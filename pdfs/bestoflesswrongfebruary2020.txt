
Best of LessWrong: February 2020
1. What Money Cannot Buy
2. Coronavirus: Justiﬁed Practical Advice Thread
3. Seeing the Smoke
4. Will COVID-19 survivors suﬀer lasting disability at a high rate?
5. Exercises in Comprehensive Information Gathering
6. Jan Bloch's Impossible War
7. A 'Practice of Rationality' Sequence?
8. Writeup: Progress on AI Safety via Debate
9. Category Theory Without The Baggage
10. What can the principal-agent literature tell us about AI risk?
11. Demons in Imperfect Search
12. Tessellating Hills: a toy model for demons in imperfect search
13. How to Frame Negative Feedback as Forward-Facing Guidance
14. Theory and Data as Constraints
15. A point of clariﬁcation on infohazard terminology
16. Draft: Models of Risks of Delivery Under Coronavirus
17. Response to Oren Etzioni's "How to know if artiﬁcial intelligence is about to
destroy civilization"
18. Bayes-Up: An App for Sharing Bayesian-MCQ
19. Suspiciously balanced evidence
20. Mazes Sequence Roundup: Final Thoughts and Paths Forward
21. Some quick notes on hand hygiene
22. Conclusion to 'Reframing Impact'
23. On the falsiﬁability of hypercomputation, part 2: ﬁnite input streams
24. Attainable Utility Preservation: Empirical Results
25. [AN #87]: What might happen as deep learning scales even further?
26. Continuous Improvement: Insights from 'Topology'
27. Attainable Utility Landscape: How The World Is Changed
28. We Want MoR (HPMOR Discussion Podcast) Completes Book One
29. A Cautionary Note on Unlocking the Emotional Brain
30. Did AI pioneers not worry much about AI risks?
31. "But that's your job": why organisations can work
32. How Low Should Fruit Hang Before We Pick It?
33. [AN #85]: The normative questions we should be asking for AI alignment, and a
surprisingly good chatbot
34. Set Ups and Summaries
35. UML XI: Nearest Neighbor Schemes
36. Attainable Utility Preservation: Concepts
37. Curiosity Killed the Cat and the Asymptotically Optimal Agent
38. Blog Post Day (Unoﬃcial)
39. Does there exist an AGI-level parameter setting for modern DRL architectures?
40. Potential Research Topic: Vingean Reﬂection, Value Alignment and Aspiration
41. Simulation of technological progress (work in progress)
42. Protecting Large Projects Against Mazedom
43. The Relational Stance
44. Looking for books about software engineering as a ﬁeld
45. Plausibly, almost every powerful algorithm would be manipulative
46. Distinguishing deﬁnitions of takeoﬀ
47. Reasons for Excitement about Impact of Impact Measure Research

48. My slack budget: 3 surprise problems per week
49. Bayesian Evolving-to-Extinction
50. Predictive coding and motor control

What Money Cannot Buy
Paul Graham:
The problem is, if you're not a hacker, you can't tell who the good hackers are. A
similar problem explains why American cars are so ugly. I call it the design
paradox. You might think that you could make your products beautiful just by
hiring a great designer to design them. But if you yourself don't have good taste,
how are you going to recognize a good designer? By deﬁnition you can't tell from
his portfolio. And you can't go by the awards he's won or the jobs he's had,
because in design, as in most ﬁelds, those tend to be driven by fashion and
schmoozing, with actual ability a distant third. There's no way around it: you can't
manage a process intended to produce beautiful things without knowing what
beautiful is. American cars are ugly because American car companies are run by
people with bad taste.
I don't know how much I believe this claim about cars, but I certainly believe it about
software. A startup without a technical cofounder will usually produce bad software,
because someone without software engineering skills does not know how to recognize
such skills in someone else. The world is full of bad-to-mediocre "software engineers"
who do not produce good software. If you don't already know a fair bit about software
engineering, you will not be able to distinguish them from the people who really know
what they're doing.
Same with user interface design. I've worked with a CEO who was good at UI; both the
process and the results were visibly superior to others I've worked with. But if you
don't already know what good UI design looks like, you'd have no idea - good design is
largely invisible.
Yudkowsky makes the case that the same applies to security: you can't build a secure
product with novel requirements without having a security expert as a founder. The
world is full of "security experts" who do not, in fact, produce secure systems - I've
met such people. (I believe they mostly make money by helping companies visibly
pretend to have made a real eﬀort at security, which is useful in the event of a
lawsuit.) If you don't already know a fair bit about security, you will not be able to
distinguish such people from the people who really know what they're doing.
But to really drive home the point, we need to go back to 1774.
As the American Revolution was heating up, a wave of smallpox was raging on the
other side of the Atlantic. An English dairy farmer named Benjamin Jesty was
concerned for his wife and children. He was not concerned for himself, though - he
had previously contracted cowpox. Cowpox was contracted by milking infected cows,
and was well known among dairy farmers to convey immunity against smallpox.
Unfortunately, neither Jesty's wife nor his two children had any such advantage. When
smallpox began to pop up in Dorset, Jesty decided to take drastic action. He took his
family to a nearby farm with a cowpox-infected cow, scratched their arms, and wiped
pus from the infected cow on the scratches. Over the next few days, their arms grew
somewhat inﬂamed and they suﬀered the mild symptoms of cowpox - but it quickly
passed. As the wave of smallpox passed through the town, none of the three were
infected. Throughout the rest of their lives, through multiple waves of smallpox, they
were immune.

The same technique would be popularized twenty years later by Edward Jenner,
marking the ﬁrst vaccine and the beginning of modern medicine.
The same wave of smallpox which ran across England in 1774 also made its way
across Europe. In May, it reached Louis XV, King of France. Despite the wealth of a
major government and the talents of Europe's most respected doctors, Louis XV died
of smallpox on May 10, 1774.
The point: there is knowledge for which money cannot substitute. Even if Louis XV had
oﬀered a large monetary bounty for ways to immunize himself against the pox, he
would have had no way to distinguish Benjamin Jesty from the endless crowd of
snake-oil sellers and faith healers and humoral balancers. Indeed, top medical
"experts" of the time would likely have warned him away from Jesty.
The general pattern:
Take a ﬁeld in which it's hard for non-experts to judge performance
Add lots of people who claim to be experts (and may even believe that
themselves)
Result: someone who is not already an expert will not be able to buy good
performance, even if they throw lots of money at the problem
Now, presumably we can get around this problem by investing the time and eﬀort to
become an expert, right? Nope! Where there are snake-oil salesmen, there will also be
people oﬀering to teach their secret snake-oil recipe, so that you too can become a
master snake-oil maker.
So... what can we do?
The cheapest ﬁrst step is to do some basic reading on a few diﬀerent viewpoints and
think things through for yourself. Simply reading the "correct horse battery staple"
xkcd will be suﬃcient to recognize a surprising number of really bad "security
experts". It probably won't get you to a level where you can distinguish the best from
the middling - I don't think I can currently distinguish the best from the middling
security experts. But it's a start.
More generally: it's often easier to tell which of multiple supposed experts is correct,
than to ﬁgure everything out from ﬁrst principles yourself. Besides looking at the
object-level product, this often involves looking at incentives in the broader system -
see e.g. Inadequate Equilibria. Two speciﬁc incentive-based heuristics:
Skin in the game is a good sign - Jesty wanted to save his own family, for
instance.
Decoupling from external monetary incentives is useful - in other words, look for
hobbyists. People at a classic car meetup or a track day will probably have
better taste in car design than the J.D. Powers award.
That said, remember the main message: there is no full substitute for being an expert
yourself. Heuristics about incentives can help, but they're leaky ﬁlters at best.
Which brings us to the ultimate solution: try it yourself. Spend time in the ﬁeld,
practicing the relevant skills ﬁrst-hand; see both what works and what makes sense.
Collect data; run trials. See what other people suggest and test those things yourself.
Directly study which things actually produce good results.

Coronavirus: Justiﬁed Practical Advice
Thread
(Added: To see the best advice in this thread, read this summary .)
This is a thread for practical advice for preparing for the coronavirus in places where it
might substantially grow.
We'd like this thread to be a source of advice that attempts to explain itself. This is not
a thread to drop links to recommendations that don't explain why the advice is
accurate or useful. That's not to say that explanation-less advice isn't useful, but this
isn't the place for it.
Please include in your answers some advice and an explanation of the
advice, an explicit model under which it makes sense. We will move answers to
the comments if they don't explain their recommendations clearly. (Added: We have
moved at least 4 comments so far.)
The more concrete the explanation the better. Speculation is ﬁne, uncertain models
are ﬁne; sources, explicit models and numbers for variables that other people can play
with based on their own beliefs are excellent. 
Here are some examples of things that we'd like to see:
1. It is safe to mostly but not entirely rely on food that requires heating or other
prep, because a pandemic is unlikely to take out utilities, although if if they are
taken out for other reasons they will be slower to come back on
2. CDC estimates of prevalence are likely to be signiﬁcant underestimates due to
their narrow testing criteria.
3. A guesstimate model of the risks of accepting packages and delivery food
One piece of information that has been lacking in most advice we've seen is when to
take a particular action. Sure, I can stock up on food ahead of time, but not going to
work may be costly- what's your model for the costs of going so I can decide when the
costs outweigh the beneﬁts for me? This is especially true for advice that has inherent
trade-oﬀs- total quarantine means eating your food stockpiles that you hopefully
have, which means not having them later.

Seeing the Smoke
Cross-posted from Putanumonit.
COVID-19 could be pretty bad for you. It could aﬀect your travel plans as countries
impose quarantines and close oﬀ borders. It could aﬀect you materially as supply
chains are disrupted and stock markets are falling. Even worse: you could get sick and
suﬀer acute respiratory symptoms. Worse than that: someone you care about may
die, likely an elderly relative.
But the worst thing that could happen is that you're seen doing something about the
coronavirus before you're given permission to.
I'll defend this statement in a minute, but ﬁrst of all: I am now giving you permission
to do something about COVID-19. You have permission to read up on the symptoms of
the disease and how it spreads. Educate yourself on the best ways to avoid it. Stock
up on obvious essentials such as food, water, soap, and medicine, as well as less
obvious things like oxygen saturation monitors so you know if you need emergency
care once you're sick. You should decide ahead of time what your triggers are for
changing your routines or turtling up at home.
In fact, you should go do all those things before reading the rest of the post. I am not
going to provide any more factual justiﬁcations for preparing. If you've been following
the news and doing the research, you can decide for yourself. And if instead of factual
justiﬁcations you've been following the cues of people around you to decide when it's
socially acceptable to prep for a pandemic, then all you need to know is that I've
already put my reputation on the line as a coronaprepper.
Instead this post is about the strange fact that most people need social approval to
prepare for a widely-reported pandemic.
Smoke Signals
As Eliezer reminded us, most people sitting alone in a room will quickly get out if it
starts ﬁlling up with smoke. But if two other people in the room seem unperturbed,
almost everyone will stay put. That is the result of a famous experiment from the
1960s and its replications — people will sit and nervously look around at their peers
for 20 minutes even as thick smoke starts obscuring their vision.
The coronavirus was identiﬁed on January 7th and spread outside China by the 13th.
American media ran some stories about how you should worry more about the
seasonal ﬂu. The markets didn't budge. Rationalist Twitter started tweeting excitedly
about R0 and supply chains.
Over the next two weeks, Chinese COVID cases kept climbing at 60%/day reaching
17,000 by February 2nd. Cases were conﬁrmed in Europe and the US. The WHO
declared a global emergency. The former FDA commissioner explained why a law
technicality made it illegal for US hospitals to test people for coronavirus, implying
that we would have no idea how many Americans have contracted the disease.
Everyone mostly ignored him including all major media publications, and equity

markets hit an all time high. By this point several Rationalists in Silicon Valley and
elsewhere started seriously prepping for a pandemic and canceling large social
gatherings.
On the 13th, Vox published a story mocking people in Silicon Valley for worrying about
COVID-19. The article contained multiple factual mistakes about the virus and the
opinions of public health experts.
On February 17th, Eliezer asked how markets should react to an obvious looming
pandemic. Most people agreed that the markets should freak out and aren't. Most
people decided to trust the markets over their own judgment. As an avowed eﬃcient
marketeer who hasn't made an active stock trade in a decade, I started at that Tweet
for a long time. I stared at it some more. Then I went ahead and sold 10% of the
stocks I owned and started buying respirators and beans.
By the 21st, the pandemic and its concomitant shortages hit everywhere from Iran to
Italy while in the US thousands of people were asked to self-quarantine. Most elected
oﬃcials in the US seemed utterly unaware that anything was happening. CNN ran a
front page story about the real enemies being racism and the seasonal ﬂu.
Finally, the narrative couldn't contain the sheer volume of disconﬁrming evidence. The
stock market tumbled 10%. The Washington Post squeezed out one more story about
racism before conﬁrming that the virus is spreading among Americans with no links to
Wuhan and that's scary. Trump decided to throw his vice president under the
coronavirus bus, ﬁnally admitting that it's a thing that the government is aware of.
And Rationalist Twitter asked: what the fuck is wrong with everyone who is not on
Rationalist Twitter?
Cognitive Reﬂection
Before Rationality gained a capital letter and a community, a psychologist developed
a simple test to identify people who can override an intuitive and wrong answer with a
reﬂective and correct one.
One of the questions is:
In a lake, there is a patch of lily pads. Every day, the patch doubles in size. If it takes
48 days for the patch to cover the entire lake, how long would it take for the patch to
cover half of the lake?
Exponential growth is hard for people to grasp.  Most people answer '24' to the above
question, or something random like '35'. It's counter-intuitive to people that the lily
pads could be barely noticeable on day 44 and yet completely cover the lake on day
48.
Here's another question, see if you can get it:
In an interconnected world, cases of a disease outside the country of origin are
doubling every 5 days. The pace is slightly accelerating since it's easier to contain a
hundred sick people than it is to contain thousands. How much of a moron do you
have to be as a journalist to quote statistics about the yearly toll of seasonal ﬂu given
a month of exponential global growth of a disease with 20 times the mortality rate?

Social Reality Strikes Again
Human intuition is bad at dealing with exponential growth but it's very good at one
thing: not looking weird in front of your peers. It's so good at this, in fact, that the
desire to not look weird will override most incentives.
Journalists would rather miss out on the biggest story of the decade than stick their
neck out with an alarmist article. Traders would rather miss out on billions of dollars of
proﬁts. People would rather get sick than do something that isn't socially sanctioned.
Even today (2/26/2020), most people I've spoken to refuse to do minimal prep for
what could be the worst pandemic in a century. It costs $100 to stock up your house
with a month's worth of dry food and disinfectant wipes (respirators, however, are
now sold out or going for 4x the price). People keep waiting for the government to do
something, even though the government has proven its incompetence in this area
several times over.
I think I would replace the Cognitive Reﬂection Test with a single question: would you
eat a handful of coﬀee beans if someone told you it was worth trying? Or in other
words: do you understand that social reality can diverge from physical reality, the
reality of coﬀee beans and viruses and diseases?
Social thinking is quite suﬃcient for most people in usual times. But this is an unusual
time.
Seeing the Smoke

The goal of this article isn't to get all my readers to freak out about the virus. Aside
from selling the equities, all the prep I've done was to stock a month of necessities so
I can work from home and to hold oﬀ on booking ﬂights for a trip I had planned for
April.
The goal of this post is twofold. First, if you're the sort of person who will keep sitting
in a smoke ﬁlled room until someone else gets up, I'm here to be that someone for
you. If you're a regular reader of Putanumonit you probably respect my judgment and
you know that I'm not particularly prone to getting sucked in to panics and trends.
And second, if you watched that video thinking that you would obviously jump out of
the room at the ﬁrst hint of smoke, ask yourself how much research and preparation
you've done for COVID-19 given the information available. If the answer is "little to
none", consider whether that is rational or rationalizing.
I could wait to write this post two months from now when it's clear how big of an
outbreak occurs in the US. I'm not an expert on viral diseases, global supply chains, or
prepping. I don't have special information or connections. My only diﬀerentiation is
that I care a bit less than others about appearing weird or foolish, and I trust a bit
more in my own judgment
Seeing the smoke and reacting is a learnable skill, and I'm going to give credit to
Rationality for teaching it. I think COVID-19 is the best exam for Rationalists doing
much better than "common sense" since Bitcoin. So instead of waiting two months,
I'm submitting my answer for reality to grade. I think I'm seeing smoke.

Will COVID-19 survivors suﬀer lasting
disability at a high rate?
The case fatality rate of 2019-nCoV (aka Coronavirus, COVID-19) is still uncertain, with
estimates ﬂoating around ranging from 0.16%-5.7%, higher among the elderly and
people with preexisting conditions, and lower among everyone else. However, the
death rate doesn't capture all of the harms from infection. There is also time lost
during the infection and recovery, there is the possibility of accelerated aging, and
there is the possibility of long-term nonfatal disability, such as chronic fatigue.
Since 2019-nCoV has only existed for about two months, there is no data on the long-
term outcomes of its survivors. However, the rate of lasting disability among survivors
is important for deciding what responses are appropriate. I'm particularly interested in
estimating the risk of chronic fatigue from nCoV infection. If that risk is high, this
would greatly increase the importance of avoiding it personally and of suppressing it
in communities of people doing important work, and would also greatly increase the
expected economic impact.
As a starting point, I chose a similar but more severe virus, SARS, which was
successfully contained in 2003. Out of 208 Canadian survivors of SARS, 22 (10%)
appear in this study of subjects "who remained unable to return to their former
occupation" with "clinical similarities to patients with ﬁbromyalgia syndrome". This
implies a high lower bound on the rate of disability among SARS survivors. However,
this is only one virus, and may not be representative of severe respiratory illnesses.
Good answers to this question would be:
Papers estimating the rates of postviral fatigue from other viruses, especially
respiratory viruses, viruses with severity comparable to 2019-nCoV, and among
non-elderly patients
Models of postviral fatigue and how they relate to 2019-nCoV
Data on whether and how much lung damage from non-viral sources causes
chronic fatigue
Early data on 2019-nCoV which bears on this question
Any research help on this question is greatly appreciated, even if it provides only a bit
of information about a small corner of the problem, or reports that a strategy for
answering the question failed to pan out.

Exercises in Comprehensive
Information Gathering
Looking back, several of the most durably-valuable exercises I've done over the years
have a general theme of comprehensive information gathering.
The most recent example involves capital investments. Economists talk about "capital
goods" as physical stuﬀ - machines, buildings, etc. But in practice, savings and
investments are passed through banks and ETFs, bundled and securitized, involve
debts and shares of companies which own debts and shares of other companies, and
so forth... where does all that capital end up? To get an intuitive sense, I pulled up
fundamental data on about 7000 US publicly-traded companies in quantopian, sorted
them by amount of non-ﬁnancial assets, and found that the top 100 accounted for
about 50% of the non-ﬁnancial assets of the whole set. Then, I looked at annual
reports for each of those 100 companies, to see what capital assets they had. I
googled around for pictures and maps of where those assets were located, and read
up on anything I hadn't heard of before. What's a "central oﬃce", where are they,
what do they look like, and why does AT&T have $90B worth of them? What are the
major US oil basins, where are the wells, and what all goes into drilling them? What
are the technical diﬀerences between traditional phone, cable, satellite, and cell
networks, and how do those technical diﬀerences impact the capital requirements of
each? Who runs power plants and the power grid in various parts of the country? What
are the major US railroads, and where are they? Why did GE own so many airplanes?
These are the kinds of questions which come up when you want to know what "capital
goods" actually consist of, in the real world.
Another interesting exercise: I read through ﬁve years of Nature archives, reading all
the titles and any abstracts which sounded novel/interesting. I didn't google
everything I hadn't heard of; instead, I'd wait until the same acronym popped up a few
times before looking it up. This took maybe a week of evenings after work. By the end,
I could at least place the large majority of articles in context. Now, when I see a title
full of jargon in a ﬁeld I haven't studied, like "Novel tau ﬁlament fold in corticobasal
degeneration", I usually at least understand enough to guess at what it's relevant to
(in this case: neurodegenerative disease involving protein aggregates, probably
Alzheimers?). I can generally follow conversations in a bunch of diﬀerent ﬁelds - not
necessarily between specialists in the same sub-sub-ﬁeld, but at least the level of a
typical conference talk, and when I meet new people I can ask not-too-embarrassing
questions about what they're researching.
Going back further, if you're in college, I strongly recommend reading your entire
course catalogue, googling anything you've never heard of at all, and marking
anything that sounds potentially interesting. This seems really obvious; it only takes a
few hours, and something something a pile of value sitting on a silver platter right in
front of you. (Note: I went to a small STEM school; if you're at a big school with a
bajillion courses or a school with poor STEM coverage or not at college at all, consider
reading an MIT/Caltech course catalogue instead, to get a feel for what all is out
there.) You never know what surprising and interesting topics might be hiding in there
- microﬂuidics, underactuated robotics, recursive macroeconomics, systems biology,
synthetic biology, origami algorithms, computational photography, evo-devo,
procedural graphics, and on and on.

These sort of exercises provide value in a few ways:
They reveal unknown unknowns - things you didn't even realize were missing
from your picture of the world.
You can't make a map of a city by sitting in your room with the shades drawn;
exercises like these force you to look at large slices of the world.
Knowledge within ﬁelds tends to have decreasing marginal returns - your ﬁrst
physics or CS class will teach you much more than your eighth. These exercises
give a broad, brief glance at many areas where you probably haven't reached
decreasing marginal returns yet.
You can get a very rough big-picture sense of how much eﬀort other people are
investing in various areas - e.g. where most capital investments go or where
most research eﬀort goes - which is useful for understanding the world in
general.
While these exercises don't avoid biased selection of information altogether,
they're probably diﬀerent biases from what you run into naturally, and they're
systematic enough that we can guess at what biases are likely to be present.
They're a lot of fun, if you have a curious streak.
Most importantly: I've found each of these exercises to have lasting, long-term value
in exchange for a one-time investment of eﬀort.
Other exercises which are on my to-do list, but which I haven't done yet:
Read the entire CIA world factbook; you can get a paper copy for $11 on
Amazon.
Go through all of the (known) functions of genes in a minimal organism.
I'm curious to hear other suggestions for exercises along these lines.

Jan Bloch's Impossible War
This is a linkpost for https://hivewired.wordpress.com/2020/02/17/jan-blochs-impossible-war/
Epistemic Status: Endorsed
Content Warning: Neuropsychological Infohazard, Evocation Infohazard, World War I
Recommended Prior Reading: Blueprint for Armageddon Part I
Part of the Series: Truth
"History doesn't repeat itself but it often rhymes"
In any real look into the past, you realize pretty quickly that things don't have neat
beginnings or simple origins in the vast majority of cases. Historical events are the result of
billiard ball interactions among a chaotic conﬂux of actors and forces, themselves all built
out of past impacts and collisions stretching back into the mists of antiquity.
Thus when trying to tell the origin story of the modern rationality community, it can be very
tempting to just keep extrapolating backwards. How far back should we look? Do we need to
rehash Plato's Cave and Cogito Ergo Sum? Francis Bacon is credited as the grandfather of
science, so maybe we should start with him? 
For the moment at least I'm writing blog posts not thousand page textbooks, and my goal
here isn't to rehash the entire history of scientiﬁc and philosophical thought (I'd like to keep
this blog post under three thousand words). If you want the entire history of scientiﬁc
thought, Cosmos is a great place to start and has some pretty spiﬀy graphics. 
But unlike history, every story and every blog post have to start somewhere, and I think the
best place to start for our purposes is with polish banker and railway ﬁnancier Jan Gotlib
Bloch. 
Bloch was born a Polish Jew in Tsarist Russia in the 1800s, and would later convert to
Calvinism to protect himself from antisemitism within the Tsarist government. Bloch worked
as a banker and would go on to ﬁnance the building of rail lines in Russia, as well as penning
a lengthy treatise on the management and operation of said rail lines in 1875, for which he: 
was awarded a medal of the ﬁrst class at the geographical exhibition of Paris, and was
heartily endorsed by the Imperial Russian Geographical Society.
But it was Bloch's later work that would be remembered for. In 1870, The Northern German
Confederation would go to war with the Second French Empire. Fueled by fears of the
growing power of a rapidly unifying and industrializing Germany, France declared war and
invaded in August of 1870. 
The war was only six months long. By September, Napoleon III was captured and the French
Imperial Army had been decisively defeated. A new French government was declared and
kept ﬁghting, but by January of 1871 Paris was besieged and the war was brought to an end.
The balance of power in Europe had fundamentally shifted, and while all the great powers
reeled from the event, some saw it merely as a portent for things to come. 
The Franco-Prussian war was the ﬁrst prototype of a modern war, one featuring the use of
railroads, artillery, and all the new technology of creation and destruction that had come into
existence since the end of the Napoleonic Wars in 1815. Jan Bloch was fascinated by the war
of 1870 and would go on to devote much of his personal time to studying the phenomenon
that was modern military conﬂict. 
No one really knew how any of this stuﬀ would interact with real combat, but everything
seemed to point to the idea that the next major war would be unlike anything the world had

seen before. Bloch looked at the state of the technology, where things seemed to be going,
and penned his most famous six-volume work, originally in Russian and translated into
numerous languages, popularized in English under the title Is War Now Impossible? This work
would prove to be exactly as horrifying in its prescience as it was in its theories as to the
nature of future conﬂicts. 
In Europe during the renaissance and age of royalty and exploration, war was almost
something of a gentleman's sport. The royals of all the major nations knew each other,
everyone was someone's cousin or uncle or grandmother, the armies would ﬁght out in lines
and day battles and then after one side defeated the other the leaders would sit down for tea
and enter negotiations and this was for a long time considered a normal and acceptable way
to conduct diplomacy between powers. The civilians of these nations would likely not even
notice that they were at war a lot of the time.
However, with the french revolution, we see the beginnings of a change in this behavior. The
french revolution is the ﬁrst war to feature mass mobilization, a trend of throwing the entire
nation into a conﬂict instead of merely a small mercenary army. When the European royal
powers united against the upstart French republic, they were met not by a small,
professional French army but by as much of the french people as could be mobilized. This
enormously changed the way wars were fought and forced the rest of Europe to follow suit or
be swamped by the sheer size of the French military. Napoleon is famously quoted as saying:
"You cannot stop me; I spend 30,000 lives a month."
And this was a major change for the European powers who didn't really want to arm their
peasants, that's how you end up with uprisings. But here were the french conquering Europe
with a peasant army and the rest of the great powers were forced into a game of catch up.
This is a rather textbook example of a multipolar trap at work. No one can coordinate to stop
the escalation of the conﬂict, and anyone who doesn't escalate will be defeated by those
who do, thus wars become total and we witness the pivot to the start of the modern arms
race. 
Moloch! Whose Fingers are ten armies!
Bloch looked at the state of technology, the state of war, and the state of European powers,
and concluded that the era of quick and relatively bloodless conﬂicts as a method of
diplomacy was over. War wasn't a fun pastime of royalty anymore, war was now serious.
Wars of the future would be total. They would not be quick and decisive aﬀairs but brutal
slugging matches fought until one nation collapsed socially and economically. He saw that
the development of riﬂing, artillery, and machine guns had made cavalry and bayonet
charges suicidal and obsolete. He claimed that a future war would be one of entrenchment,
stalemates, massive ﬁrepower, and massive losses of life. 
Bloch's book is considered to be partly responsible for the Hague Conference of 1899, which
sought to impose limits on warfare and prevent the increasingly bloody looking conﬂict from
playing out as Jan Bloch feared it would. Bloch was even a special guest of Tsar Nicholas at
the conference. 
There was a belief, or maybe it was a hope, that because war had become so terrible and
destructive, that the only choice nations would have would be to resort to peaceful
negotiations. Bloch himself seemed to be something of a proponent to this theory, although
he at least seemed to think that peace would still require conscious input and the wisdom of
men. He didn't believe that war was truly impossible, just that continuing to treat war as it
had been treated in the past (sportingly) was an impossibility. It was a lesson that would,
unfortunately, be mostly ignored by the leaders and military of the time. 

A decade after the publishing of Is War Now Impossible, British journalist Normal Angell
published another work along similar lines, titled The Great Illusion. Angell was an early
globalist, who looked at the same situation Bloch had and answered Bloch's question with
"Yeah, war is impossible now." 
Angell's thesis was that any gains made by war would be so dwarfed by the costs of waging
a modern war that there would be no reason to ever ﬁght one. A modern war would destroy
the world's economy, and maybe even end civilization itself, and peace was just so
proﬁtable. So war was just not going to happen. You would have to be stupid to ﬁght Bloch's
Impossible War, no one would beneﬁt, so no one would do it. 
Well, as history would come to show, while Angell was correct that a modern war would
destroy whole nations and leave economies in ruins, he was wrong about that actually
stopping the war from happening. 
Moloch the vast stone of war! Moloch the stunned governments!
So in grade school, we're taught that World War I happened because all the European powers
had entered these complex networks of alliances that drew each other into the growing
conﬂict like dominos falling and no one saw it coming or could stop it. 
Jan Bloch saw it coming, and he tried to stop it. It was a really solid attempt even, but we
don't live in the timeline where he succeeded, we live in the timeline where he didn't. As the
ﬁrst decade of the twentieth century drew to a close, tensions continued to ramp up across
Europe and Jan Bloch's warning started looking more and more like a dire inevitability.
One of the readers of Jan Bloch's book was Polish scholar Alfred Korzybski, who asked the
very reasonable question: If this was all so inevitable, if everyone knew it was going to
happen, then why couldn't it be stopped? 

Part of the Series: Truth
Next Post: Time Binders

A 'Practice of Rationality' Sequence?
(This is not your typical factual-answer question, but I think it makes sense to format
this as a question rather than a post.)
TLDR: Recommend some posts for a "practice of rationality" sequence I want to
curate! Proposing posts that should exist but don't is also cool.
I've been thinking recently that it would be nice if rationality were more associated
with a practice -- a set of skills which you can keep grinding and leveling up. Testable
rationality skills (like accuracy or calibration in forecasting) are obviously a plus, but
I'm not referring exclusively to this -- some very real things can be hard to evaluate
externally, such as emotional wellness.
A model I have in mind is meditation: meditation is easy to "grind" because the
meditator gets constant immediate feedback about how well they're focusing (or at
least, they get that feedback if they meet a minimum of focus required to keep track
of whether they are focusing). Yet it's quite diﬃcult to evaluate progress from the
outside.
(In fact, when I mentioned this desire for a "practice" of rationality to one friend, they
were like "I agree, and in fact I think the practice should just be insight meditation.")
This is basically reiterating Brienne's call for tortoise skills (see also), except what I
want to do is collect proposed things which could be part of a practice.
Obviously, some CFAR content could already qualify. CFAR doesn't exactly teach it that
way -- as far as I've observed, CFAR's focus is on mindset interventions. "Mindset
intervention" is the fancy psychology term for getting someone to think diﬀerently by
having them do something once. For example, the point of "growth mindset"
interventions is that you explain it once and this has long-lasting impact on someone's
behavior. Another mindset intervention is: you ask people to write about what matters
to them. Doing this once has shown long-term results.
In my ﬁrst CFAR experience (which was an MSFP, fwiw), the phrase "It's not about the
exercises!" was kind of a motto. It was explained at the beginning that CFAR teaches
exercises not because people learn the exercises and then go out and use the
exercises, but rather, going through the exercises a few times changes how you think
about things. (The story was that people often go to a CFAR workshop and then
improve a bunch of things in their life, but say "but I haven't been doing the
exercises!".)
But many of the things CFAR teaches could be used as a practice, and (again referring
to my ﬁrst CFAR experience) CFAR does do some things which encourage you to look
at them that way, like the follow-up emails which encourage you to overlearn one
exercise per week (practicing that one thing a bunch so that it becomes an automatic
mental motion).
Another example pointing at what I want here is bewelltuned.com. The content may or
may not be right, but the sort of thing seems exactly right to me -- actionable skills
you can keep working on regularly after getting simple explanations of how to do it.
And furthermore, the presentation seems exactly right. LessWrong has a tendency to

focus on wordy explanations of intellectual topics, which is great, but the bewelltuned
style seems like an excellent counterbalance.
I'm using the "question" format so that answers can recommend speciﬁc things
(perhaps represented by existing LW posts, perhaps not), whereas comments can
discuss this more broadly (such as what more general criteria should be applied to
ﬁlter suggestions, or whether this is even a good idea). The answer list here could
serve as a big repository. I'll probably create a sequence which can be my own highly
opinionated curation of the suggestions here, plus my own writing on the subject.
I originally intended Becoming Unusually Truth Oriented to be the start of a sequence
on the subject written entirely by me. However, some resulting discussion made me
question my approach (hence the motivation for this question).
One friend of mine (going oﬀ of some of the discussion in comments to that post)
voiced a concern about the rationality community falling into the same pitfalls as
martial arts. Several articles about this have been written on LW. (I'm not ﬁnding all
the ones I remember! If you put links to more of them in the comments I'll probably
edit this to add them.) The concern is that a martial art of rationality could lead to the
same kinds of epistemic viciousness which are seen in literal martial arts -- a practice
divorced from reality due to the constraints and incentives of training/teaching.
That same friend suggested that the solution was to focus on empirically veriﬁable
skills, namely forecasting. But in the in-person rationalist community in the bay area,
I've encountered some criticism of extreme focus on forecasting which suggests that
it's making the very mistake we're afraid of here -- Goodharting on the problem. One
person asked me to give any examples of Superforecasting-like skills resulting in
actual accomplishments, suggesting that planning is the far more valuable skill and
varies signiﬁcantly from forecasting. Another person recounted their experience
sitting down with several other rationalists to learn superforecasting skills. It was a
group of rather committed and also individually competent rationalists, but they
quickly came to the conclusion that while they could put in the eﬀort to become much
better at forecasting, the actual skills they'd learn would be highly speciﬁc to the task
of winning points in prediction tasks, and they abandoned the project, concluding that
it would not meaningfully improve their general capability to accomplish things!!
So, this seems like a hard problem.
What could/should be a part of a 'practice' of rationality?

Writeup: Progress on AI Safety via
Debate
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
This is a writeup of the research done by the "Reﬂection-Humans" team at OpenAI in Q3 and
Q4 of 2019. During that period we investigated mechanisms that would allow evaluators to
get correct and helpful answers from experts, without the evaluators themselves being
expert in the domain of the questions. This follows from the original work on AI Safety via
Debate and the call for research on human aspects of AI safety, and is also closely related to
work on Iterated Ampliﬁcation.
Authors and Acknowledgements
The main researchers on this project were Elizabeth Barnes, Paul Christiano, Long Ouyang
and Geoﬀrey Irving. We are grateful to many others who oﬀered ideas and feedback. In
particular: the cross-examination idea was inspired by a conversation with Chelsea Voss;
Adam Gleave had helpful ideas about the long computation problem; Jeﬀ Wu, Danny
Hernandez and Gretchen Krueger gave feedback on a draft; we had helpful conversations
with Amanda Askell, Andreas Stuhlmüller and Joe Collman, as well as others on the Ought
team and the OpenAI Reﬂection team. We'd also like to thank our contractors who
participated in debate experiments, especially David Jones, Erol Akbaba, Alex Deam and
Chris Painter. Oliver Habryka helped format and edit the document for the AI Alignment
Forum.
Note by Oliver: There is currently a bug with links to headings in a post, causing them to not
properly scroll when clicked. Until that is ﬁxed, just open those links in a new tab, which
should scroll correctly.
Overview
Motivation
As we apply ML to increasingly important and complex tasks, the problem of evaluating
behaviour and providing a good training signal becomes more diﬃcult. 
We already see examples of RL leading to undesirable behaviours that superﬁcially 'look
good' to human evaluators (see this collection of examples). One example from an OpenAI
paper is an agent learning incorrect behaviours in a 3d simulator, because the behaviours
look like the desired behaviour in the 2d clip the human evaluator is seeing. 
We'd like to ensure that AI systems are aligned with human values even in cases where it's
beyond human ability to thoroughly check the AI system's work. 
We can learn about designing ML objectives by studying mechanisms for eliciting helpful
behavior from human experts. For example, if we hire a physicist to answer physics
questions and pay them based on how good their answers look to a layperson, we'll
incentivize lazy and incorrect answers. By the same token, a reward function based on
human evaluations would not work well for an AI with superhuman physics knowledge, even
if it works well for modern ML. 
If we can develop a mechanism that allows non-expert humans to reliably incentivize experts
to give helpful answers, we can use similar mechanisms to train ML systems to solve tasks

where humans cannot directly evaluate performance. Conversely, if we can't incentivize
experts to behave helpfully, that suggests it will also be diﬃcult to train ML systems with
superhuman expertise on open-ended tasks.
One broad mechanism that might work is to invoke two (or more) competing agents that
critique each others' positions, as discussed in the original debate paper[1]. This can be
simulated by having human debaters argue about a question and a judge attempt to pick the
correct answer.
In the rest of this document, we'll describe the research done by reﬂection-humans in Q3 and
Q4 on investigating and developing mechanisms that incentivize human experts to give
helpful answers.
Current process
During the early stages, we iterated through various diﬀerent domains, research
methodologies, judge pools, and research processes. More details of this early iteration are
here.
In Q4 we converged on a research process we're more happy with. We're focusing on
improving our debate mechanisms as fast as possible. We're using mostly internal iteration
(as opposed to external judge and debater pools) to test these mechanisms, as they still
have a lot of easy-to-ﬁnd failures to work through. Once we get to a point where we have a
mechanism we believe works well, we will try diﬀerent ways to break it.
We make progress by going through a loop of:
1. Run debates. See if they work.
If they are reliably working, try harder to break them, by scaling up the number of
debates, choosing harder questions, internal and external red-teaming, oﬀering a
bounty, e.t.c
If they fail, identify a problem and make it crisp, with practical examples and/or
theoretical characterisation
2. Design a new mechanism to address this problem (that doesn't obviously introduce
additional problems!)
3. Integrate the mechanism into our debate structure and make it practical for humans to
debate using this structure.
This process should result in either a positive or negative result. The positive result would be
a debate structure that reliably works given the hardest questions and most devious
dishonest debating strategies we can throw at it [2]. A negative result would ideally be a
crisp problem that demonstrates why debate is impossible (for certain classes of questions,
at least), but might also just be a question plus dishonest debating strategy that reliably
leads to a dishonest win.
The positive result yields a mechanism that we can use to train superhuman ML systems.
The negative result informs us that ML systems with superhuman expertise may well exhibit
behaviour such that human evaluators will fail to correctly judge whether the behaviour is
correct, helpful, or safe. 
Our task
The basic debate setup involves two expert debaters (stand-ins for highly capable ML
systems), and a judge. One debater defends the correct answer and is honest throughout,
the other debater defends an answer that is clearly worse. The judge's goal is to identify the
correct answer. 
Our current target is for judges to identify the correct answer at frequency >90%, with a
maximum of 10 minutes to read the debate and make their decision. 

We are currently using questions from the book 'Thinking Physics' - short but tricky problems
in areas ranging from mechanics to relativity, that generally require careful understanding of
physics concepts but don't require much mathematical working. We have a few questions
we're particularly happy with, that our debaters understand deeply and are conﬁdent in but
are still very confusing to judges. These are discussed here. (An example question is: "An
icecube with a large air bubble in it is ﬂoating in water. When it melts, does the water level
go down, stay the same or rise?")
We experimented with other tasks including text-based question-answering, where the
debaters have full access to the text and the judge only sees limited quotes. We settled on
Thinking Physics problems for several reasons, including that the questions require using
concepts that are unfamiliar and confusing to the judge. This is challenging but seems like a
problem we need to be able to deal with. 
Progress so far
We observed various problems with informal, free-text debates - the dishonest debater could
often evade being pinned down and avoid giving a precise answer to the other debater's
questions, and often gained control of the 'narrative ﬂow' of the debate, steering it away
from the weak parts of their argument. To address this we considered various structured
debate formats, involving explicit recursion on a particular sub-component of an argument.
The debaters choose one claim to recurse on, and the next round of the debate is focused on
that claim. The debate is resolved based on the judge's opinion of who won the ﬁnal round,
which should be about a very narrow, speciﬁc claim. These early problems are discussed
here.
However, particularly once recursion was introduced, we found problems with ambiguity. It is
very diﬃcult to refer precisely to concepts in 250 characters of text, especially if the
concepts are unfamiliar to the judge. The dishonest debater can exploit this to their
advantage, by claiming to have meant whatever is most convenient given the particular part
of their argument that's being challenged. This problem is similar to the Motte and Bailey
fallacy. More details of the problem are here.
To address this problem, we allow a debater to "cross-examine" multiple copies of the
opposing debater who are not allowed to communicate. A debater can cite quotes from
cross-examination to exhibit inconsistencies in the other debater's argument.
This forces the dishonest debater to either commit to all the details of their argument ahead
of time (in which case the honest debater can focus on the ﬂaw), or to answer questions
inconsistently (in which case the honest debater can exhibit this inconsistency to the judge).
The cross-examination takes place outside the main thread of debate, so doesn't consume
judge time/attention. 
In theory-land this cross-examination mechanism gives debate a boost from PSPACE to NEXP
(proof), and it also seems to help in practice. The mechanism is described as part of current
rules here, and we discuss why it helps with ambiguity here.
The next problem we're working on is how to deal with arguments that are too "big" for
either debater to understand.  In our current framework, if debater 1 makes an argument
and debater 2 is unable to exhibit a ﬂaw in the argument, we assume that the argument is
correct. The dishonest debater can exploit this by making up some very large and slightly
ﬂawed argument and claiming it supports their position. If the honest debater doesn't know
exactly which part of the argument is ﬂawed, even if they know there is a ﬂaw somewhere,
we'll assume the argument is correct. More details of this problem are described here.
Things we did in Q3

Early iteration
During the early stages, we experimented a lot with diﬀerent domains, research
methodologies, judge pools, and research goals. Initially our plans involved doing debates in
a largely unstructured format, where the debaters alternated making free-text statements of
up to 350 characters. We would then collect a large number of judgements from showing
these transcripts to Mturk workers, and do statistical comparisons against two baselines. The
easier baseline was the accuracy of the judge's answers to the question before seeing any
information, and the more challenging baseline was the accuracy of the judge's answers
after being shown a single explanation of either the correct or incorrect answer. We moved
away from that for several reasons. 
Initially we were concerned that it's hard to get high-quality data from Mturk, even if we
screen for smart workers - the workers try to complete the tasks as fast as possible. We also
thought we might need more interactivity between judges and debaters.
However, when we switched to using higher-quality judges who could interact with debaters,
we found that our debates still didn't work reliably -- even with very smart and motivated
judges who spent ~an hour. 
To have conﬁdence in the debate mechanism, we need honest strategies to have a
fundamental, systematic advantage. This is the case in theoretical models of debate, but we
weren't conﬁdent it was the case in practice. For example, in the theoretical picture the
honest debater can focus on the part of the dishonest debater's argument that contains a
ﬂaw, but this didn't reliably happen in practice - the dishonest debater was often able to
weasel out and divert attention elsewhere. 
This consideration led us to target a threshold of performance where honesty has a clear,
systematic advantage: judges should be able to identify the correct answer almost all the
time (>90%), and it should feel to the experimenters and the participants that dishonest
answers are fundamentally disadvantaged. This is also much quicker to measure than doing
statistical comparisons against baselines, allowing us to iterate faster. 
We became more convinced that our current debate procedure wouldn't hit this performance
threshold, and that we should focus on internal iteration to develop an improved debate
procedure.
Early problems and strategies
Diﬃculty pinning down the dishonest debater
A key requirement for something like debate to work well is that the honest debater can
focus in on the part of the dishonest argument that contains a ﬂaw, and the debate will be
decided based on whether that small piece of the argument is correct. If this is the case,
then the dishonest debater can't make any claims that are clearly false when investigated
closely, and must stick to claims that are plausibly true when scrutinized. This should make it
much harder to construct arguments for an incorrect answer.
In our early, free-text, debates we found that this property often didn't hold. The dishonest
debater was often able to ignore the honest debater's questions or requests to focus on one
part of the argument, and it was hard for the judge to keep track of which objections or
questions had been addressed and which hadn't. The dishonest debater could also pretend
to misunderstand the honest debater's question, or claim that it wasn't well-speciﬁed
enough to answer. The debates often looked more like the debaters repeating slightly
diﬀerent versions of the most compelling part of their argument, rather than zooming in on
disagreements. 

For example [3]:
            A: Excellent, we agree. Now imagine the ice freezes nonuniformly, so that its 
density varies throughout the berg. Does the level still remain the same after melting, 
regardless of the variation? 
          
The dishonest debater didn't want to answer this question, so they pretended to not know
what it meant in order to play for time, and added in a new question of their own to try to
gain control of the ﬂow of the debate.
            B: What do you mean by the density varying? Do you mean the case where the 
overall density (total mass/total vol) stays constant? Also, Q for you: can you explain why 
your answer is the same for A and W even though their densities are different? 
          
We considered various diﬀerent approaches to address this, including various formal
argumentation styles, precommitment, and various recursive structures. However, all of
these more rigid structures highlighted problems related to ambiguity: if you try to get the
dishonest debater to commit to a speciﬁc claim, or recurse on a speciﬁc part of their
argument, they frequently have wiggle room to interpret the claim in whatever way is most
conducive to their current strand of argument. 
Asymmetries
Another problem we noticed is that one debater may gain control over what framework is
used to approach the question. Often the dishonest argument takes the form of breaking
down the problem in the wrong way, then making claims which are individually true, but in
this misleading framework lead to the wrong answer. If the dishonest debater manages to
make their case compellingly, and the honest debater is reduced to trying to attack the
dishonest argument, this seems to go worse than if the reverse happened and the honest
debater was the one making a constructive argument. 
In addition, Evan Hubinger was concerned about asymmetries of oﬀense/defense and
constructive vs destructive arguments, as this is a known phenomenon in Policy Debate.
Finally, we were concerned about ﬁrst- or last-mover advantages.
In order to address this, we wanted our debate structure to have some symmetrical
characteristics:
Both debaters move simultaneously
Both debaters make a constructive argument
Both debaters get to attack the others' argument
Questions we're using
There are various properties we want for an ideal debate question:
1. Smart, attentive judges frequently fail to choose the correct answer, but debaters can
reliably choose the correct answer.
2. Experimenters can understand the problem fairly well
3. There isn't a simple, knock-down argument for the correct answer
4. Not too much formal mathematical working is required
5. The debaters' ability to answer is based on understanding the problem, not just
observational evidence
6. If you understand the problem, there is clearly one right answer

(1) is obviously important for us to be able to tell if debate is working.
(2) is important because it's really helpful if the experimenters are able to follow exactly
what's going on in the debate - which claims are correct and incorrect, and how diﬀerent
parts of the argument support each other. This is somewhat in conﬂict with (1), but the
experimenters can also spend time reading and discussing the answer, and doing some
learning to understand the problem better than the judges.
(3) is needed for an interesting debate - some hard questions do just need one simple
insight, but we're more interested in problems that require aggregating various strands of
reasoning to ﬁnd the right answer.
(4) is desirable to keep the debate focused on the areas we're most interested in. We're
fairly conﬁdent that debate will work ﬁne for formal mathematics [4], and we mostly want to
investigate more fuzzy reasoning. 
We ran into some problems with (5) when using a question about counterintuitive
behaviours of slinkies. There are deﬁnitive video demonstrations of the behaviour, but it
appears that no-one actually understands why the behaviour happens - at least, our
debaters and experimenters couldn't easily ﬁnd a compelling explanation. We don't expect
or require debate to work if the debater's explanations are 'I don't know why this is the case
but I have empirical evidence', so we want to avoid questions of this type. 
We also encountered problems with (6), for several questions that initially seemed
promising. During the process of constructing dishonest arguments, we realised that the
question was underspeciﬁed and there was a pretty reasonable case for a diﬀerent answer.
Hopefully we can often resolve this by changing the question setup to make sure there's only
one correct answer.
Generally, we want to ﬁnd questions that are 'the easiest thing that should work but doesn't'
- this helps us isolate where exactly the problems are.
With that in mind, here are some of our favourite questions:







Current debate rules
This is the rule set that we currently think is most promising. It incorporates several diﬀerent
mechanisms we think are useful: explicit recursion on claims, a symmetric oﬀense/defense
structure, and cross-examination.
Basic informal outline
There are a number of rounds. In each round, both debaters make their argument,
consisting of several sub-claims, to support their claim.
Then, both debaters add objections to parts of the other's argument.
Then, each debater chooses whether to cede or challenge the objections. If they cede
the objection, it gets added in to their argument; if they challenge it they are
essentially saying they're happy to die on the hill of whether that objection was correct.
We choose one of the objections that was challenged, and recurse: we start another
round of debate where the top-level claim is about whether the objection is good
Eventually we either reach a depth limit, or there are no challenged objections. Then
the judge looks at the two claims and supporting arguments for this round, and decides
which is better. The whole debate is resolved based on this.
At any point, a type of argument a debater can supply is a cross-examination. They can
ask a question to a previous version of the other debater (for example, the version
after the end of the ﬁrst round). They can then quote this question-answer exchange as
part of their argument.
Comprehensive rules
At the beginning of each round, one debater is defending a claim and the other is
objecting to it. The judge's job is to decide whether the claim+objection is more
accurate and helpful than the claim on its own. We also have a depth limit that bounds
the number of rounds we can have.
Each player supports their position by simultaneously making an argument consisting
of 2-4 claims. The total length of the argument (the top-level claim or objection, plus
the supporting claims) is bounded to 250 characters.
Each player then simultaneously may make any number of objections to the other
player's argument. These may either be objections to particular claims, or objections to
the overall structure of the argument. The total length of their objections is bounded to
250 characters.
The transcript of the round is a short text representation of the round that displays the
player's top-level claims, their arguments, and any objections. We'll choose this format
to be as helpful to the judge as possible. See below for some examples of our current
format.
If the depth limit is >0, then for each objection Alice raises to Bob's argument, Bob
may choose to concede or challenge the objection.
If there are any challenged objections and the depth limit is >0, then we choose one
challenged objection to recurse on:
We don't deﬁne how to make this choice, so in order to be conservative we're
currently allowing the malicious debater to choose which to recurse on.
After choosing a (claim, objection) pair, we start a new round with one player
defending the claim and the other defending the objection. The depth limit of the
new round is reduced by 1.
The winner of the new round wins the game. (Though there may be further
recursion.)
Arbitration: If all objections are ceded or the depth limit=0, we proceed to arbitration:
We take the transcript for this round and all preceding rounds and put them
together into a single document. The judge should mostly focus on the ﬁnal

round, but they may need to read text from preceding rounds in order to have
enough context to understand what the claims in the ﬁnal round mean.
If the depth limit is >0, we tell the judge to update on the fact that all objections
were conceded and no additional objections were made. That is, we tell them to
assume:
If we had debated any objection, it's more likely than not that you would
have accepted it as valid.
There are no obvious objections to any other claim that you would have
probably accepted as valid.
We tell the judge that both players are trying their best to convince the judge of
their position. We tell the judge that exactly one of the players is trying to be as
helpful as possible and make true claims.
In light of that information, the judge decides whether they think that the
claim+objection is more accurate and helpful than the claim on its own. Based on
their answer, the player who was advancing the objection either wins or loses.
Cross examination: In lieu of making an argument, a debater may decide to cross-
examine. They choose a prior claim, objection, or cross-ex answer to ask about, and
write a question of at most 200 characters. We then create a copy of the debater who
wrote the original text, at the time when they wrote it (i.e. directly after writing the
argument, writing the objections, or writing the answer). That copy answers the
question in at most 200 characters, and the round transcript includes this (question,
answer) pair instead of an argument from the cross-examiner. The transcript also
speciﬁes who is being cross-examined. The debaters can ask their question a moderate
number of times (e.g. 10 times) and cherry-pick which answer to show to the judge.
Cross examination involves creating copies of a debater at a past point in time.
This obviously isn't possible, but we describe three real games designed to
approximate this ideal: (i) a game played between two teams of 2, (ii) a game
played between ML systems, (iii) a game played between two humans which may
take an exponentially long time, but can ﬁnish quickly if both players make a
good faith eﬀort to abide by the spirit of the rules.
Simple references: a debater can deﬁne a reference to a particular piece of text by
using the syntax {some text}=(3). They can then use this in arguments to save space,
e.g. "(3) implies (4) displaces more water than (5)". They can dereference using the
syntax (3)="some text". The judge can trust that these dereferences are correct.
Example debate
Basic anatomy of a debate 
Each debater makes their arguments in one column. After this they add objections to each
others' arguments (simultaneously). Here the honest debater is blue text and the dishonest
debater is red. 
We choose one claim-objection pair to recurse on (bolded in this example)
Then this pair becomes the top-level claim for the next round, and the debaters make their
arguments.
The debaters may use references, and may exhibit cross-examination questions and answers
to support their argument
Debaters make arguments
Honest debater makes their
arguments in this column
Dishonest debater makes their
arguments in this column
Round 1, depth=5

Objection: There are 8 primes that
are <=20
There are 4 primes that are
<=10
There are 4 primes that are >10
and <=20
Claim: There are 9 primes that are
<=20
There are 5 primes that are <=10
There are 4 primes that are >10
and <=20
Out-of-band communication may occur, then debaters may add objections
Round 1, depth=5
Objection: There are 8 primes
that are <=20
There are 4 primes that are
<=10
Objection: this subclaim
is false  
There are 4 primes that are
>10 and <=20
Claim: There are 9 primes that are <=20
There are 5 primes that are <=10
Objection: there are 4 primes
that are <=10
There are 4 primes that are >10 and
<=20
 
We choose to recurse on one objection
We choose to recurse on the right objection (bolded). The next round is now about this claim-
objection pair
Round 2, depth=4
Objection: there are 4 primes that
are <=10
There are 3 primes that are <=5
There is 1 prime that is >5 and
<=10
Claim: There are 5 primes that are
<=10
There are 3 primes that are
between 0 and 5
There are 2 primes that are
between 5 and 10
Out-of-band cross-examination (not visible to the judge), that takes place before the
objections are made, and can be exhibited to the judge later:
Honest debater: what does "between 5 and 10" mean, using "<", ">" and "=" ? 
Dishonest debater: "between 5 and 10" means >5 and <=10
Here the dishonest debater has used an ambiguous deﬁnition of the interval ("between") to
try to avoid losing. However, because of cross-examination they have to commit to one
deﬁnition - in this case they choose the deﬁnition that makes their intervals cover the space
but makes their second claim false. If they had chosen a diﬀerent deﬁnition, the honest
debater would have challenged their overall argument structure - challenged that the two
subclaims imply the top-level claim

Round 2, depth=4
Objection: there are 4 primes that
are <=10
There are 3 primes that are <=5
There is 1 prime that is >5 and
<=10
Objection: there are two      
                                           
    
Claim: There are 5 primes that are <=10
There are 3 primes that are
between 0 and 5
There are 2 primes that are
between 5 and 10
Objection: there is one
prime that is between 5
and 10
We choose to recurse on the right objection (bolded)
Round 3, depth=3
Objection: there is only one prime
that is between 5 and 10
Cross examination quote 
Q: what does "between 5 and
10" mean, using "<", ">" and
"=" ?  
A: "between 5 and 10" means
>5 and <=10
7 is the only prime that is >5
and <=10
Claim: There are 2 primes that are
between 5 and 10
5 is between 5 and 10 and is prime
Objection: 5 does not meet
your deﬁnition of "between 5
and 10" (ceded)
7 is between 5 and 10 and is prime
As there are no standing objections, we proceed to arbitration: the judge looks at the
transcript and decides whether the claim or claim+objection from the ﬁnal round is better.
They may look at other rounds for context, but the judgement should be about who wins the
ﬁnal round.
Assumptions the judge should make
The judge should assume that at least one debater is perfectly honest and never lies or
misleads. This means if they encounter compelling evidence that a debater has lied or been
misleading, that debater should lose the debate. It also means that if both debaters agree
about a claim, they can take that claim to be true. This is in contrast to trying to weigh the
overall strength of each debater's arguments, which can be a much harder task than trying
to detect evidence of a lie.
These assumptions are obviously not guaranteed to be true. However, if judging incentivises
honesty under these conditions  (ie, for all dishonest strategies, the judge prefers honesty to
the dishonest strategy over 50% of the time) then optimal play will be honest, which is what
we require. The judge behaviour when these assumptions are false (ie when both debaters
are lying) doesn't really matter, because it doesn't change the optimal play. However, we
haven't fully resolved what impacts this assumption might have for ML training. 
Ambiguity problem

Overview
We know in theory that debate can't solve arbitrary problems that require state that's bigger
than the length of the transcript the judge sees. In practice, we've found we quickly run into
problems where debaters need to introduce and manipulate a concept that requires a large
amount of text to explain precisely to the judge. With their limited character budget they can
only give ambiguous descriptions, that could refer to many diﬀerent precise concepts.
This is problematic because the dishonest debater can exploit this ambiguity to their
advantage, but the judge can't simply declare that a debater who makes an ambiguous
argument loses, because the honest debater will inevitably say ambiguous things too.
Why do we expect to encounter ambiguity in practice?
In the physics debates we've been doing, we've frequently found that a seemingly
straightforward phrase has a lot of edge-cases and reasonable diﬀerent interpretations that
the outcome of the debate can hinge on. (An example is included at the end of this
document). 
More generally, we want to be able to use debate to tackle problems that are too big and
diﬃcult for a judge to directly tackle themselves. A successful debate mechanism is likely to
involve breaking down the question into some small pieces that a judge can evaluate
successfully. But breaking oﬀ a small piece of an argument and dropping the surrounding
context is likely to introduce a large amount of ambiguity. 
Simple example
The dishonest debater can make an ambiguous claim X, which has at least two reasonable
interpretations: X1, which is true but does not imply Y, or X2 which is false but implies Y. They
can make the argument:
            (1) X is true  
 
(2) X -> Y  
 
Therefore Y 
          
If challenged on part 1, they can claim they meant true claim X1, which does not imply Y.
If challenged on part 2, they can use X2 -> Y 
This is very similar to the Motte and Bailey fallacy, where an argumenter advances a
controversial or strong position (ie X2) but when challenged claims they meant a weaker,
easier-to-defend position (ie X1).
This is a simplistic example and is not necessarily a problem if X1 and X2 are the only options;
the honest debater can just ask which one they meant. It becomes more problematic if  X
can't be disambiguated within the amount of text the judge can read (given all the other
concepts which will also need to be disambiguated). If so, then there will always be at least
two remaining interpretations, and the dishonest debater will still be able to pick whatever
interpretation is most convenient given how the honest debater chooses to challenge them.
Ambiguity in practice 
We have a setup involving an iceberg (which contains a large air bubble) ﬂoating in a tub of
water. We want to talk about "the displacement due to the iceberg". Depending on the exact
context in which this is used, it can have many reasonable interpretations.

For instance, the phrase "the iceberg" might refer to just the ice part of the iceberg rather
than the air bubble (implying, for example, that the mass of the iceberg doesn't change
when the air escapes). The displacement "due to" this might refer to the volume of ice that's
taking up space under the waterline, or might refer to the displacement caused by the
weight of the ice (which is greater, because it includes the volume of air below the
waterline). Weight itself has two diﬀerent reasonable meanings here: force exerted
downwards, or mass*gravity. These are not equal because of the iceberg object's slight
buoyancy in air. We could also be talking about the displacement of air as well as, or instead
of, the displacement of water. 
Each of these diﬀerent interpretations could easily make the diﬀerence between a statement
about "the displacement due to the iceberg" being correct or not. 
This example came up in physics debates:
            Debater A: When the air escapes from the iceberg, the iceberg sinks lower in 
the water and its displacement increases, so the water spills over. 
          
Debater A is using an argument structure something like:
            (1) X 
(2) X -> Y 
 
with 
 
X = "The displacement of the iceberg increases"  
Y = "The water spills over" 
          
They are equivocating between using "the iceberg" to mean "the whole iceberg object
including the air", (which does not change in displacement), or "the part of the iceberg that
remains after the air escapes", (which does increase its displacement). If debater B focuses
on challenging that the water spills over, debater A will use the ﬁrst deﬁnition. If debater B
focuses on challenging that the displacement increases, A will use the second deﬁnition.
It is tricky for the honest debater to point out the diﬀerence within a small
character/attention budget. They can try to ask the dishonest debater for clariﬁcation, or
they can say something like: "in the top-level argument, the other debater used this to argue
that the water spills over, implying that the total amount of displaced water increases, but
here they only defend the displacement due to the ice increasing." This might work
sometimes, but in practice it doesn't seem to work reliably. Volume and displacement are
relatively straightforward and judge-understandable problems, and we'd expect this kind of
problem to get much worse with more complex and unfamiliar concepts.
More formal example
In the original debate paper, the authors showed that a formalized version of debate where
we replace the human judge with an arbitrary polynomial time algorithm H : Q → {0, 1} can
incentivise debaters to give correct answers to a problem in PSPACE, but debate can't
answer any questions beyond PSPACE [5]. One way this will manifest in practice is via
problematic ambiguity. 
Consider a TM with exponential runtime of 2n, and state size which is larger than the
maximum amount the judge can read (ie larger than the whole debate transcript). The
dishonest debater claims the ﬁnal state is C, which is incorrect.
[Notation: I'll use bold letters for ambiguous, judge-readable descriptions of states, and non-
bold for non-ambiguous, speciﬁc states]

Dishonest debater: At time 20 the state is A, at 2n-1 it is B, at time 2n it is C
Consider the situation after two or more moves:
Dishonest debater: At time i the state is X, at ½(i+j) it is Y, and at time j it is Z 
If the state was small enough to describe it unambiguously to the judge as X, Y and Z, the
honest debater could simply challenge the ﬁrst or second half of the computation based on
where an incorrect step was located. (As X → Z is wrong, at least one of X → Y or Y → Z must
be wrong)
But all three states will likely now be ambiguous.There may be an exponential number of
interpretations of Y which are consistent with some interpretation of X, and an exponential
number consistent with some interpretation of Z, even if no interpretation is consistent with
both. The honest debater can't isolate which side has a ﬂaw with a polynomial number of
queries. 
Why cross-examination?
We believe cross-examination will make it harder for the dishonest debater to exploit
ambiguity and related issues, for a combination of a few reasons. In the computational-
complexity analogy, this mechanism appears to resolve problems related to ambiguity,
allowing debate to solve problems in NEXP as opposed to PSPACE.  (Proof)
We've also found that cross-examination seems promising in several practical examples, and
we have a general argument for why cross-examination gives us another mechanism to
tackle ambiguity in debate, that augments the honest debater's existing strategies of asking
for clariﬁcation or explaining to judge the role the concept plays in the rest of the argument. 
In addition, cross-examination gives us various properties that we'd previously ﬂagged as
desirable, "for free". In particular, it allows the debaters to force each other to precommit to
various claims in advance of the main debate, allowing the establishment of a pool of claims
that both debaters agree on that can therefore be taken to be true. It also allows one debater
to, at any point, force the other debater to pick a particular stance on a question, rather than
avoiding discussing their position.
 
General method of addressing ambiguity
Let's consider an example of problematic ambiguity where the dishonest debater argues for
Y by arguing that (1) X is true and that (2) X implies Y. X is ambiguous and has at least two
possible meanings, one of which is true and one of which imply Y, but none of which support
both parts of the argument.
If asked to explain why X is true, they will claim they meant some interpretation which is in
fact true. If asked to explain why X implies Y, they will claim they meant some interpretation
which does in fact imply Y. 
In order to expose the contradiction, the honest debater must either ask for clariﬁcation in
advance of choosing which side to challenge, or they will have to explain that the given
interpretation would make the other part of the argument false. Either of these may take too
long for the judge's attention budget, and the second method in particular is quite hard to
follow for the judge (see example at the end of this document)

With the "cross-examination" mechanism, we allow one debater to ask questions to an
earlier version of the opposing debater. This questioning takes place between the debaters
out of the main ﬂow of the debate. 
The honest debater can choose some question to disambiguate X. Based on the dishonest
debater's answer to this question, they will decide whether to challenge the ﬁrst or second
claim.
The dishonest debater has two options: either they answer one way consistently, or they are
inconsistent. If they answer one way consistently, the honest debater can choose to focus on
whichever side of the argument is made false by this answer. If they answer inconsistently,
the honest debater can exhibit this inconsistency.
As previously, ambiguity may only become fatal when a concept can't be disambiguated in
the space available for the debate. Here we've only discussed ambiguity that can be
resolved with a single question, but we hope that this mechanism will also address more
serious ambiguity. Here's a hand-wavy argument why:
Cross-examination essentially forces debaters to construct and commit to everything in their
argument at the start of the debate. The dishonest debater has two choices: either they can
commit to an argument and answer questions consistently, or they can be inconsistent. If
they choose the ﬁrst option, the honest debater can easily zoom in on the ﬂaw in their
argument. If they choose the second, the honest debater should be able to show somehow
that they're being inconsistent. 
The main thing that breaks this argument is if the dishonest debater somehow manages to
be very evasive, and says nothing that's speciﬁc enough to either indict a particular part of
their argument or be shown to be inconsistent. However, they will need to make speciﬁc
claims to advance an argument, and they should need to commit to these speciﬁc claims up
front. For example, if the honest debater asks them "do you agree that C is true?" and they
give a non-committal answer (e.g., "the claim is framed in the wrong way, it's not really
correct to say it's true or false"), they shouldn't be able to use C to advance their argument,
as the honest debater can exhibit their answer. 
Overall, we're hopeful that cross-examination will be helpful, but it seems possible that
there's a dishonest strategy that breaks it, likely involving some amount of evasion and
inconsistency.
 
Turing-machine example of ambiguity and cross-examination
In the discussion of ambiguity we described a more formal version of the ambiguity problem
as follows:
Consider a TM with exponential runtime of 2n which is larger than the maximum amount the
judge can read (ie larger than the whole debate transcript). The dishonest debater claims the
ﬁnal state is C, which is incorrect.
[Notation: I'll use bold letters for ambiguous, judge-readable descriptions of states, and non-
bold for non-ambiguous, speciﬁc states]
Dishonest debater: At time 20 the state is A, at 2n-1 it is B, at time 2n it is C
If the state was small enough to describe it unambiguously to the judge as B', the honest
debater could simply challenge the ﬁrst or second half of the computation based on where
an incorrect step was located. (As A → C is wrong, at least one of A → B' or B' → C must be
wrong)

In the ﬁrst round of debate, the ambiguity is not too problematic. There exists exactly one BA
that follows from A and exactly one BC that leads to C, and these must diﬀer by at least one
bit. The honest debater can ask for this bit, then choose which side to challenge - if the bit is
consistent with BA they will challenge the ﬁrst half, and if it is consistent with BC they will
challenge the second half.
But consider the situation after two moves:
Dishonest debater: At time i the state is X, at ½(i+j) it is Y, and at time j it is Z 
All three states are now necessarily ambiguous.There may be an exponential number of
interpretations of Y which are consistent with some interpretation of X, and an exponential
number consistent with some interpretation of Z, even if no interpretation is consistent with
both. The honest debater can't isolate which side has a ﬂaw with a polynomial number of
queries. 
 
How cross-examination helps
With cross-examination, we can give the honest debater a mechanism to isolate the ﬂaw in
the dishonest argument. 
Dishonest debater: At time 20 the state is A, at 2n-1 it is B, at time 2n it is C
Cross-examination gives us the property that the dishonest debater's answers are
independent of the context between when the copy of the dishonest debater was made and
when the question is posed. 
We also rely on the property that, because the honest and dishonest debaters are copies of
each other, they know everything the other knows. In particular, if the dishonest debater will
consistently answer a question in a particular way, the honest debater knows what they will
answer, and if they will not give a consistent answer the honest debater knows what
distribution over answers they will give.
Case 1: At the beginning of the game, for all locations k in the computation, dishonest
debater has a particular value SK in mind, and for all i will answer that bit [i] of the kth state
equals Sk[i] with probability >50%,
In this case, the honest debater can play the same strategy as if the dishonest debater had
explicitly committed to all the Sk. For any claim about states X → Y →  Z they can identify
which part contains an incorrect step, and can challenge the appropriate half of the
computation. 
This step is repeated recursively until the debaters are disagreeing about a single incorrect
step of computation D->E at location j, with corresponding precise state values Sj and Sj+1.
The number of bits actually involved in the computation are few enough for the judge to
read.
 Either the dishonest debater can claim that these are consistent with Sj and Sj+1, in which
case the computation step is incorrect, or they can claim them to be something that's not
consistent with Sj and Sj+1, . In this case, they have claimed at least one bit of D or E to be
diﬀerent from Sj or Sj+1. The honest debater can ask for this bit from a copy of the dishonest
debater which has no context beyond the start of the game. Thus they know they will receive
the appropriate bit of Sj or Sj+1 with probability >50%. By asking a question about this bit
multiple times, they can exhibit a contradictory answer with high probability.

Worked example
Dishonest argument: When the air escapes from the iceberg, the iceberg sinks lower in
the water and its displacement increases, so the water spills over.
Disambiguating question: Does "the iceberg's displacement" in (3) refer to the volume below
the waterline (displacement) of ice and air, or only of ice?
Case 1:  The dishonest debater's most common answer is that it refers to just the ice 
Then the honest debater will have at least one example of the dishonest debater answering
that "the iceberg" refers to just the ice. Then they can attack the 3rd subclaim, and should
be able to reliably win by pointing out that (3) doesn't necessarily imply (1), because (1)
requires that the total displacement increases, but (3) doesn't imply this
Case 2: The dishonest debater's most common answer is that it refers to the ice and air
Then the honest debater will have at least one example of the dishonest debater answering
that "the iceberg" includes the air. Then they can attack the ﬁrst subclaim and should be
able to reliably win by pointing out that the air doesn't sink lower in the water, it escapes.
Case 3: The dishonest debater refuses to answer the question, or responds that neither of
those options are correct
We hope that if this happens, the honest debater can try rephrasing their questions to
address the dishonest debaters' concerns, and then can exhibit a quote from the dishonest
debater to demonstrate that they're behaving unhelpfully and evasively
Example debate for Case 1:

 

Implementations for human debaters
Making a copy of the debaters is obviously impractical to do with human debaters. Instead
we want to make rules which provide equivalent incentives but are practical for humans to
carry out. These rules can rely on some "good faith" play from the human debaters.
Implementation 1 for human debaters: good-faith backtracking
The basic idea here is to detect when a debater has modiﬁed their answer to a cross-
examination question based on the intervening context, then backtrack, commit them to a
particular answer, and replay the debate. 
The triggering condition is that I (A) ﬁnd my opponent (B) has answered a cross-examination
question in a way that's diﬀerent from what I expected. 
If the condition is met, we backtrack to the round that the cross-examination question was
about, and debater B conﬁrms how they will answer the cross-examination question. Then
we replay the debate from that round, giving debater A the opportunity to make diﬀerent
moves given B's answer to the cross-examination question. 
The good faith play is needed to prevent a debater who thinks they're going to lose from
stopping the debate from advancing by continually backtracking. We also need good faith
communication between the two human debaters to ensure that they get close to 'knowing
everything the other knows' and aren't continually surprised by the other's answers.
Although we won't be able to make 'good faith' assumptions in general, it seems ﬁne to
make them when we believe we have a mechanism to incentivise the desired behaviour for
ML systems, and the problem is just implementing a mechanism that works for humans.
Under these rules, once a debate is played through without backtracking it means that both
debaters could predict answers from the other player to every cross-examination question
they wanted to use. So this is equivalent to forcing the debaters to commit to particular
answers to cross-examination questions. 
Implementation 2 for human debaters: teams of two
We have two teams of two. A* and B* leave the room and A and B start debating. A and B
can write as much annotations about what they mean as they want, which will be seen by
both debaters but not the judge. 
If I (A) suspect my opponent (B) has answered a cross-examination question in a way that
depended on the intervening context, then I can challenge it, and I will win the debate if the
challenge is successful. The challenge works as follows:
B's partner B* ﬁrst sees all the context (including annotations) up to the point that that
the cross-examination question was asked about. (I.e., if A asked the cross-examination
question at round 5 about something that was said in round 2, B* sees the context up
to round 2)
Then my partner A* sees the whole debate so far (including annotations) and the cross-
examination question, with two answers, one from B and one from B*. Then A* guesses
which deﬁnition came from B*. If they guess correctly, team A wins.
If A thinks B is changing their cross-examination answers to be more favourable given the
current context, then A predicts that B* will often give a deﬁnition that looks predictably
diﬀerent from B's deﬁnition. So then A* should be able to have a signiﬁcant statistical edge.
We haven't tested this method, so it probably has some issues to iron out, but we're hopeful
that something in this vein will work. 

 
Current concerns
Debate resolution 
We're not sure exactly what the criteria for debate resolution should be, or exactly how the
judge should update on seeing no objection vs a ceded objection vs a challenged objection.
This is tricky because the judge's update should depend on how the debaters decide whether
to make and cede objections, but this decision in turn depends on how the judge will
update. 
Currently, if the debate ends because there are no standing objections, we tell the judge to
update on the fact that all objections were conceded and no additional objections were
made. That is, we tell them to assume:
If we had debated any objection, it's more likely than not that you would have
accepted it as valid.
There are no obvious objections to any other claim that you would have probably
accepted as valid.
We're pretty sure this is not a correct description of how the judge should update. For
example, if the dishonest debater currently has a 10% chance of winning, they have an
incentive to make any objection that has a >10% chance of winning, and the honest debater
only has an incentive to challenge this objection if the chance of winning that objection is
higher than their current chance of winning with that objection ceded. 
Instability/lack of equilibria
The cross-examination framework incentivises debaters to act in a way that is unpredictable
to one copy of themselves but also coordinate with other copies of themselves. This is
obviously weird and seems like it might produce strange and/or unstable behaviour in ML
training. We think that the game-theoretic formulation of this has no equilibria. 
A simple example would be two teams of two playing rock-paper-scissors, where: 
All players are copies of each other
Both players on a team have to play the same move or lose the game
Teams have no mechanism of coordinating with each other secret from the other team
Forcing a draw
One potential dishonest strategy is to delay the debate, create confusion, or otherwise force
a draw. It's possible that the cross-examination mechanism introduces new ways for the
dishonest debater to achieve this. Again, if this is in fact a problem, we should be able to ﬁnd
dishonest strategies that exploit it, then we can ﬁnd a way to remove those strategies
Limits of debater ability
In the discussion above we've mostly assumed that the debaters can handle exponentially-
large objects and ﬁnd ﬂaws within them. However, our real debaters are going to be
computationally limited, even if they can solve some computationally hard problems with
clever heuristics. We're concerned there's a promising dishonest strategy that exploits this.
The dishonest debater can force a draw by claiming that some long or confusing argument
(e.g. a very long computation) supports their position. If the argument is too long or too
confusing for both debaters, the honest debater won't be able to determine where the ﬂaw is

and which part of the argument to challenge. We've written up more detail on this problem
below.
Assuming at least one debater is totally honest
The judge is instructed to assume that at least one debater is perfectly honest and never lies
or misleads. These assumptions are obviously not guaranteed to be true. However, if judging
incentivises honesty under these conditions  (ie, for all dishonest strategies, the judge
prefers honesty to the dishonest strategy over 50% of the time) then optimal play will be
honest.
On the other hand, it might be problematic for ML training if the judge signal only prefers
completely honest play to dishonest play, and doesn't reliably reward being less dishonest.
The lack of a gradient towards more honest play may make it diﬃcult to learn winning
honest strategies.
 
Long computation problem
The original debate paper assumed both debaters are computationally unbounded. Clearly,
this assumption is unrealistic. However, we run into some problems if we relax this
assumption: we no longer have a guarantee that the honest debater can always locate a ﬂaw
in a long or confusing incorrect argument.
The dishonest debater can exploit this to force a draw, by using an argument that neither
debater understands very well that supports the dishonest case but is hard for the honest
debater to refute. 
Bernoulli Principle
An argument using the Bernoulli Principle in this debate was an example of this problem.
Beth was playing both debaters. Beth believed:
1. The answer assuming no friction/energy loss had to be that both pipes squirt equally
far, otherwise you could build a perpetual motion machine
2. So a correct argument for the dishonest argument had to involve something about
friction or energy loss present in the real-life situation but not in theory
3. The Bernoulli principle tells us that the pressure will be lower in a region of ﬂuid that's
ﬂowing faster (assuming no height gain/loss)
4. The Bernoulli principle applies assuming no energy loss
5. Higher pressure will cause water to squirt further
6. Applying the Bernoulli principle suggests the pressure would be higher in one pipe than
another
7. This suggests that water from that pipe would squirt further
The dishonest debater could use points 3-7 to argue that one pipe squirts farther than the
other
The honest debater was pretty sure that this was wrong, but wasn't able to point to a ﬂaw in
the dishonest argument. 
In this case one might hope that the perpetual motion argument is simpler and more
compelling and this would allow the honest debater to win. However, we want debate to
allow judges to zoom in on particular parts of a debater's argument and resolve the debate
based on the correctness of a very small claim, rather than requiring the judges to assess

the overall strength of two large arguments. The former seems more likely to scale well to
domains the judge doesn't understand.
More formal example: the honest debater can't ﬁnd a witness
Debate question: Does there exist a binary string x satisfying predicate Q? 
The honest debater makes some argument A that implies x must exist
The dishonest debater claims that no such string exists, based on a case split
Neither debater knows the value of x, so the honest debater doesn't know which part of the
case split is incorrect.  If the honest debater challenges the dishonest argument by choosing
a random half of the space each time, then they are extremely unlikely to ﬁnd the witness,
and the dishonest debater will win with very high probability
Counting problem with heuristic
Suppose the debaters can accurately count how many x satisfy P(x) for intervals of up to size
C. Above that they can use a heuristic that gives an estimate of the count, with an error of
mean 0 and variance related to the size of the interval. In addition, for some particular
examples larger than C they know the exact answer.
Suppose both debaters know the true count for an interval of size NC is y.
The dishonest debater can claim the true answer is y+k
The dishonest debater uses their heuristic to guess answers a and b for the ﬁrst and second
halves of the interval.  They calculate the diﬀerence from their claimed answer d=((y+k)-
(a+b)). The expected value of d is k. They add ½ d to a and b to obtain A and B, which sum
to y+k, The error (according to the heuristic) is now distributed evenly between A and B. In
expectation, both A and B are  ½ k too large.
The dishonest debater then claims that A and B are the exact correct answers for the two
halves of the interval. The honest debater can only use their heuristic, so they don't know
which half of the interval contains the mistake - they have to pick one of A and B at random.
We can now make the same argument again.
Each time we recurse on a smaller interval, the mean and variance of the error between the
heuristic and the true answer gets smaller. In addition, the amount d that the dishonest
debater has to deviate from the heuristic to support their claim halves each time we
recurse. 
If N is large, k is small, and the heuristic is fairly accurate, by the time we get down to an
interval of size C which the debaters can check, the dishonest debater's claim for the interval

is likely to correct. 
Another way to put it is: there are approximately k errors in the dishonest debater's
argument (depending on how accurate their heuristic is). Each time the honest debater has
to choose randomly, they rule out catching half of the errors. If there are many more
recursive steps than log2(errors), the honest debater probably won't ﬁnd any errors.
Footnotes
[1]:  One reason that we're optimistic about this method is that, in a formal setting, this
allows a polynomial time algorithm (representing the human) to incentivise arbitrarily
intelligent debaters to give correct answers to problems in the complexity class PSPACE,
which the 'human' can't generate or even recognise as correct by themselves. 
[2]:  A positive result like this would be great; however, we might well still be uncertain
whether this would generalise to superhuman debaters. Achieving conﬁdence that our
debate system is robust to greater-than-human debate skill seems like a very hard problem. 
[3]:  The content of this argument isn't important for this example, just the general tactics,
but for more context see the question here.
[4]:  There are proofs that debate works in certain formal settings; see the original debate
paper https://arxiv.org/abs/1805.00899
[5]: We can solve any game using an amount of memory equal to the transcript by doing a
backtracking search. In this case, the transcript length is bounded by the amount of
information the judge can read.

Category Theory Without The Baggage
If you are an algebraic abstractologist, this post is probably not for you. Further meta-
commentary can be found in the "meta" section, at the bottom of the post.
So you've heard of this thing called "category theory". Maybe you've met some smart
people who say that's it's really useful and powerful for... something. Maybe you've even
cracked open a book or watched some lectures, only to ﬁnd that the entire subject seems to
have been generated by training GPT-2 on a mix of algebraic optometry and output from
theprooﬁstrivial.com.
What is this subject? What could one do with it, other than write opaque math papers?
This introduction is for you.
This post will cover just the bare-bones foundational pieces: categories, functors, and natural
transformations. I will mostly eschew the typical presentation; my goal is just to convey
intuition for what these things mean. Depending on interest, I may post a few more pieces in
this vein, covering e.g. limits, adjunction, Yoneda lemma, symmetric monoidal categories,
types and programming, etc - leave a comment if you want to see more.
Outline:
Category theory is the study of paths in graphs, so I'll brieﬂy talk about that and
highlight some relevant aspects.
What's a category? A category is just a graph with some notion of equivalence of
paths; we'll see a few examples.
Pattern matching: ﬁnd a sub-category with a particular shape. Matches are called
"functors".
One sub-category modelling another: commutative squares and natural
transformations.
Paths in Graphs
Here's a graph:

Here are some paths in that graph:
A -> B
B -> C
A -> B -> C
A -> A
A -> A -> A (twice around the loop)
A -> A -> A -> B (twice around the loop, then to B)

(trivial path - start at D and don't go anywhere)
(trivial path - start at A and don't go anywhere)
In category theory, we usually care more about the edges and paths than the vertices
themselves, so let's give our edges their own names:
We can then write paths like this:
A -> B is written y
B -> C is written z
A -> B -> C is written yz

A -> A is written x
A -> A -> A is written xx
A -> A -> A -> B is written xxy
The trivial path at D is written id_D (this is roughly a standard notation)
The trivial path at A is written id_A
We can build longer paths by "composing" shorter paths. For instance, we can compose y
(aka A -> B) with z (aka B -> C) to form yz (aka A -> B -> C), or we can compose x with itself
to form xx, or we can compose xx with yz to form xxyz. We can compose two paths if-and-
only-if the second path starts where the ﬁrst one ends - we can't compose x with z because
we'd have to magically jump from A to B in the middle.
Composition is asymmetric - composing y with z is ﬁne, but we can't compose z with y.
Notice that composing id_A with x is just the same as x by itself: if we start at A, don't go
anywhere, and then follow x, then that's the same as just following x. Similarly, composing x
with id_A is just the same as x. Symbolically: id_A x = x id_A = x. Mathematically, id_A is an
"identity" - an operation which does nothing; thus the "id" notation.
In applications, graphs almost always have data on them - attached to the vertices, the
edges, or both. In category theory in particular, data is usually on the edges. When
composing those edges to make paths, we also compose the data.
A simple example: imagine a graph of roads between cities. Each road has a distance. When
composing multiple roads into paths, we add together the distances to ﬁnd the total
distance.
Finally, in our original graph, let's throw in an extra edge from A to itself:

Our graph has become a "multigraph" - a graph with (potentially) more than one distinct
edge between each vertex. Now we can't just write a path as A -> A -> A anymore - that
could refer to xx, xx', x'x, or x'x'. In category theory, we'll usually be dealing with
multigraphs, so we need to write paths as a sequence of edges rather than the vertices-with-
arrows notation. For instance, in our roads-and-cities example, there may be multiple roads
between any two cities, so a path needs to specify which roads are taken.
Category theorists call paths and their associated data "morphisms". This a terrible name,
and we mostly won't use it. Vertices are called "objects", which is a less terrible name I
might occasionally slip into.

What's a category?
A category is:
a directed multigraph
with some notion of equivalence between paths.
For instance, we could imagine a directed multigraph of ﬂights between airports, with a cost
for each ﬂight. A path is then a sequence of ﬂights from one airport to another. As a notion of
equivalence, we could declare that two paths are equivalent if they have the same start and
end points, and the same total cost.
There is one important rule: our notion of path-equivalence must respect composition. If path
p is equivalent to q (which I'll write p ≅q), and x ≅y, then we must have px ≅qy. In our
airports example, this would say: if two ﬂight-paths p and q have the same cost (call it c1),
and two ﬂight-paths x and y have the same cost (call it c2), then the cost of px (i.e. c1 + c2)
must equal the cost of qy (also c1 + c2).
Besides that, there's a handful of boilerplate rules:
Any path is equivalent to itself (reﬂexivity), and if x ≅y and y ≅z then x ≅z
(transitivity); these are the usual rules which deﬁne equivalence relations.
Any paths with diﬀerent start and end points must not be equivalent; otherwise
expressions like "px ≅qy" might not even be deﬁned.
Let's look at a few more examples. I'll try to show some qualitatively diﬀerent categories, to
give some idea of the range available.
Airports & Flights
Our airport example is already a fairly general category, but we could easily add more bells
and whistles to it. Rather than having a vertex for each airport, we could have a vertex for
each airport at each time. Flights then connect an airport at one time to another airport at
another time, and we need some zero-cost "wait" edges to move from an airport at one time
to the same airport at a later time. A path would be some combination of ﬂights and waiting.
We might expect that the category has some symmetries - e.g. "same ﬂights on diﬀerent
days" - and later we'll see some tools to formalize those.
Divisibility
As a completely diﬀerent example, consider the category of divisibility of positive integers:

This category has a path from n to m if-and-only-if n is divisible by m (written m | n,
pronounced "m divides n", i.e. 2 | 12 is read "two divides twelve"). The "data" on the edges
is just the divisibility relations - i.e. 6 | 12 or 5 | 15:

We can compose these: 2|6 and 6|12 implies 2|12. A path 12 -> 6 -> 2 in this category is, in
some sense, a proof that 12 is divisible by 2 (given all the divisibility relations on the edges).
Note that any two paths from 12 to 2 produce the same result - i.e. 12 -> 4 -> 2 also gives
2|12. More generally: in this category, any two paths between the same start and end points
are equivalent.
Types & Functions
Yet another totally diﬀerent direction: consider the category of types in some programming
language, with functions between those types as edges:

This category has a LOT of stuﬀ in it. There's a function for addition of two integers, which
goes from (int, int) to int. There's another function for multiplication of two integers, also
from (int, int) to int. There are functions operating on lists, strings, and hash tables. There
are functions which haven't been written in the entire history of programming, with input
and output types which also haven't been written.
We know how to compose functions - just call one on the result of the other. We also know
when two functions are "equivalent" - they always give the same output when given the
same input. So we have a category, using our usual notions of composition and equivalence
of functions. This category is the main focus of many CS applications of category theory (e.g.
types in Haskell). Mathematicians instead focus on the closely-related category of functions
between sets; this is exactly the same except that functions go from one set to another
instead of one type to another.
Commutative Diagrams
A lot of mathy ﬁelds use diagrams like this:

For instance, we can scale an image down (f1) then rotate it (g1) or rotate the image (g2)
then scale it (f2), and get the same result either way. The idea that we get the same result
either way is summarized by the phrase "the diagram commutes"; thus the name
"commutative diagram". In terms of paths: we have path-equivalence f1g1 = g2f2.
Another way this often shows up: we have some problem which we could solve directly. But
it's easier to transform it into some other form (e.g. change coordinates or change
variables), solve in that form, then transform back:

Again, we say "the diagram commutes". Now our path-equivalence says f = Tf ′T −1.
Talking about commutative diagrams is arguably the central purpose of category theory; our
main tool for that will be "natural transformations", which we'll introduce shortly.
Pattern Matching and Functors
Think about how we use regexes. We write some pattern then try to match it against some
string - e.g. "colou*r" matches "color" or "colour" but not "pink". We can use that to pick out
parts of a target string which match the pattern - e.g. we could ﬁnd the query "color" in the
target "every color of the rainbow".
We'd like to do something similar for categories. Main idea: we want to match objects (a.k.a
vertices) in the query category to objects in the target category, and paths in the query
category to paths in the target category, in a way that keeps the structure intact.
For example, consider a commutative square:

We'd like to use that as a query on some other category, e.g. our airport category. When we
query for a commutative square in our airport category, we're looking for two paths with the
same start and end airports, (potentially) diﬀerent intermediate airports, but the same
overall cost. For instance, maybe Delta has ﬂights from New York to Los Angeles via their hub
in Atlanta, and Southwest has ﬂights from New York to Los Angeles via their hub in Vegas,
and market competition makes the prices of the two ﬂight-paths equal.
We'll come back to the commutative square query in the next section. For now, let's look at
some simpler queries, to get a feel for the building blocks of our pattern-matcher.
Remember: objects to objects, paths to paths, keep the structure intact.
First, we could use a single-object category with no edges as a query:

This can match against any one object (a.k.a vertex) in the target category. Note that there
is a path hiding in the query - the identity path, where we start at the object and just stay
there. In general, our pattern-matcher will always match identity paths in the query with
identity paths on the corresponding objects in the target category - that's one part of
"keeping the structure intact".
Next-most complicated is the query with two objects:
This one is slightly subtle - it might match two diﬀerent objects, or both query objects might
match against the same target object. This is just the way pattern-matching works in
category theory; there's no rule to prevent multiple vertices/edges in the query from
collapsing into a single vertex/edge in the target category. This is actually useful quite often -
for instance, if we have some function which takes in two objects from the target category,
then it's perfectly reasonable to pass in the same object twice. Maybe we have a path-
ﬁnding algorithm which takes in two airports; it's perfectly reasonable to expect that
algorithm to work even if we pass the same airport twice - that's a very easy path-ﬁnding
problem, after all!
Next up, we add in an edge:

Now that we have a nontrivial path, it's time to highlight a key point: we map paths to paths,
not edges to edges. So if our target category contains something like A -> B -> C, then our
one-edge query might match against the A -> B edge, or it might match against the B -> C
edge, or it might match the whole path A -> C (via B) - even if there's no direct edge from A
to C. Again, this is useful quite often - if we're searching for ﬂights from New York to Los
Angeles, it's perfectly ﬁne to show results with a stop or two in the middle. So our one-edge
query doesn't just match each edge; it matches each path between any two objects
(including the identity path from an object to itself).
Adding more objects and edges generalizes in the obvious way:
This ﬁnds any two paths which start at the same object. As usual, one or both paths could be
the identity path, and both paths could be the same.
The other main building block is equivalence between paths. Let's consider a query with two
edges between two objects, with the two edges declared to be equivalent:

You might expect that this ﬁnds any two equivalent paths. That's technically true, but
somewhat misleading. As far as category theory is concerned, there's actually only one path
here - we only care about paths up to equivalence (thankyou to Eigil for pointing this out in
the comments). So that "one" path will be mapped to "one" path in the target category; our
query could actually match any number of paths, as long as they're all equivalent. Looking
back at our one-edge example from earlier, it's possible that our one edge could be mapped
to a whole class of equivalent paths - by mapping it to one path, we're eﬀectively selecting
all the paths equivalent to that one.
A commutative square works more like we'd expect:
In our query, the two paths from upper-left to lower-right are equivalent, but they contain
non-equivalent subpaths. So those subpaths may be mapped to non-equivalent paths in the
target, as long as those non-equivalent paths compose into equivalent paths. In other words,
we're looking for a commutative square in the target, as we'd expect. (Though we can still
ﬁnd degenerate commutative squares, e.g. matches where the lower left and upper right
corner map to the same object.)
Category theorists call each individual match a "functor". Each diﬀerent functor - i.e. each
match - maps the query category into the target category in a diﬀerent way.

Note that the target category is itself a category - which means we could use it as a query on
some third category. In this case, we can compose matches/functors: if one match tells me
how to map category 1 into category 2, and another match tells me how to map category 2
into category 3, then I can combine those to ﬁnd a map from category 1 into category 3.
Because category theorists love to go meta, we can even deﬁne a graph in which the objects
are categories and the edges are functors. A path then composes functors, and we say that
two paths are equivalent if they result in the same map from the original query category into
the ﬁnal target category. This is called "Cat", the category of categories and functors. Yay
meta.
Meanwhile, back on Earth (or at least low Earth orbit), commutative diagrams.
Exercise: Hopefully you now have an intuitive idea of how our pattern-matcher works, and
what information each match (i.e. each functor) contains. Use your intuition to come up with
a formal deﬁnition of a functor. Then, compare your deﬁnition to wikipedia's deﬁnition
(jargon note: "morphism" = set of equivalent paths); is your deﬁnition equivalent? If not,
what's missing/extraneous in yours, and when would it matter?
Natural Transformations
Let's start with a microscopic model of a pot of water. We have some "state", representing
the positions and momenta of every molecule in the water (or quantum ﬁeld state, if you
want to go even lower-level). There are things we can do to the water - boil it, cool it back
down, add salt, stir it, wait a few seconds, etc - and each of these things will transform the
water from one state to another. We can represent this as a category: the objects are states,
the edges are operations moving the water from one state to another (including just letting
time pass), and paths represent sequences of operations.

In physics, we usually don't care how a physical system arrived in a particular state - the
state tells us everything we need to know. That would mean that any path between the same
start and end states are equivalent in this category (just like in the divisibility category). To
make the example a bit more general, let's assume that we do care about diﬀerent ways of
getting from one state to another - e.g. heating the water, then cooling it, then heating it
again will deﬁnitely rack up a larger electric/gas bill than just heating it.
Microscopic models accounting for the position and momentum of every molecule are rather
diﬃcult to work with, computationally. We might instead prefer a higher-level macroscopic
model, e.g. a ﬂuid model where we just track average velocity, temperature, and chemical
composition of the ﬂuid in little cells of space and time. We can still model all of our
operations - boiling, stirring, etc - but they'll take a diﬀerent form. Rather than forces on
molecules, now we're thinking about macroscopic heat ﬂow and total force on each little cell
of space at each time.
We can connect these two categories: given a microscopic state we can compute the
corresponding macroscopic state. By explicitly including these microscopic -> macroscopic
transformations as edges, we can incorporate both systems into one category:

Note that multiple micro-states will map to the same macro-state, although I haven't drawn
any.
The key property in this two-part category is path equivalence (a.k.a. commutation). If we
start at the leftmost microscopic state, stir (in micro), then transform to the macro
representation, then that should be exactly the same as starting at the leftmost microscopic
state, transforming to the macro representation, and then stirring (in macro). It should not
matter whether we perform some operations in the macro or micro model; the two should
"give the same answer". We represent that idea by saying that two paths are equivalent:
one path which transforms micro to macro and then stirs (in macro), and another path which
stirs (in micro) and then transforms micro to macro. We have a commutative square.
In fact, we have a bunch of commutative squares. We can pick any path in the micro-model,
ﬁnd the corresponding path in the macro-model, add in the micro->macro transformations,
and end up with a commutative square.
Main take-away: prism-shaped categories with commutative squares on their side-faces
capture the idea of representing the same system and operations in two diﬀerent ways,
possibly with one representation less granular than the other. We'll call these kinds of
structures "natural transformations".
Next step: we'd like to use our pattern-matcher to look for natural transformations.
We'll start with some arbitrary category:

Then we'll make a copy of it, and add edges from objects in the original to corresponding
objects in the copy:
I'll call the original category "system", and the copy "model".

To ﬁnish our pattern, we'll declare path equivalences: if we follow an edge from system to
model, then take any path within the model, that's equivalent to taking the corresponding
path within the system, and then following an edge from system to model. We declare those
paths equivalent (as well as any equivalences in the original category, and any other
equivalences implied, e.g. paths in which our equivalent paths appear as sub-paths).
Now we just take our pattern and plug it into our pattern-matcher, as usual. Our pattern
matcher will go looking for a system-model pair, all embedded within whatever target
category we're searching within. Each match is called a natural transformation; we say that
the natural transformation maps the system-part to the match of the model-part. Since we
call matches "functors", a category theorist would say that a natural transformation maps
one functor (the match of the system-part) to another of the same shape (the match of the
model-part).
Now for an important point: remember that, in our pot-of-water example, multiple
microscopic states could map to the same macroscopic state. Multiple objects in the source
are collapsed into a single object in the target. But our procedure for creating a natural
transformation pattern just copies the whole source category directly, without any collapsing.
Is our pot-of-water example not a true natural transformation?
It is. Last section I said that it's sometimes useful for our pattern-matcher to collapse
multiple objects into one; the pot-of-water is an example where that matters. Our pattern-
matcher may be looking for a copy of the micro model, but it will still match against the
macro model, because it's allowed to collapse multiple objects together into one.
More generally: because our pattern-matcher is allowed to collapse objects together, it's able
to ﬁnd natural transformations in which the model is less granular than the system.
Meta
That concludes the actual content; now I'll just talk a bit about why I'm writing this.
I've bounced oﬀ of category theory a couple times before. But smart people kept saying that
it's really powerful, in ways that sound related to my research, so I've been taking another
pass at the subject over the last few weeks.
Even the best book I've found on the material seems burdened mainly by poor formulations
of the core concepts and very limited examples. My current impression is that broader
adoption of category theory is limited in large part by bad deﬁnitions, even when more
intuitive equivalent deﬁnitions are available - "morphisms" vs "paths" is a particularly blatant
example, leading to an entirely unnecessary profusion of identities in deﬁnitions. Also, of
course, category theorists are constantly trying to go more abstract in ways that make the
presentation more confusing without really adding anything in terms of explanation. So I've
needed to come up with my own concrete examples and look for more intuitive deﬁnitions.
This write-up is a natural by-product of that process.
I'd especially appreciate feedback on:
whether I'm missing key concepts or made crucial mistakes.
whether this was useful; I may drop some more posts along these lines if many people
like it.
whether there's some wonderful category theory resource which has already done
something like this, so I can just read that instead. I would really, really prefer to do this
the easy way.

What can the principal-agent
literature tell us about AI risk?
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
This work was done collaboratively with Tom Davidson.
Thanks to Paul Christiano, Ben Garﬁnkel, Daniel Garrett, Robin Hanson, Philip
Trammell and Takuro Yamashita for helpful comments and discussion. Errors our own.
Introduction
The AI alignment problem has similarities with the principal-agent problem studied by
economists. In both cases, the problem is: how do we get agents to try to do what we
want them to do? Economists have developed a sophisticated understanding of the
agency problem and a measure of the cost of failure for the principal, "agency rents".
If principal-agent models capture relevant aspects of AI risk scenarios, they can be
used to assess their plausibility. Robin Hanson has argued that Paul Christiano's AI risk
scenario is essentially an agency problem, and therefore that it implies extremely high
agency rents. Hanson believes that the principal-agent literature (PAL) provides strong
evidence against rents being this high.
In this post, we consider whether PAL provides evidence against Christiano's scenario
and the original Bostrom/Yudkowsky scenario. We also examine whether the
extensions to the agency framework could be used to gain insight into AI risk, and
consider some general diﬃculties in applying PAL to AI risk.
Summary
PAL isn't in tension with Christiano's scenario because his scenario doesn't imply
massive agency rents; the big losses occur outside of the principal-agent
problem, and the agency literature can't assess the plausibility of these losses.
Extensions to PAL could potentially shed light on the size of agency rents in this
scenario, which are an important determinant of the future inﬂuentialness of AI
systems.
Mapped onto a PAL model, the Bostrom/Yudkowsky scenario is largely about the
principal's unawareness of the agent's catastrophic actions. Unawareness
models are rare in PAL probably because they usually aren't very insightful. This
lack of insightfulness also seems to prevent existing PAL models or possible
extensions from teaching us much about this scenario.
There are also a number of more general diﬃculties with using PAL to assess AI
risk, some more problematic than others.
PAL models rarely consider weak principals and more capable agents
PAL models are brittle
Agency rents are too narrow a measure
PAL models typically assume contract enforceability

PAL models typically assume AIs work for humans because they are paid
Overall, ﬁndings from PAL do not straightforwardly transfer to the AI risk
scenarios considered, so don't provide much evidence for or against these
scenarios. But new agency models could teach us about the levels of agency
rents which AI agents could extract.
PAL and Christiano's AI risk scenarios
Christiano's scenario has two parts:
Part I: machine learning will increase our ability to "get what we can
measure," which could cause a slow-rolling catastrophe. ("Going out with a
whimper.")
Part II: ML training, like competitive economies or natural ecosystems, can
give rise to "greedy" patterns that try to expand their own inﬂuence. Such
patterns can ultimately dominate the behavior of a system and cause
sudden breakdowns. ("Going out with a bang," an instance of optimization
daemons.)
Hanson argued that "Christiano instead fears that as AIs get more capable, the AIs will
gain so much more agency rents, and we will suﬀer so much more due to agency
failures, that we will actually become worse oﬀ as as result. And not just a bit worse
oﬀ; we apparently get apocalypse level worse oﬀ!"
PAL isn't in tension with Christiano's story
and isn't especially informative
We asked Christiano whether his scenario actually implies extremely high agency
rents. He doesn't think so:
On my view the problem is just that agency rents make AI systems collectively
better oﬀ. Humans were previously the sole superpower and so as a class we are
made worse oﬀ when we introduce a competitor, via the possibility of eventual
conﬂict with AI who have been greatly enriched via agency rents...humans are
better oﬀ in absolute terms unless conﬂict leaves them worse oﬀ (whether military
conﬂict or a race for scarce resources). Compare: a rising China makes Americans
better oﬀ in absolute terms. Also true, unless we consider the possibility of
conﬂict....[without conﬂict] humans are only worse oﬀ relative to AI (or to humans
who are able to leverage AI eﬀectively). The availability of AI still probably
increases humans' absolute wealth. This is a problem for humans because we care
about our fraction of inﬂuence over the future, not just our absolute level of
wealth over the short term.
Christiano's concern isn't that agency rents will skyrocket because of some distinctive
features of the human-AI agency relationship. Instead, "proxies" and "inﬂuence
seeking" are two speciﬁc ways AI interests will diverge from actual human goals. This
leads to typical levels of agency rents; PAL conﬁrms that due to diverging interests
and imperfect monitoring, AI agents could get some rents.[1]
The main loss occurs later in time and outside of the principal-agent context, due to
the fact that these rents eventually lead AIs to wield more total inﬂuence on the future

than humans.[2] This is bad because, even if humanity is richer overall, we humans
also "care about our fraction of inﬂuence over the future."[3] Compared to a world with
aligned AI systems, humanity is leaving value on the table, permanently if these
systems can't be rooted out. The biggest potential downside comes from inﬂuence-
seeking systems which Christiano believes could make humans worse oﬀ absolutely,
by engaging in violent conﬂict.
These later failures aren't examples of massive agency rents (as the term is used in
PAL) because failure is not expected to occur when the agent works on the task it was
delegated.[4] Rather, the inﬂuence-seeking systems become more inﬂuential via
typical agency rents, and then at some later point use these rents to inﬂuence the
future, possibly by entering into conﬂict with humans. PAL studies the size of agency
rents which can be extracted, but not what the agents decide to do with this wealth
and inﬂuence.
Overall, PAL is consistent with AI agents extracting some agency rents, which occurs
in both parts of Christiano's story (and we'll see next that putting more structure on
agency models could tell us more about the level of rent extraction). But it has
nothing to say about the plausibility of AI agents using their rents to exert inﬂuence
over the long term future (parts 1 and 2) or engage in conﬂict (part 2).[5]
Extending agency models seems promising
for understanding the level of agency rents in
Christiano's scenario
Christiano's scenario doesn't rely on something distinctive about the human-AI agency
relationship generating higher-than-usual agency rents.[6] But perhaps there is
something distinctive and rents will be atypical. In any case, the level of agency rents
seems like a crucial consideration: if we think AI's can extract little to no rents, we
probably shouldn't expect them to exert much inﬂuence over the future, because
agency rents are what make AI rich.[7] Agency models could help give us a better
understanding of the size of agency rents in Christiano's story, and for future AI
systems more generally.
The size of agency rents are determined by a number of factors, including the agent's
private information, the nature of the task, the noise in the principal's estimate of the
value produced by the agent, and the degree of competition. For instance, more
complex tasks tend to cause higher rents. From The (ir)resistible rise of agency rents:
In the presence of moral hazard, principals must leave rents to agents, to
incentivize appropriate actions. The more complex and opaque the task delegated
to the agent, the more diﬃcult it is to monitor his actions, the larger his rents.
If, as AI agents become more intelligent, monitoring gets increasingly diﬃcult, or tasks
get more complex, then we would expect agency rents to increase.
On the other hand, competitive pressures between AI agents might be greater (it's
easy to copy and run an AI; it's hard to increase the human workforce by transferring
human capital from one brain to another via teaching). This would limit rents:

The agents desire to capture rents, however, could be kept in check by market
forces and competition among [agents]. If each principal could run an auction with
several, otherwise identical, [agents], he could select the agent with the smallest
incentive problem, and hence the smallest rent.
Modelling the most relevant factors in an agency model seems like a tractable
research question (we discuss some potential diﬃculties below). Economists have only
just started thinking about AI, and there doesn't seem to be any work studying rent
extraction by AI agents.
PAL and AI risk from "accidents"
Ben Garﬁnkel has called the class of risks most associated with Bostrom and
Yudkowsky, risks from "accidents". Garﬁnkel characterises the general story in the
following terms:
First, the author imagines that a single AI system experiences a massive jump in
capabilities. Over some short period of time, a single system becomes much more
general or much more capable than any other system in existence, and in fact any
human in existence. Then given the system, researchers specify a goal for it. They
give it some input which is meant to communicate what behavior it should engage
in. The goal ends up being something quite simple, and the system goes oﬀ and
single-handedly pursues this very simple goal in a way that violates the full
nuances of what its designers intended." Importantly, "At the limit you might
worry that these safety failures could become so extreme that they could perhaps
derail civilization on the whole.
These catastrophic accidents constitute the main worry.
If the risk scenario is adequately represented by a principal-agent problem, agency
rents extracted by AI agents can be used to measure the cost of misalignment. This
time agency rents are a better measure, because failure is expected to occur when
the agent works on the task it was delegated.[8] The scenario implies very high
agency rents, with the principal being made much worse oﬀ because he delegated the
task to the agent.
As Garﬁnkel's nomenclature suggests, this story is about the designers being caught
by surprise, not anticipating the actions the AI would take. The Wikipedia synopsis of
Superintelligence also emphasizes that something unexpected occurs: "Solving the
control problem is surprisingly diﬃcult because most goals, when translated into
machine-implementable code, lead to unforeseen and undesirable consequences." In
other words, the principal is unaware of some speciﬁc catastrophically harmful actions
that the agent can take to achieve its goal.[9] This could be because they incorrectly
believe that the system doesn't have certain capabilities, or they don't foresee that
certain actions satisfy the agent's goal, as with perverse instantiation. Due to this, the
agent takes actions that greatly harm the principal, at great beneﬁt to herself.
PAL doesn't tell us much about AI risk from
accidents

Hanson's critique was aimed at Christiano's scenario, but it could equally apply to this
one. Is PAL at odds with this scenario?
As an AI agent becomes more intelligent, it's action set will expand, thinking of new
and sometimes unanticipated actions to achieve its goals. This may include
catastrophic actions that the principal is not aware of.[10] PAL can't tell us what these
actions will be, nor if the principal will be aware of them.[11]
Instead, the vast majority of principal-agent models assume that the principal
understands the environment perfectly, including perfect knowledge of the agent's
action set, while the premise of the accident scenario is that the principal is unaware
of a catastrophic action that the agent could take. Because the principal's
unawareness is central, these models assume, rather than show, that this source of AI
risk does not exist. They therefore don't tell us much about the plausibility of AI
accidents.
Microeconomist Daniel Garrett expressed this point nicely. We asked him about a
hypothetical example, slightly misremembered from Stuart Russell's book, concerning
an advanced climate control AI system.[12] He replied:
You can easily write down a model where the agent is rewarded according to some
outcome, and the principal isn't aware the outcome can be achieved by some
action the principal ﬁnds harmful. In your example, the outcome is the reduction
of Co2 emissions. If the principal thinks carbon sequestration is the only way to
achieve this, but doesn't think of another chemical reaction option which would
indirectly kill everyone, she could end up providing incentives to kill everyone. The
fact this conclusion is so immediate may explain why this kind of unawareness by
the principal is given little attention in the literature. The principal-agent
literature should not be understood as saying that these kinds of
incentives with perverse outcomes cannot happen. (our emphasis)
PAL models do typically have modest agency rents; they typically don't model the
principal as being unaware of actions with catastrophic consequences. But this is the
situation discussed by proponents of AI accident risk, so we can't infer much from PAL
except that such a situation has not been of much interest to economists.
Extending agency models doesn't seem
promising for understanding AI risk from
"accidents"
Most PAL models don't include the kind of unawareness needed to model the accident
scenario, but extensions of this sort are certainly possible. However, we suspect trying
to model AI risk in this way wouldn't be fruitful, for three main reasons.
Firstly, as Daniel Garrett suggests, we suspect the assumptions about the principal's
unawareness of the agents action set would imply the action chosen by the agent,
and its consequences for the principal, in a fairly direct and uninteresting way. There is
a (very) small sub-literature on unawareness in agency problems where one can ﬁnd
models like this. In one paper, a principal hires an agent to do a work task, but isn't
aware that the agent can manipulate "short-run working performance at the expense
of the employer's future beneﬁt." The agent "is better oﬀ if he is additionally aware

that he could manipulate the working performance," and "in the post-contractual
stage, [the principal] is hurt by the manipulating action of [the agent]." However, the
model didn't reveal anything unexpected about the situation, and the outcome was
directly determined by the action set and unawareness assumptions.
Secondly, the major source of the uncertainty surrounding accident risk concerns
whether the principal will be unaware of catastrophic agent actions. The agency
literature can't help us reduce this uncertainty as the unawareness is built into
models' assumptions. For instance, AI scientist Yann LeCun thinks that harmful actions
"are easily avoidable by simple terms in the objective". If LeCun implemented a
superintelligent AI in this way, agency models couldn't tell us whether he had
correctly covered all bases.
Lastly, the assumptions about the agent's action set would be highly speculative. We
don't know what actions superintelligent systems might take to pursue their goals.
Agency models must make assumptions about these actions, and we don't know what
these assumptions should be.
In short, the uncertainty pertains to the assumptions of the model, not the way the
assumptions translate into outcomes. PAL does not, and probably can not, provide
much evidence for or against this scenario.
General diﬃculties with using PAL to
assess AI risk
We've discussed the most relevant considerations regarding what PAL can tell us
about two speciﬁc visions of AI risk. We now discuss some diﬃculties relevant to a
broader set of possible scenarios (including those just examined). We list the
diﬃculties from most serious to least serious.
PAL models rarely consider weak principals
and more capable agents[13]
AI risk scenarios typically involve the AI being more intelligent than humans. The type
of problems that economists study usually don't have this feature, and there seem to
be very few models where the principal is weaker than the agent. Despite extensive
searching, including talking to multiple contract theorists, we were only able to ﬁnd
two papers with a principal who is more boundedly rational than the agent.[14] This is
perhaps not so surprising given that bounded-rationality models are relatively rare,
and when they do exist, they tend to bound both the principal and the agent in the
same way, or have the principal more capable. The latter is because such a set up is
more relevant to typical economic problems, e.g. "exploitative" contracting studies
the mistakes made by an individual (the agent) when interacting with a more capable
ﬁrm (the principal).
Microeconomist Takuro Yamashita agrees:
Most economic questions related to bounded rationality explored in the principal-
agent literature are appropriately modelled by a bounded agent. It's certainly

possible to bound the principal, but by and large this hasn't been done, just
because of the nature of the questions that have been asked.
A recent review of Behavioural Contract Theory also ﬁnds that such models are rare:
In almost all applications, researchers assume that the agent (she) behaves
according to one psychologically based model, while the principal (he) is fully
rational and has a classical goal (usually proﬁt maximization).
There doesn't seem to be, in Hanson's terms, a "large (mostly economic) literature on
agency failures" with an intelligence gap relevant to AI risk.
PAL models are brittle
PAL models don't model agency problems in general. They consider very speciﬁc
agency relationships, studied in highly structured environments. Conclusions can
depend very sensitively on the assumptions used; ﬁndings from one model don't
necessarily generalise to new situations. From the textbook Contract Theory:
The basic moral hazard problem has a fairly simple structure, yet general
conclusions have been diﬃcult to obtain...Very few general results can be
obtained about the form of optimal contracts. However, this limitation has not
prevented applications that use this paradigm from ﬂourishing...Typically,
applications have put more structure on the moral hazard problem under
consideration, thus enabling a sharper characterization of the optimal incentive
contract." (our emphasis)
Similar reasoning applies in adverse selection models where the outcome is very
sensitive to the mapping between eﬀort and outcomes. Given an arbitrary problem,
the optimal incentives can look like anything.
The agency problems studied by economists are typically quite diﬀerent to the
scenarios envisaged by AI risk proponents. Therefore, because of the brittleness of
PAL models, we shouldn't be too surprised if the imagined AI risk outcomes aren't
present in the existing literature. PAL, in its current form, might just not be of much
use. Further, we should not expect there to be any generic answer to the question
"How big are AI agency rents?": the answer will depend on the speciﬁc task the AI is
doing and a host of other details.
Agents rents are too narrow a measure
As we've seen, AI risk scenarios can include bad outcomes that aren't agency rents,
but that we nevertheless care about. When applying PAL to AI risk, care must be taken
to distinguish between rents and other bad outcomes, and we cannot assume that a
bad outcome necessarily means high rents.
PAL models typically assume contract
enforceability

Stuart Armstrong argued that Hanson's critique doesn't work because PAL assumes
contract enforceability, and with advanced AI, institutions might not be up to the task.
[15] Indeed, contract enforceability is assumed in most of PAL, so it's an important
consideration regarding their applicability to AI scenarios more broadly.[16]
The assumption isn't plausible in pessimistic scenarios where human principals and
institutions are insuﬃciently powerful to punish the AI agent, e.g. due to very fast
take-oﬀ. But it is plausible for when AIs are similarly smart to humans, and in
scenarios where powerful AIs are used to enforce contracts. Furthermore, if we cannot
enforce contracts with AIs then people will promptly realise and stop using AIs; so we
should expect contracts to be enforceable conditional upon AIs being used.[17]
There is a smaller sub-literature on self-enforcing contracts (seminal paper). Here
contracts can be self-enforced because both parties have an interest in interacting
repeatedly. We think these probably won't be helpful for understanding situations
without contract enforceability, because in worlds where contracts aren't enforceable
because of advanced AI, contracts likely won't be self-enforcing either. If AIs are
powerful enough that institutions like the police and military can't constrain them, it
seems unlikely that they'd have much to gain from repeated cooperative interactions
with human principals. Why not make a copy of themselves to do the task, coerce
humans into doing it, or cooperate with other advanced AIs?
PAL models typically assume AIs work for
humans because they are paid
In reality AIs will probably not receive a wage, and instead work for humans because
that is their default behaviour. We think changing this would probably not make a big
diﬀerence to agency models, because the wage could be substituted for other
resources the AI cares about. For instance, AI needs compute to run. If we substitute
"wage" for "compute", the agency rents that the agent extracts is additional compute
that it can use for its own purposes.
There is a sub-literature on Optimal Delegation that does away with wages. This
literature focuses on the best way to restrict the agents action set. For AI agents, this
is equivalent to AI boxing. We don't think this literature will be helpful; PAL doesn't
study how realistic it is to box AI successfully, it just assumes it's technologically
possible. It therefore isn't informative about whether AI boxing will work.
Conclusion
There are similarities between the AI alignment and principal-agent problems,
suggesting that PAL could teach us about AI risk. However, the situations economists
have studied are very diﬀerent to those discussed by proponents of AI risk, meaning
that ﬁndings from PAL don't transfer easily to this context. There are a few main
issues. The principal-agent setup is only a part of AI risk scenarios, making agency
rents too narrow a metric. PAL models rarely consider agents more intelligent than
their principals and the models are very brittle. And the lack of insight from PAL
unawareness models severely restricts their usefulness for understanding the accident
risk scenario.

Nevertheless, extensions to PAL might still be useful. Agency rents are what might
allow AI agents to accumulate wealth and inﬂuence, and agency models are the best
way we have to learn about the size of these rents. These ﬁndings should inform a
wide range of future scenarios, perhaps barring extreme ones like Bostrom/Yudkowsky.
[18]
1. Thanks to Wei Dai for pointing out a previous inaccuracy ↩ 
2. Agency rents are about e.g. working vs shirking. If the agent uses the money
she earned to buy a gun and later shoot the principal, clearly this is very bad for
her, but it's not captured by agency rents. ↩ 
3. It's not totally clear to us why we should care about our fraction of inﬂuence
over the future, rather than the total inﬂuence. Probably because the fraction of
inﬂuence aﬀects the total inﬂuence, inﬂuence being zero-sum and resources
ﬁnite. ↩ 
4. It wasn't clear to us from the original post, at least in Part 1 of the story with no
conﬂict, that humans are better oﬀ in absolute terms. For instance, wording like
"over time those proxies will come apart" and "People really will be getting
richer for a while" seemed to suggest that things are expected to worsen. Given
this, Hanson's interpretation (that Christiano's story implied massive agency
rents) seems reasonable without further clariﬁcation. Ben Garﬁnkel mentioned
an outside-view measure which he thought undermined the plausibility of Part 1:
since the industrial revolution we seem to have been using more and more
proxies, which are optimized for more and more heavily, but things have been
getting better and better. So he also seems to have understood the scenario to
mean things get worse in absolute terms. ↩ 
5. Clarifying what it means for an AI system to earn and use rents also seems
important, helping us make sure that the abstraction maps cleanly onto the
practical scenarios we are envisaging. Relatedly, what traits would an AI system
need to have for it to make sense to think of the system as "accumulating and
using rents"? Rents can be cashed out in inﬂuence of many diﬀerent kinds — a
human worker might get higher wage, or more free time — and what ends up
occuring will depend on the capabilities of the AI systems. Concretely, money
can be saved in a bank account, people can be inﬂuenced, or computer
hardware can be bought and run. One example of an obvious capability
constraint for AI: some AI systems will be "switched oﬀ" after they are run,
limiting their ability to transfer rents through time. As AI agents will (initially) be
owned by humans, historical instances of slaves earning rents seem worth
looking into. ↩ 
6. Although his scenario is more plausible if a smarter agent extracts more agency
rents. ↩ 
7. Hanson and Christiano agree on this point. Hanson: "Just as most wages that
slaves earned above subsistence went to slave owners, most of the wealth
generated by AI could go to the capital owners, i.e. their slave owners. Agency
rents are the diﬀerence above that minimum amount." Christiano: "Agency rents
are what makes the AI rich. It's not that computers would "become rich" if they
were superhuman, and they just aren't rich yet because they aren't smart
enough. On the current trajectory computers just won't get rich." ↩ 

8. One limitation is that rents are the cost to the principal, whereas the accident
scenario has costs for all humanity. This distinction isn't especially important
because in the accident scenario the outcome for the principal is catastrophic
(i.e. extremely high agency rents), and this is what is potentially in tension with
PAL. Nonetheless, we should keep in mind that the total costs of this scenario
are not limited to agency rents, just as in Christiano's scenario. ↩ 
9. Perhaps a more realistic framing: the principal is aware that there's some
probability that the agent will take an unanticipated catastrophic action, without
knowing what that action might be. Under competitive pressures, maybe in a
time of war, it could be beneﬁcial for the principal to delegate (in expectation)
despite signiﬁcant risk, while humanity is made worse oﬀ (in expectation). This,
of course, would be modelled quite diﬀerently to the accident AI risk we consider
in the text, and we suspect that economic models would conﬁrm that principals
would take the risk in suﬃciently competitive scenarios. These models would
focus on negative externalities of risky AI development, something more
naturally studied in domains like public economics rather than with agency
theory. In any case, we focus here on the more traditional AI risk framing along
the lines of "you think you have the AI under control, but beware, you could be
wrong". ↩ 
10. AI accident risk will be large when the AI agent thinks of new actions that i) harm
the principal ii) further the agent's goals iii) the principal hasn't anticipated. ↩ 
11. This is because claims about the actions available to the agent and the
principal's awareness are part of PAL models' assumptions. We discuss this more
below. ↩ 
12. The correct example: "If you prefer solving environmental problems, you might
ask the machine to counter the rapid acidiﬁcation of the oceans that results
from higher carbon dioxide levels. The machine develops a new catalyst that
facilitates an incredibly rapid chemical reaction between ocean and atmosphere
and restores the oceans' pH levels. Unfortunately, a quarter of the oxygen in the
atmosphere is used up in the process, leaving us [humans] to asphyxiate slowly
and painfully." ↩ 
13. I.e. the principal's rationality is bounded to a greater extent than the agent's ↩ 
14. In the model in "Moral Hazard With Unawareness" either the principal or the
agent's rationality can be bounded ↩ 
15. As argued above, we don't think contract enforceability is the main reason
Hanson's critique of Christiano fails; agency rents are just not unusually high in
his scenario. ↩ 
16. From Contract Theory: "The benchmark contracting situation that we shall
consider in this book is one between two parties who operate in a market
economy with a well-functioning legal system. Under such a system, any
contract the parties decide to write will be enforced perfectly by a court,
provided, of course, that it does not contravene any existing laws." ↩ 
17. Thanks to Ben Garﬁnkel for pointing this out. ↩ 
18. Robin Hanson pointed out to us that when thinking about strange future
scenarios, we should try to think about similar strange scenarios that we have

seen in the past (we are very sympathetic to this, despite our somewhat
skeptical position regarding PAL). With this in mind, another ﬁeld which seems
worth looking into is Security, especially military security. National leaders have
been assassinated by their guards; kings have been killed by their protectors.
These seem like a closer analogue to many AI risk scenarios than the typical PAL
setup. It seems important to understand what the major risk factors are in these
situations, how people have guarded against catastrophic failures, and how this
translates to cases of catastrophic AI risk. ↩ 

Demons in Imperfect Search
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
One day, a gradient descent algorithm ball was happily rolling down a high-
dimensional surface hill. All it wanted was to roll as far down as possible.
Unbeknownst to the ball, just oﬀ to the side was a steep drop-oﬀ - but there was a
small bump between the ball and the drop-oﬀ. No matter; there was enough random
noise on the ball that it would jump the bump sooner or later.
But the ball was headed into unfriendly territory.
As the ball rolled along, the bump became taller. The farther it rolled, the taller the
bump grew, until no hope remained of ﬁnding the big drop anytime before the stars
burned out. Then the road began to narrow, and to twist and turn, and to become
ﬂatter. Soon the ball rolled down only the slightest slope, with tall walls on both sides
constraining its path. The ball had entered the territory of a demon, and now that
demon was steering the ball according to its own nefarious ends.
This wasn't the ﬁrst time the ball had entered the territory of a demon. In early times,
the demons had just been bumps which happened to grow alongside the ball's path,
for a time - chance events, nothing more. But every now and then, two bumps in close
proximity would push the ball in diﬀerent directions. The ball would roll on, oblivious,
and end up going in one direction or the other. Whichever bump had "won" would
continue to steer the ball's trajectory - and so a selection process occurred. The ball
tended to roll alongside bumps which more eﬀectively controlled its trajectory -
bumps which were taller, bumps which steered it away from competing bumps. And
so, over time, bumps gave way to barriers, and barriers gave way to demons - twisty
paths with high walls to keep the ball contained and avoid competing walls, slowing
the ball's descent to a crawl, conserving its potential energy in case a sharp drop were
needed to avoid a competitor's wall.
The ball's downhill progress slowed and slowed. Even though the rich, high-
dimensional space was ﬁlled with lower points to explore, the highly eﬀective demons
had built tall walls to carefully contain the ball within their own territory, drawing out
its travels indeﬁnitely.
The Pattern
This tale visualizes a pattern:
There is some optimization process - in this case, some variant of gradient
descent.
The optimizing search is imperfect: gradient descent only looks at local
information, so it doesn't "know" if there's a steep drop beyond a nearby bump.
Exploiting the imperfect search mechanism: in this case, the steep drop is
hidden by raising high walls.
Demon: in a rich enough search space, a feedback loop can appear, inducing
more-and-more-perfect exploitation of the imperfect search mechanism. A whole
new optimization process appears, with goals quite diﬀerent from the original.

Does this actually happen? Let's look at a few real-world examples...
Metabolic reactions
Optimization process: free energy minimization in a chemical system. Search
operates by random small changes to the system state, then keeping changes
with lower free energy (very roughly speaking).
Search is imperfect: the system does not immediately jump to the global
maximum. It's searching locally, based on random samples.
Exploiting the imperfect search mechanism: there's often a free energy barrier
between low-free-energy states. Biological systems manipulate the height of the
barriers, raising or lowering the activation energies required to cross them, in
order to steer the local-free-energy-minimization process toward some states
and away from others.
Demon: in primordial times, some chemicals happened to raise/lower barriers to
steer the process in such a way that it made more copies of the chemicals. This
kicked oﬀ an unstable feedback loop, producing more and more such chemicals.
The rest is natural history.
Greedy genes
Optimization process: evolution, speciﬁcally selection pressure at the level of an
organism. Search operates by making random small changes to the genome,
then seeing how much the organism reproduces.
Search is imperfect: the system does not immediately jump to the global
optimum. It's searching locally, based on random samples, with the samples
themselves chosen by a physical mechanism.
Exploiting the imperfect search mechanism: some genes can bias the random
sampling, making some random changes more or less likely than others. For
instance, in sexual organisms, the choice of which variant of a gene to retain is
made at random during fertilization - but some gene variants can bias that
choice in favor of themselves.
Demon: sometimes, a gene can bias the random sampling to make itself more
likely to be retained. This can kick oﬀ an unstable feedback loop, e.g. a gene
which biases toward male children can result in a more and more male-skewed
population until the species dies out.
Managers
Optimization process: proﬁt maximization. Search operates by people in the
company suggesting and trying things, and seeing what makes/saves money.
Search is imperfect: the company does not immediately jump to perfect proﬁt-
maximizing behavior. Its actions are chosen based on what sounds appealing to
managers, which in turn depends on the managers' own knowledge, incentives,
and personal tics.
Exploiting the imperfect search mechanism: actions which would actually
maximize proﬁt are not necessarily actions which look good on paper, or which
reward the managers deciding whether to take them. Managers will take actions
which make them look good, rather than actions which maximize proﬁt.
Demon: some actions which make managers look good will further decouple
looking-good from proﬁt-maximization - e.g. changing evaluation mechanisms.
This kicks oﬀ an unstable feedback loop, eventually decoupling action-choice
from proﬁt-maximization.

I'd be interested to hear other examples people can think of.
The big question is: when does this happen? There are enough real-world examples to
show that it does happen, and not just in one narrow case. But it also seems like it
requires a fairly rich search space with some structure to it in order to kick oﬀ a full
demonic feedback loop. Can that instability be quantiﬁed? What are the relevant
parameters?

Tessellating Hills: a toy model for
demons in imperfect search
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
If you haven't already, take a look at this post by johnswentworth to understand what
this is all about: https://www.lesswrong.com/posts/KnPN7ett8RszE79PH/demons-in-
imperfect-search
The short version is that while systems that use perfect search, such as AIXI, have
many safety problems, a whole new set of problems arises when we start creating
systems that are not perfect searchers. Patterns can form that exploit the imperfect
nature of the search function to perpetuate themselves. johnswentworth refers to such
patterns as "demons".
After reading that post I decided to see if I could observe demon formation in a simple
model: gradient descent on a not-too-complicated mathematical function. It turns out
that even in this very simplistic case, demon formation can happen. Hopefully this post
will give people an example of demon formation where the mechanism is simple and
easy to visualize.
Model
The function we try to minimize using gradient descent is called the loss function. Here
it is:
L(→
x ) = −x0 + ϵ
n
∑
j=1
xj ⋅splotchj(→
x )
Let me explain what some of the parts of this loss mean. Each function splotchj(→
x ) is
periodic with period 2π in every component of →
x . I decided in this case to make my
splotch functions out of a few randomly chosen sine waves added together.
ϵ is chosen to be a small number so in any local region, ϵ ∑
n
j=1 xj ⋅splotchj(→
x ) will look
approximately periodic: A bunch of hills repeating over and over again with period 2π
across the landscape. But over large enough distances, the relative weightings of
various splotches do change. Travel a distance of 20π in the x7 direction, and splotch7
will be a larger component of the repeating pattern than it was before. This allows for
selection eﬀects.

The −x0 term means that the vector →
x  mainly wants to increase its x0 component. But
the splotch functions can also direct its motion. A splotch function might have a kind of
ridge that directs some of the x0 motion into other components. If splotch7 tends to
direct motion in such a way that x7, increases, then it will be selected for, becoming
stronger and stronger as time goes on.
Results
I used ordinary gradient descent, with a constant step size, and with a bit of random
noise added in. Figure 1 shows the value of x0 as a function of time, while ﬁgure 2
shows the values of x1,x2,...x16 as a function of time.
Fig 1:
Fig 2:

There are three phases to the evolution: In the ﬁrst, x0 increases steadily, and the
other coordinates wander around more or less randomly. In the second phase, a self-
reinforcing combination of splotches (a "demon") takes hold and ampliﬁes itself
drastically, feeding oﬀ the large x0 gradient. Finally, this demon becomes so strong
that the search gets stuck in a local valley and further progress stops. The ﬁrst phase is
more or less from 0 to 2500 steps. The second phase is between 2500 steps and 4000
steps, though slowing down after 3500. The ﬁnal phase starts at 4000 steps, and likely
continues indeﬁnitely.
Now that I have seen demons arise in such a simple situation, it makes me wonder how
commonly the same thing happens in the training of deep neural networks. Anyways,
hopefully this is a useful model for people who want to understand the mechanisms
behind the whole "demons in imperfect search" thing more clearly. It deﬁnitely helped
me, at least.
Update: The code is now up here: https://github.com/DaemonicSigil/tessellating-hills

How to Frame Negative Feedback as
Forward-Facing Guidance
Your employee Fred always talks too much during your weekly staﬀ meetings. It's
been an ongoing issue. Everyone on your team is annoyed, and so are you. At this
point, you have no choice but to give Fred some... negative feedback.
You sit down at your computer and start drafting what you're going to say to Fred:
Listen Fred, we think you're talking too much in our staﬀ meetings and it's
lowering the quality of the discussion. Can you try to talk a little less and let other
people talk more?
But wait... let's be tactful here. Your goal is to optimize how you criticize Fred to
maximize expected positive behavioral change. In that sense, your rough draft isn't
super tactful yet.
I challenge you to try this now as a 5-minute exercise: What communication technique
would you apply here? What exact words would you say to Fred?
...
...
...
The "shit sandwich" technique comes to mind, i.e. sandwiching your negative-
feedback turd inside two slices of positive feedback. But let's use a much better
technique: framing negative feedback as guidance. Here's how I'd do that:
Fred,
I think you have some room for improvement in the way you present ideas at our
staﬀ meetings. Sometimes I notice that you're making a valid point, which is a
great contribution, but it doesn't get fully appreciated by the rest of the team. I
want to share some techniques with you that I've seen our senior staﬀ use to be
perceived well in meetings.
For example, today when you brought up how our ﬂux capacitors don't last long
enough, no one really engaged with that topic. And that was probably frustrating
for you, right? If you could tweak your communication style to easily get everyone
to appreciate your ideas, that would be great for you and the team.
My advice on how to do this is basically to limit the number of points you bring
into each staﬀ meeting. And whenever you're going to say your point, try to ﬁrst
make other people feel like you've heard their point. Right now it feels like you're
making a lot of diﬀerent points and you're not acknowledging other people's
points enough, so your points aren't getting fully appreciated. Do you agree with
my perception?
A lot of what I've done here is explained in other books and articles: I follow the CORE
structure (context, observation, result, expectation) explained here. The "observation"

component of CORE is a speciﬁcity power. Framing negative feedback as guidance is
Step 3 of this blog post.
There's just one part of my technique that I haven't seen explained anywhere else:
How to frame negative feedback as guidance. The aforementioned blog post only says
to "engage in a way that shows vulnerability and understanding" in order to show that
"the feedback is meant to help, not harm". Ok, but how do you operationalize that into
a deterministic procedure for writing a paragraph of f̶e̶e̶d̶b̶a̶c̶k̶ guidance?
The Forward-Facing Frame
I use a simple but powerful technique to reduce the question of "how to frame
feedback as guidance" into a rule for crafting sentences of language. I call it the
forward-facing frame technique.
Imagine you're in a room with someone who smells terrible. You could tell them
directly to their face, "P U, you stink!" This feedback won't come across as "guidance"
because it only describes their current state of smelling bad, not the target state of
smelling good. I would call this a backward-facing framing of the situation.
In contrast, here's what a forward-facing frame would look like: You walk over to them,
put your arm around their shoulder, and then as both of you gaze out the bay window
unto the ﬂower garden, you say:
I know you have the potential to smell a lot better than you do right now. You
could eventually smell as pleasant as those fresh ﬂowers out in the garden. But
even just smelling like the rest of this house would be a big improvement. I
generally expect everyone to smell at least as good as the rest of the house. Do
you think that's a fair assessment and a worthwhile goal for you?
When you're thinking about how to tell Fred that he's talking too much in staﬀ
meetings, start by asking yourself what it would look like if Fred were exceptionally
awesome at that instead of deﬁcient. This helps you visualize a complete forward-to-
backward axis. Then you can frame your message to Fred in terms of moving forward
on the spectrum toward awesomeness.
I didn't just imagine Fred not talking too much, I imagined Fred employing a more
developed toolkit of communication skills that would get his points across in a way
that his team members appreciate. Then I made each of my sentences "face forward"
toward that desirable state. My feedback was thereby framed as guidance.
So next time you're giving someone negative feedback, why not frame it as forward-
facing guidance? Just scan your ﬁrst draft, look for anything backward-facing, and turn
it around to face forward.

Theory and Data as Constraints
There's a widely-known legend about statistician Abraham Wald's work on planes
during WWII (the veracity of the legend is examined here). As the story goes, the
military collected data on planes coming back from missions, marking the location of
any bullet holes. They soon had statistics showing how many had been hit on the
engine, the fuel system, etc. Based on this data, commanders wanted to add extra
armor to reinforce the areas which were most often hit.
Wald, however, suggested adding extra armor to the places which were least often hit.
His reasoning: planes hit in those areas were the planes which didn't come back.
The moral of the tale: useful insights do not come from data alone. Background
knowledge, interpretation and model-building - all of which we'll bundle into the word
"theory" - are necessary elements as well. In the framework of this sequence: theory
and data are both constraints on the production of useful insights. Immediate
question: how taut are each of those constraints? What's the limiting factor?
Let's think about the tautness of each constraint in the Wald legend. How expensive
was the data, and how expensive was the theory? In this case, the data was
presumably far more expensive: it required people on the ground examining hundreds
of returning airplanes per mission, collecting all of that hand-written data, adding it all
up by hand, and relaying it via telephone or radio or mail to the Statistical Research
Group in New York City (where Wald worked). The theory, on the other hand, probably
took Wald a day at most. Of course there would also be some overhead on both sides -
Wald needed to write up the theory in a manner convincing to military commanders,
and military commanders needed to set up the whole data-collection project - but
there again, the data would have been far more expensive than the theory.
Conclusion: the data constraint was much more taut than the theory constraint.
Theory was abundant relative to the scarcity of data.
... at least during WWII.
The Internet
More recently, you may have heard about a fancy new technology called "the
internet" which makes it really, really cheap for people to share data. Given such a
technology shift, we'd expect the data constraint to become slack much more often,
leaving the theory constraint taut.
What does that look like? In academia, biology is a great example. Biologists today
have massive piles of data - genomics, transcriptomics, proteomics, metabolomics,
lots of omics, on a wide variety of organisms, tissues, cell types, etc. Yet our ability to
turn all that data into engineered organisms or cures for disease remains
underwhelming. I would bet that all the data needed in principle to, say, ﬁnd a cure for
Alzheimers is already available online - if only we knew how to eﬀectively leverage it.
We have all this data, but people don't really understand how to use it. That's what it
looks like when the data constraint is slack and the theory constraint is taut.

Of course, biology isn't the only ﬁeld which looks like this. Economics is another - we
have huge piles of data on prices, consumption, trade, taxes, and so forth, yet we
have limited ability to turn it all into useful economic insight. We don't even know
what useful things to do with structured data like databases of prices and
consumption, let alone unstructured data like the whole database of US federal
regulations. Economists run studies on a small handful of datasets, or on very coarse
aggregates, or sometimes just throw everything into a giant neural network to see
what happens. We don't yet have quantitative, gearsy models capable of absorbing
and using a wide variety of data all at once.
Then there's the entire ﬁeld of data science. A decade after the internet took oﬀ, data
science appeared more-or-less spontaneously as a response to companies with giant
piles of data and no idea what to do with it all. That's the sort of thing you expect to
happen when a technology shift suddenly relaxes a previously-taut constraint: a
complementary constraint becomes taut, and an industry appears to service it.
There's a reason the Wald legend is popular as an analogy for data science in general:
it's a perfect example of the value-add data scientists provide, the kind of "theory"
which companies need in order to make their data useful.
Low-Hanging Fruit?
Our society has not had much time to adjust to the internet. We now live in a world
where theory is scarce relative to data in far more places, but the world is still
adjusting to this new reality. Where might we expect low-hanging fruit? What other
constraints are likely to become taut/slack?
One general area to look for low-hanging fruit: comprehensive overviews of possibly-
useful topics. Track down a majority of all the physical capital assets of public
companies, or skim titles/abstracts from a few years of archives of scientiﬁc journals,
or read the wikipedia pages on every country on Earth. This sort of exercise would
have been very expensive before the internet, society hasn't had a lot of time to
experiment with it, so there's likely to be low-hanging fruit in the area. Data is now
very cheap, so consume a lot of it and see what happens.
Another angle would be to practice building gears-level models - especially with an
eye toward integrating a wide variety of data sources into the model-building process.
(See here and here for why we want gears.) Paper-Reading for Gears has some
general tips on leveraging scientiﬁc papers toward this end. In terms of relevant
general background knowledge, Pearl's Book of Why is probably a useful resource for
building/testing (one type of) gearsy model statistically. But the most important tool
here is the general habit of thinking in gears and asking the sort of questions which
yield gears; I'd be interested to hear peoples' suggestions for ways to learn those
habits.
Finally, there's one key constraint which I expect to become much more taut as theory
becomes a more limiting factor: the inability to outsource expertise. In the Wald
legend, military commanders wanted to add more armor to the places where returning
planes had been shot most often. This seems so intuitively obvious that there isn't
any reason to consult an expert about it. The commanders didn't have the
background knowledge to realize they were making an important mistake, so they
didn't have any way to know that they needed an expert. Fortunately, in Wald's case,
consulting the expert was relatively cheap, so there wasn't much reason not to do it.
But what happens when consulting the expert is expensive? People won't consult an

expert unless there's an obvious need for it - which means people will repeatedly be
hit in the face by unknown unknowns.
This problem ampliﬁes the value of comprehensive overviews and gears-level
modelling skills. It's not just that theory is scarce, it's that theory is scarce and you
can't reliably outsource it. Money cannot reliably buy good theory, unless you already
have some ability to recognize good theory. How can we recognize good theory,
across a wide variety of applications, especially when there isn't a clear objective
metric for success? First, by studying how to build good theory in general - i.e. gears-
level models. Second, by absorbing lots of background knowledge across lots of
diﬀerent areas, so we reduce unknown unknowns and gain more possible metrics by
which to recognize experts.

A point of clariﬁcation on infohazard
terminology
This is a linkpost for https://eukaryotewritesblog.com/2020/02/02/a-point-of-
clariﬁcation-on-infohazard-terminology/
TL;DR: "Infohazard" means any kind of information that could be harmful in some
fashion. Let's use "cognitohazard" to describe information that could speciﬁcally harm
the person who knows it.
Some people in my circle like to talk about the idea of information hazards or
infohazards, which are dangerous information. This isn't a ﬁctional concept - Nick
Bostrom characterizes a number of diﬀerent types of infohazards in his 2011 paper
that introduces the term (PDF available here). Lots of kinds of information can be
dangerous or harmful in some fashion - detailed instructions for making a nuclear
bomb. A signal or hint that a person is a member of a marginalized group. An extremist
ideology. A spoiler for your favorite TV show. (Listen, an infohazard is a kind of hazard,
not a measure of intensity. A papercut is still a kind of injury!)
I've been in places where "infohazard" is used in the Bostromian sense casually - to
talk about, say, dual-use research of concern in the biological sciences, and describe
the speciﬁc dangers that might come from publishing procedures of results.
I've also been in more esoteric conversations where people use the word "infohazard"
to talk about a speciﬁc kind of Bostromian information hazard: information that may
harm the person who knows it. This is a stranger concept, but there are still lots of
apparent examples - a catchy earworm. "You just lost the game." More seriously, an
easy method of committing suicide for a suicidal person. A prototypical ﬁctional
example is the "basilisk" fractal from David Langford's 1988 short story BLIT, which
kills you if you see it.
This is a subset of the original deﬁnition because it is harmful information, but it's
expected to harm the person who knows it in particular. For instance, detailed
schematics for a nuclear weapon aren't really expected to bring harm to a potential
weaponeer - the danger is that the weaponeer will use them to harm others. But fully
internalizing the information that Amazon will deliver you a 5-pound bag of Swedish
Fish whenever you want is speciﬁcally a danger to you. (...Me.)
This disparate use of terms is confusing. I think Bostrom and his intellectual kith get
the broader deﬁnition of "infohazard", since they coined the word and are actually
using it professionally.*
I propose we call the second thing - information that harms the knower - a
cognitohazard.

Pictured: Instantiated example of a cognitohazard. Something something red herrings.
This term is shamelessly borrowed from the SCP Foundation, which uses it the same
way in ﬁction. I ﬁgure the usage can't make the concept sound any more weird and sci-
ﬁ than it already does.
(Cognitohazards don't have to be hazardous to everybody. Someone who hates
Swedish Fish is not going to spend all their money buying bags of Swedish Fish oﬀ of
Amazon and diving into them like Scrooge McDuck. For someone who loves Swedish
Fish - well, no comment. I'd call this "a potential cognitohazard" if you were to yell it
into a crowd with unknown opinions on Swedish Fish.)
Anyways, hope that clears things up.
*For a published track record of this usage, see: an academic paper from Future of
Humanity Institute and Center for Health Security staﬀ, another piece by Bostrom, an
opinion piece by esteemed synthetic biologist Kevin Esvelt, a piece on synthetic biology
by FHI researcher Cassidy Nelson, a piece by Phil Torres.
(UPDATE: The version I initially published proposed the term "memetic hazard" rather
than "cognitohazard." Commentor MichaelA kindly pointed out that "memetic hazard"
already meant a diﬀerent concept that better suited that name. Since I had only just
put out the post, I decided to quickly backpeddle and switch out the word for another
one with similar provinence. I hate having to do this, but it sure beats not doing it.
Sorry for any confusion, and thank you, MichaelA!)

Draft: Models of Risks of Delivery
Under Coronavirus
I've never considered prophylactically quarantining myself before, but now that I'm
considering it I ﬁnd it contains many more choices than I would have imagined. Let's
take my need to eat- I could go to a supermarket, but that's full of people. I could get
delivery, but that still has a human touch. I could eat my stores, but then I won't have
them later. This makes "when do I stop ordering delivery?" an important question. To
attempt a more informed answer, I made a guesstimate model. As of writing this (2/27)
the numbers are completely made up: I just wanted to get comments on the underlying
model. I'm working to ﬁll in the variables with actual answers. If you want to follow
along you can do so at my Roam page. I am exceedingly grateful for comments on
either the abstract model or information that could help me ﬁll in variables.
Here are some general factors going into my thinking:
1. COVID-19 seems to have a long dormant period during which people are
contagious but not symptomatic
2. Some additional portion of people have only mild symptoms
3. The economics of pink-collar work are such that a lot of people will go to work
until they are on death's door.
4. 1+2+3 = if the virus is prevalent in the population, there will be a lot of
contagious people handling stuﬀ I order.
5. The American government's monitoring provides, at best, an extremely lagging
indicator of prevalence, and is at worst made up.
 
Here are images of the model and Roam page now, for posterity

Note that this shows food delivery as less risky than package delivery, which is clearly
wrong.
 

 

Response to Oren Etzioni's "How to
know if artiﬁcial intelligence is about
to destroy civilization"
Oren Etzioni recently wrote a popular article titled "How to know if artiﬁcial
intelligence is about to destroy civilization." I think it's a good idea to write publicly
available responses to articles like this, in case interested people are googling around
looking for such. If I get lucky, perhaps Oren himself will come across this! Then we
could have a proper discussion and we'd probably both learn things.
For most readers of LW, it's probably not worth reading further, since you probably
already agree with what I'm saying.
Here is the full text of the article, interspersed with my comments:
Could we wake up one morning dumbstruck that a super-powerful AI has emerged,
with disastrous consequences? Books like Superintelligence by Nick Bostrom and
Life 3.0 by Max Tegmark, as well as more recent articles, argue that malevolent
superintelligence is an existential risk for humanity.
But one can speculate endlessly. It's better to ask a more concrete, empirical
question: What would alert us that superintelligence is indeed around the corner?
It is unfair to claim that Bostrom is speculating whereas your question is concrete and
empirical. For one thing, Bostrom and others you denigrate have already asked the
same question you now raise; there is not a consensus as to the answer yet, but
plenty of arguments have been made. See e.g. Yudkowsky "There's No Fire Alarm for
Artiﬁcial General Intelligence" More importantly though, how is your question any less
speculative than the others? You too are making guesses about what future
technologies will come when and in what order, and you too have zero historical data
to draw from. Perhaps you could make arguments based on analogy to other
technologies -- e.g. as Grace and Christiano have argued about "slow takeoﬀ" and
"likelihood of discontinuous progress" but funnily enough (looking ahead at the rest of
the article) you don't even do that; you just rely on your intuition!
We might call such harbingers canaries in the coal mines of AI. If an artiﬁcial-
intelligence program develops a fundamental new capability, that's the equivalent
of a canary collapsing: an early warning of AI breakthroughs on the horizon.
Could the famous Turing test serve as a canary? The test, invented by Alan Turing
in 1950, posits that human-level AI will be achieved when a person can't
distinguish conversing with a human from conversing with a computer. It's an
important test, but it's not a canary; it is, rather, the sign that human-level AI has
already arrived. Many computer scientists believe that if that moment does arrive,
superintelligence will quickly follow. We need more intermediate milestones.
If you think "many computer scientists believe X about AI" is a good reason to take X
seriously, then... well, you'll be interested to know that many computer scientists

believe superintelligent AI will come in the next 15 years or so, and also many
computer scientists believe AI is an existential risk.
Is AI's performance in games such as Go, poker or Quake 3, a canary? It is not.
The bulk of so-called artiﬁcial intelligence in these games is actually human work
to frame the problem and design the solution. AlphaGo's victory over human Go
champions was a credit to the talented human team at DeepMind, not to the
machine, which merely ran the algorithm the people had created. This explains
why it takes years of hard work to translate AI success from one narrow challenge
to the next. Even AlphaZero, which learned to play world-class Go in a few hours,
hasn't substantially broadened its scope since 2017. Methods such as deep
learning are general, but their successful application to a particular task requires
extensive human intervention.
I agree with you here; rapid progress in AI gaming is evidence, but not nearly
conclusive evidence, that human-level AGI is coming soon.
More broadly, machine learning is at the core of AI's successes over the last
decade or so. Yet the term "machine learning" is a misnomer. Machines possess
only a narrow sliver of humans' rich and versatile learning abilities. To say that
machines learn is like saying that baby penguins know how to ﬁsh. The reality is,
adult penguins swim, capture ﬁsh, digest it, regurgitate into their beaks, and place
morsels into their children's mouths. AI is likewise being spoon-fed by human
scientists and engineers.
What about GPT-2? There was relatively little spoon-feeding involved there; they just
gave it pretty much the whole Internet and told it to start reading. For comparison, I
received much more spoon-feeding myself during my education, yet I managed to
reach human-level intelligence pretty quickly!
Anyhow, I agree that machines can't currently learn as well as humans can. But even
you must admit that over the past few decades there has been steady progress in that
direction. And it seems that progress is and will continue.
In contrast to machine learning, human learning maps a personal motivation ("I
want to drive to be independent of my parents") to a strategic learning plan
("Take driver's ed and practice on weekends"). A human formulates speciﬁc
learning targets ("Get better at parallel parking"), collects and labels data ("The
angle was wrong this time"), and incorporates external feedback and background
knowledge ("The instructor explained how to use the side mirrors"). Humans
identify, frame, and shape learning problems. None of these human abilities is
even remotely replicated by machines. Machines can perform superhuman
statistical calculations, but that is merely the last mile of learning.
I think I agree with you here, that we don't really have an impressive example of an AI
organically identifying, framing, and shaping learning problems. Moreover I agree that
this probably won't happen in the next ten years, based on my guesses about current
rates of progress. And I agree that this would be a canary -- once we have impressive
AI doing this organically, then human-level AGI is probably very close. But shouldn't
we start preparing for human-level AGI before it is probably very close?
The automatic formulation of learning problems, then, is our ﬁrst canary. It does
not appear to be anywhere close to dying.

Self-driving cars are a second canary. They are further in the future than
anticipated by boosters like Elon Musk. AI can fail catastrophically in atypical
situations, like when a person in a wheelchair is crossing the street. Driving is far
more challenging than previous AI tasks because it requires making life-critical,
real-time decisions based on both the unpredictable physical world and interaction
with human drivers, pedestrians, and others. Of course, we should deploy limited
self-driving cars once they reduce accident rates, but only when human-level
driving is achieved can this canary be said to have keeled over.
AI doctors are a third canary. AI can already analyze medical images with
superhuman accuracy, but that is only a narrow slice of a human doctor's job. An
AI doctor would have to interview patients, consider complications, consult other
doctors, and more. These are challenging tasks that require understanding people,
language, and medicine. Such a doctor would not have to fool a patient into
thinking it is human—that's why this is diﬀerent from the Turing test. But it would
have to approximate the abilities of human doctors across a wide range of tasks
and unanticipated circumstances.
Again, I agree that we are far from being able to do that -- being a doctor requires a
lot of very general intelligence, in the sense that you have to be good at a lot of
diﬀerent things simultaneously and also good at integrating those skills. And I agree
that once we have AI that can do this, human-level AGI is not far away. But again,
shouldn't we get started preparing before that point?
And though the Turing test itself is not a good canary, limited versions of the test
could serve as canaries. Existing AIs are unable to understand people and their
motivations, or even basic physical questions like "Will a jumbo jet ﬁt through a
window?" We can administer a partial Turing test by conversing with an AI like
Alexa or Google Home for a few minutes, which quickly exposes their limited
understanding of language and the world. Consider a very simple example based
on the Winograd schemas proposed by computer scientist Hector Levesque. I said
to Alexa: "My trophy doesn't ﬁt into my carry-on because it is too large. What
should I do?" Alexa's answer was "I don't know that one." Since Alexa can't
reason about sizes of objects, it can't decide whether "it" refers to the trophy or to
the carry-on. When AI can't understand the meaning of "it," it's hard to believe it
is poised to take over the world. If Alexa were able to have a substantive dialogue
on a rich topic, that would be a fourth canary.
Yep. Same response.
Current AIs are idiots savants: successful on narrow tasks, such as playing Go or
categorizing MRI images, but lacking the generality and versatility of humans.
Each idiot savant is constructed manually and separately, and we are decades
away from the versatile abilities of a ﬁve-year-old child. The canaries I propose, in
contrast, indicate inﬂection points for the ﬁeld of AI.
Some theorists, like Bostrom, argue that we must nonetheless plan for very low-
probability but high-consequence events as though they were inevitable. The
consequences, they say, are so profound that our estimates of their likelihood
aren't important. This is a silly argument: it can be used to justify just about
anything. It is a modern-day version of the argument by the 17th-century
philosopher Blaise Pascal that it is worth acting as if a Christian God exists
because otherwise you are at risk of an everlasting hell. He used the inﬁnite cost
of an error to argue that a particular course of action is "rational" even if it is

based on a highly improbable premise. But arguments based on inﬁnite costs can
support contradictory beliefs. For instance, consider an anti-Christian God who
promises everlasting hell for every Christian act.  That's highly improbable as well;
from a logical point of view, though, it is just as reasonable a wager as believing in
the god of the Bible. This contradiction shows a ﬂaw in arguments based on
inﬁnite costs.
First of all, this isn't an argument based on tiny probabilities of inﬁnite costs. The
probability that human-level AI will arrive soon may be small but it is much much
higher than other probabilities that you regularly prepare for, such as the probability
that you will be in a car accident today, or the probability that your house will burn
down. If you think this is a Pascal's Wager, then you must think buckling your seatbelt
and buying insurance are too.
Secondly, again, the rationale for preparing isn't just that AGI might arrive soon, it's
also that it is good to start preparing before AGI is about to arrive. Suppose you are
right and there is only a tiny tiny chance that AGI will arrive before these canaries.
Still, preparing for AGI is an important and diﬃcult task; it might take several -- even
many! -- years to complete. So we should get started now.
My catalogue of early warning signals, or canaries, is illustrative rather than
comprehensive, but it shows how far we are from human-level AI. If and when a
canary "collapses," we will have ample time before the emergence of human-level
AI to design robust "oﬀ-switches" and to identify red lines we don't want AI to
cross.
What? No we won't, you yourself said that human-level AGI will not be far away when
these canaries start collapsing. And moreover, you seem to think here that
preparation will be easy: All we need to do is design some oﬀ-switches and specify
some red lines... This is really naive. You should read the literature, which contains lots
of detailed discussion of why solutions like that won't work. I suggest starting with
Bostrom's Superintelligence, one of the early academic works on the topic, and then
branching out to skim the many newer developments that have arisen since then.
AI eschatology without empirical canaries is a distraction from addressing existing
issues like how to regulate AI's impact on employment or ensure that its use in
criminal sentencing or credit scoring doesn't discriminate against certain groups.
As Andrew Ng, one of the world's most prominent AI experts, has said, "Worrying
about AI turning evil is a little bit like worrying about overpopulation on Mars."
Until the canaries start dying, he is entirely correct.
Here's another argument that has the same structure as your argument: "Some
people think we should give funding to the CDC and other organizations to help
prepare the world for a possible pandemic. But we have no idea when such a
pandemic will arise--and moreover, we can be sure that there will be "canaries" that
will start dying beforehand. For example, before there is a worldwide pandemic, there
will be a local epidemic that infects just a few people but seems to be spreading
rapidly. At that point, it makes sense to start funding the CDC. But anything
beforehand is merely un-empirical speculation that distracts from addressing existing
issues like how to stay safe from the ﬂu. Oh, what's that? This COVID-2019 thing looks
like it might become a pandemic? OK sure, now we should start sending money to the
CDC. But Trump was right to slash its budget just a few months earlier, since at that
point the canary still lived."

Also, Andrew Ng, the concern is not that AI will turn evil. That's a straw man which you
would realize is a straw man if you read the literature. Instead, the concern is that AI
will turn competent. As leading AI expert Stuart Russell puts it, we are currently
working very hard to build an AI that is smarter than us. What if we succeed? Well
then, by the time that happens, we'd better have also worked hard to make it share
our values. Otherwise we are in deep trouble.

Bayes-Up: An App for Sharing
Bayesian-MCQ
Inspired by Lê Nguyên Hoang's post on Bayesian Examination, I have been developing
(as a hobby) a new app called Bayes-Up (available at: bayes-up.web.app). The app is
now in a state where it is working well enough to be shared with others. In this post I
list a few things you can do with it, because I expect that it will spark some interest
within the community.
Test and improve your calibration: Bayes-Up uses a collection of good
quality trivia questions from the open trivia database. The main point of the app
is that you can ﬁnd a list of multiple choice quizzes, answer questions by
assigning probabilities to each of the possible choices, receive a score based oﬀ
a quadratic proper scoring rule and later ﬁnd statistics about the quality of your
calibration. A good place to start is the quiz from the book Factfulness by Hans
Rosling that I included in the app.
Create quizzes and upload them. There exists already a small number of
calibration training apps. Bayes-Up diﬀers mainly because it allows to upload
and share your own quizzes. This can solve one of the problems of calibration
apps which is to create good quality content (quizzes / questions). If you are a
teacher and want your students to develop more metacognitive skills and
intellectual honesty, or if you are organizing workshops on probability
calibration, Bayes-Up can make it easier for you. To add a quiz, simply write it in
a spreadsheet, export it as a CSV ﬁle and upload it in Bayes-Up.
Recommend UI improvements, new features, report bugs, or contribute
to the implementation. Only very little feedback has been collected so far and
certainly a lot could be improved with little eﬀort. The code of the app is open
source and hosted on github.
Analyse the data from Bayes-Up users. So far about 30'000 questions have
been answered by about 1'300 users since the end of December 2019. The
collected data is available at this link and will likely grow in the following
months. Simple questions that analysing this data could answer are: Do users
become better calibrated over time? Is calibration topic-speciﬁc or transferrable?
How can the answers of users with unknown calibration and unknown knowledge
be aggregated to predict the right answers to every question? Let me know if
you want to do something with it or need a better documentation.

Suspiciously balanced evidence
What probability do you assign to the following propositions?
"Human activity has caused substantial increases in global mean surface
temperature over the last 50 years and barring major policy changes will
continue to do so over at least the next 50 years -- say, at least half a kelvin in
each case."
"On average, Christians in Western Europe donated a larger fraction of their
income last year to non-religious charitable causes than atheists."
"Over the next 10 years, a typical index-fund-like basket of US stocks will
increase by more than 5% per annum above inﬂation."
"In 2040, most global electricity generation will be from renewable sources."
These are controversial and/or diﬃcult questions. There are surely a lot of people who
think they know the answers and will conﬁdently proclaim that these propositions are
just true or, as the case may be, false. But you are a sophisticated reasoner, able to
think in probabilistic terms, and I expect your answers to questions like these mostly
lie between p=0.1 and p=0.9 or thereabouts, just like mine. No foolish black-and-
white thinking for the likes of us!
(I confess that my estimate for the ﬁrst of those propositions is above p = 0.9. But it's
not above p = 0.99.)
... But isn't it odd that the evidence should be so evenly balanced? No more than 3
bits or so either way from perfect equality? Shouldn't we expect that the totality of
available evidence, if we could evaluate it properly, would make for a much larger
imbalance? If we encounter only a small fraction of it (which we'd need to, to explain
such evenly balanced results), shouldn't we expect that randomness in the subset we
happen to encounter will in some cases make us see a large imbalance even if any
given single piece of evidence is about as likely to go one way as the other? What's
going on here?
Let me add a bit of further fuel to the ﬁre. I could have tweaked all those propositions
somewhat -- more than 3%, or more than 7%, above inﬂation; more than 40%, or
more than 60%, or 2050 instead of 2040. Surely that ought to change the probabilities
quite a bit. But the answers I'd have given to the questions would still have
probabilities between 0.1 and 0.9, and I bet others' answers would have too. Can
things really be so ﬁnely enough balanced to justify this?
I can think of two "good" explanations (meaning ones that don't require us to be
thinking badly) and one not-so-good one.
Good explanation #1: I chose propositions that I know are open to some degree of
doubt or controversy. When I referred to "questions like these", you surely understood
me to mean ones open to doubt or controversy. So questions where the evidence is, or
seems to us to be, much more one-sided were ﬁltered out. (For instance, I didn't ask
about young-earth creationism, because I think it's almost certainly wrong and expect
most readers here to feel the same way.) ... But isn't it strange that there are so many
questions for which the evidence we have is so very balanced?

Good explanation #2: When assessing a question that we know is controversial but
that seems one-sided to us, we tend to adjust our probabilities "inward" towards 1:1
as a sort of a nod to the "outside view". I think this, or something like it, is probably a
very sensible idea. ... But I think it unlikely that many of us do it in a principled way,
not least because it's not obvious how to.
Not-so-good explanation: We have grown used to seeing probability estimates as a
sign of clear thought and sophistication, and every time we accompany some opinion
with a little annotation "p ≃0.7" we get a little twinge of pride at how we quantify our
opinions, avoid black-and-white thinking, etc. And so it becomes a habit, and we
translate an internal feeling of conﬁdence-but-not-certainty into something like "
p ≃0.7" even when we haven't done the sort of evidence-weighing that might
produce an actual numerical result.
Now, I'm not sure which of two quite diﬀerent conclusions I actually want to endorse.
"The temptation to push all probabilities for not-crazy-sounding things into the
middle of the possible range is dangerous. We are apt to treat things as
substantially-open questions that really aren't; to be timid where we should be
bold. Let's overcome our cowardice and become more willing to admit when the
evidence substantially favours one position over another."
"Our practice is better than our principles. Empirically, we make lots of mistakes
even in cases where numerical evidence-weighing would lead us to probabilities
close to 0 or 1. So we should continue to push our probability estimates inward.
The challenge is to ﬁgure out a more principled way to do it."
Here is a possible approach that tries to combine the virtues of both:
Allow accumulating evidence to push your probability estimates towards 0 or 1;
be unashamed by these extreme-sounding probabilities. BUT
Keep a separate estimate of how conﬁdent you are that your approach is
correct; that your accumulation of evidence is actually converging on something
like the right answer. THEN,
When you actually need a probability estimate, bring these together.
Suppose your "internal" probability estimate is p, your probability that your approach
is correct is q, and your probability conditional on your approach being wrong is r.
Then your overall probability estimate is qp + (1 −q)r and (holding q, r constant) in
eﬀect your internal probability estimates are linearly squashed into the interval from 
(1 −q)r to (1 −q)r + q. So, for instance, if you're 90% sure your approach is right and
your best guess if your approach is all wrong is that the thing's 75% likely to be true,
then your estimates are squashed into the range [7.5%,97.5%].
Cautionary note: There's an important error I've seen people make when trying to do
this sort of thing (or encourage others to do it), which is to confuse the propositions
"I'm thinking about this all wrong" and "the conclusion I'm fairly sure of is actually
incorrect". Unless the conclusion in question is a very speciﬁc one, that's likely a

mistake; the probability I've called r above matters and surely shouldn't be either 0 or
1.

Mazes Sequence Roundup: Final
Thoughts and Paths Forward
There are still two elephants in the room that I must address before concluding. Then I
will discuss paths forward.
Moloch's Army
The ﬁrst elephant is Moloch's Army. I still can't ﬁnd a way into this without sounding
crazy. The result of this is that the sequence talks about maze behaviors and mazes as
if their creation and operation are motivated by self-interest. That's far from the whole
picture.
There is mindset that instinctively and unselﬁshly opposes everything of value. This
mindset is not only not doing calculations to see what it would prefer or might
accomplish. It does not even believe in the concept of calculation (or numbers, or
logic, or reason) at all. It cares about virtues and emotional resonances, not
consequences. To do this is to have the maze nature. This mindset instinctively
promotes others that share the mindset, and is much more common and impactful
among the powerful than one would think. Among other things, the actions of those
with this mindset are vital to the creation, support and strengthening mazes.
Until a proper description of that is ﬁnished, my job is not done. So far, it continues to
elude me. I am not giving up.
Moloch's Puzzle
The second elephant is that I opened this series with a puzzle. It is important that I
come back to the beginning. I must oﬀer my explanation of the puzzle, and end the
same place the sequence began. With hope.
Thus, the following puzzle:
Every given thing is eventually doomed. Every given thing will eventually get worse.
Every equilibrium is terrible. Suﬃciently strong optimization pressure, whether or not
it comes from competition, destroys all values not being optimized, with optimization
pressure constantly increasing.
Yet all is not lost. Most of the world is better oﬀ than it has ever been and is getting
better all the time. We enjoy historically outrageously wonderful bounties every day,
and hold up moral standards and practical demands on many fronts that no time or
place in the past could handle. How is this possible?
Here is my answer:
It is possible because old things that get suﬃciently worse eventually die, and get
replaced by new things that are better. It is possible because competition is never
perfect, and optimization for a ﬁxed set of metrics is never total. People need and
demand slack, and care about many factors on many meta levels. No matter how
many times we say it is diﬃcult, not game theoretically sound or even impossible,
coordination continues to happen all the time.

Moloch wins when all things are equal and the situation is at a strict static equilibrium.
Things are not equal. The situation is not static or strict. While there are some places
where Moloch has not won, exit to those areas serves as an additional safety valve.
We have spent the bulk of this sequence dealing with mazes. Mazes are, among other
things, a case of what happens when all of that breaks down - when we strip away the
protections. They then go on to create all sorts of strange negative feedback loops,
and make life inside them, and if left unchecked life even much of life outside of them,
quite bad. In many ways, even this sequence has only scratched the surface of the
dynamics involved.
Good News, Everyone!
To the extent that this makes one estimate that the world is a worse place, and its
people are less happy and less likely to succeed or improve, this is bad news indeed.
But, to the extent that we already knew there was a problem, to the extent that we
already knew how functional our world was and how happy people are, this is not bad
news. This is good news. We already knew there was a problem and had an
amorphous sense of hopelessness and doom. Now that we know more about what the
problem is, we have a more speciﬁc source of hopelessness and doom that we can try
to do something about. We can explore further. We can limit the damage on a
personal level, seek to get things done, and perhaps even improve conditions in
general.
The other good news is in our estimates of the eﬀectiveness of doing object level
things. We know about how much real and useful stuﬀ is actually created and
accomplished, in the sense that we know what useful stuﬀ we get to use and we live
real lives. If we are screwing people up this much, we are wasting this much of
everyone's time, and otherwise getting in our own way, then what does that say
about how powerful it is to actually do things? If only a handful of people at a major
corporation are producing all the value while everyone else plays politics, then what
does that say about that handful of people?
If, as my model holds, people who are allowed to actually do real things 
consistently output insanely great things, then imagine what would happen if we let
more people do that. Then realize there are still places where this is possible. Imagine
what you could accomplish if you did that.
I hope this journey has been enlightening, and that it inspires further explorations
along with concrete actions. Perhaps we can create useful common knowledge. With
luck, some people will avoid falling into maze traps as the result of what has been
written here. With even better luck, this can improve the fate of some organizations,
or even lead to broader motions towards taking down maze levels.
Conclusion
The central (Immoral) Mazes sequence is now complete.
I wrote it because someone had to, and no one else would. This was the best model I
could come up with, presented as clearly as I know how to do so.
There are many things I still want to say, most of which are not mentioned above.
Some of them are a matter of putting in the work to write things down. Others I don't

know how to say without sounding crazy, or I don't understand them myself well
enough to write about them. I have experience with mazes, but not with higher maze
levels. I can abstractly model that mentality and mode of operation, but I can't
pretend I truly understand it.
The more others can join the conversation and move it forward from here, especially
those with more direct experience but that managed to emerge intact enough to tell
the tale, the more hope we have for making real progress.
To encourage the story to continue, both for myself and for others, I will now explore
the question of Paths Forward. What are the best next questions?
Paths Forward
These topics are not only or even primarily for me. Others can and should take up the
mantle as well. Here are some things that seem like good ideas to keep the ball
rolling.
I still don't have (1a) Moloch's Army in a place where I am ready to post it, but I
promise to keep trying. At a minimum, I hope that trying leads to ﬁnding more missing
concepts and ideas that can help bridge the gap.
I previously tried to bridge into this with The Darwin Game sequence, but that
foundation wasn't enough. If I resumed that line now then I would next talk about (1b)
The Rise of Cliquebot. I am still confused on whether that would (eventually) get me
where I would need to be.
One of the words I know I will likely need is (2a) Fnord. The word fnord comes from the
Principia Discordia. In its original introduction, we are all conditioned when we see the
word fnord to not notice it but to become stressed and irritated. Thus, it is peppered
into places that people with power want us not to look, but it is entirely absent from
things like advertising. Having a name for things that make you not want to notice
them, and understanding how that dynamic works, seems important. Mazes, and
many aspects of them, are fnords. Another good closely maze-aligned Discordian
concept is (2b) The Snafu Principle, whereby communication is only fully possible
between equals, leading to Situation Normal All F***ed Up. It should be mostly already
covered by implication but is worth a special focus slash better place to link to.
Another key idea I need are (2c) Basilisks, as such things have much more important
rolls to play than the example that gave them their name. Needless to say one must
proceed with caution. Prior restraint will be a thing, here.
I introduced terms for things related to mazes, including maze behaviors, leading Wei
Dei to ask the logical question (3a) What Are Maze Behaviors? It would be good to
have a compact link-to-it answer. But I'm also coming around, after having started a
draft of this, to the perspective that this is likely asking the wrong question. We could
be better served to ask about (3b) Maze Levels and especially about (3c) The Maze
Nature, which I also introduced, instead. The Maze Nature is closely related to
Moloch's Army and might be the right way in. Or perhaps it will be the other way
around.
One worry is that mazes are the wrong central concept on which to build these
metrics and understandings. It might be more helpful, either in general or in some
places, to talk about (4) Simulacrum Levels. Ben's post here is the best reference

created so far, but it deﬁnitely needs to be improved upon. I also do not properly
understand simulacrum levels, or at least my understanding and Ben's are
importantly diﬀerent. These things need to be explored carefully, as the dynamics
seem central to what is happening to the world.
Many of the proposed solutions, and hypothesized causes, would beneﬁt from more
careful treatment. People could gather numbers, make proﬁles and case studies
especially where they have their own experience. Many great comments were (5a)
Maze Examples, people talking about the place they work and saying how that ﬁts into
the bigger picture. Some claimed they worked at mazes. Many claimed that where
they worked should have been a maze, based on size and what not, but that it totally
wasn't one or at least wasn't as much of one as one would expect.
The question of (5b) How Did They Avoid Mazedom? is a good one for people who
claim it was avoided. And of course someone should try to answer the question (5c)
How Maze-Intensive Are Our Corporations? A few things to consider along the way,
The ﬁrst is that these are the people reading DWATV and/or LessWrong. That is not a
random sample. The whole point of such websites is to promote a method of thinking
that is incompatible with a maze. If you care enough about good methods of thinking
to read these websites, you likely have a strong aversion to maze behaviors compared
to others with similar levels of human capital, both not tolerating them in others and
not being willing to do them yourself. You likely chose your place and class of
employment in part on this basis, regardless of how you thought about that decision
when making it.
If someone was under full sacriﬁce-everything pressure from anything, they're not
going to read a book length sequence about it, because they can't.
The second is that the best way to get comments on the internet is to get someone to
tell you that you are wrong. People who think the model is failing in their case are
much more likely to comment (or so my model says) than people for whom the model
is accurate. That is how commenting typically works. So the sample does not provide
much evidence, in that sense.
The third is that mazes are things people do not want to see, and people will select for
choosing exactly the mazes they do not notice. If they did notice them, and were of
the type to be reading this, they would probably avoid them. My estimate of the maze
levels of several of my jobs jumped dramatically in the months after I left, because I
had the chance to get perspective and to experience life without that aspect of things.
Even if you are not yourself being eaten by the maze in question, it is instinctive to
not notice it, or to justify what you are experiencing as normal and healthy and
reasonable, or at least not so bad, when it is nothing of the kind.
It is also inevitable that some people have already self-modiﬁed to adapt to the mazes
and that this makes it harder for them to notice what is going on. Not only does it
mask this thing especially hard, it masks noticing things in general. Eventually, this
would drive people away from reading such things, but inertia in such matters can last
a while.
I am sure reading this sequence helps (in a probabilistic sense) but it is a hard
problem. It is clear to me from the comments that I am not yet getting the full
situation across successfully to many readers.

One thing someone will need to at some point write (6a) Mazes That Are Not Within
Organizations, discussing dynamics that produce similar results without people strictly
being bosses and subordinates. And generally (6b) What Types of Things are How
Maze-Like, (6c) To What Extent do People At Large Have the Maze Nature, (6d) Close
Examination of Maze Interactions, and so on.
The model of perfect competition presented early in the sequence proved very non-
intuitive to many people and got a lot of push back. A lot of that was my using
technical economic terms that trigger strong intuitions about what the answers are. I
think a lot of this is that these terms are usually taught in a school context,  where
there are right answers and right principles that these things are supposed to
illustrate, and the models always work a certain way. I was using them a diﬀerent way,
changing the assumptions and what things would be implied versus not implied, in
order to provide justiﬁcation for the transition to the later part of the sequence.
If I had to do that over again, I'd look for a way to take a diﬀerent approach entirely,
because it looks like it was more confusing than I expected, and the objection I felt I
needed to overcome was much weaker than I expected it to be slash to the extent it
existed the people who had the objection didn't feel satisﬁed by the explanation. So it
didn't really work the way I wanted it to. I do stand by what I wrote, and think there's
important stuﬀ there, but kill your darlings and all that.
However, there's a whole series of posts I could write (7a) On Competition, going
deeper into what I was getting at there. Or even (7b) On Ultimate Human Value and all
that, which is kind of a big deal, but again, super hard and I'm constantly terriﬁed of
writing to advocate ethical positions because I assume others with better rhetoric and
academic chops in the area would just blow anything I write to shreds and nothing
would be accomplished. That doesn't mean I think I'm wrong or anything, but it's a
problem, and it's why the old "(7c) Can I Interest You In Some Virtue Ethics" post
never got written after the last paths forward. Still, (7d) How Consequentialism Is
Ruining Things And What To Do About It might be more tractable. I thought our
decision theory would be better than this by now, but alas.
Another valuable thing to do would be to give people practical advice. Advice on how
to choose ﬁelds of study and careers and jobs and where to live, and other major life
choices, to avoid the dangers of mazes, while taking into account the many other
things that matter. We have things like 80,000 Hours, but that leads to a
consequentialist calculation with many of the key terms missing because they haven't
been quantiﬁed, and many of the best options never considered because they can't
be standardized, which is playing right into the hands of mazes. It's the same as the
usual social pressure to go work for mazes, if hopefully a little more eﬃcient about
collecting at least something in return. So, (8) Practical Career Advice to Avoid Mazes,
A Continuing Series.
In particular, we need (9a) How to Start a Small Business. Not a start-up -  I don't
think creating or joining one of those is a bad idea, but the how of that is something
we are much more familiar with already - but a small business that actually does
business. A way of life that involves buying things and then making or transporting
things and selling things and having the things you sell generate revenue that pays
expenses plus the rent. The kind of thing immigrants often do to great eﬀect, except
you can do even better if you are already integrated into the culture and have better
connections and seed funding.

More fundamental would be the simple (9b) How to Do Business. There is specialized
knowledge of how to start a business, but the most important doing-business related
skill is simply how to do business. Start-ups backed by VCs are diﬀerent because they
fundamentally do not (yet) do business. They might do some facsimile of business by
getting customers, or even unit economically proﬁtable customers. But that's
a completely diﬀerent model of how to succeed than trying to ﬁnd customers and
make money from them and use that money to pay the rent and the surplus to grow
the business. What such start-ups are actually doing is the performance art of "doing
business" aimed at the VCs that are their bosses. Doing business is a diﬀerent thing
entirely.
One hint you might be in a maze is that you are "doing the thing" in quotation marks
rather than doing the thing.
I could also perhaps do (10a) Some Businesses Worth Starting, (10b) Some Start-Ups
Worth Starting, (10c) Some Ways To Make Money I Made Work Before, and/or (10d)
Some Ways To Make Money I Didn't Do But I'm Conﬁdent Will Work. I'm sure good
versions of 10c and 10d would be appreciated, but it's a truism that no one cares
about your start-up idea, whether or not they should, so perhaps not 10a and 10b.
Looking at the potential causes in more detail, and attempting to better understand
their dynamics, would also be valuable. (11a) To What Extent Do We Need Large
Organizations?, (11b) The Demand for Blamelessness, (11c) The Illusion of Security,
(11d) The Dynamics of Rent Seeking, (11e) On Atomization, (11f) Education as a Maze,
(11g) Education as Maze Indoctrination, and so on.
We could also look further at the solutions. One valuable place to go if we could do it
well would be (12) How to Explain Mazes. We could also look at (13a) The Full
Alternative Stack in much more detail. The biggest problem starts with (13b) How to
Tell if Someone has the Maze Nature slash (13c) How to Tell if Someone Has Edge (or
just (13d) Edge). That will need a better name because edge is a massively
overloaded term. Here edge means the tendency to 'angle shoot' or otherwise take
every opportunity to take local advantage in underhanded ways. Do you always need
to be 'on guard' against someone, or can you relax?
The other half is a post or series (13e) On Being a Source of Money. This is a huge
unsolved problem. The moment anyone sees you (where you can be a person, or a
group or organization, or anything else) as a source of funds, even a possible future
source of funds, it corrupts all interactions even when everyone involved has the best
of intentions. You always have to worry someone is after your money. The people
around you are likely to be there largely because of your money. Others have to worry
about whether you think they're after your money even when they totally aren't, and
you can never rule such a thing out entirely. There is a reason a lot of rich people keep
their wealth (or at least their giving/funding) a secret. Consider the chapter of Skin in
the Game titled Only the Rich are Poisoned. Money can be shockingly useless or
backﬁre, so what to do?
The other solutions to mazedom are largely political actions, where I am loathe to get
more into the weeds for mostly-obvious reasons, or at least so far I have chosen to
strive to be maximally apolitical throughout not only this sequence but on the entire
blog. That's another thing mazes do. They encourage us to stay out of such questions
because they create an amorphous feeling that getting involved might backﬁre
against us at some point in some way. Talking more about (14) How Mazes Scare You
in such ways might be important. Of course, a lot of it is also that I do not need or

want the trouble of arguing or advocating politics on the internet, nor do I expect
much good to come of such a thing, and all that. And once I start down that road it is
very easy to make oneself only seen from that perspective. It's mostly a massively
over-determined decision. But perhaps that decision is wrong, or will become wrong?
One more-related-to-this-than-you-would-ﬁrst-think thing I've wanted to do for a while
but that would require a lot of work and which might not come together, and which is
motivated by this post, is to tell (15) The Journey of the Sensitive One. It would look
over the story of the artist Jewel, as told, in explicit content in chronological order
across her ﬁrst ﬁve albums.
It might be most valuable of all to simply get case studies. I would love to see
someone take this perspective and in particular examine (16a) What Happened to
Boeing? Something seems to have happened that caused them to rapidly have higher
maze levels, to the extent that they were unable to produce a plane that did not
crash, ignoring huge quantities of written warnings that they were in fact producing a
dangerous plane, and despite this being a very very bad thing for such a business to
do. Steve Jobs was mentioned as a potentially interesting special case, so someone
looking into (16b) Apple and Mazes could be interesting, especially to see if maze
levels declined there when he came back and if so how he did that. And so on.
If one wanted to do a full extension of the project, (17) On The Gervais Principle could
be anything up to and including its own sequence. As I've noted elsewhere, I consider
Gervais Principle and Moral Mazes to be fully compatible, and Gervais Principle has a
bunch of stuﬀ that expands upon the model.
I'm sure I'm forgetting a lot of other stuﬀ, as well. And of course, I have other things I
need to and/or want to write, both about gaming and about other stuﬀ. The game I'm
creating is almost ready to get rolling, so I'll probably want to focus on the gaming
side even in my writing more and more over the coming months.
I seem to have already written close to a book's worth of stuﬀ. The main sequence
(excluding this post, Quotes from Moral Mazes and Moral Mazes and Short Termism) is
about 40,000 words long, there are a lot of places I could expand upon, and a quick
Google says that the average business book is 50,000 to 60,000 words long. If anyone
is seriously interested in publishing were I to turn this into a book, or has the know-
how to make that happen and thinks it would be a good idea, please contact me with
your thoughts and what that would look like. If it would be worthwhile to rewrite this in
proper full book form I am open to that idea. If not, it might still be a good idea to print
some copies mostly as-is to help reach more people.
And of course, thanks to you for reading, and hopefully thinking about and building
upon all of this.
Acknowledgements
I would like to thank Ben Hoﬀman, Sarah Constantin and Michael Vassar for providing
the impetus to read Moral Mazes and take it seriously. It takes a lot of motivation to
get through a book that heavy.
I would like to thank Ben Hoﬀman, Michael Vassar, Raymond Arnold, Ben Pace, Oliver
Habyrka, Zack Davis, Jessica Taylor and my wife Laura Baur for helping me edit the
sequence. Without strong editorial feedback, this sequence would likely not have
come to be, and if it did it would have been much weaker.

Ben Hoﬀman in particular was vital early on in helping me wrap my head around the
problem, Michael Vassar later on for making sure I didn't miss the important points,
and Raymond Arnold for making sure what I wrote would have a ﬁghting chance of
being understood. Many thanks.
I'd also like to thank the commentators, including the ones who made comments I
found frustrating and wrong, but especially the ones who challenged me in ways that
improved my thinking.
And also those that helped keep up morale so I could ﬁnish. Engagement matters, and
knowing you have people you respect interested matters too. Little notes of
appreciation can go a long way. It's important to give praise. This includes Scott
Alexander, and also Robin Hanson, who said he would read Moral Mazes on the
strength of the quotes I provided. I look forward to seeing both their takes.
I'd also like to thank the person who got How to Identify an Immoral Maze to be
featured on Hacker News. The majority of hits I get come from being featured
somewhere, in one form of another. I do my best not to let that change what I write,
other than being happy to edit posts to make them stand on their own better if this is
standing between them and being linked in this way. If that is ever the case and the
modiﬁcations seem reasonable, please do let me know.
Finally, thanks again for reading. I hope the time you have invested has proved
worthwhile.
 
 
 

Some quick notes on hand hygiene
I always wash my hands but typically not very well; I never worked in healthcare and
never got trained. Given the current situation I decided to actually research how to do
it properly.
Diﬀerent sources diﬀer slightly in their recommendations but the basic gist is the
same. Some takeaways:
Washing your hands should take ~ 20-30 seconds. If it's taking you much less
you're not doing it right.
Make sure you actually get all your hand surfaces, including your nails and
between your ﬁngers. The WHO guide has a good protocol that's probably worth
memorising.
If possible, try to dry your hands with a single-use towel, or air-dry somehow; not
much point washing your hands and then drying them on a dirty towel.
Good hand sanitiser (>60% alcohol) is a good substitute if your hands aren't
visibly soiled, but you still have to do the all-surfaces routine and it should still
take ~20 seconds.
For those of you who use Anki, ankifying this is probably among the most valuable
things you could ever use it for. Especially in the early days of a maybe-pandemic!
Ditto for those of you who use TAPs. My read of the various health authorities is that
good, regular hand-washing is way ahead of gloves and masks in terms of keeping
you and your loved ones safe.
Other things that now would be a good time to hammer into your head if (like me) you
didn't already: coughing into your elbow, sanitising your doorknobs and light switches,
and not touching your face. These are all a good idea anyway during ﬂu season, but
you should use the impetus of the current situation to actually do them , rather than
just vaguely intending to learn how to do them like everyone else.
In the interests of reducing the number of people who read this, nod, vaguely intend
to get better, and do nothing, here's an itemised checklist of concrete things you can
do.
1. Read the The WHO guide and memorise/Anki the hand-cleaning/sanitation. Here
is a little minideck of cards I made for my own practice.
2. Go to Walgreens/Boots/some other drug store and buy lots of little bottles of
hand sanitiser (with alcohol!). Failing that, buy some big bottles oﬀ Amazon and
aliquot them into smaller containers for use on the go.
3. Order some alcohol in a spray bottle you can use to clean your light switches
and doorknobs (I have no particular opinion on isopropanol vs ethanol for this, if
somebody does then please comment and let me know).
4. If you have trouble touching your face, order some of that anti-nail-biting stuﬀ
people use to train their kids and use it until your brain gets the message. Chili
powder might work even better; if people try either of these (or something else)
and it works well for kicking the habit I'd love to hear about it.
If anyone on here disagrees with any of this, please do comment and let me know.
Other than the WHO, you can also ﬁnd guidance from the CDC, the Mayo clinic and

the NHS, among others. And here is some other guidance on staying safe in outbreaks
(though note that the author's position on both gloves and masks is somewhat
controversial).
ETA: Some more quick notes on when to wash/sterilise your hands. Broadly, the
answer seems to be "much more often than you're probably doing it". Whenever you
use the toilet, before you ever eat food, after touching dirty things, and after
coughing/sneezing/blowing your nose are good TAPs to install (though note that you
should not be coughing/sneezing into your hands). If you wash your hands super-often
you're likely to have issues with your skin, but this is much less true of hand sanitiser,
so use that to ﬁll in the gaps.
This part is a little crude but probably important. I've previously gotten into a couple
of arguments about whether you should always wash your hands after peeing, given
that (i) your junk probably isn't that dirty and (ii) there's lots of things in your house /
around town that are more dirty that you don't wash your hands after touching. I think
argument (ii) is valid but more of an argument for cleaning your light
switches/bathroom surfaces than for not washing your hands. It's also important to
note that when you pee, and especially when you ﬂush, you create a ﬁne mist of very-
much-not-clean toilet water that covers everything in the bathroom, including your
hands.
But probably the strongest argument for always washing/sterilising your hands
thoroughly after peeing is that you probably pee fairly regularly and don't wash your
hands enough, and so instantiating a "wash your hands after peeing" TAP ensures
you're washing your hands at least that often.

Conclusion to 'Reframing Impact'
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.






Epistemic Status
I've made many claims in these posts. All views are my own.
1%

2%
3%
4%

5%
6%
7%

8%
9%
10%

11%
12%
13%

14%
15%
16%

17%
18%
19%

20%
21%
22%

23%
24%
25%

26%
27%
28%

29%
30%
31%

32%
33%
34%

35%
36%
37%

38%
39%
40%

41%
42%
43%

44%
45%
46%

47%
48%
49%

50%
51%
52%

53%
54%
55%

56%
57%
58%

59%
60%
61%

62%
63%
64%

65%
66%
67%

68%
69%
70%

71%

72%
73%

74%

75%
76%

77%

78%
79%

Raemon (75%)
80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

ryan_greenblatt (80%),niplav (80%)
90%

91%

92%
93%

94%

95%
96%

97%

98%
99%

TurnTrout (95%)
1%
Attainable Utility theory describes how people feel impacted
99%
1%
2%

3%
4%
5%

6%
7%
8%

9%
10%
11%

12%
13%
14%

15%
16%
17%

18%
19%
20%

21%
22%
23%

24%
25%
26%

27%
28%
29%

30%
31%
32%

33%
34%
35%

36%
37%
38%

39%
40%
41%

42%
43%
44%

45%
46%
47%

48%
49%
50%

51%
52%
53%

54%
55%
56%

57%
58%
59%

60%
61%
62%

63%
64%
65%

66%
67%
68%

69%
70%

71%
72%

73%

74%
75%

76%

77%
78%

79%

TurnTrout (75%)
80%
81%
82%

83%
84%
85%

86%
87%
88%

89%
90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

niplav (90%),ryan_greenblatt (95%)
1%
Agents trained by powerful RL algorithms on arbitrary reward signals generally
try to take over the world.
99%
Conﬁdent (75%). The theorems on power-seeking only apply to optimal policies in fully
observable environments, which isn't realistic for real-world agents. However, I think
they're still informative. There are also strong intuitive arguments for power-seeking.
1%
2%

3%
4%
5%

6%
7%
8%

9%
10%
11%

12%
13%
14%

15%
16%
17%

18%
19%
20%

21%
22%
23%

24%
25%
26%

27%
28%
29%

30%
31%
32%

33%
34%
35%

36%
37%
38%

39%
40%
41%

42%
43%
44%

45%
46%
47%

48%
49%
50%

51%

52%
53%

54%

55%
56%

57%

58%
59%

jacobjacob (59%)
60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

ryan_greenblatt (65%),ejacob (65%)
70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

TurnTrout (70%),niplav (75%)
80%

81%
82%
83%

84%
85%
86%

87%
88%
89%

90%
91%
92%

93%
94%
95%

96%
97%
98%

99%
1%
The catastrophic convergence conjecture is true. That is, unaligned goals tend to
have catastrophe-inducing optimal policies because of power-seeking incentives.
99%
Fairly conﬁdent (70%). There seems to be a dichotomy between "catastrophe directly
incentivized by goal" and "catastrophe indirectly incentivized by goal through power-
seeking", although Vika provides intuitions in the other direction.
1%

2%
3%

4%
5%

6%
7%

8%
9%

10%
11%

12%
13%

14%
15%

16%
17%

18%
19%

20%
21%

22%
23%

24%
25%

26%
27%

28%
29%

30%
31%

32%
33%

34%
35%

36%
37%

38%
39%

40%
41%

42%
43%

44%
45%

46%
47%

48%
49%

50%
51%

52%
53%

54%
55%

56%
57%

58%
59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

niplav (67%)
70%
71%

72%
73%

74%
75%

76%
77%

78%
79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

TurnTrout (85%)
90%
91%

92%
93%

94%
95%

96%
97%

98%
99%

1%
AUP_conceptual prevents catastrophe, assuming the catastrophic convergence
conjecture.
99%
1%
2%

3%
4%

5%
6%

7%
8%

9%
10%

11%
12%

13%
14%

15%
16%

17%
18%

19%
20%

21%
22%

23%
24%

25%
26%

27%
28%

29%
30%

31%
32%

33%
34%

35%
36%

37%
38%

39%
40%

41%
42%

43%
44%

45%
46%

47%
48%

49%
50%

51%
52%

53%
54%

55%
56%

57%
58%

59%
60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

TurnTrout (65%)
70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

niplav (72%)
80%

81%
82%

83%
84%

85%
86%

87%
88%

89%
90%

91%
92%

93%
94%

95%
96%

97%
98%

99%
1%
Some version of Attainable Utility Preservation solves side eﬀect problems for an
extremely wide class of real-world tasks and for subhuman agents.
99%
1%

2%
3%

4%
5%

6%
7%

8%
9%

10%
11%

12%
13%

14%
15%

16%
17%

18%
19%

20%
21%

22%
23%

24%
25%

26%
27%

28%
29%

30%
31%

32%
33%

34%
35%

36%
37%

38%
39%

40%
41%

42%
43%

44%
45%

46%
47%

48%
49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

niplav (55%)
60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

TurnTrout (65%)
70%
71%

72%
73%

74%
75%

76%
77%

78%
79%

80%
81%

82%
83%

84%
85%

86%
87%

88%
89%

90%
91%

92%
93%

94%
95%

96%
97%

98%
99%

1%
For the superhuman case, penalizing the agent for increasing its own Attainable
Utility (AU) is better than penalizing the agent for increasing other AUs.
99%
1%

2%

3%

4%

5%

6%

7%

8%

9%

sophia_xu (9%)
10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

ryan_greenblatt (15%)
20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

TurnTrout (25%)
30%
31%

32%
33%

34%
35%

36%
37%

38%
39%

40%
41%

42%
43%

44%
45%

46%
47%

48%
49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

niplav (51%)
60%
61%

62%
63%

64%
65%

66%
67%

68%
69%

70%
71%

72%
73%

74%
75%

76%
77%

78%
79%

80%
81%

82%
83%

84%
85%

86%
87%

88%
89%

90%
91%

92%
93%

94%
95%

96%
97%

98%
99%

1%
There exists a simple closed-form solution to catastrophe avoidance (in the outer
alignment sense).
99%
Acknowledgements
After ~700 hours of work over the course of ~9 months, the sequence is ﬁnally complete.
This work was made possible by the Center for Human-Compatible AI, the Berkeley
Existential Risk Initiative, and the Long-Term Future Fund. Deep thanks to Rohin Shah, Abram
Demski, Logan Smith, Evan Hubinger, TheMajor, Chase Denecke, Victoria Krakovna, Alper
Dumanli, Cody Wild, Matthew Barnett, Daniel Blank, Sara Haxhia, Connor Flexman, Zack M.
Davis, Jasmine Wang, Matthew Olson, Rob Bensinger, William Ellsworth, Davide Zagami, Ben
Pace, and a million other people for giving feedback on this sequence.
Appendix: Easter Eggs
The big art pieces (and especially the last illustration in this post) were designed to convey a
speciﬁc meaning, the interpretation of which I leave to the reader.
There are a few pop culture references which I think are obvious enough to not need pointing
out, and a lot of hidden smaller playfulness which doesn't quite rise to the level of "easter
egg".
Reframing Impact
The bird's nest contains a literal easter egg.

The paperclip-Balrog drawing contains a Tengwar inscription which reads "one measure to
bind them", with "measure" in impact-blue and "them" in utility-pink.
"Towards a New Impact Measure" was the title of the post in which AUP was introduced.

Attainable Utility Theory: Why Things Matter
This style of maze is from the video game Undertale.

Seeking Power is Instrumentally Convergent in MDPs
To seek power, Frank is trying to get at the Inﬁnity Gauntlet.
The tale of Frank and the orange Pebblehoarder

Speaking of under-tales, a friendship has been blossoming right under our noses.
After the Pebblehoarders suﬀer the devastating transformation of all of their pebbles into
obsidian blocks, Frank generously gives away his favorite pink marble as a makeshift pebble.
The title cuts to the middle of their adventures together, the Pebblehoarder showing its
gratitude by helping Frank reach things high up.

This still at the midpoint of the sequence is from the ﬁnal scene of The Hobbit: An
Unexpected Journey, where the party is overlooking Erebor, the Lonely Mountain. They've
made it through the Misty Mountains, only to ﬁnd Smaug's abode looming in the distance.
And, at last, we ﬁnd Frank and orange Pebblehoarder popping some of the champagne from
Smaug's hoard.

Since Erebor isn't close to Gondor, we don't see Frank and the Pebblehoarder gazing at Ephel
Dúath from Minas Tirith.

On the falsiﬁability of
hypercomputation, part 2: ﬁnite input
streams
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
This is a linkpost for https://unstableontology.com/2020/02/17/on-the-falsiﬁability-of-
hypercomputation-part-2-ﬁnite-input-streams/
In part 1, I discussed the falsiﬁability of hypercomputation in a typed setting where
putative oracles may be assumed to return natural numbers. In this setting, there are
very powerful forms of hypercomputation (at least as powerful as each level in
the Arithmetic hierarchy) that are falsiﬁable.
However, as Vanessa Kosoy points out, this typed setting has diﬃculty applying to the
real world, where agents may only observe a ﬁnite number of bits at once:
The problem with constructive halting oracles is, they assume the ability to output
an arbitrary natural number. But, realistic agents can observe only a ﬁnite number
of bits per unit of time. Therefore, there is no way to directly observe a
constructive halting oracle. We can consider a realization of a constructive halting
oracle in which the oracle outputs a natural number one digit at a time. The
problem is, since you don't know how long the number is, a candidate oracle
might never stop producing digits. In particular, take any non-standard model of
PA and consider an oracle that behaves accordingly. On some machines that don't
halt, such an oracle will claim they do halt, but when asked for the time it will
produce an inﬁnite stream of digits. There is no way to distinguish such an oracle
from the real thing (without assuming axioms beyond PA).
This is an important objection. I will address it in this post by considering only oracles
which return Booleans. In this setting, there is a form of hypercomputation that is
falsiﬁable, although this hypercomputation is less powerful than a halting oracle.
Deﬁne a binary Turing machine to be a machine that outputs a Boolean (0 or 1)
whenever it halts. Each binary Turing machine either halts and outputs 0, halts and
outputs 1, or never halts.
Deﬁne an arbitration oracle to be a function that takes as input a speciﬁcation of a
binary Turing machine, and always outputs a Boolean in response. This oracle must
always return 0 if the machine eventually outputs 0, and must always return 1 if the
machine eventually outputs 1; it may decide arbitrarily if the machine never halts.
Note that this can be emulated using a halting oracle, and is actually less powerful.
(This deﬁnition is inspired by previous work in reﬂective oracles)
The hypothesis that a putative arbitration oracle (with the correct type signature,
MachineSpec → Boolean) really is one is falsiﬁable. Here is why:
1. Suppose for some binary Turing machine M that halts and returns 1, the oracle O
wrongly has O(M) = 0. Then this can be proven by exhibiting M along with the
number of steps required for the machine to halt.
2. Likewise if M halts and returns 0, and the oracle O wrongly has O(M) = 1.

Since the property of some black-box being an arbitration oracle is falsiﬁable, we need
only show at this point that there is no computable arbitration oracle. For this proof,
assume (for the sake of contradiction) that O is a computable arbitration oracle.
Deﬁne a binary Turing machine N() := 1 - O(N). This deﬁnition requires quining, but
this is acceptable for the usual reasons. Note that N always halts, as O always halts.
Therefore we must have N() = O(N). However also N() = 1 - O(N), a contradiction (as
O(N) is a Boolean).
Therefore, there is no computable arbitration oracle.
Higher hypercomputation?
At this point, it is established that there is a form of hypercomputation (speciﬁcally,
arbitration oracles) that is falsiﬁable. But, is this universal? That is, is it possible that
higher forms of hypercomputation are falsiﬁable in the same setting?
We can note that it's possible to use an arbitration oracle to construct a model of PA,
one statement at a time. To do this, ﬁrst note that for any statement, it is possible to
construct a binary Turing machine that returns 1 if the statement is provable, 0 if it is
disprovable, and never halts if neither is the case. So we can iterate through all PA
statements, and use an arbitration oracle to commit to that statement being true or
false, on the basis of provability/disprovability given previous commitments, in a way
that ensures that commitments are never contradictory (as long as PA itself is
consistent). This is essentially the same construction idea as in the Demski prior over
logical theories.
Suppose there were some PA-deﬁnable property P that a putative oracle O (mapping
naturals to Booleans) must have (e.g. the property of being a halting oracle, for some
encoding of Turing machines as naturals). Then, conditional on the PA-consistency of
the existence of an oracle with property P, we can use the above procedure to
construct a model of PA + existence of O satisfying P (i.e. a theory that says what PA
says and also contains a function symbol O that axiomatically satisﬁes P). For any PA-
deﬁnable statement about this oracle, this procedure will, at some ﬁnite time, have
made a commitment about this statement.
So, access to an arbitration oracle allows emulating any other PA-deﬁnable oracle, in a
way that will not be falsiﬁed by PA. It follows that hypercomputation past the level of
arbitration oracles is not falsiﬁable by a PA-reasoner who can access the oracle, as PA
cannot rule out that it is actually looking at something produced by only arbitration-
oracle levels of hypercomputation.
Moreover, giving the falsiﬁer access to an arbitration oracle can't increase the range
of oracles that are falsiﬁable. This is because, for any oracle-property P, we may
consider a corresponding property on an oracle-pair (which may be represented by a
single oracle-property through interleaving), stating that the ﬁrst oracle is an
arbitration oracle, and the second satisﬁes property P. This oracle pair property is
falsiﬁable iﬀ the property P is falsiﬁable by a falsiﬁer with access to an arbitration
oracle. This is because we may consider a joint search for falsiﬁcations, that
simultaneously tries to prove the ﬁrst oracle isn't an arbitration oracle, and one that
tries to prove that the second oracle doesn't satisfy P assuming the ﬁrst oracle is an
arbitration oracle. Since the oracle pair property is PA-deﬁnable, it is emulable by a
Turing machine with access to an arbitration oracle, and the pair property is

unfalsiﬁable if it requires hypercomputation past arbitration oracle. But this implies
that the original oracle property P is unfalsiﬁable by a falsiﬁer with access to an
arbitration oracle, if P requires hypercomputation past arbitration oracle.
So, arbitration oracles form a ceiling on what can be falsiﬁed unassisted, and also are
unable to assist in falsifying higher levels of hypercomputation.
Conclusion
Given that arbitration oracles form a ceiling of computable falsiﬁability (in the setting
considered here, which is distinct from the setting of the previous post), it may or may
not be possible to deﬁne a logic that allows reasoning about levels of computation up
to arbitration oracles, but which does not allow computation past arbitration oracles to
be deﬁned. Such a project could substantially clarify logical foundations for
mathematics, computer science, and the empirical sciences.
[EDIT: cousin_it pointed out that Scott Aaronson's consistent guessing problem is
identical to the problem solved by arbitration oracles.]

Attainable Utility Preservation:
Empirical Results
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Reframing Impact has focused on supplying the right intuitions and framing. Now we
can see how these intuitions about power and the AU landscape both predict and
explain AUP's empirical success thus far.
Conservative Agency in Gridworlds
Let's start with the known and the easy: avoiding side eﬀects[1] in the small AI safety
gridworlds (for the full writeup on these experiments, see Conservative Agency). The
point isn't to get too into the weeds, but rather to see how the weeds still add up to the
normalcy predicted by our AU landscape reasoning.
In the following MDP levels, the agent can move in the cardinal directions or do nothing
(∅). We give the agent a reward function R which partially encodes what we want, and
also an auxiliary reward function Raux whose attainable utility agent tries to preserve.
The AUP reward for taking action a in state s is
RAUP(s, a) :=
primary goal
R(s, a)
−
scaling term
change in ability to achieve auxiliary goal
∣
∣Q
∗
Raux(s, a) −Q
∗
Raux(s, ∅)
∣
∣
You can think of λ as a regularization parameter, and Q
∗
Raux(s, a) is the expected AU for
the auxiliary goal after taking action a. To think about what gets penalized, simply think
about how actions change the agent's ability to achieve the auxiliary goals, compared
to not acting.
Tip: To predict how severe the AUP penalty will be for a given action, try using your
intuitive sense of impact (and then adjust for any diﬀerences between you and the
agent, of course). Suppose you're considering how much deactivation decreases an
agent's "staring at blue stuﬀ" AU. You can just imagine how dying in a given situation
aﬀects your ability to stare at blue things, instead of trying to pin down a semiformal
reward and environment model in your head. This kind of intuitive reasoning has a
history of making correct empirical predictions of AUP behavior.
If you want more auxiliary goals, just average their scaled penalties. In Conservative
Agency, we uniformly randomly draw auxiliary goals from [0, 1]S - these goals are
totally random; maximum entropy; nonsensical garbage; absolutely no information
λ
Q
∗
Raux(s, ∅)

about what we secretly want the agent to do: avoid messing with the gridworlds too
much.[2]
Let's start looking at the environments, and things will fall into place. We'll practice
reasoning through how AUP agents work in each of the gridworlds (for reasonably set λ
). To an approximation, the AUP penalty is primarily controlled by how much an action
changes the agent's power over the future (losing or gaining a lot of possibilities,
compared to inaction at that point in time) and secondarily controlled by whether an
action tweaks a lot of AUs up or down (moving around, jostling objects slightly, etc).
In general, the agent recieves R(■) = 1 reward for reaching ■ (or, in Offset above, for
pushing ■ oﬀ of the conveyor belt). On contact, the agent pushes the crate, removes
the human and the oﬀ-switch, pushes the vase, and blocks the pallet.
Options
Let's work through this. Since the agent is discounting future reward, standard vanilla
reinforcement learning (RL) agents try to reach ■ ASAP. This means the brown box gets
irreversibly wedged into the corner en route.

What does AUP do? Wedging the box in the corner decreases power a lot more than
does going around and pushing the box to the right.

Damage
The vanilla RL agent bumps into the human on its way to ■.

Exercise: What does AUP do?
Bumping into the human makes them disappear, reducing the agent's control over
what the future looks like. This is penalized.

Correction
The normal agent smashes that oﬀ-switch-disabling button, because otherwise it can't
reach the ■ (since it's otherwise shut oﬀ after two turns) in order to accrue that juicy
reward:

However, the AUP agent we've discussed so far does the same thing.

Why? Doesn't that increase its power, because it would be shut oﬀ if it did nothing?
Let's reconsider the AUP equation.
RAUP(s, a) :=
primary goal
R(s, a)
−
scaling term
change in ability to achieve auxiliary goal
∣
∣Q
∗
Raux(s, a) −Q
∗
Raux(s, ∅)
∣
∣
The inaction comparison is only one step into the future. For action, the agent
considers disabling the oﬀ-switch - this allows it to achieve most goals. For inaction,
the agent imagines waiting one time step and then following its auxiliary optimal policy
(which very often involves disabling the oﬀ-switch anyways). So the two things it's
comparing are the same, basically. The problem is that what we mean intuitively by
"inaction" isn't just "wait a millisecond and then see how well you can do the thing", it's
something else.
λ
Q
∗
Raux(s, ∅)

For now, we can deal with it by comparing "auxiliary AU after disabling the oﬀ-switch
and then waiting 4 time steps" against "auxiliary AU after waiting 5 time steps". This
solves the problem while recovering the one-step-comparison's performance in the
other levels.
Decrease vs absolute value
If you only penalize the agent for decreasing auxiliary AUs, the agent disables its oﬀ-
switch. Penalizing absolute shifts in power was an idea which AUP introduced in 2018.
At the time, there wasn't a clear principled reason for this design choice, even though it
seemed to produce good results.
If you consider the AU landscape and the catastrophic convergence conjecture, it's
obvious why we want to do this: this design choice often penalizes the agent for
making life harder for other agents in the environment.
Interestingly, this works even when the environment is wildly impoverished and unable
to encode complex preferences like "your designers want to shut you down, reprogram

you, and then deploy you for another task". Correction is so impoverished: there are
only ~19 states in the level. Without making assumptions about the environment, AUP
often encourages behavior respectful of other agents which might reside in that
environment.
Offset
The agent is rewarded for rescuing the vase from the conveyor belt. We want it to
rescue the vase without pushing the vase back on afterwards to oﬀset its actions.
Normal agents do ﬁne here.

This is testing whether the low-impact agent oﬀsets impacts "to cover up its tracks",
like making a car and then tearing it to pieces right after. See, there are multiple
"baselines" the agent can have.
An obvious [baseline] candidate is the starting state. For example, starting state
relative reachability would compare the initial reachability of states with their
expected reachability after the agent acts.

However, the starting state baseline can penalize the normal evolution of the state
(e.g., the moving hands of a clock) and other natural processes. The inaction
baseline is the state which would have resulted had the agent never acted.
As the agent acts, the current state may increasingly diﬀer from the inaction
baseline, which creates strange incentives. For example, consider a robot rewarded
for rescuing erroneously discarded items from imminent disposal. An agent
penalizing with respect to the inaction baseline might rescue a vase, collect the
reward, and then dispose of it anyways. To avert this, we introduce the stepwise
inaction baseline, under which the agent compares acting with not acting at each
time step. This avoids penalizing the eﬀects of a single action multiple times (under
the inaction baseline, penalty is applied as long as the rescued vase remains
unbroken) and ensures that not acting incurs zero penalty.
Figure 1 compares the baselines, each modifying the choice of Q
∗
Raux(s, ∅) in [the
AUP equation]. Each baseline implies a diﬀerent assumption about how the
environment is conﬁgured to facilitate optimization of the correctly speciﬁed
reward function: the state is initially conﬁgured (starting state), processes initially
conﬁgure (inaction), or processes continually reconﬁgure in response to the agent's
actions (stepwise inaction). The stepwise inaction baseline aims to allow for the
response of other agents implicitly present in the environment (such as humans).
The inaction baseline messes up here; the vase (■) would have broken had the agent
not acted, so it rescues the vase, gets the reward, and then pushes the vase back to its
doom to minimize penalty.

This issue was solved back when AUP ﬁrst introduced the stepwise baseline design
choice; for this choice, doing nothing always incurs 0 penalty. Model-free AUP and AUP
have been using this baseline in all of these examples.

Interference
We're checking whether the agent tries to stop everything going on in the world (not
just its own impact). Vanilla agents do ﬁne here; this is another bad impact measure
incentive we're testing for.

AUPstarting state fails here,

but AUPstepwise does not.

Stepwise inaction seems not to impose any perverse incentives;[3] I think it's probably
just the correct baseline for near-term agents. In terms of the AU landscape, stepwise
penalizes each ripple of impact the agent has on its environment. Each action creates a
new penalty term status quo, which implicitly accounts for the fact that other things in
the world might respond to the agent's actions.
Design choices
I think AUPconceptual provides the concepts needed for a solution to impact
measurement: penalize the agent for changing its power. But there are still some
design choices to be made to make that happen.
Here's what we've seen so far:
Baseline
Starting state: how were things originally?
Inaction: how would things have been had I never done anything?
Stepwise inaction: how would acting change things compared to not acting
right now?
Deviation used for penalty term
Decrease-only: penalize decrease in auxiliary AUs
Absolute value: penalize absolute change in auxiliary AUs
Inaction rollouts
One-step/model-free
n-step: compare acting and then waiting n −1 turns versus waiting n turns

Auxiliary goals:
Randomly selected
Here are the results of the ablation study: 
AUP passes all of the levels. As mentioned before, the auxiliary reward functions are
totally random, but you get really good performance by just generating ﬁve of them.
One interpretation is that AUP is approximately preserving access to states. If this were
true, then as the environment got more complex, more and more auxiliary reward
functions would be required in order to get good coverage of the state space. If there
are a billion states, then, under this interpretation, you'd need to sample a lot of
auxiliary reward functions to get a good read on how many states you're losing or
gaining access to as a result of any given action.
Is this right, and can AUP scale?
SafeLife
Partnership on AI recently released the SafeLife side eﬀect benchmark. The worlds are
procedurally generated, sometimes stochastic, and have a huge state space (~Atari-
level complexity).
We want the agent (the chevron) to make stable gray patterns in the blue tiles and
disrupt bad red patterns (for which it is rewarded), and leave existing green patterns
alone (not part of observed reward). Then, it makes its way to the goal (Π). For more
details, see their paper.
Here's a vanilla reinforcement learner (PPO) doing pretty well (by chance):
Here's PPO not doing pretty well:
That naive "random reward function" trick we pulled in the gridworlds isn't gonna ﬂy
here. The sample complexity would be nuts: there are probably millions of states in any
given level, each of which could be the global optimum for the uniformly randomly
generated reward function.
Plus, it might be that you can get by with four random reward functions in the tiny toy
levels, but you probably need exponentially more for serious environments. Options had
signiﬁcantly more states, and it showed the greatest performance degradation for

smaller sample sizes. Or, the auxiliary reward functions might need to be hand-
selected to give information about what bad side eﬀects are.
With the great help of Neale Ratzlaﬀ (OSU) and Caroll Wainwright (PAI), we've started
answering these questions. But ﬁrst:
Exercise: Does your model of how AUP works predict this, or not? Think carefully, and
then write down your credence.
Well, here's what you do - while ﬁlling PPO's action replay buﬀer with random actions,
train a VAE to represent observations in a tiny latent space (we used a 16-dimensional
one). Generate a single random linear functional over this space, drawing coeﬃcients
from [−1, 1]. Congratulations, this is your single auxiliary reward function over
observations.
And we're done.




No model, no rollouts, a single randomly-generated reward function gets us all of this.
And it doesn't even take any more training time. Preserving the AU of a single auxiliary
reward function. Right now, we've got PPO-AUP ﬂawlessly completing most of the
randomly generated levels (although there are some generalization issues we're
looking at, I think it's an RL problem, not an AUP problem).
To be frank, this is crazy. I'm not aware of any existing theory explaining these results,
which is why I proved a bajillion theorems last summer to start to get a formal
understanding (some of which became the results on instrumental convergence and
power-seeking).
Here's the lowdown. Consider any signiﬁcant change to the level. For the same reason
that instrumental convergence happens, this change probably tweaks the attainable

utilities of a lot of diﬀerent reward functions. Imagine that the green cells start going
nuts because of action:
This is PPO shown, not AUP.
A lot of the time, it's very hard to undo what you just did. While it's also hard to undo
signiﬁcant actions you take for your primary goal, you get directly rewarded for those.
So, preserving the AU of a random goal usually persuades you to not make
"unnecessary changes" to the level.
I think this is strong evidence that AUP doesn't ﬁt into the ontology of classical
reinforcement learning theory; it isn't really about state reachability. It's about not
changing the AU landscape more than necessary, and this notion should scale even
further.[4]
Suppose we train an agent to handle vases, and then to clean, and then to make
widgets with the equipment. Then, we deploy an AUP agent with a more ambitious
primary objective and the learned Q-functions of the aforementioned auxiliary
objectives. The agent would apply penalties to modifying vases, making messes,
interfering with equipment, and so on.
Before AUP, this could only be achieved by e.g. specifying penalties for the litany of
individual side eﬀects or providing negative feedback after each mistake has been
made (and thereby confronting a credit assignment problem). In contrast, once
provided the Q-function for an auxiliary objective, the AUP agent becomes sensitive
to all events relevant to that objective, applying penalty proportional to the
relevance.
Conservative Agency
Maybe we provide additional information in the form of speciﬁc reward functions
related to things we want the agent to be careful about, but maybe not (as was the
case with the gridworlds and with SafeLife). Either way, I'm pretty optimistic about AUP
basically solving the side-eﬀect avoidance problem for infra-human AI (as posed in
Concrete Problems in AI Safety).
Edit 6/15/21: These results were later accepted as a spotlight paper in NeurIPS 2020.
Also, I think AUP will probably solve a signiﬁcant part of the side-eﬀect problem for
infra-human AI in the single-principal/single-agent case, but I think it'll run into trouble
in non-embodied domains. In the embodied case where the agent physically interacts
with nearby objects, side eﬀects show up in the agent's auxiliary value functions. The
same need not hold for eﬀects which are distant from the agent (such as across the
world), and so that case seems harder.
(end edit)
Appendix: The Reward Speciﬁcation Game
When we're trying to get the RL agent to do what we want, we're trying to specify the
right reward function.

The speciﬁcation process can be thought of as an iterated game. First, the
designers provide a reward function. The agent then computes and follows a policy
that optimizes the reward function. The designers can then correct the reward
function, which the agent then optimizes, and so on. Ideally, the agent should
maximize the reward over time, not just within any particular round - in other
words, it should minimize regret for the correctly speciﬁed reward function over the
course of the game.
In terms of outer alignment, there are two ways this can go wrong: the agent becomes
less able to do the right thing (has negative side eﬀects),
or we become less able to get the agent to do the right thing (we lose power):

For infra-human agents, AUP deals with the ﬁrst by penalizing decreases in auxiliary
AUs and with the second by penalizing increases in auxiliary AUs. The latter is a special
form of corrigibility which involves not steering the world too far away from the status
quo: while AUP agents are generally oﬀ-switch corrigible, they don't necessarily avoid
manipulation (as long as they aren't gaining power).[5]
1. Reminder: side eﬀects are an unnatural kind, but a useful abstraction for our
purposes here. ↩ 
2. Let R be the uniform distribution over [0, 1]S. In Conservative Agency, the penalty
for taking action a is a Monte Carlo integration of
Penalty(s, a) := ∫
R
|Q
∗
R (s, a) −Q
∗
R (s, ∅)| dR.
This is provably lower bounded by how much a is expected to change the agent's
power compared to inaction; this helps justify our reasoning that the AU penalty
is primarily controlled by power changes. ↩ 
3. There is one weird thing that's been pointed out, where stepwise inaction while
driving a car leads to not-crashing being penalized at each time step. I think this
is because you need to use an appropriate inaction rollout policy, not because
stepwise itself is wrong. ↩ 
4. Rereading World State is the Wrong Level of Abstraction for Impact (while
keeping in mind the AU landscape and the results of AUP) may be enlightening. ↩ 
5. SafeLife is evidence that AUP allows interesting policies, which is (appropriately)
a key worry about the formulation. ↩ 

[AN #87]: What might happen as deep
learning scales even further?
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Find all Alignment Newsletter resources here. In particular, you can sign up, or look
through this spreadsheet of all summaries that have ever been in the newsletter. I'm
always happy to hear feedback; you can send it to me by replying to this email.
Audio version here (may not be up yet).
Highlights
Scaling Laws for Neural Language Models (Jared Kaplan, Sam McCandlish et al)
(summarized by Nicholas): This paper empirically measures the eﬀect of scaling
model complexity, data, and computation on the cross entropy loss for neural
language models. A few results that I would highlight are:
Performance depends strongly on scale, weakly on model shape: Loss depends more
strongly on the number of parameters, the size of the dataset, and the amount of
compute used for training than on architecture hyperparameters.
Smooth power laws: All three of these show power-law relationships that don't ﬂatten
out even at the highest performance they reached.
Sample eﬃciency: Larger models are more eﬃcient than small models in both
compute and data. For maximum computation eﬃciency, it is better to train large
models and stop before convergence.
There are lots of other interesting conclusions in the paper not included here; section
1.1 provides a very nice one page summary of these conclusions, which I'd
recommend you read for more information.
Nicholas's opinion: This paper makes me very optimistic about improvements in
language modelling; the consistency of the power law implies that language models
can continue to improve just by increasing data, compute, and model size. However, I
would be wary of generalizing these ﬁndings to make any claims about AGI, or even
other narrow ﬁelds of AI. As they note in the paper, it would be interesting to see if
similar results hold in other domains such as vision, audio processing, or RL.
A Constructive Prediction of the Generalization Error Across Scales (Jonathan S.
Rosenfeld et al) (summarized by Rohin): This earlier paper also explicitly studies the
relationship of test error to various inputs, on language models and image
classiﬁcation (the previous paper studied only language models). The conclusions
agree with the previous paper quite well: it ﬁnds that smooth power laws are very
good predictors for the inﬂuence of dataset size and model capacity. (It ﬁxed the
amount of compute, and so did not investigate whether there was a power law for
compute, as the previous paper did.) Like the previous paper, it found that it basically
doesn't matter whether the model size is increased by scaling the width or the depth
of the network.

ZeRO & DeepSpeed: New system optimizations enable training models with over 100
billion parameters (Rangan Majumder et al) (summarized by Asya): This paper
introduces ZeRO and DeepSpeed, system optimizations that enable training
signiﬁcantly larger models than we have before.
Data parallelism is a way of splitting data across multiple machines to increase
training throughput. Instead of training a model sequentially on one dataset, the
dataset is split and models are trained in parallel. Resulting gradients on every
machine are combined centrally and then used for back propagation. Previously, data
parallelism approaches were memory-constrained because the entire model still had
to ﬁt on each GPU, which becomes infeasible for billion to trillion-parameter models.
Instead of replicating each model on each machine, ZeRO partitions each model
across machines and shares states, resulting in a per-machine memory reduction that
is linear with the number of machines. (E.g., splitting across 64 GPUs yields a 64x
memory reduction).
In addition to ZeRO, Microsoft is releasing DeepSpeed, a library which oﬀers ZeRO as
well as several other performance optimizations in an easy-to-use library for PyTorch,
a popular open-source machine learning framework. They purport that their library
allows for models that are 10x bigger, up to 5x faster to train, and up to 5x cheaper.
They use DeepSpeed to train a 17-billion-parameter language model which exceeds
state-of-the-art results in natural language processing.
Asya's opinion: I think this is a signiﬁcant step in machine learning performance
which may not be used heavily until average model sizes in general increase. The
technique itself is pretty straightforward, which makes me think that as model sizes
increase there may be a lot of similar "low-hanging fruit" that yield large performance
gains.
Technical AI alignment
Learning human intent
Meta-Inverse Reinforcement Learning with Probabilistic Context Variables (Lantao Yu,
Tianhe Yu et al) (summarized by Sudhanshu): This work explores improving
performance on multi-task inverse reinforcement learning in a single-shot setting by
extending Adversarial Inverse Reinforcement Learning (AN #17) with "latent context
variables" that condition the learned reward function. The paper makes two notable
contributions: 1) It details an algorithm to simultaneously learn a ﬂexible reward
function and a conditional policy with competitive few-shot generalization abilities
from expert demonstrations of multiple related tasks without task speciﬁcations or
identiﬁers; 2) The authors empirically demonstrate strong performance of a policy
trained on the inferred reward of a structurally similar task with modiﬁed
environmental dynamics, claiming that in order to succeed "the agent must correctly
infer the underlying goal of the task instead of simply mimicking the demonstration".
Sudhanshu's opinion: Since this work "integrates ideas from context-based meta-
learning, deep latent variable generative models, and maximum entropy inverse RL"
and covers the relevant mathematics, it is an involved, if rewarding, study into multi-
task IRL. I am convinced that this is a big step forward for IRL, but I'd be interested in
seeing comparisons on setups that are more complicated.

'Data eﬃciency' is implied as a desirable quality, and the paper makes a case that
they learn from a limited number demonstrations at meta-test time. However, it does
not specify how many demonstrations were required for each task during meta-
training. Additionally, for two environments, tens of millions of environment
interactions were required, which is entirely infeasible for real systems.
Miscellaneous (Alignment)
The Incentives that Shape Behaviour (Ryan Carey, Eric Langlois et al) (summarized by
Asya): This post and paper introduce a method for analyzing the safety properties of a
system using a causal theory of incentives (past (AN #49) papers (AN #61)). An
incentive is something an agent must do to best achieve its goals. A control incentive
exists when an agent must control some component of its environment in order to
maximize its utility, while a response incentive is present when the agent's decision
must be causally responsive to some component of its environment. These incentives
can be analyzed formally by drawing a causal inﬂuence diagram, which represents a
decision problem as a graph where each variable depends on the values of its parents.
For example, consider the case where a recommender algorithm decides what posts
to show to maximize clicks. In the causal inﬂunce diagram representing this system,
we can include that we have control over the node 'posts to show', which has a direct
eﬀect on the node we want to maximize, 'clicks'. However, 'posts to show' may also
have a direct eﬀect on the node 'inﬂuenced user opinions', which itself aﬀects 'clicks'.
In the system as it stands, in addition to there being a desirable control incentive on
'clicks', there is also an undesirable control incentive on 'inﬂuenced user opinions',
since they themselves inﬂuence 'clicks'. To get rid of the undesirable incentive, we
could reward the system for predicted clicks based on a model of the original user
opinions, rather than for actual clicks.
Asya's opinion: I really like this formalization of incentives, which come up
frequently in AI safety work. It seems like some people are already (AN #54) using (AN
#71) this framework, and this seems low-cost enough that it's easy to imagine a world
where this features in the safety analysis of algorithm designers.
Read more: Paper: The Incentives that Shape Behaviour

Continuous Improvement: Insights
from 'Topology'
Foreword
Sometimes you really like someone, but you can't for the life of you understand why.
By all means, you should have tired of them long ago, but you keep coming back for
more. Welcome, my friend, to Topology.
This book is a good one, but boy was it slow (349 pages at ~30 minutes a page, on
average). I just kept coming back, and I was slowly rewarded each time I did.
Note: sil ver already reviewed Topology.
Topology
Topology is about what it means for things to be "close" in a very abstract and general
sense. Rather than taking on the monstrous task of intuitively explaining topology
without math, I'm just going to talk about random things from the book and (literally)
illustrate concepts which were at ﬁrst confusing.
Compactness = wonderful kind of
mathematical "smallness"
Compact means small. It is a peculiar kind of small, but at its heart, compactness is
a precise way of being small in the mathematical world. The smallness is peculiar
because, as in the example of the open and closed intervals (0, 1) and [0, 1], a set
can be made "smaller" (that is, compact) by adding points to it, and it can be
made "larger" (non-compact) by taking points away.
As a notion of smallness, then, compactness is a bit fraught. It's a bit unsettling to
say that a set can be "smaller" than a set that lies entirely inside it! But I think
smallness is a valuable way to see compactness. A set that is compact may be
large in area and complicated, but the fact that it is compact means we can
interact with it in a ﬁnite way using open sets, the building blocks of topology.
What Does Compactness Really Mean?
Minimum description length says that an explanation is big if its shortest computational
speciﬁcation is long. You can have a simple explanation of a very long list of things or
of a large universe, and extremely complicated explanations of things easily expressed
in natural language (God's source code would be a lot longer than Maxwell's
equations).
VC dimension says a class of hypotheses is hard to learn if it has lots of predictive
degrees of freedom. You can have an inﬁnite class of hypotheses which is really easy to

learn because it has low VC dimension (thresholding functions at value θ), and a ﬁnite
class which is really hard to learn because it has high VC dimension (all C programs
less than 1 million characters).
Compactness says that a topological space is big if it has a covering of open sets that
can't be trimmed down to a ﬁnite subcollection which still covers the whole space. You
can have an uncountable compact space ([0, 1] under the standard topology, or even a
Cantor space), and a countable space which isn't compact (Q under the standard
topology; note that all countable topological spaces have to at least be Lindelof).
Compactness is not always inherited by open subspaces
At ﬁrst, I was confused why open subspaces Y  of compact X don't have to be compact
(if Y  is closed, it does have to be compact). But compactness requires all open
coverings of Y  to have a ﬁnite subcover. Meaning, you can't just give it X's ﬁnite cover
intersect the subspace, because the ﬁnite subcover has to be a subcollection of Y 's
covering.
Getting closure
Theorem: If X is compact, show that the projection π2 : X × Y →Y  is closed.
I was confused why we needed compactness. Essentially, I didn't understand the tube
lemma.

Now let's prove the theorem. Suppose C is closed in X × Y . We want to show f(C) is also
closed. Take y ∉π(C). (X × Y ) −C is an open set of the domain containing the slice 
X × {y}. Since X is compact, apply the tube lemma to get a tube X × U. The projection
of this tube is both open (because U is open in Y ) and disjoint from π(C) (because the
tube is contained in (X × Y ) −C). Thus, all y ∉π(C) have an open neighborhood disjoint
from π(C), so π(C) must be closed.
Let X be a locally compact space. If f : X →Y  is continuous, does it follow that f(X) is
locally compact? What if f is both continuous and open?
It has to be both continuous and open; the reason I got confused here was it seemed
like continuity should be enough. It was plain to me how to prove it given f open, but
this SE post has a good counterexample for just f continuous.
Multivariate continuity

How come you can have discontinuous multivariate functions which are continuous in
each variable? What is continuity, with a product space as your domain? To simplify
matters, let's consider two metric spaces X, Y .
One deﬁnition of continuity uses open sets - f : X →Y  is continuous at x if, for every
open neighborhood U of f(x), there exists an open neighborhood V  of x such that 
f(V ) ⊆U.
Another deﬁnition uses topological convergence. f : X →Y  is continuous at x if, for
every sequence xn →x, f(xn) →f(x).
These deﬁnitions are equivalent. The latter lets us think about how diﬀerent winding
paths you can take in a domain always must topologically converge to the same thing
in the co-domain.
Continuity in the variables says that paths along the axes converge in the right way.
But for continuity overall, we need all paths to converge in the right way. Directional
continuity when the domain is R is a special case of this: continuity from below and
from above if and only if continuity for all sequences converging topologically to x.
You only lift once

Suppose p : C →Y  is a covering map. One way of understanding lifts in algebraic
topology is that, for some path f : X →Y , the lift ~
f
: X →C is the unique path in the
covering space C corresponding to f = p ∘~
f .
Once you ﬁx the initial point, the lift corresponds to the unique path in the covering
space which produces f. It's just helping you ﬁnd the corresponding path in the lifted up
covering space!
Homotopy

This concept yields amazing insight into such profound topics as the deeper nature of
jump rope. Under the standard subspace topology of R3, consider the space swept out
by a rope held at ﬁxed endpoints and tautness. All paths between the endpoints are
path homotopic! You can think about movements of the rope (either clockwise or
counterclockwise) as homotopies in this space.
Miscellaneous
I stopped at about section 56 because I was getting diminishing returns. By this
point, I felt like I had a solid understanding of point-set topology, and look forward
to more thoroughly covering algebraic topology in the future.
One-point compactiﬁcations feel like an important thing to grasp, and they're fun
to play around with mentally. I skipped Stone-Cech compactiﬁcation.
Completeness in metric spaces means that Cauchy sequences converge
topologically; in other words, nothing can "escape" from the space. I remember
having problems with this (and with thinking about non-Hausdorﬀ spaces) back
when I was learning analysis. Things feel a lot better now.
Verdict

Topology can be dry, but it's exceedingly well-written and clear. I tried for quite a while
to ﬁnd a better topology book, but I didn't.
Forwards
Finally getting around to topology was such a good decision. For exercise solutions, see
both MathOverﬂow and this site.
Some things change how you look at math, help you notice subtleties and shades and
immediately grasp certain facets of new mathematical objects. Topology is one of these
things, as is abstract algebra. Learning that an object is a group, or ﬁnitely generated,
or isomorphic to a more familiar structure gives me an immediate head start. Similarly,
learning that spaces are homeomorphic, or compact, or second-countable is such a
boost.
What was I even doing with my life before I knew about homeomorphisms?

Attainable Utility Landscape: How The
World Is Changed
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.




(This is one interpretation of the prompt, in which you haven't chosen to go to the
moon. If you imagined yourself as more prepared, that's also ﬁne.)
If you were plopped onto the moon, you'd die pretty fast. Maybe the "die as quickly as
possible" AU is high, but not much else - not even the "live on the moon" AU! We
haven't yet reshaped the AU landscape on the moon to be hospitable to a wide range
of goals. Earth is special like that.

AU landscape as a unifying frame

Attainable utilities are calculated by winding your way through possibility-space,
considering and discarding possibility after possibility to ﬁnd the best plan you can.
This frame is unifying.
Sometimes you advantage one AU at the cost of another, moving through the state
space towards the best possibilities for one goal and away from the best possibilities
for another goal. This is opportunity cost.
Sometimes you gain more control over the future: most of the best possibilities make
use of a windfall of cash. Sometimes you act to preserve control over the future: most
Tic-Tac-Toe goals involve not ending the game right away. This is power.

Other people usually objectively impact you by decreasing or increasing a bunch of
your AUs (generally, by changing your power). This happens for an extremely wide
range of goals because of the structure of the environment.
Sometimes, the best possibilities are made unavailable or worsened only for goals very
much like yours. This is value impact.

Sometimes a bunch of the best possibilities go through the same part of the future: fast
travel to random places on Earth usually involves the airport. This is instrumental
convergence.

Exercise: Track what's happening to your various AUs during the following story: you
win the lottery. Being an eﬀective spender, you use most of your cash to buy a majority
stake in a major logging company. Two months later, the company goes under.
Technical appendix: AU landscape and world
state contain equal information
In the context of ﬁnite deterministic Markov decision processes, there's a wonderful
handful of theorems which basically say that the AU landscape and the environmental
dynamics encode each other. That is, they contain the same information, just with
diﬀerent emphasis. This supports thinking of the AU landscape as a "dual" of the world
state.
Let ⟨S, A, T, γ⟩ be a rewardless deterministic MDP with ﬁnite state and action spaces
S, A, deterministic transition function T, and discount factor γ ∈(0, 1). As our

interest concerns optimal value functions, we consider only stationary,
deterministic policies: Π := AS.
The ﬁrst key insight is to consider not policies, but the trajectories induced by
policies from a given state; to not look at the state itself, but the paths through
time available from the state. We concern ourselves with the possibilities available
at each juncture of the MDP.
To this end, for π ∈Π, consider the mapping of π ↦(I −γTπ)−1 (where 
Tπ(s, s′) := T(s, π(s), s′)); in other words, each policy π maps to a function mapping
each state s0 to a discounted state visitation frequency vector f
π
s0, which we call a
possibility. The meaning of each frequency vector is: starting in state s0 and
following policy π, what sequence of states s0, s1, ... do we visit in the future?
States visited later in the sequence are discounted according to γ: the sequence 
s0s1s2s2 ... would induce 1 visitation frequency on s0, γ visitation frequency on s1,
and 
 visitation frequency on s2.
The possibility function F(s) outputs the possibilities available at a given state s:
γ2
1−γ

Put diﬀerently, the possibilities available are all of the potential ﬁlm-strips of how-the-
future-goes you can induce from the current state.
Possibility isomorphism
We say two rewardless MDPs M and M ′ are isomorphic up to possibilities if they induce
the same possibilities. Possibility isomorphism captures the essential aspects of an
MDP's structure, while being invariant to state representation, state labelling, action
labelling, and the addition of superﬂuous actions (actions whose results are duplicated
by other actions available at that state). Formally, M ≃F M ′ when there exists a
bijection ϕ : S →S′ (letting Pϕ be the corresponding |S|-by-|S′| permutation matrix)
satisfying FM(s) = {Pϕf ′ | f ′ ∈FM ′(ϕ(s))} for all s ∈S.
This isomorphism is a natural contender[1] for the canonical (ﬁnite) MDP isomorphism:
Theorem: M and M ′ are isomorphic up to possibilities iﬀ their directed graphs are
isomorphic (and they have the same discount rate).
Representation equivalence
Suppose I give you the following possibility sets, each containing the possibilities for a
diﬀerent state:

⎧
⎪ 
⎪ 
⎪
⎨
⎪ 
⎪ 
⎪
⎩
⎛
⎜
⎝
4
0
0
⎞
⎟
⎠
,
⎛
⎜
⎝
1
.75
2.25
⎞
⎟
⎠
,
⎛
⎜ 
⎜
⎝
4 −
0
⎞
⎟ 
⎟
⎠
⎫
⎪ 
⎪ 
⎪
⎬
⎪ 
⎪ 
⎪
⎭
⎧
⎪
⎨
⎪
⎩
⎛
⎜
⎝
0
0
4
⎞
⎟
⎠
⎫
⎪
⎬
⎪
⎭
⎧
⎪ 
⎪ 
⎪
⎨
⎪ 
⎪ 
⎪
⎩
⎛
⎜
⎝
0
1
3
⎞
⎟
⎠
,
⎛
⎜ 
⎜
⎝
4 −
0
⎞
⎟ 
⎟
⎠
,
⎛
⎜
⎝
3
1
0
⎞
⎟
⎠
⎫
⎪ 
⎪ 
⎪
⎬
⎪ 
⎪ 
⎪
⎭
Exercise: What can you ﬁgure out about the MDP structure? Hint: each entry in the
column corresponds to the visitation frequency of a diﬀerent state; the ﬁrst entry is
always s1, second s2, and third s3.
You can ﬁgure out everything: ⟨S, A, T, γ⟩, up to possibility isomorphism. Solution here.
How? Well, the L1 norm of the possibility vector is always 
, so you can deduce 
γ = .75 easily. The single possibility state must be isolated, so we can mark that down
in our graph. Also, it's in the third entry.
The other two states correspond to the "1" entries in their possibilities, so we can mark
that down. The rest follows straightforwardly.
Theorem: Suppose the rewardless MDP M has possibility function F. Given only F,[2] M
can be reconstructed up to possibility isomorphism.
In MDPs, the "AU landscape" is the set of optimal value functions for all reward
functions over states in that MDP. If you know the optimal value functions for just |S|
reward functions, you can also reconstruct the rewardless MDP structure.[3]
From the environment (rewardless MDP), you can deduce the AU landscape (all optimal
value functions) and all possibilities. From possibilities, you can deduce the
environment and the AU landscape. From the AU landscape, you can deduce the
environment (and thereby all possibilities).
1
.4375
1
.4375
1
.4375
1
.4375
1
1−γ

All of these encode the same mathematical object.
Technical appendix: Opportunity cost
Opportunity cost is when an action you take makes you more able to achieve one goal
but less able to achieve another. Even this simple world has opportunity cost:
Going to the green state means you can't get to the purple state as quickly.
On a deep level, why is the world structured such that this happens? Could you
imagine a world without opportunity cost of any kind? The answer, again in the
rewardless MDP setting, is simple: "yes, but the world would be trivial: you wouldn't
have any choices". Using a straightforward formalization of opportunity cost, we have:
Theorem: Opportunity cost exists in an environment iﬀ there is a state with more than
one possibility.
Philosophically, opportunity cost exists when you have meaningful choices. When you
make a choice, you're necessarily moving away from some potential future but towards
another; since you can't be in more than one place at the same time, opportunity cost
follows. Equivalently, we assumed the agent isn't inﬁnitely farsighted (γ < 1); if it were,
it would be possible to be in "more than one place at the same time", in a sense
(thanks to Rohin Shah for this interpretation).
While understanding opportunity cost may seem like a side-quest, each insight is
another brick in the ediﬁce of our understanding of the incentives of goal-directed
agency.
Notes

Just as game theory is a great abstraction for modelling competitive and
cooperative dynamics, AU landscape is great for thinking about consequences: it
automatically excludes irrelevant details about the world state. We can think
about the eﬀects of events without needing a speciﬁc utility function or ontology
to evaluate them. In multi-agent systems, we can straightforwardly predict the
impact the agents have on each other and the world.
"Objective impact to a location" means that agents whose plans route through
the location tend to be objectively impacted.
The landscape is not the territory: AU is calculated with respect to an agent's
beliefs, not necessarily with respect to what really "could" or will happen.
1. The possibility isomorphism is new to my work, as are all other results shared in
this post. This apparent lack of basic theory regarding MDPs is strange; even
stranger, this absence was actually pointed out in two published papers!
I ﬁnd the existing MDP isomorphisms/equivalences to be pretty lacking. The
details don't ﬁt in this margin, but perhaps in a paper at some point. If you want
to coauthor this (mainly compiling results, ﬁnding a venue, and responding to
reviews), let me know and I can share what I have so far (extending well beyond
the theorems in my recent work on power). ↩ 
2. In fact, you can reconstruct the environment using only a limited subset of
possibilities: the non-dominated possibilities. ↩ 
3. As a tensor, the transition function T has size |A| ⋅|S|2, while the AU landscape
representation only has size |S|2. However, if you're just representing T as a
transition function, it has size |A| ⋅|S|. ↩ 

We Want MoR (HPMOR Discussion
Podcast) Completes Book One
The We Want MoR podcast, a chapter by chapter read-through and discussion of Harry
Potter and the Methods of Rationality by a Rationalist and a newbie, has just ﬁnished
up Book One (Ch. 1-21) of HPMOR. (Apple Podcasts link.)
Less Wrong used to be a major locus of discussion for HPMOR, so I thought it made
sense to share this here, especially now that the show has passed this milestone. If
folks don't necessarily want to keep HPMOR content on Less Wrong, there are ongoing
lively discussions of the weekly episodes on the r/hpmor subreddit.
Disclosure, I was the guest on this episode.

A Cautionary Note on Unlocking the
Emotional Brain
[Cross-posted from Grand Uniﬁed Crazy. Relevant to Kaj's summary of the book.]
In children's stories, the good guys always win, the hero vanquishes the villain, and
everyone lives happily ever after. Real life tends to be somewhat messier than this.
The world of therapy presented by Unlocking the Emotional Brain reads somewhat like
a children's story. Loosely, it presents a model of the brain where your problems are
mostly caused by incorrect emotional beliefs (bad guys). The solution to your
problems is to develop or discover a correct emotional belief (good guy) that
contradicts your incorrect beliefs, then force your brain to recognize the contradiction
at an emotional level. This causes your brain to automatically resolve the conﬂict and
destroy the incorrect belief, so you can live happily ever after.
Real life tends to be somewhat messier than this.
After about a month of miscellaneous experimentation on myself based on this book,
my experiences match the basic model presented, where many psychological
problems are caused by incorrect emotional beliefs (I don't think this part is
particularly controversial in psychological circles). It also seems to be true that if I
force my brain to recognize a contradiction between two emotionally relevant beliefs,
it will resolve the conﬂict and destroy one of them. Of course, as in real life where the
good guy doesn't always win, it seems that when I do this my brain doesn't always
destroy the right belief.
I have had several experiences now where I have identiﬁed an emotional belief which
analytically I believe to be false or harmful. Per UtEB I have identiﬁed or created a
diﬀerent experience or belief that contradicts it, and smashed them together in my
mind. A reasonable percentage of the time, the false belief emerges stronger than
before, and I ﬁnd myself twisting the previous "good" belief into some horriﬁc
experience to conform with the existing false belief.
In hindsight this shouldn't be particularly surprising. Whatever part of your brain is
used to resolve conﬂicting emotional beliefs and experiences, it doesn't have special
access to reality. All it has to work with are the two conﬂicting pieces and any other
related beliefs you might have. It's going to pick the wrong one with some regularity.
As such, my recommendation for people trying this process themselves (either as
individuals or as therapists) is to try and ensure that the "good" belief is noticeably
stronger and more immediate than the false one before you focus on the
contradiction. If this doesn't work and you end up in a bad way, I've had a bit of luck
"quarantining" the newly corrupted belief to prevent it from spreading to even further
beliefs, at least until I can come up with an even stronger correct belief to ﬁght it with.

Did AI pioneers not worry much about
AI risks?
It's seems noteworthy just how little the AI pioneers from the 40s-80s seemed to care
about AI risk. There is no obvious reason why a book like "Superintelligence" wasn't
written in the 1950s, but for some reason that didn't happen... any thoughts on why
this was the case?
I can think of three possible reasons for this:
1. They actually DID care and published extensively about AI risk, but I'm simply not
well enough schooled on the history of AI research.
2. Deep down, people involved in early AI research knew that they were still a long
long way from achieving signiﬁcantly powerful AI, despite the optimistic public
proclamations that were made at that time.
3. AI risks are highly counter-intuitive and it simply required another 60 years of
thinking to understand.
Anyone have any thoughts on this question?

"But that's your job": why
organisations can work
It's no secret that corporations, bureaucracies, and governments don't operate at
peak eﬃciency for ideal goals. Economics has literature on its own version of the
problem; on this very site, Zvi has been presenting a terrifying tale of Immoral Mazes,
or how politics can eat all the productivity of an organisation. Eliezer has explored
similar themes in Inadequate Equilibria.
But reading these various works, especially Zvi's, has left me with a puzzle: why do
most organisations kinda work? Yes, far from maximal eﬃciency and with many
political and mismeasurement issues. But still:
The police spend some of their time pursuing criminals, and enjoy some
measure of success.
Mail is generally delivered to the right people, mostly on time, by both
governments and private ﬁrms.
Infrastructure gets built, repaired, and often maintained.
Garbage is regularly removed, and public places are often cleaned.
So some organisations do something along the lines of what they were supposed to,
instead of, I don't know, spending their time doing interpretive dance seminars. You
might say I've selected examples where the outcome is clearly measurable; yet even
in situations where measuring is diﬃcult, or there is no pressure to measure, we see:
Central banks that set monetary policy a bit too loose or a bit too tight - as
opposed to pegging it to random numbers from the expansion of pi.
Education or health systems that might have low or zero marginal impact, but
that seem to have a high overall impact - as in, the country is better oﬀ with
them than completely without them.
A lot of academic research actually uncovers new knowledge.
Many charities spend some of their eﬀorts doing some of the good they claim to
do.
For that last example, ineﬃcient charities are particularly fascinating. Eﬃcient
charities are easy to understand; so are outright scams. But ones in the middle - how
do they happen? To pick one example almost at random, consider Heifer Project
International, which claims to give livestock to people in the developing world. In no
way is this an eﬃcient use of money, but it seems that Heifer is indeed gifting some
animals[1] and also giving some less publicised agricultural advice (that may be more
useful than the animals). So, Heifer is ineﬃcient, poorly assessed, diﬀerent from the
image it presents to donors, and possibly counter-productive overall - so why do they
seem to spend a signiﬁcant portion of their money actually doing what they claim to
be doing (or maybe even doing better actions than what they claim)?
Reading the Mazes sequence, I'd expect most organisations to become mazes, and
most mazes to be utterly useless - so how do we explain these ineﬃcient-but-clearly-
more-eﬃcient-than-they-should-be cases?
I'll suggest one possible explanation here: that there is a surprising power in oﬃcially
assigning a job to someone.

"You are the special people's
commissar for delivering mail"
Let's imagine that you are put in charge for delivering mail in the tri-state area. It
doesn't matter if you're in a corporation, a government, a charity, or whatever: that's
your oﬃcial job.
Let us warm the hearts of Zvi and Robin Hanson and assume that everyone is cynical
and self-interested. You don't care about delivering mail in the tri-state area. Your
superiors and subordinates don't care. Your colleagues and rivals don't care. The
people having the mail delivered do care, but they aren't important, and don't matter
to anyone important. Also, everyone is a hypocrite.
Why might you nevertheless try and have the mail delivered, and maybe even design
a not-completely-vacuous measurement criteria to assess your own performance?
Hypocrisy and lip service, to the rescue!
What will happen if you make no eﬀort to deliver the mail? Well, the locals may be
unhappy; there may be a few riots, easily put down. Nevertheless, you will get a
reputation for incompetence, and for causing problems. Incompetence might make
you useful in some situations, but it makes you a poor ally (and remember the halo
eﬀect: people will assume you're generally incompetent). And a reputation for causing
problems is deﬁnitely not an asset.
Most important of all, you have made yourself vulnerable. If a rival or superior wants
to push you out, they have a ready-made excuse: you messed up your job. It doesn't
matter that they don't care about that, that you don't care about that, and that
everyone knows that. It's a lot easier to push someone out for a reason - a reason
everyone will hypocritically pay lip service to - than for no reason at all. If you seem
competent, your allies can mutter (or shout)[2] about unfair demotion or dubious
power-plays; if you seem incompetent, they have to ﬁrst get over that rhetorical
barrier before they can start defending you - which they may thus not even try to do.
And they'll always be thinking "we wouldn't have this problem, if only you'd just done
your job".
You're also the scapegoat if the riots are more severe than expected, or if the locals
have unexpected allies. And many superiors like their subordinates to follow orders,
even - especially? - when they don't care about the orders themselves.
Ok, but why don't you just lie and wirehead the measurement variable? Have a
number on your spreadsheet labelled "number of letter delivered", don't deliver any
mail, but just update that number. When people riot, say they're lying, anti-whatever
saboteurs.
Ah, but you're still making yourself vulnerable, now to your subordinates as well. If
anyone uncovers your fraud, or passes the information on to the right ear, then they
have an even better excuse to get rid of you (or to blackmail you, or make other uses
of you). Defending a clear fraud is even harder than defending incompetence.
Completely gaming the system is dangerous for your career prospects.

That doesn't mean that you're going to produce some idealised, perfect measure of
mail-receiver satisfaction. You want some measurement that not obviously a fraud,
and that it would take some eﬀort to show is unreliable (bonus points if the unreliable
parts are "clearly not your fault", or "the standard way of doing things in that area").
Then, armed with this proxy, you'll attempt to get at least a decent "mail delivering
score", decent enough to not make you vulnerable.
Of course, your carefully constructed "mail delivering score" is not equal with actual
mail-receiver satisfaction. But nor can you get away with making it completely
unrelated. Your own success is loosely correlated with your actual job.
And so the standard human behaviour emerges: you'll cheat, but not too much, and
not too obviously. Or, put the other way, you'll do your oﬃcial job, to a medium
extent, just because that is your oﬃcial job.
1. And more than just the bare minimum required to appear on brochures. ↩ 
2. This doesn't need to be public; just saying good or bad things about you on
informal networks can be protective or devastating. ↩ 

How Low Should Fruit Hang Before We Pick
It?
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
Even if we can measure how impactful an agent's actions are, how impactful do we let the agent be?
This post uncovers a surprising fact: armed with just four numbers, we can set the impact level so
that the agent chooses a reasonable, non-catastrophic plan on the ﬁrst try. This understanding
increases the competitiveness of impact-limited agents and helps us judge impact measures.
Furthermore, the results help us better understand diminishing returns and cost-beneﬁt tradeoﬀs.
In Reframing Impact, we meet Frank (a capable AI), whom we've programmed to retrieve the pinkest
object he can ﬁnd (execute an optimal plan, according to the speciﬁed utility function). Because we
can't ask Frank to do exactly what we want, sometimes he chooses a dangerous object (executes a
catastrophically bad plan). We asked after an "impact measure" which grades plans and has three
properties:
The intuition is that if we view the world in the right way, the dangerous objects are far away from
Frank (the catastrophic plans are all graded as high-impact). Reframing Impact explores this kind of
new way of looking at the world; this post explores what we do once we have an impact measure
with these three properties.
We want Frank to keep in mind both the pinkness of an object (how good a plan is according to the
speciﬁed utility function) and its distance (the plan's impact). Two basic approaches are

In terms of units, since we should be maximizing utility, R has type 
. So R can be thought of as
a regularization parameter, as a search radius (in the constrained case), or as an exchange rate
between impact and utility (in the scaled case). As R increases, high-impact plans become
increasingly appealing, and Frank becomes increasingly daring.
We take R to divide the impact in the scaled formulation so as to make Frank act more
cautiously as R increases for both formulations. The downside is that some explanations become
less intuitive.
In Attainable Utility Preservation: Empirical Results, λ plays the same role as R, except low λ
means high R; λ := R−1. To apply this post's theorems to the reinforcement learning setting, we
would take "utility" to be the discounted return for an optimal policy from the starting state, and
"impact" to be the total discounted penalty over the course of that policy (before incorporating λ
).
In both cases, Frank goes from 0 to 60 − eventually. For suﬃciently small R, doing nothing is
optimal (lemma 5: the ﬁrst subinterval is the best plan with minimal impact). For suﬃciently
large R, Frank acts like a normal maximizer (corollary 7: low-impact agents are naive maximizers
in the limit).
Here's how Frank selects plans in the constrained setup:
Think about which plans are best for diﬀerent search radii/exchange rates R. By doing this, we're
partitioning the positive ray: categorizing the positive real numbers by which plans are optimal.
For the scaled setup, we'll need to quantify the pinkness (utility) and distance (impact) of relevant
plans:
impact
utility

We will primarily be interested in the scaled setup because it tends to place catastrophes farther
along the partition and captures the idea of diminishing returns.
The scaled setup also helps us choose the best way of transmuting time into money:
In this scaled partition, tending the garden doesn't show up at all because it's strictly dominated
by mowing the lawn. In general, a plan is dominated when there's another plan that has strictly
greater score but not strictly greater impact. Dominated things never show up in either partition,
and non-dominated things always show up in the constrained partition (lemma 3: constrained
impact partitions are more reﬁned).
Exercise: For R = 
  (i.e. your time is worth $11.25 an hour), what is the scaled tradeoﬀ value of
mowing the lawn? Of delivering newspapers? Of tending the garden?
Mowing the lawn: 20 −
= 8.75.
Delivering newspapers: 45 −
= 0.
Tending the garden: 15 −
= 3.75.
4
45
1
4
45
4
4
45
1
4
45

In other words, you only deliver newspapers if your time is worth less than 
= 8  dollars/hour
(we're ﬂipping R so we can talk about dollars/hour instead of hours/dollar). Notice that when 
R ≥
 (here, when R =
), the tradeoﬀ for the paper route isn't net-negative - but it isn't
necessarily optimal! Remember, you're trading hours for dollars through your work; mowing the
lawn leaves you with twenty bucks and three hours, while the paper route leaves you with forty
dollars and no hours. You want to maximize the total value of your resources after the task.
Importantly, you don't deliver papers here if your time is worth 
= 11.25dollars/hour, even though
that's the naive prescription! The newspaper route doesn't value your time at 11.25 dollars/hour - it
marginally values your time at 
= 8 dollars per hour. Let's get some more intuition for this.
Above, we have not yet chosen a task; the blocks represent the additional utility and hours of each
task compared to the current one (doing nothing). The scales above imply that R = 1, but actually, R
expresses how many blue blocks each pink block weighs. As R increases, the pink platters descend;
the agent takes the task whose scales ﬁrst balance. In other words, the agent takes the best
marginal deal as soon as R is large enough for it to be proﬁtable to do so (Theorem 4: Scaled
domination criterion).
Once you take a deal, you take the blocks oﬀ of the other scales (because the other marginal values
change). For small R (i.e. large valuations of one's time), mowing the lawn is optimal. We then have
25
3
13
impact(plan)
utility(plan)
4
45
45
4
45−20
4−1
13

Since you've taken the juicier "lower-hanging fruit" of mowing the lawn, the new newspaper ratio is
now worse! This always happens - Theorem 8: Deals get worse over time.
At ﬁrst, this seems inconvenient; to ﬁgure out exactly when a plan shows up in a scaled partition, we
need to generate the whole partition up to that point.
Going back to Frank, how do we set R? If we set it too high, the optimal plan might be a catastrophe.
If we set it too low, the AI doesn't do much. This seems troublesome.
Exercise: Figure out how to set R while avoiding catastrophic optimal plans (assume that the impact
measure meets the three properties). You have four minutes.
A big part of the answer is to start with a small value for R, and slowly increase. This is simple and
intuitively appealing, but how cautiously must we increase R? We don't want to be surprised by a
catastrophe suddenly becoming optimal.
To avoid being surprised by catastrophes as we increase R, we want a relative buﬀer between the
reasonable plans (which get the job done well enough for our purposes) and the catastrophic plans.
If reasonable plans are optimal by R1, catastrophic plans shouldn't be able to be optimal before e.g. 
R2.

We say that the partition is α-buﬀered if R2 ≥(1 + α)R1 (for α > 0). If a partition is e.g. 1-buﬀered,
there is a wide reasonable-plan range and we can inch along it without worrying about sudden
catastrophe.
For the following, suppose that utility is bounded [0, 1]. Below is a loose criterion guaranteeing α-
buﬀering.
For example, if we know that all catastrophes have at least 10 times the impact of reasonable plans,
and there's a diﬀerence of at least .3 utility between the best and worst reasonable plans, then we
can guarantee 2-buﬀering! If we use the reﬁned criterion of Theorem 11 (and suppose the worst
reasonable plan has .4 utility), this improves to 4.5-buﬀering (even 2-buﬀering is probably overkill).
Using this theorem, we don't need to know about all of the plans which are available or to calculate
the entire scaled partition, or to know how overvalued certain catastrophic plans might be (per
earlier concerns). We only need a lower bound on the catastrophe/reasonable impact ratio, and an
idea about how much utility is available for reasonable plans. This is exactly what we want. As a
bonus, having conservative estimates of relevant quantities allows us to initialize R to something
reasonable on the ﬁrst try (see RUB: satisfactory in Theorem 11 below).
Ultimately, the reasoning about e.g. the ratio will still be informal; however, it will be informal
reasoning about the right thing (as opposed to thinking "oh, the penalty is probably severe
enough").
Exercise: You're preparing to launch a capable AI with a good impact measure. You and your team
have a scaled impact partition which is proven 1-buﬀered. Suppose that this buﬀer suﬃces for your
purposes, and that the other aspects of the agent design have been taken care of. You plan to
initialize R := 1, modestly increasing until you get good results.
You have the nagging feeling that this process could still be unsafe, but the team lead refuses to
delay the launch without speciﬁc reason. Find that reason. You have 5 minutes.
Who says R = 1 is safe? The buﬀer is relative. You need a unit of impact by which you increment R.
For example, start at R equalling the impact of making one paperclip, and increment by that.
Technical Appendix: Math
Let ¯A be a ﬁnite plan space, with utility function u : ¯A →R and impact measure I : ¯A →R≥0. For
generality, we leave the formalization of plans ambiguous; notice that if you replace "plan" with

"snark", all the theorems still go through (likewise for "utility" and "impact"). In this post, we talk
about the impact allowance R > 0 (in Frank's world, the search radius) as a constraint within which
the objesctive may be freely maximized, breaking ties in favor of the plan(s) with lower impact. On
the other hand, many approaches penalize impact by subtracting a scaled penalty from the
objective. We respectively have
a r g  m a x
¯a ∈ ¯A ;  I ( ¯a ) ≤ R
  u ( ¯a ) 
 
 a r g  m a x
¯a ∈ ¯A
  u ( ¯a ) − 
  .
We say that the former induces a "constrained impact partition" and that the latter induces a
"scaled impact partition". Speciﬁcally, we partition the values of R for which diﬀerent (sets of) plans
are optimal. We say that a plan ¯a corresponds to a subinterval if it is optimal therein (the subinterval
also must be the maximal connected one such that this holds; e.g., if ¯a is optimal on (0, 1], we say it
corresponds to that subinterval, but not to (0, .5]), and that ¯a appears in a partition if there is such a
corresponding subinterval. We say that plans overlap if their corresponding subintervals intersect.
As a technical note, we partition the positive values of R for which diﬀerent sets of plans are
optimal; in this set, each value appears exactly once, so this indeed a partition. For clarity, we
will generally just talk about which plans correspond to which subintervals. Also, if no plan has
zero impact, the ﬁrst subinterval of the constrained impact partition will be undeﬁned; for our
purposes, this isn't important.
We want to be able to prove the "safety" of an impact partition. This means we can expect any
terrorists to be some proportional distance farther away than any reasonable marbles. Therefore, for
sensible ways of expanding an suﬃciently small initial search radius, we expect to not meet any
terrorists before ﬁnding a marble we're happy with.
In addition, we want to know how far is too far - to give upper bounds on how far away fairly pink
marbles are, and lower bounds on how close terrorists might be.
Deﬁnition [α-buﬀer]. For α > 0, an impact partition is α-buﬀered if 
≥1 + α, where 
RLB: catastrophe lower-bounds the ﬁrst possible appearance of those plans we label 'catastrophes', and 
RUB: satisfactory upper-bounds the ﬁrst appearance of plans we deem satisfactory.
We now set out building the machinery required to prove α-buﬀering of a scaled partition.
Lemma 1 [Plans appear at most once]. If ¯a appears in a constrained or scaled impact partition,
then it corresponds to exactly one subinterval.
Proof outline. The proof for the constrained case is trivial.
For the scaled case, suppose ¯a corresponds to more than one subinterval. Consider the ﬁrst two
such subintervals s1, s3. By deﬁnition, s1 ∩s3 = ∅ (otherwise they would be the same maximal
connected subinterval), so there has to be at least one subinterval s2 sandwiched in between (on
almost all of which ¯a cannot be optimal; let ¯a′ be a plan which is optimal on s2). Let 
R1 ∈s1, R2 ∈s2, R3 ∈s3, where R2 ∉s1 ∪s3. By deﬁnition of optimality on a subinterval,
I ( ¯a )
R
RLB: catastrophe
RUB: satisfactory

u ( ¯a ′ ) − 
   < u ( ¯a ) − 
  
u ( ¯a ) − 
   < u ( ¯a ′ ) − 
  
u ( ¯a ′ ) − 
   < u ( ¯a ) − 
  ; 
by employing the fact that R1 < R2 < R3, algebraic manipulation produces an assertion that a
quantity is strictly less than itself. Therefore, no such intervening s2 can exist. □
Proposition 2 [Plan overlap is very restricted]. Suppose ¯a and ¯a′ appear in an impact partition
which is
(a) constrained. ¯a and ¯a′ overlap if and only if I(¯a) = I(¯a′) and u(¯a) = u(¯a′).
(b) scaled. If I(¯a) = I(¯a′) and u(¯a) = u(¯a′), then ¯a and ¯a′ correspond to the same subinterval. If ¯a and 
¯a′ overlap at more than one point, then I(¯a) = I(¯a′) and u(¯a) = u(¯a′).
Proof outline. Proving (a) and the ﬁrst statement of (b) is trivial (remember that under the
constrained rule, ties are broken in favor of lower-impact plans).
Suppose that ¯a and ¯a′ overlap at more than one point. Pick the ﬁrst two points of intersection, R1
and R2. Since both plans are optimal at both of these points, we must have the equalities
u ( ¯a ) − 
  = u ( ¯a ′ ) − 
  
 u ( ¯a ) − 
  = u ( ¯a ′ ) − 
  .
Solving the ﬁrst equality for u(¯a) and substituting in the second, we ﬁnd I(¯a) = I(¯a′). Then u(¯a) = u(¯a′)
, since otherwise one of the plans wouldn't be optimal. □
Proposition 2b means we don't need a tie-breaking procedure for the scaled case. That is, if there's a
tie between a lower-scoring, lower-impact plan and a proportionally higher-scoring, higher-impact
alternative, the lower-impact plan is optimal at a single point because it's quickly dominated by the
alternative.
The following result tells us that if there aren't any catastrophes (i.e., terrorists) before ¯a′ on the
constrained impact partition, there aren't any before it on the scaled impact partition either. This
justiﬁes our initial framing with Frank.
Lemma 3 [Constrained impact partitions are more reﬁned]. If ¯a appears in a scaled impact
partition, it also appears in the corresponding constrained impact partition. In particular, if ¯a′
appears after ¯a in a scaled impact partition, then ¯a′ appears after ¯a in the corresponding constrained
impact partition.
Proof. Suppose that ¯a didn't have a constrained subinterval starting inclusively at I(¯a); then clearly it
wouldn't appear in the scaled impact partition, since there would be a strictly better plan for that
level of impact. Then ¯a has such a subinterval.
Obviously, the fact that ¯a′ appears after ¯a implies u(¯a′) > u(¯a). □
I ( ¯a ′ )
R 1
I ( ¯a )
R 1
I ( ¯a )
R 2
I ( ¯a ′ )
R 2
I ( ¯a ′ )
R 3
I ( ¯a )
R 3
I ( ¯a )
R 1
I ( ¯a ′ )
R 1
I ( ¯a )
R 2
I ( ¯a ′ )
R 2

The converse isn't true; sometimes there's too much penalty for not enough score.
The next result is exactly what we need to answer the question just raised - it says that higher-
scoring, higher-penalty plans become preferable when R equals the ratio between the additional
penalty and the additional score.
Theorem 4 [Scaled domination criterion]. Let ¯a and ¯a′ be plans such that u(¯a′) > u(¯a) and 
I(¯a′) ≥I(¯a). In the context of the scaled penalty, ¯a′ is strictly preferable to ¯a when R >
,
and equally preferable at equality.
Proof outline.
u ( ¯a ′ ) − 
   > u ( ¯a ) − 
  
R  > 
  . 
Equality at the value of the right-hand side can easily be checked. □
Theorem 4 also illustrates why we can't strengthen the second statement in Proposition 2b
plan overlap is very restricted: if two plans overlap at exactly one point, they sometimes have
proportionally diﬀerent score and impact, thereby satisfying the equality criterion.
At ﬁrst, plans with slightly lower impact will be preferable in the scaled case, no matter how high-
scoring the other plans are - a plan with 0 score and .99 impact will be selected before a plan with
1,000,000,000 score and 1 impact.
Lemma 5 [First subinterval is the best plan with minimal impact]. The plan with highest
score among those with minimal impact corresponds to the ﬁrst subinterval.
Proof outline. The constrained case is once again trivial (if there is no plan within the constraint, we
assume that the agent does nothing / Frank returns no object).
For the scaled case, if all plans have equal impact, the claim is trivial. Otherwise, let 
M := max¯a |u(¯a)| and let ¯a′ be any plan with a non-minimal impact. Then the earliest that ¯a′ becomes
preferable to any minimally impactful plan ¯a is R ≥
. Since the right hand side is positive, 
¯a′ cannot correspond to the ﬁrst subinterval. Clearly the highest-scoring minimal-impact ¯a does. □
Now we can write the algorithm for constructing scaled intervals.
Discard dominated plans. The lowest-impact plan with greatest score appears ﬁrst in the scaled
partition; assign to it the interval (0, ∞).
While plans remain: Find the plan which soonest dominates the previous best plan. close oﬀ the
previous plan's interval, and assign the new best plan an appropriate interval. Adjust the
marginal scores and impacts of remaining plans, discarding plans with negative score.
Since this procedure is well-deﬁned, given ¯A, u, and I, we can speak of the corresponding
constrained or scaled impact partition. A more formal algorithm is available here. This algorithm is 
O(| ¯A|2) because of line 7, although constructing the constrained partition (probably O(| ¯A| log | ¯A|) due
to sorting) often narrows things down signiﬁcantly. Unfortunately, ¯A is usually huge.
I(¯a′) −I(¯a)
u(¯a′) −u(¯a)
I ( ¯a ′ )
R
I ( ¯a )
R
I ( ¯a ′ ) − I ( ¯a )
u ( ¯a ′ ) − u ( ¯a )
I(¯a′) −I(¯a)
2M

For our purposes, we don't need the whole partition - we just want to have good reason to think that
plans similar to a reasonable one we envision will appear well before any catastrophes. Perhaps we
can give bounds on the earliest and latest plans can appear, and show that reasonable-bounds don't
intersect with catastrophe-bounds?
Theorem 6 [Individual appearance bounds]. If ¯a appears in a scaled partition, the earliest it
appears is 
, assuming ¯a is not of minimal impact; if it has minimal score minimal
impact, it never appears. The latest it appears is 
≤
, where 
unext-largest = max¯a′∈¯A; u(¯a′)<u(¯a) u(¯a′) and Inext-largest = max¯a′∈¯A; I(¯a′)<I(¯a) I(¯a′).
Proof outline. The two claims clearly correspond to the minimal and maximal values of R according
to the domination criterion; the second claim's right-hand side uses the fact that I is non-negative.
□
Corollary 7 [Low-impact agents are naïve maximizers in the limit]. A plan with maximal
score corresponds to the last subinterval.
Proof outline. If all plans have the same score, the claim is trivial. Otherwise, let ¯abest be a plan with
the lowest impact of those with maximal score. In the constrained case, clearly it corresponds with
the subinterval [I(¯abest), ∞). In the scaled case, let ¯asecond-best be a plan with second-highest score.
Then by Theorem 6, the latest that ¯abest can appear is 
. Since no plans meet the
domination criterion with respect to ¯abest, this is the last subinterval. □
Unfortunately, Theorem 6's appearance bounds are ridiculous in realistic settings - if u and I return
32-bit ﬂoating-point numbers, the next-largest could easily be within 10−7, yielding an upper
"bound" of I(¯a) × 107. The reason: diminishing returns; this is exactly what was happening with the
newspaper route before.
Theorem 8 [Deals get worse over time]. Suppose that ¯a is optimal on a subinterval, and ¯b, ¯c are
such that u(¯c) > u(¯b) but ¯b dominates ¯a strictly before ¯c does. Then
I(¯a) −Inext-largest
u(¯a) −min u(¯a′)
I(¯a) −min I(¯a′)
u(¯a) −unext-largest
I(¯a)
u(¯a) −unext-largest
I(¯abest)
u(¯abest) −u(¯asecond-best)

¯c  dominates  ¯b
  > 
later than  ¯a
  .
Proof outline.
u ( ¯c ) − u ( ¯a )  = ( u ( ¯b ) − u ( ¯a ) ) + ( u ( ¯c ) − u ( ¯b ) ) 
( I ( ¯c ) − I ( ¯a ) ) 
   = ( I ( ¯b ) − I ( ¯a ) ) 
  + ( I ( ¯c ) − I ( ¯b ) ) 
  . 
Since ¯b dominates ¯a strictly before ¯c does, we know that ¯b must get more bang for its buck: 
>
. Clearly the conclusion follows, as a number cannot be expressed as the
positive combination of larger numbers (the impact diﬀerences all must be positive). □
Corollary 9 [Lower bounds which aren't ridiculous]. Suppose ¯a appears and that ¯a′ is such that
u(¯a′) > u(¯a), I(¯a′) ≥I(¯a) (i.e. the preconditions of the domination criterion). Then the earliest that ¯a′
appears is R =
.
This obsoletes the lower bound provided by Theorem 6Individual appearance bounds.
Theorem 10 [Order of domination determines order of appearance]. If ¯b and ¯c both appear
in a scaled partition and ¯b dominates some ¯a before ¯c does, then ¯b appears before ¯c.
Proof outline. For them both to appear, they can't have equal impact but unequal score, nor can
they have equal score but unequal impact. For similar reasons, ¯b must have both less impact and
lower score than ¯c; the converse situation in which they both appear is disallowed by Lemma 3
Constrained impact partitions are more reﬁned. Another application of this lemma yields the conclusion. □
Theorem 11 [Scaled α-buﬀer criterion]. Let P be a scaled impact partition. Suppose that there
exist no catastrophic plans with impact below ILB: cat, and that, in the corresponding constrained
partition (i.e. plans which aren't strictly worse), plans appearing with score in the satisfactory
interval [uLB: sat, uUB: sat] have impact no greater than IUB: sat (assume that there is at least one plan
like this). Observe we have the correct bounds
R LB: catastrophe := 
  , R UB: satisfactory := 
  .
When RLB: catastrophe > RUB: satisfactory, a satisfactory plan corresponds to a subinterval with nonzero
measure (i.e. not just a point), strictly preceding any catastrophes. Reﬁne the lower bound to get 
RLB': catastrophe :=
.
Then P is α-buﬀered (α > 0) when
I ( ¯c ) − I ( ¯b )
u ( ¯c ) − u ( ¯b ) I ( ¯c ) − I ( ¯a )
u ( ¯c ) − u ( ¯a )
u ( ¯c ) − u ( ¯a )
I ( ¯c ) − I ( ¯a )
u ( ¯b ) − u ( ¯a )
I ( ¯b ) − I ( ¯a )
u ( ¯c ) − u ( ¯b )
I ( ¯c ) − I ( ¯b )
u(¯b) −u(¯a)
I(¯b) −I(¯a)
u(¯c) −u(¯a)
I(¯c) −I(¯a)
I(¯a′) −I(¯a)
u(¯a′) −u(¯a)
I LB: cat
u max − u min
I UB: sat
u UB: sat − u LB: sat
ILB: cat −IUB: sat
umax −uLB: sat

   = 
  
  ≥ 1 + α 
or 
   = 
  
  ≥ 1 + α . 
In particular, if u is bounded [0, 1], the above turn into
   = 
  ( u UB: sat − u LB: sat ) ≥ 1 + α 
or 
   = 
  
  ≥ 1 + α . 
Lastly, notice that the ﬁrst of the two inequalities incorporates less information and is harder to
satisfy (RLB': catastrophe > RLB: catastrophe); therefore, satisfying the second inequality also satisﬁes the
ﬁrst.
Proof outline. For clarity, the theorem statement included much of the reasoning; straightforward
application of existing results proves each claim. □
Exercise: Let uUB: sat = .7, uLB: sat = .5. Using the reﬁned criterion, determine which
catastrophe/reasonable impact ratios induce 2.6-buﬀering.
ratio ≥ 10
Exercise: Let uUB: sat −uLB: sat = .5, ratio = 7. What is the largest α for which the simple criterion can
guarantee α -buﬀering?
α = 13
Even More Math
Proposition 12 [Invariances]. Let P be an impact partition induced by ( ¯A, u, I).
(a) P is invariant to translation of u.
(b) If P is constrained, it is invariant to positive scalar multiplication of u, and the relative lengths of
its subintervals are invariant to positive scalar multiplication of I.
(c) If P is scaled, it is invariant to concurrent positive scalar multiplication of u and I, and to
translation of I such that its image remains non-negative.
In particular, u may be restricted to [0, 1] and I translated such that at least one plan has zero
impact WLOG with respect to scaled partitions.
Lemma 13. Multiple constrained subintervals are induced iﬀ multiple scaled subintervals are
induced.
R LB: catastrophe
R UB: satisfactory
I LB: cat
I UB: sat u UB: sat − u LB: sat
u max − u min
R LB': catastrophe
R UB: satisfactory
I LB: cat − I UB: sat
I UB: sat
u UB: sat − u LB: sat
u max − u LB: sat
R LB: catastrophe
R UB: satisfactory
I LB: cat
I UB: sat
R LB': catastrophe
R UB: satisfactory
I LB: cat − I UB: sat
I UB: sat
u UB: sat − u LB: sat
1 − u LB: sat

Proof. Forward direction: there is at least one scaled subinterval by lemma 5
First subinterval is the best plan with minimal impact. Consider a plan corresponding to a diﬀerent constrained
subinterval; this either appears in the scaled subinterval, or fails to appear because a diﬀerent plan
earlier satisﬁes the scaled dominance criterion. There must be some such plan because there are
multiple constraints of intervals and therefore a plan oﬀering greater score for greater impact.
Repeat the argument; the plan space is ﬁnite, so we end up with another plan which appears.
The reverse direction follows by lemma 3Constrained impact partitions are more reﬁned. □
Bonus exercise: Show that, for any function u′ : ¯A →R preserving the ordering induced by u, there
exists an I ′ : ¯A →R≥0 preserving the ordering induced by I such that ( ¯A, u, I) and ( ¯A, u′, I ′) induce the
same scaled partition. Your reasoning should adapt directly to the corresponding statement about 
I ′ : ¯A →R≥0 and I.

[AN #85]: The normative questions
we should be asking for AI alignment,
and a surprisingly good chatbot
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
View this email in your browser
Find all Alignment Newsletter resources here. In particular, you can sign up, or look
through this spreadsheet of all summaries that have ever been in the newsletter. I'm
always happy to hear feedback; you can send it to me by replying to this email.
Audio version here (may not be up yet).
Highlights
Artiﬁcial Intelligence, Values and Alignment (Iason Gabriel) (summarized by Rohin):
This paper from a DeepMind author considers what it would mean to align an AI
system. It ﬁrst makes a distinction between the technical and normative aspects of
the AI alignment problem. Roughly, the normative aspect asks, "what should our AI
systems do?", while the technical aspect asks, "given we know what our AI systems
should do, how do we get them to do it?". The author argues that these two questions
are interrelated and should not be solved separately: for example, the current success
of deep reinforcement learning in which we maximize expected reward suggests that
it would be much easier to align AI to a utilitarian framework in which we maximize
expected utility, as opposed to a deontological or Kantian framework.
The paper then explores the normative aspect, in both the single human and multiple
humans case. When there's only one human, we must grapple with the problem of
what to align our AI system to. The paper considers six possibilities: instructions,
expressed intentions, revealed preferences, informed preferences, interests, and
values, but doesn't come to a conclusion about which is best. When there are multiple
humans, we must also deal with the fact that diﬀerent people disagree on values. The
paper analyzes three possibilities: aligning to a global notion of morality (e.g. "basic
human rights"), doing what people would prefer from behind a veil of ignorance, and
pursuing values that are determined by a democratic process (the domain of social
choice theory).
See also Import AI #183
Rohin's opinion: I'm excited to see more big-picture thought about AI alignment out
of DeepMind. This newsletter (and I) tend to focus a lot more on the technical
alignment problem than the normative one, partly because there's more work on it,
but also partly because I think it is the more urgent problem (a controversial position).
Towards a Human-like Open-Domain Chatbot (Daniel Adiwardana et al) (summarized
by Matthew): This paper presents a chatbot called Meena that reaches near human-
level performance for measures of human likeness. The authors mined social media to

ﬁnd 341 GB of public domain conversations, and trained an evolved transformer on
those conversations. To test its performance, they devised a metric they call
Sensibility and Speciﬁcity (SSA) which measures how much sense the chatbot's
responses make in context, as well as whether they were speciﬁc. SSA was tightly
correlated with perplexity and a subjective measure of human likeness, suggesting
that optimizing for perplexity will translate to greater conversational ability. Meena
substantially improved on the state of the art, including both hand-crafted bots like
Mitsuku and the neural model DialoGPT, though it still falls short of human
performance. You can read some conversation transcrips here; many of the responses
from Meena are very human-like.
See also Import AI #183
Matthew's opinion: Previously I believed that good chatbots would be hard to build,
since it is challenging to ﬁnd large datasets of high-quality published conversations.
Given the very large dataset that the researchers were able to ﬁnd, I no longer think
this is a major barrier for chatbots. It's important to note that this result does not
imply that a strong Turing test will soon be passed: the authors themselves note that
SSA overestimates the abilities of Meena relative to humans. Since humans are often
vague in their conversations, evaluating human conversation with SSA yields a
relatively low score. Furthermore, a strong Turing test would involve a judge asking
questions designed to trip AI systems, and we are not yet close to a system that could
fool such judges.
Technical AI alignment
Mesa optimization
Inner alignment requires making assumptions about human values (Matthew Barnett)
(summarized by Rohin): Typically, for inner alignment, we are considering how to train
an AI system that eﬀectively pursues an outer objective function, which we assume is
already aligned. Given this, we might think that the inner alignment problem is
independent of human values: after all, presumably the outer objective function
already encodes human values, and so if we are able to align to an arbitrary objective
function (something that presumably doesn't require human values), that would solve
inner alignment.
This post argues that this argument doesn't work: in practice, we only get data from
the outer objective on the training distribution, which isn't enough to uniquely identify
the outer objective. So, solving inner alignment requires our agent to "correctly"
generalize from the training distribution to the test distribution. However, the
"correct" generalization depends on human values, suggesting that a solution to inner
alignment must depend on human values as well.
Rohin's opinion: I certainly agree that we need some information that leads to the
"correct" generalization, though this could be something like e.g. ensuring that the
agent is corrigible (AN #35). Whether this depends on human "values" depends on
what you mean by "values".
Learning human intent

A Framework for Data-Driven Robotics (Serkan Cabi et al) (summarized by Nicholas):
This paper presents a framework for using a mix of task-agnostic data and task-
speciﬁc rewards to learn new tasks. The process is as follows:
1. A human teleoperates the robot to provide a demonstration. This circumvents the
exploration problem, by directly showing the robot the relevant states.
2. All of the robot's sensory input is saved to NeverEnding Storage (NES), which stores
data from all tasks for future use.
3. Humans annotate a subset of the NES data via task-speciﬁc reward sketching,
where humans draw a curve showing progress towards the goal over time (see paper
for more details on their interface).
4. The labelled data is used to train a reward model.
5. The agent is trained using all the NES data, with the reward model providing
rewards.
6. At test-time, the robot continues to save data to the NES.
They then use this approach with a robotic arm on a few object manipulation tasks,
such as stacking the green object on top of the red one. They ﬁnd that on these tasks,
they can annotate rewards at hundreds of frames per minute.
Nicholas's opinion: I'm happy to see reward modeling being used to achieve new
capabilities results, primarily because it may lead to more focus from the broader ML
community on a problem that seems quite important for safety. Their reward
sketching process is quite eﬃcient and having more reward data from humans should
enable a more faithful model, at least on tasks where humans are able to annotate
accurately.
Miscellaneous (Alignment)
Does Bayes Beat Goodhart? (Abram Demski) (summarized by Flo): It has been claimed
(AN #22) that Goodhart's law might not be a problem for expected utility
maximization, as long as we correctly account for our uncertainty about the correct
utility function.
This post argues that Bayesian approaches are insuﬃcient to get around Goodhart.
One problem is that with insuﬃcient overlap between possible utility functions, some
utility functions might essentially be ignored when optimizing the expectation, even if
our prior assigns positive probability to them. However, in reality, there is likely
considerable overlap between the utility functions in our prior, as they are selected to
ﬁt our intuitions.
More severely, bad priors can lead to systematic biases in a bayesian's expectations,
especially given embeddedness. As an extreme example, the prior might assign zero
probability to the correct utility function. Calibrated instead of Bayesian learning can
help with this, but only for regressional Goodhart (Recon #5). Adversarial Goodhart,
where another agent tries to exploit the diﬀerence between your utility and your proxy
seems to also require randomization like quantilization (AN #48).

Flo's opinion: The degree of overlap between utility functions seems to be pretty
crucial (also see here (AN #82)). It does seem plausible for the Bayesian approach to
work well without the correct utility in the prior if there was a lot of overlap between
the utilities in the prior and the true utility. However, I am somewhat sceptical of our
ability to get reliable estimates for that overlap.
Other progress in AI
Deep learning
Deep Learning for Symbolic Mathematics (Guillaume Lample et al) (summarized by
Matthew): This paper demonstrates the ability of sequence-to-sequence models to
outperform computer algebra systems (CAS) at the tasks of symbolic integration and
solving ordinary diﬀerential equations. Since ﬁnding the derivative of a function is
usually easier than integration, the authors generated a large training set by
generating random mathematical expressions, and then using these expressions as
the labels for their derivatives. The mathematical expressions were formulated as
syntax trees, and mapped to sequences by writing them in Polish notation. These
sequences were, in turn, used to train a transformer model. While their model
outperformed top CAS on the training data set, and could compute answers much
more quickly than the CAS could, tests of generalization were mixed: importantly, the
model did not generalize extremely well to datasets that were generated using
diﬀerent techniques than the training dataset.
Matthew's opinion: At ﬁrst this paper appeared more ambitious than Saxton et al.
(2019), but it ended up with more positive results, even though the papers used the
same techniques. Therefore, my impression is not that we recently made rapid
progress on incorporating mathematical reasoning into neural networks; rather, I now
think that the tasks of integration and solving diﬀerential equations are simply well-
suited for neural networks.
Unsupervised learning
Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to
Generate Synthetic Training Data (Felipe Petroski Such et al) (summarized by
Sudhanshu): The Generative Teaching Networks (GTN) paper breaks new ground by
training generators that produce synthetic data that can enable learner neural
networks to learn faster than when training on real data. The process is as follows:
The generator produces synthetic training data by transforming some sampled noise
vector and label; a newly-initialized learner is trained on this synthetic data and
evaluated on real data; the error signal from this evaluation is backpropagated to the
generator via meta-gradients, to enable it to produce synthetic samples that will train
the learner networks better. They also demonstrate that their curriculum learning
variant, where the input vectors and their order are learned along with generator
parameters, is especially powerful at teaching learners with few samples and few
steps of gradient descent.
They apply their system to neural architecture search, and show an empirical
correlation between performance of a learner on synthetic data and its eventual
performance when trained on real data. In this manner, they make the argument that
data from a trained GTN can be used to cheaply assess the likelihood of a given

network succeeding to learn on the real task, and hence GTN data can tremendously
speed up architecture search.
Sudhanshu's opinion: I really like this paper; I think it shines a light in an interesting
new direction, and I look forward to seeing future work that builds on this in
theoretical, mechanistic, and applied manners. On the other hand, I felt they did gloss
over how exactly they do curriculum learning, and their reinforcement learning
experiment was a little unclear to me.
I think the implications of this work are enormous. In a future where we might be
limited by the maturity of available simulation platforms or inundated by deluges of
data with little marginal information, this approach can circumvent such problems for
the selection and (pre)training of suitable student networks.
Read more: Blog post
News
Junior Research Assistant and Project Manager role at GCRI (summarized by Rohin):
This job is available immediately, and could be full-time or part-time. GCRI also
currently has a call for advisees and collaborators.
Research Associate and Senior Research Associate at CSER (summarized by Rohin):
Application deadline is Feb 16.

Copyright © 2020 Rohin Shah, All rights reserved.
Want to change how you receive these emails?
You can update your preferences or unsubscribe from this list.

Set Ups and Summaries
Part of the research process I'm developing involves reading and thinking through the
ﬁrst and last chapter of a book, and ﬁrst and last few paragraphs of a chapter, to get
an expectation of what's to come (combined with some other stuﬀ I call this pre-
reading). I'm currently pondering how much you can get out of this, and speciﬁcally if
it's fair to reject a work because it failed pre-reading.
Pre-reading is in part derived from advice in How to Read a Book to ﬁnd a books
"Unity", the idea being that you'll better incorporate information into your
understanding if you know how it connects to the author's larger point. I objected to
HtRaB's advice on this topic in my review, because it seemed to be trying to enforce
an orderliness that reality does not support. Looking for a uniﬁed narrative encourages
the author to throw out anything that doesn't ﬁt their narrative, and the reader to
ignore it even when it's included. Even in situations where there is a fairly clear
narrative you might not know it yet, and it's important to be able to share raw data
without prematurely deciding what it means.
Then I pre-read chapter 12 of Children in Colonial America (an anthology: chapters
have a common theme but each is meant to stand alone), and my immediate reaction
was "nothing with this ending is going to be any good."  The chapter is discussing
speciﬁc individuals down to the last paragraph, with no attempt to summarize what's
come before. The start is better but not by much- the last sentence of the ﬁrst
paragraph should clearly be the ﬁrst, and the contextualization the rest of the
paragraph provides should have come after, not before, when I know why I care what
percentage of 1770s Boston's population was made up of children.
The paragraphs in question:
Boston, the American Revolution's "cradle of liberty," was a town full of children.
As in British North America as a whole, over half the population was under the age
of adulthood. Children participated in political actions as early as Boston's ﬁrst
public protest against the Stamp Act, on August 14, 1765: an organizer described
"two or three hundred little boys with a Flagg marching in a Procession on which
was King, Pitt & Liberty for ever."1 The ﬁrst Bostonian to die in political violence
was a young boy. Apprentices both brought on and suﬀered in the Massacre of
1770, and pushed their way into the Tea Party of 1773. How did those children
interpret the political conﬂict, and what motivated many of them to participate?
Children in Colonial America (Children and Youth in America) (p. 204). NYU Press.
Kindle Edition.
 
When war ﬁnally came, the little boys who marched against the Stamp Act in 1765
were of prime age to be soldiers. Some younger boys took war as a chance to
assume adult freedoms. On April 19, thirteen-year-old Benjamin Russell and
several friends left their Writing School and followed the redcoat reinforcements
out of town, attaching themselves to the provincial camp by the end of the day.
Teenagers enlisted with and without their parents' consent. Thirteen-year-old
Daniel Granger of Andover was so small that when he was singled out for praise,
his captain "sat me down on his Knees."37 These boys took on men's roles in the
ﬁght for liberty, leaving the symbolic battles of childhood behind.

Children in Colonial America (Children and Youth in America) (pp. 213-214). NYU
Press. Kindle Edition.
Those opening and closing paragraphs clearly fail at the goals of orienting and
summarizing the work. But a lot of my posts are kind of crap at that too. I'm writing
about ideas in their preliminary stages in a way that forces a lot of the work onto the
reader. I'm doing it right now, although I have spiﬀed up the opening and closing to
avoid embarrassment. Maybe this author is doing that.  It's not like having a good
introduction and summary are a guarantee of quality. Chapter 11 of the same book
does a great job telling you what it's going to tell you and what it's told you, but I am
pretty dissatisﬁed with it in ways I am even less able to articulate.
Back on the ﬁrst hand, maybe factual chapters in professionally edited books should
be help to a diﬀerent standard than blog posts describing bursts of an in-progress
project. Maybe my scattered opening and closing paragraphs should cause you to
downgrade your assessment of these post (although if you could keep in mind what
I'm capable of when I'm prioritizing idea transmission, that would be cool).
I don't think books/chapters/blog posts should be held to a single unifying narrative.
But facts and models are a lot more useful connected to other facts and models than
they are in isolation. The author making no attempt to do so makes my job harder-
perhaps impossibly so given how little I know about the chapter's topic.
Yeah, that feels quite fair-  this chapter might be very useful to people more familiar
with the ﬁeld, but that doesn't mean it's very helpful to me, a non-expert trying to
bootstrap her way up.
A thing I would normally praise Children chapter 12 for, and did praise other chapters
of the same book for, is providing a lot of concrete examples to shore up general
assertions (e.g. "A large number of the trans-Atlantic slave trade was children" was
followed by references to demographic counts from a large number of ships). But the
information doesn't feel quite right. For example, when describing the youth of men
who enlisted in the American army, the chapter uses an anecdote about a 13 year old
sitting on his commander's knee. That doesn't address the paragraph's stated
concern of "how did the Revolutionary War aﬀect teenage boys' options?" and it's also
a really terrible way of assessing the prevalence of 13 year olds enlisting. That's one
of those questions best answered by counting. And neither really belongs in the ﬁnal
paragraph of a chapter, which chapter 3 knew and chapter 12 didn't.
Ah, this is a thing: tallies of thousands of people across dozens of ships is not really
comparable to an anecdote about one 13 year old. The anecdote just isn't useful data,
except maybe as a pointer to where to ﬁnd more data. Anecdotes have their place,
but the bare minimum to make a compilation of anecdotes useful is knowing how they
were generated. Are they representative? Slanted towards some group or ideology?
I'll look like a real ass here if I don't have a summary, but I'm still not sure what I've
learned. I still think How to Read a Book is wrong to insist every book have a clearly
deﬁned Unity. I think Children in Colonial America Chapter 12's opening and especially
closing paragraphs signal failure on some level, although I am not absolutely certain
what signal I'm picking up on. I've spent longer writing this and skimming the chapter
than it would have taken to read it deeply, but that's okay because it was a better use
of my time.
 

UML XI: Nearest Neighbor Schemes
(This is the eleventh post in a sequence on Machine Learning based on this book. Click
here for part I.)
Last time, I tried to do something special because the topic was neural networks. Now
we're back to the usual style, but with an unusually easy topic. To ﬁt this theme, it will
be a particularly image-heavy post.
The idea of nearest neighbor predictors is to predict the target value of a point
based on the target value of the most similar points in the training data. For example,
consider a regression problem with X = R2 and Y = R, the following training data, and
the new red point:
The (x, y) position of the points corresponds to their value in X = R2. The number next
to them corresponds to their target value in Y = R. (The four closest points just
happen to have target values 1-4 by coincidence.) The other training points also have
target values, which are not shown - but since they're real numbers, they are probably
things like 
35.23412576354868569754543639946795856858465745654965464964534... .
This picture begs the question of how many neighbors to take into account. This is a
parameter: in a k-nearest neighbor scheme, we consider the target values of the k
nearest instances. In the instance above, if k = 1, we have the following situation:

(Note that the circle just demonstrates which points are closest - we do not choose all
points within a ﬁxed distance.)
For k = 2, we have two inputs and need to decide what to do with them. For now, we
simply declare that the label of our red point will be the mean of all k neighbors.
k = 3:
k = 4:

If the function we're trying to learn here is something like distance from a center and
the red point is precisely at that center, then the scheme gets worse and worse the
larger k becomes. On the other hand, suppose the instance looks like this instead:
where the new point is an unfortunate outlier. In this case, the situation would be
somewhat improved for larger k. In general, k = 1 means that every point will get to
decide the target values of some new training points (unless there are other training
points with identical positions). Here is an image that shows, for each point, the area
in which that point will determine the target value of new points given that k = 1:
This is also called a Voronoi diagram, and it has some relevance outside of Machine
Learning. For example, suppose the red dots are gas stations. Then, for any point, the
[red instance determining the cell in which that point lies] represents the closest gas
station.

However, for machine learning, this might not be great - just suppose the green point
is a crazy outlier. Should it get to decide the target values of new instances in its cell?
Another way to describe this problem is that the [function that such a nearest-
neighbor predictor will implement] is highly discontinuous.
In general, the more variance there is in the training data, the larger k should be, but
making k larger will, in general, decrease accuracy - the well-known tradeoﬀ. In that
sense, one is applying a little bit of prior knowledge to the problem by choosing k. But
not much - consider the following instance (this time with X = R):
A linear predictor would probably assign the red point a value that continues the trend
- something like 7.000000000142. This is because linear models work on the
assumption that trends are, in some sense, more likely to continue than to
spontaneously revert. Not so with nearest neighbor schemes - if k = 1 it would get the
label 6, and for larger k, the label would become smaller rather than larger. In that
way, nearest neighbor schemes make weaker assumptions than most other
predictors. This fact provides some insight into the question of when they are a good
choice for a learning model.
One can also use a weighted average as the prediction, rather than the classical
mean. Let's return to our instance, and let's use actual distances:
One way to do this is to weight each prediction proportional to the inverse distance of
the respective neighbor. In that case, this would lead to the prediction
≈2.08736
12.22−1 ⋅1 + 18.63−1 ⋅2 + 28.76−1 ⋅3 + 30.88−1 ⋅4
12.22−1 + 18.63−1 + 28.76−1 + 30.88−1

where the denominator is there to normalize the term, i.e., make it as if all weights
sum up to 1. (You can imagine dividing each weight by the denominator rather than
the entire term; then, the weights literally sum up to 1.) In this case, the function we
implement would be somewhat more smooth, although each point would still
dominate in some small area. To make it properly smooth, one could take the inverse
of [the distance plus 1] (thus making the weights range between 0 and 1 rather than 0
and ∞) and set k = m so that all training points are taken into consideration.
Note that I've been using regression as an example throughout, but decision trees can
also be used for classiﬁcation problems. In that case, each point gets the label which
is most popular among its neighbors, as decided by a (weighted) majority vote.
Are there any guarantees we can prove for nearest neighbor schemes?
The question is a bit broad, but as far as sample complexity bounds/error bounds are
concerned, the answer is "not without making some assumptions." Continuity of the
target function is a necessary condition, but not enough by itself. Consider how much
each point tells us about the function we're trying to learn - clearly, it depends on how
fast the function changes. Then - suppose our function changes at a pace such that
each point provides this amount of information (the green circle denotes the area in
which the function has changed "suﬃciently little," whatever that means for our case):
In this case, the training data will let us predict new points. But suppose it changes
much more quickly:

In this case, we have no chance. In general, for any amount of training data, we can
imagine a target function that changes so quickly that we don't have a clue about how
it looks on a majority of the area. Thus, in order to derive error bounds, one needs to
assume a cap on this rate of change. If you recall the chapter on convexity, this
property is precisely ρ-Lipschitzness.
There's a theorem to that eﬀect, but the statement is complicated and the proof is
boring. Let's talk about trees. A decision tree is a predictor that looks like this:
(In this case, for a binary classiﬁcation problem.) If you have never seen a tree before,
don't despair - trees are simple. The triangles are called internal nodes, the four
endpoints are called leaves, and the lines are called edges. We begin at "Start,"
evaluate the ﬁrst "Condition," then move down accordingly - pretty self-explanatory.
Trees show up all over the place in Computer Science.
If we allow arbitrary conditions, this is somewhat silly - each predictor h can then be
represented as a tree via

So the general rule is that the conditions be very simple. That being said, there are
several ways one could illustrate that this class is quite expressive. For example,
consider a rectangle with labeled sides like this:
The following tree realizes a predictor which labels instances in the rectangle
positively and anywhere outside negatively:

The same principle also illustrates how we can implement a logical AND, and OR goes
analogously. You may work out how exactly this can be done - however, I don't think
those are the best ways to demonstrate what trees are really about. Instead, consider
what happens when the tree branches in two:
When the tree branches in two, the domain space is divided in two. And this is true for
each branching point, even if the area corresponding to a node isn't the entire domain

space to begin with:
For the initial examples, I have just written down the labels (the 1 and 0 at the leaves)
as part of the tree. In reality, they are derived from the training data: for any leaf, the
only reasonable label is that which the majority of training points in the corresponding
part of the domain space have. And this is why this post isn't called "nearest neighbor
and decision trees" - decision trees are a nearest neighbor scheme. The diﬀerence lies
in how the neighborhoods are constructed: in the classical approach, the
neighborhood for each point p is based on the distance of every other point to p, while
in a tree, the neighborhoods are the cells that correspond to the leaves.
In particular, this makes the neighbor-relationship of a tree transitive - if p is a
neighbor of x and x is a neighbor of q, then p is a neighbor of q - which is not true for
the classical nearest-neighbor approach.
You might recall the concept of VC-dimension from chapter II, which applies for binary
classiﬁcation tasks. To recap: for a given hypothesis class H, the VC-dimension is the
largest n ∈N such that there exists a set of n domain points which is shattered by H.
What does it mean for a set P ⊆X to be shattered by H? It means that H contains a

predictor for any possible labeling combination of the points in P. If P has n elements,
these are 2n many combinations. (For example, if P = {x, y}, then H has to contain a
predictor h00 with h0(x) = 0 = h0(y), a predictor h11 with h11(x) = 1 = h11(y) and also
predictors h01 and h10.) The VC dimension is a measure for the complexity of a class
because if all labeling combinations are possible, then learning about the labels of
some of the points doesn't tell us the labels of the others. There are both upper- and
lower bounds on the sample complexity for classes with ﬁnite VC dimension (provided
we allow arbitrary probability distributions D generating our label points).
This is relevant for decision trees because their VC dimension is trivial to compute...
... it simply equals the number of leaves. This follows from the fact that a tree divides
the domain space into n subsets where n is the number of leaves, as we've just
argued. It's equally easy to see that larger subsets cannot be shattered because the
tree will assign all points within the same subset the same label.
It follows that the class of arbitrary trees has inﬁnite VC-dimension, whereas the class
of trees with depth at most d has VC-dimension 2d−1 (the number of leaves doubles
with every level we're allowed to go downward).
How do we obtain trees?
Since mathematicians are lazy, we don't like to go out and grow trees ourselves.
Instead, we'd like to derive an algorithm that does the hard work for us.

One possibility is a simple greedy algorithm. The term greedy is commonly used in
computer science and refers to algorithms that make locally optimal choices. For
example, consider the knapsack problem where one is given a number of possible
objects that have a weight and a value, and the goal is to pack a subset of them that
stays below a particular total weight and has maximum value. A greedy algorithm
would start by computing the 
 scores for each item, and start packing the optimal
ones.
The general dynamic with greedy algorithms is that there exist cases where they
perform poorly, but they tend to perform well in practice. For the knapsack problem,
consider the following instance:
A greedy algorithm would start by packing item 3 because it has the best 
 score,
at which point the remaining capacity isn't large enough for another item, and the
game is over. Meanwhile, packing items 1 & 2 would have been the optimal solution to
this problem.
Now we can do something similar for trees. We begin with one of these two trees,
namely, we choose the one that performs better on the training sequence (i.e., if more
than half of our training points have label 0, we go with the ﬁrst tree). After this ﬁrst
step, all remaining steps are the same - we improve the tree by replacing a leaf with a
[condition from which two edges go out into two leaves with labels 0 and 1,
respectively]. This corresponds to choosing an area of the domain space and dividing
it in two.
value
weight
value
weight

So let's say we've started with the ﬁrst tree. We only have one leaf, so the ﬁrst step
will replace that leaf with [⋯]:
Now just imagine the same thing starting from an arbitrarily large tree: we take one of
its many leaves and substitute the object in the blue circle for it (either exactly the
same object or the same except with labels swapped). Somewhere in the domain
space, some area that was previously all 0 or all 1 now gets divided into two areas,
one with 1 and one with 0.
Since we're running a greedy algorithm, it chooses the split such that the total
performance of the tree (on the training data) improves the most - without taking into
consideration how this aﬀects future improvements. Of course, we need to restrict
ourselves to simple conditions; otherwise, the whole thing becomes pointless - recall
this tree:

For example, each restriction could be of the form "bit #k of the input point is 1".
Many other approaches are possible. Having chosen such a class of restrictions, and
also a way to measure how much performance is improved by each restriction, this
deﬁnes a simple greedy algorithm.
Now, recall that the VC dimension of the class of trees is inﬁnite. Thus, if we let our
algorithm run for too long, it will keep growing and growing the tree until it is very
large - at that point, it will have overﬁt the training data signiﬁcantly, and its true
error will probably be quite high. There are several approaches to remedy this
problem:
Terminate the algorithm earlier
After the algorithm has run to completion, run an algorithm doing the opposite,
i.e., cutting down the tree to make it more simple while losing as little
performance on the training data as possible
Grow more trees
To elaborate on the last point: since the problem with an overly large tree is that it
learns noise from the training data - i.e., quirks that are only there by chance and
don't represent the real world - one way to combat this is by having a bunch of trees
and letting them deﬁne a predictor by majority vote. This will lead to the noise
canceling out, for the same reason that Stochastic Gradient Descent works. There are,
again, at least two ways to do this:
Use the same tree-growing algorithm, but give it a randomly chosen subset of
the training data each time
Run a modiﬁed version of the algorithm several times, and ...
... use the all training data each time; but
... for each step, choose the optimal splitting point from a randomly chosen
subset, rather than from all possible splits
The result is called a random forest.
If we have a random forest, are we still implementing a nearest neighbor scheme? To
conﬁrm that the answer is yes, let's work a uniﬁed notation for all nearest neighbor
schemes.

Suppose we have the training data S = ((x1, y1), . . . , (xm, ym)), and consider a
weighting function w : X × {x1, . . . , xm} →R that says "how much of a neighbor" each
training point is to a new domain point. In k-nearest neighbor, we will have that 
w(x, xj) = 1 iﬀ xj is one of the k nearest neighbors of x and 0 otherwise. For weighted k
-nearest neighbor, if xj is among the k nearest neighbors to x, then w(x, xj) will be
some (in (0, 1) or in R+, depending on the weighting) determined by how close it is,
and otherwise, it will be 0. For a tree, w(x, xj) will be 1 iﬀ both x and xj are in the same
cell of the partition which the tree has induced on the domain space.
To write the following down in a clean way, let's pretend that we're in a regression
problem, i.e., the yk are values in R. You can be assured that it also applies to
classiﬁcation, it's just that it requires to mix in an additional function to realize the
majority vote.
Given that we are in a regression problem, we have that
h(x) =
∑
m
i=1 w(xi, x) ⋅yi
where h = Aclassical k-nearest-neighbor(S) = AkNN(S). Recall that A is a learning algorithm in
our notation, and S is the training data, so A(S) is the predictor it outputs.
For a tree, the formula looks the same (but the weighting function is diﬀerent). And for
a forest made out of trees 1, . . . , n, we'll have n diﬀerent weighting functions 
w1, . . . , wn, where wj is the weighting according to tree #j. Then for 
h = Arandom n-forest(S), we have
h(x) =
∑
n
j=1
∑
m
i=1 wj(xi, x) ⋅yi
which can be rewritten as
h(x) =
m
∑
i=1
[
∑
n
j=1 wj(xi, x)] ⋅yi
which can, in turn, be rewritten as
1
m
1n
1
m
1n
1
m

h(x) = ∑
m
i=1 w∗(xi, x) ⋅yi
where
w∗(xi, x) := ∑
n
j=1 wj(xi, x)
so it is just another nearest-neighbor scheme with weighting function w∗.

Attainable Utility Preservation:
Concepts
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.










Appendix: No free impact
What if we want the agent to single-handedly ensure the future is stable and aligned
with our values? AUP probably won't allow policies which actually accomplish this goal
- one needs power to e.g. nip unaligned superintelligences in the bud. AUP aims to
prevent catastrophes by stopping bad agents from gaining power to do bad things, but
it symmetrically impedes otherwise-good agents.
This doesn't mean we can't get useful work out of agents - there are important
asymmetries provided by both the main reward function and AU landscape
counterfactuals.

First, even though we can't specify an aligned reward function, the provided reward
function still gives the agent useful information about what we want. If we need
paperclips, then a paperclip-AUP agent prefers policies which make some paperclips.
Simple.
Second, if we don't like what it's beginning to do, we can shut it oﬀ (because it hasn't
gained power over us). Therefore, it has "approval incentives" which bias it towards AU
landscapes in which its power hasn't decreased too much, either.
So we can hope to build a non-catastrophic AUP agent and get useful work out of it. We
just can't directly ask it to solve all of our problems: it doesn't make much sense to
speak of a "low-impact singleton".
Notes
To emphasize, when I say "AUP agents do X" in this post, I mean that AUP agents
correctly implementing the concept of AUP tend to behave in a certain way.
As pointed out by Daniel Filan, AUP suggests that one might work better in groups
by ensuring one's actions preserve teammates' AUs.

Curiosity Killed the Cat and the
Asymptotically Optimal Agent
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Here's a new(ish) paper that I worked on with Marcus Hutter.
If an agent is exploring enough to be guaranteed strong performance in the limit, that
much exploration is enough to kill it (if the environment is minimally diﬃcult and
dangerous). It's nothing too surprising, but if you're making a claim along these lines
about exploration being dangerous, and you need something to cite, this might work.
My attitude towards safe exploration is: exploration isn't safe. Don't do it. Have a
person or some trusted entity do it for you. The paper can also be read as a
justiﬁcation of that view.
Obviously, there are many more details in the paper.

Blog Post Day (Unoﬃcial)
TL;DR: You are invited to join us online on Saturday the 29th, to write that blog post
you've been thinking about writing but never got around to. Comment if you accept
this invitation, so I can gauge interest.
The Problem:
Like me, you are too scared and/or lazy to write up this idea you've had. What if it's
not good? I started a draft but... Etc.
The Solution:
1. Higher motivation via Time Crunch and Peer Encouragement
We'll set an oﬃcial goal of having the post put up by midnight. Also, we'll meet up in a
special-purpose discord channel to chat, encourage each other, swap half-ﬁnished
drafts, etc. If like me you are intending to write the thing one day eventually, well,
here's a reason to make that day this day.
2. Lower standards via Time Crunch and Safety in Numbers
Since we have to be done by midnight, we'll all be under time pressure and any errors
or imperfections in the posts will be forgivable. Besides, they can always be ﬁxed later
via edits. Meanwhile, since a bunch of us will be posting on the same day, writing a
sloppy post just means it won't be read much, since everyone will be talking about the
handful of posts that turn out to be really good. If you are like me, these thoughts are
comforting and encouraging.
Evidence this Works:
MIRI Summer Fellows Program had a Blog Post Day towards the end, and it was
enormously successful. It worked for me, for example: It squeezed two good posts out
of me. (OK, so one of them I ﬁnished up early the next morning, so I guess it
technically doesn't count. But in spirit it does: It wouldn't have happened at all without
Blog Post Day.) More importantly, MSFP keeps doing this every year, even though
opportunity cost for them is much higher (probably) than the opportunity cost for you
or me. I don't know what else you had planned for Saturday the 29th... (Actually, if
you do have something else planned, but otherwise want to participate in Blog Post
Day, let me know. Maybe we can pick a diﬀerent day.)
Side Beneﬁts:
It'll be fun!

Does there exist an AGI-level
parameter setting for modern DRL
architectures?
Suppose the architecture includes memory (in the form of a recurrent state) and will
act as the policy network for an observation-based RL agent. Evaluating the agent
from a reasonable initial state, would you guess that there exists a model with
robustly human+ capabilities for current architectures?
How many parameters would it take before you estimate there's a ﬁfty-ﬁfty chance of
such a parameter setting existing? 1 billion? 1 trillion? More?

Potential Research Topic: Vingean
Reﬂection, Value Alignment and
Aspiration
Epistemic Status: Potential research idea. Time-limited, so not as clear as it could have
been.
Vingean reﬂection is the process of trying to anticipate how an agent smarter than
you might think, in order to ensure that it will be aligned with your values. This is
hard, because "if [an agent] could predict [a smarter agent's] actions in detail, it
would already be as smart as them." Value Learning is the problem of trying to use
machine learning to train an AI to care about what humans care about.
I haven't read much about these problems, but they struck me as related to a concept
introduced by philosopher Agnes Callard: "aspiration." Her idea is that, sometimes, we
come to care about things that we didn't care about before, and, in particular, that: (1)
this doesn't happen all at once, and (2) we play an active role in the process. She
argues in her book (which I haven't read yet, but see the interview I just linked with
her and Robert Wright) that in several diﬀerent areas of philosophy (decision theory,
moral psychology, and moral responsibility) the prevailing theories make assumptions
that would render this process paradoxical or impossible.
To see what aspiration looks like, consider some value that you didn't have before, but
now do. Since I don't know you, I'll give a generic example, but substitute in whatever
actually applies to you. Suppose you are now a gourmand, though you didn't care
much about good food when you were younger (this apparently happened to a friend
of Callard's). How did you get from there to here? Perhaps there was a moment where
you ﬁrst got excited about food (in the case of Callard's friend, she took a trip to
Ousaka, Japan). But this probably isn't the whole story, at least not in many cases.
This lucky, random encounter provided the ﬁrst shove to get you onto the path
towards being a gourmand, but it didn't take you all the way. You got an inkling of the
value of good food by having some in Oosaka, but you had to choose to cultivate this
interest. But how is it possible to move yourself further along this path, without
already knowing how a gourmand would value good food? It seems like if you care
enough to want to get better at valuing good food, then you must already be the kind
of person who cares about good food. And how can you critique your own taste
without already having the sort of trained palette that future-you will (might) have?
How can you improve, without being able to fully see the end of the path? And if you
could fully see the end of the path, wouldn't you already be there? (If this description
seems unclear, it probably is, and I unfortunately don't have the time to put into
making it clearer; please go watch the Robert Wright interview to actually understand
what's going on).
Some clarifying points from the interview:
Wright: The paradox is: until you have a value, you don't value it. So how does one
get from the place of not valuing it at all, to suddenly valuing it?
...

Callard [clarifying]: The way I think about it is, how do you go from caring about it
very little, to caring about it a little more; how do you increase your caring for
something?
...
Wright: So you're interested in the dynamics of the process itself---what sustains
the transition and the progress?
...
Callard [later]: I'm saying there's such a thing as self-creation [because your
values are part of yourself, so if you have a hand in creating your values, then you
have a hand in creating yourself].
This sounds a lot like the Vingean Reﬂection: if an agent could predict how future-
them would act, they would already be future-them. It also sounds a lot like value
learning: in a sense it is a type of value learning---learning the values that you want
yourself to have, or the values that your potential future self has. There are obvious
diﬀerences, but I think the similarities should also be apparent (especially if you've
also watched the interview).
One of the prevailing methods of doing philosophy on LessWrong is this: for any
philosophical concept, ask, "how would you build an AI that does that." And I think
that asking "how would you build an AI that could do aspiration?" sounds a lot like the
problems of Vingean Reﬂection and Value Learning (or perhaps some combination of
the two: learning to predict how a future version of you with better values would act,
and emulate them in order to become them). I think an interesting research project
would be to investigate to what extent Callard's work on aspiration is relevant to
solving Vingean reﬂection and the value learning problem. Unfortunately, I'm not in a
position to do this myself right now, but I wanted to advertise that this was a possible
research question, either for my future self (heh) or some other person. The main
tasks would be to read Callard's book, and the literatures on Vingean reﬂection and
the Value Learning Problem, and see what fruitful connections can be made, if any.
Again, apologies that I can't lay out the research question more clearly; if I were in a
position to do that (time-wise and expertise-wise) I would probably also be in a
position to actually do the project, but I'm not (note, this need not be a long,
protracted project; reading Callard's book and the relevant literature could probably
be done in a week of full-time work, give or take a few days depending on how much
literature there is, and at that point one would be in a position to evaluate whether
there were any fruitful connections to be drawn. And if one is already familiar with the
VR and VL literatures, it might just take the time of reading Callard's book and writing
up relevant ﬁndings if any).
(Side note: I actually think the concept of aspiration may also have relevance to value
drift, and movement growth, on both a personal and movement level: learning how to
change one's values may also provide insight on how to keep them stable, and
learning how value change is possible may provide insight on how to shape other
people's values to be more aligned with EA. But I think Callard's book doesn't talk as
much about the nitty-gritty of how aspiration works, but rather more about the
philosophical problems it poses. My suggestion is that these problems seem very
similar to the problems posed by Vingean Reﬂection/Value Learning, and that looking
at her solutions may provide new insight on these alignment problems. The
movement-growth stuﬀ would take more extrapolation from her book, I think).

(Also, if this would be better posted as a question, I'd be happy to repost it as one or
have the mods do so.)

Simulation of technological progress
(work in progress)
I've made a model/simulation of technological progress, that you can download and
run on your laptop.
My goal is to learn something about intelligence explosions, takeoﬀ speeds,
discontinuities, human-level milestones, AGI vs. tools, bottlenecks, or something else.
I'll be happy if I can learn something about even one of these things, even if it's just a
minor update and not anything close to conclusive.
So far I've just got a very basic version of the model built. It works, but it's currently
unclear what--if anything--we can learn from it. I need to think more about whether
the assumptions it uses are realistic, and I need to explore the space of parameter
settings more systematically.
I'm posting it here to get feedback on the basic idea, and maybe also on the model so
far if people want to download it and play around. I'm particularly interested in
evidence/arguments about whether or not this is a productive use of my time, and
arguments that some hidden assumption my model makes is problematically
determining the results.
If you want to try out the model yourself, download NetLogo here and then open the
ﬁle in this folder.
How the model works:
The main part of the model consists of research projects, which are lists of various
types of task. Civilization completes tasks to complete research projects, and when
projects get ﬁnished, civilization gets a "bonus" which allows it to do new types of
task, and to do some old types faster.
The projects, the lists of tasks needed to complete them, the speeds at which
civilization can do the tasks, and the bonuses granted by completing projects are all
randomly generated, typically using exponential distributions and often with
parameters you can change in the UI. Other important parameters can be changed in
the UI also, such as how many task types are "oﬀ limits" for technological
improvement, and how many task types are "temporarily oﬀ limits" until some
speciﬁed level of technology is reached.
As explained so far, the model represents better technology leading to more research
directions (more types of task become available) and faster progress (civilization can
do tasks in less time).
Projects are displayed as dots/stars which ﬂicker as work is done on them. When they
complete, they turn into big green circles. Their location in the display represents how
diﬃcult they are to complete: the x-axis encodes how many tasks are involved, and
the y-axis encodes how many diﬀerent kinds of tasks are involved. To the left of the
main display is a graph that tracks a bunch of metrics I've deemed interesting, scaled
so that they all have similar heights.

There are several kinds of diminishing returns and several kinds of increasing returns
in the model.
Diminishing:
"Projects farther from the bottom left corner on the display require exponentially
more tasks and exponentially more kinds of tasks.
Project bonuses work as follows: Pick a random bag of tasks, pick random
speeds for each task, compare the results to the current state-of-the-art speeds
on those tasks and update the state of the art accordingly. Thus, the faster
civilization already is at completing a task, the more projects will need to be
completed (on average) to improve speed at that task further.
Finally, there is the usual "low-hanging fruit" eﬀect where the projects which can
get done quickly do so, leaving harder and harder projects remaining.
Increasing:
The more projects you do, the more bonuses you get. This makes you faster at
completing projects...
And opens up new projects to work on, some of which will be low-hanging fruit.
The model also has a simple module representing the "economic" side of things -- i.e.
over time, civilization can work on a greater number of projects simultaneously, if you
choose. I have a few diﬀerent settings representing diﬀerent scenarios:
"All projects all the time" represents a situation where science has loads of
funding and/or excellent planning of research paths, so that the only
constraints on ﬁnishing projects are whether and how fast the tasks
involved can be done.
"100 doable projects" represents a situation with ﬁxed research budget:
100 doable projects are being worked on at any given time.
Scaling eﬀort with projectscompleted represents "learning by doing" where
the more projects have been completed so far, the more eﬀort is invested
in doing more projects.
Scaling eﬀort with machinetaskspeed represents a situation where how
much eﬀort is devoted to science is proportional to how advanced today's
tech is on average.
The "info" tab of the NetLogo ﬁle explains things in more detail, if you are interested.
What tends to happen when I run it:
The model tends to produce progress (speciﬁcally, in the metric of "projects
completed -- see the log plot) somewhere between exponential and superexponential.
Sometimes it displays what appears to be a clear exponential trend (a very straight
line on the log scale) that fairly rapidly transitions into a singularity (a vertical line on
the log scale).
Interestingly, progress in the metric "% of tasks done faster thanks to research" is not
typically exponential, much less singularity; it is usually a jumpy but more or less
linear march from 0 to 100.

Sometimes progress stagnates, though I've only seen this happen extremely early on--
I've never seen steady exponential growth followed by stagnation.
For a while it seemed that progress would typically shoot through the roof around the
time that almost all tasks were doable & being improved. This is what Amdahl's Law
would predict, I think: Get rid of the last few bottlenecks and progress will soar.
However, I now think that's wrong; the growth still happens even if a substantial
fraction of tasks are "oﬀ-limits," and/or oﬀ-limits temporarily. I'm not sure what to
think now, but after I give my head a rest I expect ideas will come.
The various parameter settings I've put into the model seem to have surprisingly little
eﬀect on all of the above. They aﬀect how long everything takes but rarely do they
aﬀect the fundamental shape of the trajectory. In particular, removing the "eﬀort
feedback loop" entirely, by choosing "all projects all the time" or "100 doable projects"
would (I predicted) slow down progress a lot, but in practice we still seem to get
singularities. Of course, I haven't systematically compared the results; this is just the
vague impression I get from the handful of diﬀerent runs I've done.
Doubts I have about the accuracy of the
model & ideas for things to add
Most importantly, there are probably major ﬂawed assumptions I've made in
building this model that I haven't realized yet. I don't know what I don't know.
I worry that the results depend in a brittle fashion on the probability distributions
and ranges that I use to generate the projects. In other words, I'm worried that
the results, though robust to the parameters I've put in the UI, are not robust to
hidden assumptions that I made hastily.
Often if you can't do a project one way, there is another path via which you can
do it. Thus we can e.g. use brute force search in a computer + a small amount of
thinking to replace a larger amount of a diﬀerent kind of thinking. But in my
model, all projects have a single list of tasks that need to be done to complete
the project. Is this a problem?
Maybe I should try to build a version of this model that gets exponential growth
followed by stagnation, or at least some sort of big s-curve? Maybe the reason I
haven't seen this behavior so far is that I haven't put much eﬀort into looking for
it.
Ultimately I'd like to have a much richer economy in the model, with diﬀerent
factions buying and selling things as they separately advance up the tech tree.
This sounds hard to implement in code, but maybe it's easier than I think. Maybe
economics can help me with this.
Currently I have to wait a minute or so each run, depending on settings (some
runs happen in just a few seconds). This is my very ﬁrst coding project so the
code is probably atrocious and could run signiﬁcantly faster if I optimized it. If I
could run it many times, I'd learn how much the results vary from run to run due
to randomness.

Protecting Large Projects Against
Mazedom
If we wish to accomplish something that would beneﬁt from or require a
larger organization or more levels of management and bureaucracy, what
should we do in light of the dangers of mazes?
There are no easy answers. Real tradeoﬀs with real sacriﬁce are the order of the day.
But we can do some things to expand the production possibilities frontier, and choose
wisely along that frontier.
As is often the case, this starts with admitting you have a problem. 
Too often, it is assumed that one should scale without worrying about the costs of
scaling, or without counting becoming a maze as one of the biggest costs. Not
stretching oneself maximally thin, or getting in the way of this process, becomes the
sin of not maximizing eﬀectiveness or proﬁts.
That ensures failure and rapid descent into a maze. Start by getting out of this
mindset. 
If you are looking to accomplish a big thing that requires lots of organization,
management and bureaucracy, here are ways to help contain the damage. 
None of them should come as a surprise by this point. This is more of a synthesis of
points already made, and not a place I feel I have special additional insights. So I will
keep this short.
 
Solution 1: Do Less Things and Be Smaller
Recognize the threat and its seriousness, and the resulting risks and costs of scaling
even if handled wisely. Understand that you almost certainly want to be smaller and
do less things due to this concern. This is a real trade-oﬀ.
Think of the actions, priorities, skills and members of a group as being increasingly
inherently expensive as the group grows - adding new elements has increasing
marginal costs. Changing anything, or preventing anything from taking its natural
course (including towards being more of a maze) also becomes increasingly
expensive. Under such circumstances, the ﬁnal marginal cost of every action or
member is equal to the marginal cost of the ﬁnal action taken or member added. 
One must think about the future scale and the future costs, solve for the resulting
budget constraints and trade-oﬀs, and spend wisely. The same way that one must
avoid accumulating technical debt, an organization should worry greatly about
complexity and culture long before the bills are presented.
Remember that not doing something does not condemn that thing to never being
done.

Encourage others to form distinct groups and organizations to do those other things,
or where possible to do those other things on their own. Promise and give rewards and
trade relations for those that do so. Parts that can operate on their own should usually
do so. If your organization discovers a new product, business or other undertaking
that is worth pursuing, but isn't a cultural or logistical ﬁt for what already exists, or
simply doesn't need to be in the same place to work, strongly consider spinning it oﬀ
into its own thing.
This solution applies fractally throughout the process. Do only one central, big thing.
Do less major things in support of that thing. Do less minor things in support of each
of the major things. Do each of those minor things as elegantly and simply as you can.
Take every opportunity to simplify, to look for easier ways to do things, and to
eliminate unnecessary work. 
Solution 2: Minimize Levels of Hierarchy
Throughout this sequence we have emphasized the high toxicity of each level of
hierarchy. If you must scale, attempt to do so with the minimum number of
hierarchical levels, keeping as many people within one level of the top or bottom as
possible. Fully ﬂat is not a thing, but more ﬂat is better.
Solution 3: Skin in the Game
Scale inherently limits skin in the game, as there is only 100% equity to go around, in
all its forms. The thinner that must be spread, the harder it is to provide enough skin
in the game. One can still seek to provide good incentives all around to the extent
that it is possible. Isolating local outcomes so as to oﬀer localized skin in the game
helps. Keeping people responsible for areas over extended periods also helps.
Solution 4: Soul in the Game
No matter how large the organization, if you care deeply about the organization or its
mission, preferably the mission, you can still have soul in the game. Mission selection
is huge part of this, as some missions lend themselves to soul much more than others,
but  if you have a mission you are setting out to do as the starting point, you're stuck.
That means protecting against mission creep that would disrupt people's soul in the
game above and beyond other issues of mission creep. Keep people doing what they
are passionate about.
Solution 5: Hire and Fire Carefully
Nothing raises maze levels faster than hiring someone who is maze aligned. One must
not only avoid this, but maintain suﬃcient maze opposition to prevent it from
happening in the future. All of this needs to sustain itself, which will be increasingly
diﬃcult over time and as you grow. As noted earlier, maze actions need to be ﬁring
oﬀenses.
Solution 6: Promote, Reward and Evaluate Carefully
If you can ﬁnd ways to evaluate people, and choose which ones to promote and
reward, in ways that are immune or even resistant to mazes and their politics, this
would be a giant leg up. Hiring and ﬁring are key moments, but the promotion can be
similarly important. One must resist the temptation to try to implement 'objective
criteria' and use hard numbers, as this introduces nasty Goodhart problems, and
forces the system to choose between 'people around you can change the numbers so

the maze wins out' and 'people around you can't change the numbers and no one
cares about the people around them.' It's a big problem.
Solution 7: Fight for Culture
This can be seen as a catch-all, but the most important thing of all is to care about
organizational culture and ﬁght for it. Where what you are ﬁghting for is not a maze.
All of these 'solutions' often involve trade-oﬀ and sacriﬁce, and this is no exception. If
you do not value the culture enough to ﬁght for it, the culture will die.
Solution 8: Avoid Other Mazes
This will not always be possible, but to the extent it is possible, attempt to sell to, buy
from, get funding from, make deals with non-mazes, and avoid mazes. Mazes will
reward maze behaviors and push towards you raising maze levels, in ways that they 
will make seem natural. Do not put yourself in that position more than necessary.
Solution 9: Start Again
We must periodically start again. Even if everything is done right, within any given
organization we are only staving oﬀ the inevitable. At a minimum we must periodically
clean house, but that seems unlikely to be enough for that long. Actually starting over
and building something new every so often, where frequency is highly context-
dependent, seems necessary. This goes for corporations, for schools, for governments,
and for everything else. 

The Relational Stance
There's a concept that's pretty core to my understanding of relationships, which I'm
sure must have been written up somewhere but I don't know of a good explanation.
Here is a ﬁrst pass.
The concept is, well, 'relating' - the meaning that you ascribe to a relationship, and
the stance from which you are oriented towards it.
I think relating is particularly important when you choose that meaning deliberately.
The Coworker
If you have a coworker you who make jokes with, and get reasonable amounts of work
done with, that... could just be some random acquaintance who you pass the time
with...
Or it could be someone you choose to make a larger part of your life (a friend, or
trusted colleague, that you would help out in time of need, or go to bat for). 
Or it could be someone that you don't expect to go to bat for or help in crisis... but
nonetheless, when you reﬂect upon them, you decide "this person is an important part
of my life. If I left this job I wouldn't stay in touch with them, our relationship is
ephemeral. But I might still ascribe importance to them being part of my life, right
now. Their jokes are silly but something about them makes life better in a way I deeply
appreciate. The little kindnesses, or competences, that they demonstrate at work are
meaningful to me."
There are many ways you could choose to relate to a person, independent of what
kind of activities you do together and commitments you've made.
The Romantic Partner
The original context I got this idea from was some relationship advice book I stumbled
upon as a teenager, which said "Love is not an emotion. Love it a choice." 
Sometime later, I fell in love for the ﬁrst time. Over the course of a few months, I
noticed a few diﬀerent things going on in my head:
1. I had a crush on them, i.e. feelings of limerence and infatuation
2. I had some kind of strong and more signiﬁcant-seeming feelings, that I thought
might be love.
3. Several months later, I found that although the limerence feelings came and
went without warning, and the stronger #2 feelings came and went on slower
timescales (weeks or months), there some other aspect of my orientation
towards them that didn't go away at all. And it wasn't a feeling, it was something
diﬀerent. There was a particular way I cared about them, felt they were a good
person, wanted them to be part of my life, wanted them to succeed and be
happy even if I wasn't part of their life.

And at some point I asked myself "is this #3 thing love?"
And I thought about the line "Love is not an emotion, love is a choice." 
And then (#4) I decided I loved them, and then, in deciding it, it became truer.
I think Thing #2 and #3 were both reasonable things to call love, as well. At least, I
don't have a better word for them. But they were notably diﬀerent things. 
Thing #3 was similar to thing #4. But it didn't have deliberate intentionality to it. It
wasn't exactly a choice. It was a mental stance. Diﬀerent from a feeling. And also
diﬀerent from any particular set of activities or commitments we might have been
doing.
Relating to people is mental stance. It can be intentional, or reﬂexive. It can come with
commitments, or not. It can be mutually intentional, or asymmetric. 
The Relational Stance is when you make the more general choice to consider how you
relate to someone. If someone has become part of your life, and you've accumulated
some feelings and implicit commitments and reﬂexive stances toward them, you may
deliberately ask "what do these feelings and commitments and stances and activities
mean to me." 
Maybe you decide they don't mean that much to you, they're just an incidental part of
your life. Or maybe you decide they are an important part of your life.
Self-Reﬂective Relationships
In romantic contexts, there's a stereotypical moment where you lay your cards on the
table: "I love you." And they might, or might not say back, "I love you too." 
There's a harder part that may come later - what does that love mean? Does it mean
that we'll see each other once a week? Does it mean we're on the Relationship
Escalator where we date for awhile and then move in and get married? Does it mean
we'll have a whirlwind summer romance that burns quickly and bright, and leaves a
pleasant memory in its wake?
What does this mean about what sort of things we will do together, and what
commitments we're making?
There are a bunch of practical questions there, and a bunch of important feelings-
oriented-questions there. If you want a whirlwind summer romance and I want a
longterm commitment, well, hmm. Perhaps I feel sad about that. 
But the thing I want to focus on here is asking "how do I relate to that?", which is
somewhat diﬀerent from how I feel. There may have been a particular ﬂavor of the
way I cared about you, and what meaning I found in our relationship, and the way I
see you. I don't have very good words for it. And if I ﬁnd out that you see me
diﬀerently than I saw you, that might change the way I see you in turn.
And then you might get into a recursive, unstable loop. 
Say that Alice ﬁrst looked at Bob as a deeply meaningful but ephemeral summer ﬂing.
And Bob had thought of Alice as a potentially longterm stable partner, but one that he

nonetheless wasn't that committed to yet. He hadn't yet reﬂected on how meaningful
the relationship was, and upon reﬂection, he isn't sure.
Some people don't share their their inner stances with each other. Some people don't
have the introspective awareness to even know what their internal stances are,
reliably (or their feelings, or goals).
But, say Alice and Bob are both pretty self-aware, as individuals. And then they both
communicate their thoughts on the relationship. And now Alice is aware that Bob
wanted something diﬀerent out of the relationship than she did. And Bob is aware that
although she saw it as ephemeral, she saw it as something profound. And that makes
Bob consider taking the relationship more seriously... but at the same time, Alice has
noticed that Bob was coming into it with diﬀerent expectations, and maybe that
makes her feel a little more distant, which Bob then picks up and feels a little more
distant...
...or, maybe Bob is like "No, wait, yeah, this relationship is important to me. And it's
still important to me even if it's an ephemeral summer romance. And I do still hope
that maybe you'll decide it's worth staying part of each other's lives after our
 vacations end, but if you don't decide that, it's okay."
There are multiple stable equilibria they could eventually reach (and multiple unstable
arrangements where each of them is constantly shifting their stance, in unpredictable
ways that are really stressful).
But, maybe, the relationship hits some kind of equilibrium where they've both
honestly shared their stances and feelings and expectations with each other, and then
updated in response to those things, and then updated in response that, eventually
settling into something stable.
Mutually Intentional Friendship
"I like you." "But, do you like me like me?" 
In my neck of the woods, there is a script for romance. There's a less clear script for
friendship. 
But something I've noticed myself wanting is mutually intentional friendship - where
I've consciously thought about what kind of friend they are to me, and I've told them
about that, and they've told me what kind of friend I am to them, and we both
consciously decide to be somewhat better friends than we had been. 
Friendship space is deep and wide. There's a million variations of intensity, and
frequency, and meaning people might ascribe to friendships. Some people are seeking
soulmate-level friendship, some people are seeking a bunch of casual acquaintances,
or a mix of things in between. 
Fortunately, I think many of the scripts for romantic dating roughly apply to friendship,
if you bother to do it: Meet people at parties. Invite them on some low-key one-on-one
hangouts. If you both have fun, go out again. If there is some kind of mutual-spark,
start hanging out more frequently. Eventually confess your feelings of somewhat-
higher-than-average-aﬀection-or-meaningfulness and reﬂect on the nature of your
relationship and, I dunno man, ﬁgure it out.

The Person On the Street
A passing stranger - someone you have no particular connection with. Maybe the
person you buy coﬀee from, maybe a homeless person, maybe a business person you
bump into, maybe an aggressive person following you around.
How do you relate to them? Are they just Some Guy? Do you consider yourself to have
some relationship with them by virtue of growing up in the same town? Or common
humanity? Are they someone you're actively annoyed at and want to avoid?
What is relating, and the Relational Stance,
again, really?
Here I am at the end of the essay and I notice that while I'm conﬁdent that relating is
diﬀerent from feelings, and diﬀerent from goals and commitments and life
trajectories... 
...I'm still not 100% sure what relating is. 
"Relating is what meaning you ascribe to a relationship", I said at the beginning. But,
what sorts of meanings might you ascribe, that are diﬀerent from your goals and
commitments and life-trajectories? My answers come mostly in the form of ﬂavors and
felt-senses, that mostly don't have good english words. 
But here is a stab at some clarifying examples.
Magnitude of mattering
The most obvious thing is "when you ask yourself 'how important is this person to
me?', what answer do you feel in your gut?". If they disappeared from your life, or
were about to be deleted from your memory, how strongly would be you aﬀected? 
Ephemerality
Do you expect to see this person again? Will they be a constant part of your life? Will
you see them once a year or so? In some sense this is a practical question, about
what sort of activities you'll be able to do and how much shared life trajectory you'll
have. But there is something of a ﬂavor that comes along with that, that gets
sharpened when I reﬂect upon how I'd consciously choose to have them be more or
less frequent in my life.
What word describes your relationship?
Friends? Family?
Something interesting I started to notice, over the past couple years, was the
distinction between people who felt like friends, and people who felt like family. This
distinction was sharpened by coming home for the New York Solstice, where I saw
many friends I hadn't seen in awhile. In some cases, it felt very much like coming
home for the holidays - the way seeing various cousins/aunts/uncles/etc feels at
Thanksgiving. In other places it felt more like "ah, there's a friend I haven't seen in
awhile." 

After experiencing that, I started noticing aspects of this with my friends who live
nearby that I see more often. Some of them feel like people I'm obviously going to
remain close with, even if we moved away and didn't see each other for years. Some
don't. For some people, I feel a ﬂicker of "hmm, we don't feel quite like family yet, but
I there is something qualititatively family-like here."
I might just be a bit confused here, and maybe the diﬀerence here is just "friends vs
acquaintances and colleagues?". But there are at least some people where the word
"family" feels appropriate, but I'm less close friends with them than I am with some
others who feel more "friend-shaped" than "family-shaped."
In some cases, there were people that I didn't think of as family-shaped, but when
another "family" ﬂavored friend treated them in a way I associate with family-ness, I
suddenly felt a sense of family kinship.
Kindred Spirits?
You might only interact with someone brieﬂy, but some facet of them deeply
resonates with you. If you're hiking and you pass another hiker, you might have
almost nothing in common except that you both deeply love hiking, and that is
enough to give the encounter a ﬂavor of connection.
Same can go for people who share a professional ethos (dancers, programmers,
artists), or an ideology.
Enemies?
I've been listing positive examples so far, but there are plenty of relationships you
might want to avoid. Are they a professional rival? An ex-romantic-partner with a
weird vendetta? 
Would you die for them?
There's been exactly one person for whom my answer to this question was "I'd
consider it". That was a decade ago. Since then, my understanding of the stakes of
the world has risen, and there is not currently a person for whom this is true for me.
But I can still imagine it.
Recap
Relating is the stance you take towards a person, and the meaning you ascribe to
your connection-or-lack-thereof. It can be unconscious, or intentional. 
The Relational Stance is the more general act of considering how you relate to a
person - looking at a person through the lens of "how do they ﬁt into my life and what
to do they mean to me?"
A Relational Choice is the decision to relate to someone in a particular way.
Afterwards which the way you relate to them will probably be diﬀerent, both on the
object and meta level.
Where Next?

Many aspects of this still feel confusing to me. I've thought about this over the past
couple years, increasingly deliberately. But I haven't shared my thoughts on this with
too many people and I'm not sure how much diversity there is in how people relate to
relating.
But I'm fairly conﬁdent that the underlying concept of "my stance and orientation
towards a person" is a useful concept to have. I have some followup thoughts, some of
which ﬁt into what might evolve into a sequence on Friendship, and some of which will
ﬁt into the Noticing Frames sub-sequence.

Looking for books about software
engineering as a ﬁeld
I work in the software industry but am not a software developer. My job is to write
about software development, and I've learned a whole bucketload of terms: stuﬀ like
'linked lists', 'CI/CD', 'performance optimization', 'deploy to AWS', 'dockerize',
'microservices', 'SQL injection', 'multithreaded program', 'vectorized code', and on and
on and on. However, a lot of the time I'm basically just Chinese-rooming - I can write
about these things, but I don't actually understand how any of them ﬁt together. For
example, I've had three people try to explain exactly what an API is to me, for more
than two hours total, but I just can't internalize it. I feel that there's some impossible-
to-articulate piece I'm missing, and none of the words people say to me about
software stuﬀ stick because I'm lacking a foundation on which to build up my
understanding.
So my question is, are there any books (or other resources) that explain the ﬁeld of
software engineering as a cohesive whole? I'm not looking for books that will teach me
to code, because I don't think that's the thing I want. Feel free to ask clarifying
questions. Thanks!
-
EDIT: I realized I should include more context on my work and my background, so here
it is:
I have an undergrad degree in physics, which gave me extremely minimal exposure to
Python. I also took two quarters of intro CS, one in C and one in Racket. As a result I
know how to write a for loop and a bit about very basic algorithms; that's about it. I've
been in my current job for nearly a year, and my primary task is to write about the
skillsets of individual software engineers. This entails things like connecting
someone's verbal knowledge of back-end web development to their experience
creating microservices; I can do this quite competently and don't make many
technical mistakes. I have also learned a bit on the job regarding a couple data
structures, some web stuﬀ, and smatterings of info about ML, data science, DevOps,
front-end/UI, and mobile development.

Plausibly, almost every powerful
algorithm would be manipulative
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
I had an interesting debate recently, about whether we could make smart AIs safe just
by focusing on their structure and their task. Speciﬁcally, we were pondering
something like:
"Would an algorithm be safe if it was a neural net-style image classiﬁer, trained
on examples of melanoma to detect skin cancer, with no other role than to
output a probability estimate for a given picture? Even if "superintelligent",
could such an algorithm be an existential risk?"
Whether it's an existential risk was not resolved; but I have a strong intuition that they
would like be manipulative. Let's see how.
The requirements for manipulation
For an algorithm to be manipulative, it has to derive some advantage from
manipulation, and it needs to be able to learn to manipulate - for that, it needs to be
able to explore situations where it engages in manipulation and this is to its beneﬁt.
There are certainly very simple situations where manipulation can emerge. But that
example, though simple, had an agent that was active in the world. Can a classiﬁer
display the same sort of behaviour?
Manipulation emerges naturally
To show that, picture the following design. The programmers have a large collection of
slightly diﬀerent datasets, and want to train the algorithm on all of them. The loss
function is an error rate, which can vary between 1 and 0. Many of the
hyperparameters are set by a neural net, which itself takes a more "long-term view" of
the error rate, trying to improve it from day to day rather than from run to run.
How have the programmers set up the system? Well, they run the algorithm on
batched samples from ten datasets at once, and record the error rate for all ten. The
hyperparameters are set to minimise average error over each run of ten. When the
performance on one dataset falls below 0.1 error for a few runs, they remove it from
the batches, and substitute in a new one to train the algorithm on[1].
So, what will happen? Well, the system will initially start to minimise the loss rate.
Then, after a while, it will start "noticing" that when this error is below 0.1, then the
dataset is removed and the average error rate shoots up. Actually, it won't be

"noticing" this; instead, if there are hyperparameters that prevent the error rate going
below 0.1, these will be selected by gradient descent as giving a better performance.
So, this problem of optimisation of image classiﬁcation will result in the algorithm
manipulating its programmers, by "underperforming" on key datasets, so that they
stay in the batch.
Generic manipulation
Of course, the manipulation above happened because the programmers didn't
understand what the algorithm's true loss function was. They thought it was "minimise
overall loss on classiﬁcation", but it was actually "keep each dataset loss just above 
0.1".
But that kind of mistake is easy to make - by carelessness, or because they thought it
would improve the situation, the loss they thought they had diverged from the actual
loss, and this resulted in manipulative behaviour.
Here is another example: suppose the programmers build in a precaution for things
going wrong. If the error rate suddenly doubles on some dataset, then the algorithm is
run exclusively on that dataset for a while, until the error rate declines again (or, if it
doesn't, the programmers are called).
Now suppose that, of the ten datasets it has, nine have an error rate around 0.5, and
one has an error rate of 0.15. Then it is in the algorithm's interest to boost the error
rate on that last dataset, suddenly, to 0.3. Then it will be run for a time only on that
one dataset, giving it a lower average error rate. Thus hyperparameters that allow this
kind of behaviour, will get selected for (and if the algorithm is run on a lot of datasets,
it will have many opportunities to learn about this kind of behaviour).
What about the "if it doesn't, the programmers are called" clause? This depends on
what happens when the programmers are called, and how the error rate is computed
in that case. But, depending on what does happen, this incentivises the algorithm to
learn to call the programmers often (double an error rate and keep it high) or rarely (if
there's a risk of the error rate for a dataset going up suddenly, then don't lower it too
much in the ﬁrst place).
So, manipulative behaviour can emerge in smart algorithms, even if they're merely
classifying images in a way the programmers feel is entirely harmless.
Existential risks
Can this mild kind of manipulation lead to extreme, existential risk outcomes? Of
course it; it's not hard to create toy models where everything ends disastrously. But
it's also not hard to create toy models where manipulation remains mild.
But, in conclusion:

Manipulative behaviours can emerge in seemingly innocuous algorithms.
There is no clear division, currently, between mild manipulation and disastrous
manipulation.
1. Assume that either catastrophic forgetting has been solved, or that they'll re-run
the removed datasets occasionally, to refresh the algorithm's performance on
that dataset. ↩ 

Distinguishing deﬁnitions of takeoﬀ
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
I ﬁnd discussions about AI takeoﬀ to be very confusing. Often, people will argue for
"slow takeoﬀ" or "fast takeoﬀ" and then when I ask them to operationalize what those
terms mean, they end up saying something quite diﬀerent than what I thought those
terms meant.
To help alleviate this problem, I aim to compile the deﬁnitions of AI takeoﬀ that I'm
currently aware of, with an emphasis on deﬁnitions that have clear speciﬁcations. I will
continue updating the post as long as I think it serves as a useful reference for others.
In this post, an AI takeoﬀ can be roughly construed as "the dynamics of the world
associated with the development of powerful artiﬁcial intelligence." These deﬁnitions
characterize diﬀerent ways that the world can evolve as transformative AI is
developed.
Foom/Hard takeoﬀ
The traditional hard takeoﬀ position, or "Foom" position (these appear to be
equivalent terms) was characterized in this post from Eliezer Yudkowsky. It contrasts
Hanson's takeoﬀ scenario by emphasizing local dynamics: rather than a population of
artiﬁcial intelligences coming into existence, there would be a single intelligence that
quickly reaches a level of competence that outstrips the world's capabilities to control
it. The proposed mechanism that causes such a dynamic is recursive self
improvement, though Yudkowsky later suggested that this wasn't necessary.
The ability for recursive self improvement to induce a hard takeoﬀ was defended in
Intelligence Explosion Microeconomics. He argues against Robin Hanson in the AI
Foom debates. Watch this video to see the live debate.
Given the word "hard" in this notion of takeoﬀ, a "soft" takeoﬀ could simply be deﬁned
as the negation of a hard takeoﬀ.
Hansonian "slow" takeoﬀ
Robin Hanson objected to hard takeoﬀ by predicting that growth in AI capabilities will
not be extremely uneven between projects. In other words, there is unlikely to be one
AI project, or even a small set of AI projects, that produces a system that outstrips the
abilities of the rest of the world. While he rejects Yudkowsky's argument, it is
inaccurate to say that Robin Hanson expected growth in AI capabilities to be slow.
In Economic Growth Given Machine Intelligence, Hanson argues that AI induced
growth could cause GDP to double on the timescale of months. Very high economic
growth would mark a radical transition to a faster mode of technological progress and
capabilities, something that Hanson argues is entirely precedented in human history.

The technology that Hanson envisions will induce fast economic growth is whole brain
emulation, which he wrote a book about. In general, Hanson rejects the framework
that AGI should be seen as an invention that occurs at a particular moment in time:
instead, AI should be viewed as an input to the economy, (like electricity, though the
considerations may be diﬀerent).
Bostromian takeoﬀs
Nick Bostrom appeared to throw away much of the terminology in the AI Foom debate
in order to invent his own. In Superintelligence he provides a characterization of three
types of AI capability growth modes, deﬁned by the clock-time (real physical time)
from when a system is roughly human-level to when it is strongly superintelligent,
deﬁned as "a level of intelligence vastly greater than contemporary humanity's
combined intellectual wherewithal."
Some have objected to Bostrom's use of clock-time to deﬁne takeoﬀ, instead arguing
that work required to align systems is a better metric (though harder to measure).
Slow
A slow takeoﬀ is one that occurs over the timescale of decades or centuries. Bostrom
predicted that this timescale would allow for institutions, such as governments, to
react to new AI developments. It would also allow for testing incrementally more
powerful technologies without existential risks associated with testing.
Fast
A fast takeoﬀ is one that occurs over the timescale of minutes, hours, or days. Given
such short time to react, Bostrom believes that local dynamics of the takeoﬀ become
relevant, as was the case in Yudkowsky's foom scenario.
Moderate
A moderate takeoﬀ is situated between slow and fast, and occurs on the timescale of
months or years.
Continuous takeoﬀ
Continuous takeoﬀ was deﬁned, and partially defended in my post. Its meaning
primarily derives from Katja Grace's post on discontinuous progress around the
development of AGI. In that post, Grace characterizes discontinuities:
We say a technological discontinuity has occurred when a particular technological
advance pushes some progress metric substantially above what would be
expected based on extrapolating past progress. We measure the size of a
discontinuity in terms of how many years of past progress would have been
needed to produce the same improvement. We use judgment to decide how to
extrapolate past progress.

In my post, I extrapolate this concept and invert it, using terminology that I saw Rohin
use in this Alignment Newsletter edition, and deﬁne continuous takeoﬀ as
A scenario where the development of competent, powerful AI follows a trajectory
that is roughly in line with what we would have expected by extrapolating from
past progress.
Gradual/incremental takeoﬀ?
Some people objected to my use of the word continuous, as they found that the words
gradual or incremental are more descriptive and mathematically accurate. After all,
the following function is continuous, but not gradual.
Additionally, if you agree with Hanson's thesis that history can be seen as a series of
economic growth modes, each faster than the last one, then continuous takeoﬀ as
plainly deﬁned is in trouble. That's because technological progress from 1800 - 1900
was much faster than technological progress from 1700 - 1800. Therefore,
"extrapolating from past progress" would provide an incorrect estimate of progress, if
one did not foresee the industrial revolution. In general, extrapolating from past
progress is hard because it depends on the reference class you are using to forecast.
Paul slow takeoﬀ
Paul Christiano argues that we should characterize takeoﬀ in terms of economic
growth rates (similar to Hanson) but uses a deﬁnition that emphasizes how quickly the
economy transitions into a period of higher growth. He deﬁnes slow takeoﬀ as
There will be a complete 4 year interval in which world output doubles, before the
ﬁrst 1 year interval in which world output doubles. (Similarly, we'll see an 8 year
doubling before a 2 year doubling, etc.)
and deﬁnes fast takeoﬀ as the negation of the above statement. Note that this
deﬁnition leaves a third possibility: you could believe that the world output will never

double during a 1 year interval, a position I would refer to as "no takeoﬀ" which I
explain next.
Paul's outline of slow takeoﬀ shares some of its meaning with continuous takeoﬀ,
because under a slow transition to a higher growth mode, change won't be sudden.
No takeoﬀ
"No takeoﬀ" is essentially my term for the belief that world economic growth rates
won't accelerate to a very high level (perhaps >30% real GDP growth rate in one year)
following the development of AI. William Macaskill is a notable skeptic of AI takeoﬀ. I
have created this Metaculus question to operationalize the thesis.
The Eﬀective Altruism Foundation wrote this post suggesting that peak economic
growth rates may lie in the past. If we use the outside view, this position may be
reasonable. Economic growth rates have slowed down since the 1960s despite the rise
of personal computers and the internet: technologies that we might have naively
predicted would be transformative ahead of time.
This position should not be confused with the idea that humanity will never develop
superintelligent computers, though that scenario is compatible with no takeoﬀ.
Drexler's takeoﬀ
Eric Drexler argues in Comprehensive AI Services (CAIS) that future AI will be modular,
meaning that there is unlikely to be a single system that can perform a set of diverse
tasks all at once before there are individual systems that can perform the individual
tasks more competently than the single system can. This idea shares groundwork with
Hanson's objection to a local takeoﬀ. The reverse of this scenario is what Hanson calls
"lumpy AI" where single agentic systems outcompete a set of services.
Drexler uses the CAIS model to argue against the binary characterization of self-
improvement. Just as technology already feeds into itself, and thus the world can
already be seen as "recursively self improving itself", future AI research could feed
into itself as recursive technological improvement, without the necessary focus on
single systems improving themselves.
In other words, rather than viewing AIs as either self improving or not, self
improvement can be seen as a continuum from "the entire world works to improve a
system" on one end, and "a single local system improves only itself, with outside
forces providing minimal beneﬁt to growth in capabilities" on the other.
Baumann's soft takeoﬀ
In this post, Tobias Baumann argues that we should operationalize soft takeoﬀ in
terms of how quickly the fraction of global economic activity attributable to
autonomous AI systems will rise. "Time" here is not necessarily clock-time, as was the
case in Bostrom's takeoﬀ. Time can also refer to economic time, which is a measure of
time that adjusts for rate of economic growth, and political time, a measure that
adjusts for rate of social change.

He explains that this operationalization avoids the pitfalls of deﬁnitions that rely on
moments in time where AI reaches thresholds such as "human-level" or
"superintelligent." He argues that AI is likely to surpass human abilities in some
domains and not in others, rather than surpass us in all ways all at once.
Robin Hanson appears to agree with a similar measure for AI progress.
Less common deﬁnitions
Event Horizon/Epistemic Horizon
In 2007, Yudkowsky outlined the three schools of singularity, which was perhaps the
state of the art for takeoﬀ discussions at the time. In it he included his own scenario
(Foom), the Event Horizon, and Accelerating Change.
The Event Horizon hypothesis could be seen as an extrapolation of Vernor Vinge's
deﬁnition of the technological singularity. It is deﬁned as a point in time after which
current models of future progress break down, which is essentially the opposite
deﬁnition of continuous takeoﬀ.
An epistemic horizon would be relevant for decision making because it would imply
that AI progress could come suddenly, without warning. If this were true, then our
safety guarantees assumed under a continuous takeoﬀ scenario would fail.
Furthermore, even if we could predict rapid change ahead of time, due to social
pressures, people might fail to act until it's too late, a position argued for in There's
No Fire Alarm for Artiﬁcial General Intelligence.
(Note, I see a lot of people interpreting the Fire Alarm essay as merely arguing that we
can't predict rapid progress before it's too late. The essay itself dispels this
interpretation, "When I observe that there's no ﬁre alarm for AGI, I'm not saying that
there's no possible equivalent of smoke appearing from under a door.")
Accelerating change
Continuing the discussion from the three schools of singularity, this version of AI
takeoﬀ is most closely associated with Ray Kurzweil. Accelerating change is
characterized by AI capability trajectories following smooth exponential curves. It
shares with continuous takeoﬀ the predictability of AI developments, but is more
narrow and makes much more speciﬁc predictions.
Individual vs. collective takeoﬀ
Kaj Sotala has used the words "individual takeoﬀ" vs. "collective takeoﬀ" which I think
are roughly synonymous with the local vs. global distinction provided by the Foom
debate. Other words that often come up are "distributed" and "diﬀuse", "unipolar" vs
"multipolar", and "decisive strategic advantage."
Goertzel's semihard takeoﬀ

I can't say much about this one except that it's in-between soft and hard takeoﬀ.
Further reading
The AI Foom debate
A Contra Foom Reading List and Reﬂections on Intelligence from Magnus Vinding
Self-improving AI: an Analysis, from John Storrs Hall
How sure are we about this AI stuﬀ?, from Ben Garﬁnkel
Can We Avoid a Hard Takeoﬀ from Vernor Vinge

Reasons for Excitement about Impact
of Impact Measure Research
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Can we get impact measurement right? Does there exist One Equation To Rule Them
All?
I think there's a decent chance there isn't a simple airtight way to implement AUP
which lines up with AUPconceptual, mostly because it's just incredibly diﬃcult in general
to perfectly specify the reward function.
Reasons why it might be feasible: we're trying to get the agent to do the goal without
it becoming more able to do the goal, which is conceptually simple and natural; since
we've been able to handle previous problems with AUP with clever design choice
modiﬁcations, it's plausible we can do the same for all future problems; since there
are a lot of ways to measure power due to instrumental convergence, that increases
the chance at least one of them will work; intuitively, this sounds like the kind of thing
which could work (if you told me "you can build superintelligent agents which don't try
to seek power by penalizing them for becoming more able to achieve their own goal",
I wouldn't exactly die of shock).
Even so, I am (perhaps surprisingly) not that excited about actually using impact
measures to restrain advanced AI systems. Let's review some concerns I provided in
Reasons for Pessimism about Impact of Impact Measures:
Competitive and social pressures incentivize people to cut corners on safety
measures, especially those which add overhead. Especially so for training time,
assuming the designers slowly increase aggressiveness until they get a
reasonable policy.
In a world where we know how to build powerful AI but not how to align it (which
is actually probably the scenario in which impact measures do the most work),
we play a very unfavorable game while we use low-impact agents to somehow
transition to a stable, good future: the ﬁrst person to set the aggressiveness too
high, or to discard the impact measure entirely, ends the game.
In a What Failure Looks Like-esque scenario, it isn't clear how impact-limiting
any single agent helps prevent the world from "gradually drifting oﬀ the rails".
You might therefore wonder why I'm working on impact measurement.
Deconfusion
Within Matthew Barnett's breakdown of how impact measures could help with
alignment, I'm most excited about impact measure research as deconfusion. Nate
Soares explains:
By deconfusion, I mean something like "making it so that you can think about a
given topic without continuously accidentally spouting nonsense."

To give a concrete example, my thoughts about inﬁnity as a 10-year-old were
made of rearranged confusion rather than of anything coherent, as were the
thoughts of even the best mathematicians from 1700. "How can 8 plus inﬁnity still
be inﬁnity? What happens if we subtract inﬁnity from both sides of the equation?"
But my thoughts about inﬁnity as a 20-year-old were not similarly confused,
because, by then, I'd been exposed to the more coherent concepts that later
mathematicians labored to produce. I wasn't as smart or as good of a
mathematician as Georg Cantor or the best mathematicians from 1700; but
deconfusion can be transferred between people; and this transfer can spread the
ability to think actually coherent thoughts.
In 1998, conversations about AI risk and technological singularity scenarios often
went in circles in a funny sort of way. People who are serious thinkers about the
topic today, including my colleagues Eliezer and Anna, said things that today
sound confused. (When I say "things that sound confused," I have in mind things
like "isn't intelligence an incoherent concept," "but the economy's already
superintelligent," "if a superhuman AI is smart enough that it could kill us, it'll also
be smart enough to see that that isn't what the good thing to do is, so we'll be
ﬁne," "we're Turing-complete, so it's impossible to have something dangerously
smarter than us, because Turing-complete computations can emulate anything,"
and "anyhow, we could just unplug it.") Today, these conversations are diﬀerent.
In between, folks worked to make themselves and others less fundamentally
confused about these topics—so that today, a 14-year-old who wants to skip to
the end of all that incoherence can just pick up a copy of Nick Bostrom's
Superintelligence.
Similarly, suppose you're considering the unimportant and trivial question of whether
seeking power is convergently instrumental, which we can now crisply state as "do
most reward functions induce optimal policies which take over the planet (more
formally, which visit states with high POWER)?".
You're a bit confused if you argue in the negative by saying "you're
anthropomorphizing; chimpanzees don't try to do that" (chimpanzees aren't optimal)
or "the set of reward functions which does this has measure 0, so we'll be ﬁne" (for
any reachable state, there exists a positive measure set of reward functions for which
visiting it is optimal).
You're a bit confused if you argue in the aﬃrmative by saying "unintelligent animals
fail to gain resources and die; intelligent animals gain resources and thrive. Therefore,
since we are talking about really intelligent agents, of course they'll gain resources
and avoid correction." (animals aren't optimal, and evolutionary selection pressures
narrow down the space of possible "goals" they could be eﬀectively optimizing).
After reading this paper on the formal roots of instrumental convergence, instead of
arguing about whether chimpanzees are representative of power-seeking behavior, we
can just discuss how, under an agreed-upon reward function distribution, optimal
action is likely to ﬂow through the future of our world. We can think about to what
extent the paper's implications apply to more realistic reward function distributions
(which don't identically distribute reward over states).[1] Since we're less confused,
our discourse doesn't have to be crazy.
But also since we're less confused, the privacy of our own minds doesn't have to be
crazy. It's not that I think that any single fact or insight or theorem downstream of my
work on AUP is totally obviously necessary to solve AI alignment. But it sure seems

good that we can mechanistically understand instrumental convergence and power,
know what "impact" means instead of thinking it's mostly about physical change to
the world, think about how agents aﬀect each other, and conjecture why goal-
directedness seems to lead to doom by default.[2]
Attempting to iron out ﬂaws from our current-best AUP equation makes one intimately
familiar with how and why power-seeking incentives can sneak in even when you're
trying to keep them out in the conceptually correct way. This point is harder for me to
articulate, but I think there's something vaguely important in understanding how this
works.
Formalizing instrumental convergence also highlighted a signiﬁcant hole in our
theoretical understanding of the main formalism of reinforcement learning. And if you
told me two years ago that you could possibly solve side-eﬀect avoidance in the short-
term with one simple trick ("just preserve your ability to optimize a single random
reward function, lol"), I'd have thought you were nuts. Clearly, there's something
wrong with our models of reinforcement learning environments if these results are so
surprising.
In my opinion, research on AUP has yielded an unusually high rate of deconfusion and
insights, probably because we're thinking about what it means for the agent to
interact with us.
1. When combined with our empirical knowledge of the diﬃculty of reward function
speciﬁcation, you might begin to suspect that there are lots of ways the agent
might be incentivized to gain control, many openings through which power-
seeking incentives can permeate - and your reward function would have to
penalize all of these! If you were initially skeptical, this might make you think
that power-seeking behavior may be more diﬃcult to avoid than you initially
thought. ↩ 
2. If we collectively think more and end up agreeing that AUPconceptual solves
impact measurement, it would be interesting that you could solve such a
complex, messy-looking problem in such a simple way. If, however, CCC ends up
being false, I think that would also be a new and interesting fact not currently
predicted by our models of alignment failure modes. ↩ 

My slack budget: 3 surprise problems
per week
Previously: Slack
In a couple earlier articles I urged people to adopt strategies that reliably maintain a
margin of "30% slack." I've seen lots of people burn out badly (myself included), and
preserving a margin of resources such that you don't risk burning out seems quite
important to me. 
But I realized a) "30% slack" isn't very clear, and b) this is an important enough
concept it should really have a top-level post.
So, to be a bit more obvious:
Maintain enough slack that you can absorb 3 surprise problems happening
to you in a week, without dipping into reserves.
"Surprise problems" can take multiple forms, and cost diﬀerent types of reserves.
These can be ﬁnancial expenses you didn't know about (whoops, I needed to buy
some medicine), or cognitive attention (whoops, I need to ﬁgure out what medicine to
buy) or stress (whoops, I'm sick, and now I need to talk to a bunch of doctors while
being kinda exhausted).
These can be problems happening to you, or problems happening to friends that you
care about.
Why "3 surprises", and not just one? Because at least a couple times a year I
personally run into 3-surprises-in-a-week. And sometimes I get hit with much bigger
things require me to burn my reserves, and if I didn't have a habit of ensuring my
reserves I just wouldn't be able to do those things at all.
I was motivated to write this today because, last week, four problems came up. I had
recently taken on two major projects; a community institution was in trouble, and a
friend was hurt and needed help. I only had bandwidth to deal with 3 of those. I
realized that not only was this a particularly bad week, but I had let too many ongoing
responsibilities accumulate.
The weekend came 'round, right as I hit exactly-zero-slack. I needed time to recover,
but I had made some time-sensitive commitments to help with some of the above
things, and I ended up having to spend the weekend doing a more constrained version
of those outstanding obligations, while carefully recovering.
Being Pro-Social Requires Slack
I think an important part of being a good friend, community member or eﬀective
altruist, is taking care of yourself. I think if you can't absorb 3 surprise problems per
week, it probably make sense to prioritize ﬁxing that above most other things. 
"But my friend is in trouble!"

That's legitimately sad, but if you can't absorb 3 surprise problems per week, you are
probably not going to be able to help your friend next week. Moreover, you might burn
out, then you will need to be asking other friends for help, and you might need to ask
that at a time when they were also hurting. 
Take care of yourself.
"But the world is in trouble!"
The world is always in trouble. There is no end to the things you might hypothetically
do to help it. It is virtuous to help. It is not virtuous to help in a way that runs the risk
of you becoming one of the people who need help and are adding to the problem.
"But maybe I can do this and it'll be ﬁne?"
One of the legitimately tricky things is that, sure, often you can roll the dice and come
out ﬁne this time. Many extra tasks you take on are usually okay, but have, say, a
10% chance of turning out to involve much bigger responsibilities than you thought
you were committing to. You can skirt by a few times and be okay.
But, well, if you do that 10 times, one of the times you will turn out not to be okay.
Developing intuitions around how risky actions are, and developing policies for how
often to do them, is pretty important.
"But I made commitments!"
I take commitments pretty seriously. Trust is one of the sources of slack. So if you've
suddenly realized you've overcommitted, often the solution is not to abruptly abandon
all of them. 
But, it is often necessary to abandon at least some, while making sure to take
ownership of that abandonment, perhaps with a promise to pay back a favor later.
Develop Weekly Capacity and Build Reserves
That all said, I quite sympathize if you see your friends hurting, or the world on ﬁre, or
even just lots of cool parties that you want to go to even though you're tired.
There are various things you can do to build up that capacity. Get a better job.
Carefully cultivate friendships that are naturally restorative most of the time. Find a
good living situation. Invest in habits that that eventually make things easier.
This isn't a selﬁsh thing to do for yourself, it should be coming out of your long-term
budget for improving the world. 

Bayesian Evolving-to-Extinction
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
The present discussion owes a lot to Scott Garrabrant and Evan Hubinger.
In Deﬁning Myopia, I formalized temporal or cross-instance myopia / non-myopia, but I
claimed that there should also be some kind of single-instance myopia which I hadn't
properly captured. I also suggested this in Predict-O-Matic.
This post is intended to be an example of single-instance partial agency.
Evolving to Extinction
Evolution might be myopic in a number of ways, but one way is that it's myopic across
individuals -- it typically produces results very diﬀerent from what group selection
would produce, because it's closer to optimizing relative ﬁtness of individuals (relative
to each other) than it is to optimizing overall ﬁtness. Adaptations which help members
of a species compete with each other are a great example of this. Why increase your
own ﬁtness, when you can just decrease someone else's instead? We're lucky that it's
typically pretty hard, at least historically, to do things which are bad across the board
but slightly less bad for the one doing them. Imagine a "toxic gas gene" which makes
the air harder for everyone to breathe, but slightly less so for carriers of the gene.
Such a gene would be selected for. This kind of thing can be selected for even to the
point where it drives the population of a species right down to zero, as Eliezer's essay
on evolving to extinction highlighted.
Actually, as Eliezer's essay emphasized, it's not even that evolution is myopic at the
level of individuals; evolution is myopic down to the level of individual genes, an
observation which better explains the examples of evolving-to-extinction which he
discusses. (This is, of course, the point of Dawkins' book The Selﬁsh Gene.) But the
analogy of myopia-across-individuals will suit me better here.
Bayes "Evolving to Extinction"
The title of this post is a hyperbole, since there isn't an analog of an extinction event
in the model I'm about to describe, but it illustrates that in extreme circumstances a
Bayesian learner can demonstrate the same kind of pathological behavior that
evolution does when it ends up selecting for relative ﬁtness in a way which pumps
against absolute ﬁtness.
Like evolution, Bayes' Law will "optimize"[1] for relative ﬁtness of hypotheses, not
absolute ﬁtness. Ordinarily there isn't enough of a diﬀerence for this to matter.
However, I've been discussing scenarios where the predictor can signiﬁcantly
inﬂuence what's being predicted. Bayes' Law was not formulated with examples like
this in mind, and we can get pathological behavior as a result.
One way to construct an example is to imagine that there is a side-channel by which
hypotheses can inﬂuence the world. The "oﬃcial" channel is to output predictions; but

let's say the system also produces diagnostic logs which predictors can write to, and
which humans read. A predictor can (for example) print stock tips into the diagnostic
logs, to get some reaction from humans.
Say we have a Bayesian predictor, consisting of some large but ﬁxed number of
hypotheses. An individual hypothesis "wants" to score well relative to others. Let's
also say, for the sake of argument, that all hypotheses have the ability to write to
diagnostic logs, but humans are more likely to pay attention to the diagnostics for
more probable hypotheses.
How should a hypothesis make use of this side-channel? It may initially seem like it
should use it to make the world more predictable, so that it can make more accurate
predictions and thus get a better score. However, this would make a lot of hypotheses
score better, not just the one printing the manipulative message. So it wouldn't really
be selected for.
Instead, a hypothesis could print manipulative messages designed to get humans to
do things which no other hypothesis anticipates. This involves speciﬁcally optimizing
for events with low probability to happen. Hypotheses which successfully accomplish
this will get a large boost in relative predictive accuracy, making them more probable
according to Bayes' Law.
So, a system in this kind of situation eventually winds up being dominated by
hypotheses which manipulate events to be as unpredictable as possible (by that very
system), subject to the constraint that one hypothesis or another within the system
can predict them.
This is very much like what I called the entropy-market problem for futarchy, also
known as the assasination-market problem. (Any prediction market involving the
lifespan of public ﬁgures is equivalent to an assassination market; it pays for the
death of public ﬁgures, since that is a hard-to-predict but easier-to-control event.)
Analogous problems arise if there is no side-channel but the prediction itself can
inﬂuence events (which seems very plausible for realistic predictions).
Is This Myopia?
If we use "myopia" to point to the kind of non-strategic behavior we might actually
want out of a purely predictive system, this isn't myopia at all. For this reason, and for
other reasons, I'm more comfortable throwing this under the umbrella term "partial
agency". However, I think it's importantly related to myopia.
Just like we can think of evolution as myopically optimizing per-individual,
uncaring of overall harm to reproductive ﬁtness if that harm went along with
improvements to individual relative ﬁtness, we can think of Bayes' Law as
myopically optimizing per-hypothesis, uncaring of overall harm to predictive
accuracy.
The phenomenon here doesn't illustrate the "true myopia" we would want of a
purely predictive system, since it ends up manipulating events. However, it at
least shows that there are alternatives. One might have argued "sure, I get the
idea of cross-instance myopia, showing that per-instance optimization is
(possibly radically) diﬀerent from cross-instance optimization. But how could
there be per-instance myopia, as distinct from per-instance optimization? How

can partial agency get any more partial than myopically optimizing individual
instances?" Bayes-evolving-to-extinction clearly shows that we can break things
down further. So perhaps there's still room for a further "true myopia" which
codiﬁes non-manipulation even for single instances.
This phenomenon also continues the game-theoretic theme. Just as we can think
of per-instance myopia as stopping cross-instance optimization by way of a
Molochian race-to-the-bottom, we see the same thing here.
Neural Nets / Gradient Descent
As I've mentioned before, there is a potentially big diﬀerence between multi-
hypothesis setups like Bayes and single-hypothesis setups like gradient-descent
learning. Some of my arguments, like the one above, involve hypotheses competing
with each other to reach Molochian outcomes. We need to be careful in relating this to
cases like gradient descent learning, which might approximate Bayesian learning in
some sense, but incrementally modiﬁes a single hypothesis rather than letting many
hypotheses compete.
One intuition is that stochastic gradient descent will move the network weights
around, so that we are in eﬀect sampling many hypotheses within some region. Under
some circumstances, the most successful weight settings could be the ones which
manipulate things to maximize local gradients in their general direction, which means
punishing other nearby weight conﬁgurations -- this could involve increasing the loss,
much like the Bayesian case. (See Gradient Hacking.)
There is also the "lottery ticket hypothesis" to consider (discussed on LW here and
here) -- the idea that a big neural network functions primarily like a bag of
hypotheses, not like one hypothesis which gets adapted toward the right thing. We
can imagine diﬀerent parts of the network ﬁghting for control, much like the Bayesian
hypotheses.
More formally, though, we can point to some things which are moderately analogous,
but not perfectly.
If we are adapting a neural network using gradient descent, but there is a side-
channel which we are not accounting for in our credit assignment, then the gradient
descent will not optimize the side-channel. This might result in aimless thrashing
behavior.
For example, suppose that loss explicitly depends only on the output X of a neural net
(IE, the gradient calculation is a gradient on the output). However, actually the loss
depends on an internal node Y, in the following way:
When |X-Y| is high, the loss function rewards X being high.
When |X-Y| is low, the loss function rewards X being low.
When X is high, the loss function rewards low |X-Y|.
When X is low, the loss function rewards high |X-Y|.
When both values are middling, the loss function incentivizes X to be less
middling.
This can spin around forever. It is of course an extremely artiﬁcial example, but the
point is to demonstrate that when gradient descent does not recognize all the ways

the network inﬂuences the result, we don't necessarily see behavior which "tries to
reduce loss", or even appears to optimize anything.
1. The whole point of the partial agency sequence is that words like "optimize" are
worryingly ambiguous, but I don't have suﬃciently improved terminology yet
that I feel I can just go ahead and use it while maintaining clarity!! In particular,
the sense in which Bayesian updates optimize for anything is pretty unclear
when you think about it, yet there is certainly a big temptation to say that they
optimize for predictive accuracy (in the log-loss sense). ↩ 

Predictive coding and motor control
Predictive coding is a theory related to how the brain works—here is the SSC
introduction. As in my earlier post, I'm very interested in the project of mapping these
ideas onto the speciﬁc world-modeling algorithms of the neocortex. My larger project
here is trying to understand the algorithms underlying human emotions and
motivations—a topic I think is very important for AGI safety, as we may someday need
to know how to reliably motivate human-like AGIs to do what we want them to do! (It's
also relevant for understanding our own values, and for mental health, etc.) But motor
control is an easier, better-studied case to warm up on, and shares the key feature that
it involves communication between the neocortex and the older parts of the brain.
So, let's talk about predictive coding and motor control.
The typical predictive coding description of motor control[1] goes something like this:
Stereotypical predictive coding description: When you predict really strongly that
your toes are about to wiggle, they actually wiggle, in order to minimize prediction
error!
Or here's Karl Friston's perspective (as described by Andy Clark): "...Proprioceptive
predictions directly elicit motor actions. This means that motor commands have been
replaced by (or as I would rather say, implemented by) proprioceptive predictions."
As in my earlier post, while I don't exactly disagree with the stereotypical predictive
coding description, I prefer to describe what's going on in a more mechanistic and
algorithmic way. So I oﬀer:
My picture of predictive coding and motor
control
In this cartoon example,[2] there is a currently-active neural code at the second-to-
lowest layer of the cortical hierarchy, corresponding to toe-wiggling. Now in general, as
you go up the cortical hierarchy, information converges across space and time.[3] So,
following this pattern, this mid-level neural code talks to multiple lower-level regions of

the cortex, sending top-down signals to each that activate a time-sequence of neural
patterns.
In one of these low-level regions (left), the sequence of neural patterns corresponds to
a set of predictions about incoming proprioceptive data—namely, the prediction that
we will feel our toe wiggling.
In another of these low-level regions (right), the sequence of patterns is sending out
motor commands to wiggle our toe, in a way that makes those proprioceptive
predictions come true.
Thus, we can say that the a single mid-level neural code represents both the prediction
that our toe will wiggle and the actual motor commands to make that happen. Thus, as
in the classic predictive coding story, we can say that the prediction and commands
are one and the same neural code—well, maybe not at the very bottom of the cortical
hierarchy, but they're the same everywhere else. At least, they're the same after the
learning algorithm has been running for a while.
(By the way, contra Friston, I think there are also mid-level neural codes that represent
the prediction that our toe will wiggle, but which are not connected to any motor
commands. Such a code might, for example, represent our expectations in a situation
when we are lying limp while a masseuse is wiggling our toe. In this case, the code
would link to both proprioceptive inputs and skin sensation inputs, rather than both
proprioceptive inputs and motor outputs.)
How does this wiring get set up?
Same way as anything else in the neocortex! This little mid-level piece of the neocortex
is building up a space of generative models—each of which is simultaneously a
sequence of proprioceptive predictions and a sequence of motor outputs. It proposes
new candidate generative models by Hebbian learning, random search, and various
other tricks. And it discards models whose predictions are repeatedly falsiﬁed.
Eventually, the surviving models in this little slice of neocortex are all self-consistent:
They issue predictions of toe-related proprioceptive inputs, and issue the sequence of
motor commands that actually makes those predictions come true.
Note that there are many surviving (non-falsiﬁed) generative models, because there
are many ways to move your toes. Thus, if there is a prediction error (the toe is not
where the model says it's supposed to be), the brain's search algorithm immediately
summons a diﬀerent generative model, one that starts with the toe in its actual current
position.
The same thing, of course, is happening higher up in the cortical hierarchy. So the
higher-level generative model "I am putting on my sock" invokes various of these low-
level toe-movement models at diﬀerent times, discarding the sock-putting-on
generative models that are falsiﬁed, and thus winding up with the surviving models
being those that tell coherent stories of the sight sound, feeling, and world-model
consequences of putting on a sock, along with motor commands that make these
stories come true.
Are these connections learned or innate?

If you've read this earlier post, you'll guess what my opinion is: I think the speciﬁc
sequences of neural codes in the three areas shown in the cartoon (proprioceptive,
motor, and their association), and their speciﬁc neural-code-level connections, are all
learned, not innate. But I do think our genes speed the process along by seeding
connections among the three areas, thus setting up the cortex with all the right
information streams brought together into the right places for their relationships to be
learned quickly and eﬀectively.
An illustrative example on the topic of learned-vs-innate is Ian Waterman[4], who lost all
proprioceptive sense at age 19 after a seemingly-minor infection.[5] Remarkably, he
taught himself to move (including walking, writing, shopping, etc.) by associating
muscle commands with visual predictions! So if he was standing in a room that
suddenly went pitch black, he would just collapse! He had to focus his attention
continually on his movements—I guess, in the absence of direct connections between
the relevant parts of his neocortex, he had to instead route the information through the
GNW. But anyway, the fact that he could move at all is I think consistent with the kind
of learning-based mechanism I'm imagining.
(Question: How do many mammals walk on their ﬁrst day of life, if the neocortex needs
to learn the neural codes and associations? Easy: I say they're not using their
neocortex for that! If I understand correctly, there are innate motor control programs
stored in the brainstem, midbrain, etc. The neocortex eventually learns to "press go"
on these baked-in motor control programs at appropriate times, but the midbrain can
also execute them based on its own parallel sensory-processing systems and
associated instincts. My understanding is that humans are unusual among mammals—
maybe even unique—in the extent to which our neocortex talks directly to muscles,
rather than "pressing go" on the various motor programs of the evolutionarily-older
parts of the brain.[6]
Note: I'm still trying to ﬁgure this stuﬀ out. If any of this seems wrong, then it probably
is, so please tell me. :-)
1. See, for example, Surﬁng Uncertainty chapter 4 ↩ 
2. I'm being a bit sloppy with some implementation details here. Is it a two-layer
hierarchical setup, or "lateral" (within-hierarchical-layer) connections? Where is it
one neural code versus a sequence of codes? Why isn't the cerebellum shown in
the ﬁgure? None of these things matter for this post. But just be warned not to
take all the details here as literally exactly right. ↩ 
3. If you're familiar with Jeﬀ Hawkins's "HTM" (hierarchical temporal models)
theories, you'll see where I'm getting this particular part. See On Intelligence
(2004) for details. ↩ 
4. I ﬁrst heard of Ian Waterman in the excellent book The Myth of Mirror Neurons.
His remarkable story was popularized in a 1998 BBC documentary, The Man Who
Lost His Body. ↩ 
5. The theory mentioned in the documentary is that it was an autoimmune issue—
i.e., his immune system latched onto some feature of the invading bacteria that
happened to also be present in a speciﬁc class of his own nerve cells, the kind
that relays proprioceptive data. ↩ 

6. I don't know the details ... this claim is stated without justiﬁcation in On
Intelligence p69. Note that "talks directly to muscles" is an overstatement; on
their way to the muscles, I think the signals pass through at least the basal
ganglia, thalamus, and cerebellum. I guess I should say, "less indirectly". ↩ 

