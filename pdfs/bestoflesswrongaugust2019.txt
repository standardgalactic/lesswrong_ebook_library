
Best of LessWrong: August 2019
1. Why Subagents?
2. Power Buys You Distance From The Crime
3. Soft takeoﬀ can still lead to decisive strategic advantage
4. The Commitment Races problem
5. A Personal Rationality Wishlist
6. Trauma, Meditation, and a Cool Scar
7. LW Team Updates - September 2019
8. Can we really prevent all warming for less than 10B$ with the mostly side-eﬀect
free geoengineering technique of Marine Cloud Brightening?
9. Intentional Bucket Errors
10. Troll Bridge
11. Inspection Paradox as a Driver of Group Separation
12. Subagents, trauma and rationality
13. Algorithmic Similarity
14. What are the reasons to *not* consider reducing AI-Xrisk the highest priority
cause?
15. What explanatory power does Kahneman's System 2 possess?
16. How Can People Evaluate Complex Questions Consistently?
17. [AN #62] Are adversarial examples caused by real but imperceptible features?
18. AI Forecasting Resolution Council (Forecasting infrastructure, part 2)
19. Problems in AI Alignment that philosophers could potentially contribute to
20. Permissions in Governance
21. Book Review: Secular Cycles
22. Call for contributors to the Alignment Newsletter
23. September Bragging Thread
24. Schelling Categories, and Simple Membership Tests
25. Understanding understanding
26. Subagents, neural Turing machines, thought selection, and blindspots
27. [Link] Book Review: Reframing Superintelligence (SSC)
28. Distance Functions are Hard
29. Status 451 on Diagnosis: Russell Aphasia
30. 2-D Robustness
31. Negative "eeny meeny miny moe"
32. When do utility functions constrain?
33. Emotions are not beliefs
34. Clarifying some key hypotheses in AI alignment
35. Calibrating With Cards
36. Predicted AI alignment event/meeting calendar
37. Alleviating Bipolar with meditation
38. Could we solve this email mess if we all moved to paid emails?
39. How to Make Billions of Dollars Reducing Loneliness
40. Which of these ﬁve AI alignment research projects ideas are no good?
41. [Site Update] Behind the scenes data-layer and caching improvements
42. Very diﬀerent, very adequate outcomes
43. "Can We Survive Technology" by von Neumann
44. Vaniver's View on Factored Cognition
45. How has rationalism helped you?
46. I'm interested in a sub-ﬁeld of AI but don't know what to call it.
47. Six AI Risk/Strategy Ideas

48. Towards an Intentional Research Agenda
49. Cartographic Processes
50. Markets are Universal for Logical Induction

Why Subagents?
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
The justiﬁcation for modelling real-world systems as "agents" - i.e. choosing actions to
maximize some utility function - usually rests on various coherence theorems. They say
things like "either the system's behavior maximizes some utility function, or it is throwing
away resources" or "either the system's behavior maximizes some utility function, or it can
be exploited" or things like that. Diﬀerent theorems use slightly diﬀerent assumptions and
prove slightly diﬀerent things, e.g. deterministic vs probabilistic utility function, unique vs
non-unique utility function, whether the agent can ignore a possible action, etc.
One theme in these theorems is how they handle "incomplete preferences": situations where
an agent does not prefer one world-state over another. For instance, imagine an agent which
prefers pepperoni over mushroom pizza when it has pepperoni, but mushroom over
pepperoni when it has mushroom; it's simply never willing to trade in either direction.
There's nothing inherently "wrong" with this; the agent is not necessarily executing a
dominated strategy, cannot necessarily be exploited, or any of the other bad things we
associate with inconsistent preferences. But the preferences can't be described by a utility
function over pizza toppings.
In this post, we'll see that these kinds of preferences are very naturally described using
subagents. In particular, when preferences are allowed to be path-dependent, subagents are
important for representing consistent preferences. This gives a theoretical grounding for
multi-agent models of human cognition.
Preference Representation and Weak Utility
Let's expand our pizza example. We'll consider an agent who:
Prefers pepperoni, mushroom, or both over plain cheese pizza
Prefers both over pepperoni or mushroom alone
Does not have a stable preference between mushroom and pepperoni - they prefer
whichever they currently have
We can represent this using a directed graph:

The arrows show preference: our agent prefers B over A if (and only if) there is a directed
path from A to B along the arrows. There is no path from pepperoni to mushroom or from
mushroom to pepperoni, so the agent has no preference between them. In this case, we're
interpreting "no preference" as "agent prefers to keep whatever they have already". Note
that this is NOT the same as "the agent is indiﬀerent", in which case the agent is willing to
switch back and forth between the two options as long as the switch doesn't cost anything.
Key point: there is no cycle in this graph. If the agent's preferences are cyclic, that's when
they provably throw away resources, paying to go in circles. As long as the preferences are
acyclic, we call them "consistent".
Now, at this point we can still deﬁne a "weak" utility function by ignoring the "missing"
preference between pepperoni and mushroom. Here's the idea: a normal utility function says
"the agent always prefers the option with higher utility". A weak utility function says: "if the
agent has a preference, then they always prefer the option with higher utility". The missing
preference means we can't build a normal utility function, but we can still build a weak utility
function. Here's how: since our graph has no cycles, we can always order the nodes so that
the arrows only go forward along the sorted nodes - a technique called topological sorting.
Each node's position in the topological sort order is its utility. A small tweak to this method
also handles indiﬀerence.

(Note: I'm using the term "weak utility" here because it seems natural; I don't know of any
standard term for this in the literature. Most people don't distinguish between these two
interpretations of utility.)
When preferences are incomplete, there are multiple possible weak utility functions. For
instance, in our example, the topological sort order shown above gives pepperoni utility 1
and mushroom utility 2. But we could just as easily swap them!
Preference By Committee
The problem with the weak utility approach is that it treats the preference between
pepperoni and mushroom as unknown - depending on which possible utility we pick, it could
go either way. It's pretending that there's some hidden preference there which we simply
don't know. But there are real systems where the preference is not merely unknown, but a
real preference to stay in the current state.
For example, maybe our pizza-agent is actually a committee which must unanimously agree
to any proposed change. One member prefers pepperoni to no pepperoni, regardless of
mushrooms; the other prefers mushrooms to no mushrooms, regardless of pepperoni. This
committee is not exploitable and does not throw away resources, nor does it have any
hidden preference between pepperoni and mushrooms. Viewed as a black box, its "true"
preference between pepperoni and mushrooms is to keep whichever it currently has.
In fact, it turns out that we can represent any consistent preferences by a committee
requiring unanimous agreement.
The key idea here is called order dimension. We want to take our directed acyclic graph of
preferences, and stick it into a multidimensional space so that there is an arrow from A to B
if-and-only-if B is higher along all dimensions. Each dimension represents the utility of one
subagent on the committee; that subagent approves a change only if the change does not
decrease the subagent's utility. In order for the whole committee to approve a change, the
trade must increase (or leave unchanged) the utilities of all subagents. The minimum
number of agents required to make this work - the minimum number of dimensions required
- is the order dimension of the graph.
For instance, our pizza example has order dimension 2. We can draw it in a 2-dimensional
space like this:

Note that, if there are inﬁnitely many possibilities, then the order dimension can be inﬁnite -
we may need inﬁnitely many agents to represent some preferences. But as long as the
possibilities are ﬁnite, the order dimension will be as well.
Path-Dependence
So far, we've interpreted "missing" preferences as "agent prefers to stay in current state".
One important reason for that interpretation is that it's exactly what we need in order to
handle path-dependent preferences.
In practice, path-dependent preferences mostly matter for systems with "hidden state":
internal variables which can change in response to the system's choices. A great example of
this is ﬁnancial markets: they're the ur-example of eﬃciency and inexploitability, yet it turns
out that a market does not have a utility function in general (economists call this
"nonexistence of a representative agent"). The reason is that the distribution of wealth
across the market's agents functions as an internal hidden variable. Depending on what path
the market follows, diﬀerent internal agents end up with diﬀerent amounts of wealth, and the
market as a whole will hold diﬀerent portfolios as a result - even if the externally-visible
variables, i.e. prices, end up the same.
Most path-dependence results from some hidden state directly, but even if we don't know
the hidden state, we can always add hidden state in order to model path-dependence.
Whenever future preferences diﬀer based on how the system reached the current state, we
just split the state into two states - one for each possibility. Then we repeat, until we have a
full set of states with path-independent preferences between them. These new states are
"full" states of the system; from outside, some of them look the same.

An example: suppose I prefer New York to Boston if I just came from DC, but Boston to New
York if I just came from Philadelphia.
We can represent that with hidden state:
We now have two separate hidden internal nodes, which both correspond to the same
externally-visible state "New York".
Now the key piece: there is no way to get to the "New York (from Philly)" node directly from
the "New York (from DC)" node. The agent does not, and cannot, have a preference between
these two nodes. Analogously, a market cannot have a preference between two diﬀerent
wealth distributions - the subagents who comprise a market will never spontaneously decide
to redistribute their wealth amongst themselves. They always "prefer" (or "decide") to stay
in whatever state they're currently in.
This is why we need to understand incomplete preferences in order to handle path-
dependent preferences: hidden state creates situations where the agent "prefers" to stay in
whatever state they're in.
Now we can easily model the system using subagents exactly as we did for incomplete
preferences. We have a directed preference graph between full states (including hidden
state), it needs to be acyclic to avoid throwing away resources, so we can ﬁnd a set of
subagents to represent the preferences. In the case of a market, this is just the subagents
which comprise the market: they'll take a trade if it does not decrease the utility of any
subagent. (Note, however, that the same externally-visible trade can correspond to multiple
possible internal state changes; the subagents will take the trade if any of the possible
internal state changes are non-utility-decreasing for all of them. For a market, this means
they can trade amongst themselves in response to the external trade in order to make
everyone happy.)

Applications & Speculations
We've just argued that a system with consistent preferences can be modelled as a
committee of utility-maximizing agents. How does this change our interpretation and
predictions of the world?
First and foremost: the subagents argument is a generalization of the standard acyclic
preferences argument. Anytime we might want to use the acyclic preferences argument, but
there's no reason for the system to be path-independent, we can apply the subagents
argument instead. In practice, we usually expect systems to be eﬃcient/inexploitable
because of some selection pressure (evolution, market competition, etc) - and that selection
pressure usually doesn't care about path dependence in and of itself.
Main takeaway: pretty much anywhere we'd use an agent with a utility function to model
something, we can apply the subagents argument and use a committee of agents with utility
functions instead. In particular, this is a good replacement for "weak" utility functions.
Humans are a particularly interesting example. We'd normally use the acyclic preferences
argument (among other arguments) to argue that humans approximate utility-maximizers in
most situations. But there's no particular reason to assume path-independence; indeed,
human behavior looks highly path-dependent. So, apply the subagents argument.
Hypothesis: human behavior approximates the choices of a committee of utility-maximizing
agents in most situations.
Sound familiar? The subagents argument oﬀers a theoretical basis for the idea that humans
have lots of internal subagents, with competing wants and needs, all constantly negotiating
with each other to decide on externally-visible behavior.
In principle, we could test this hypothesis more rigorously. Lots of people think of AI "learning
what humans want" by asking questions or oﬀering choices or running simulations.
Personally, I picture an AI taking in a scan of a full human connectome, then directly
calculating the embedded preferences. Someday, this will be possible. When the AI solves
those equations, do we expect it to ﬁnd a single generic optimizer embedded in the system,
approximately optimizing some "utility"? Or do we expect to ﬁnd a bunch of separate
generic optimizers, approximately optimizing several diﬀerent "utilities", and negotiating
with each other? Probably neither picture is complete yet, but I'd bet the second is much
closer to reality.
Conclusion
Let's recap:
The acyclic preferences argument is the easiest entry point for
eﬃciency/inexploitability-implies-utility-maximization theorems, but it doesn't handle
lots of important things, including path dependence.
Markets, for example, are eﬃcient/inexploitable but can't be represented by a utility
function. They have hidden internal state - the distribution of wealth over agents -
which makes their preferences path-dependent.
The subagents argument says that any system with deterministic,
eﬃcient/inexploitable preferences can be represented by a committee of utility-
maximizing agents - even if the system has path-dependent or incomplete preferences.
That means we can substitute committees in many places where we currently use
utilities. For instance, it oﬀers a theoretical foundation for the idea that human
behavior is described by many negotiating subagents.

One big piece which we haven't touched at all is uncertainty. An obvious generalization of
the subagents argument is that, once we add uncertainty (and a notion of
eﬃciency/inexploitability which accounts for it), an eﬃcient/inexploitable path-dependent
system can be represented by a committee of Bayesian utility maximizers. I haven't even
started to tackle that conjecture yet; it's a wide-open problem.

Power Buys You Distance From The
Crime
Introduction
Taxes are typically meant to be proportional to money (or negative externalities, but
that's not what I'm focusing on). But one thing money buys you is ﬂexibility, which can
be used to avoid taxes. Because of this, taxes aimed at the wealthy tend to end
up hitting the well-oﬀ-or-rich-but-not-truly-wealthy harder, and tax cuts aimed
at the poor end up helping the middle class. Examples (feel free to stop reading these
when you get the idea, this is just the analogy section of the essay):
Computer programmers typically have the option to work remotely in a low-tax
state; teachers need to be where the classroom is. 
Estate taxes tend to hit families with single large assets (like a business) harder
than those with diverse investments (who can simply sell assets to pay for
taxes), who are hit harder than those with enough wealth to create trust funds.
Executives can choose to receive stock (which is taxed more favorably) instead
of cash to the exact percentage they desire. Well paid employees are oﬀered
stock, but the amount will not be tailored to their needs. Lower level employees
either are not oﬀered this, or are not in a position to take advantage of it.
The legal distinction between a business (whose expenses are tax deductible)
and a hobby (deductions not allowed) is based on whether the activity nets you
income (there are complications and you can sometimes prove a money loser is
a business, but this is a good rule of thumb). Small business owners (e.g.

lawyers) can fold their occasionally-revenue-generating hobby (e.g.
photography) into their real business, enabling tax deductions for their hobby.
IRAs, 401ks, HSAs, and FSAs all lock your money up for a time or purpose, in
exchange for lower or delayed taxes. You can only take advantage of them if
you're sure you won't need the money for another purpose sooner.
More examples here.
Note that most of these are perfectly legal and the rest are borderline. But we're still
not getting the result we want, of taxes being proportional to income.
When we assess moral blame for a situation, we typically want it to be roughly in
proportion to much power a person has to change said situation. But just like money
can be used to evade taxes, power can be used to avoid blame. This results in a
distorted blame-distribution apparatus which assigns the least blame to the
person most able to change the situation. Allow me a few examples to
demonstrate this.
Examples 1 + 2: Corporate Malfeasance
Amazon.com provides a valuable service by letting any idiot sell a book, with minimal
overhead. One of the costs of this complete lack of veriﬁcation is that people will sell
things that wouldn't pass veriﬁcation, such as counterfeits, at great cost to publishers
and authors. Amazon could never sell counterfeits directly: they're a large company
that's easy to sue. But by setting themselves up as a platform on which other people
sell, they enable themselves to proﬁt from counterfeits.
Or take slavery. No company goes "I'm going to go out and enslave people today"
(especially not publicly), but not paying people is sometimes cheaper than paying
them, so ﬁnancial pressure will push towards slavery. Public pressure pushes in the
opposite direction, so companies try not to visibly use slave labor. But they can't
control what their subcontractors do, and especially not what their subcontractors'
subcontractors' subcontractors do, and sometimes this results in workers being
unpaid and physically blocked from leaving.
Who's at fault for the subcontractor(^3)'s slave labor? One obvious answer is "the
person locking them in during the ﬁre" or "the parent who gives their kid piecework",
and certainly it couldn't happen without them. But if we say "Nike's lack of knowledge
makes them not responsible", we give them an incentive to subcontract without
asking follow up questions. The executive is probably beneﬁting more from the system
of slave labor than the factory owner is from his little domain, and has more power to
change what is happening. If the small factory owner pays fair wages, he gets
outcompeted by a factory that does use slave labor. If the Nike CEO decides to
insource their manufacturing to ensure fair working conditions, something actually
changes.
...Unless consumers switch to a cheaper, slavery-driven shoe brand.
Which is actually really hard to not do. You could choose more expensive shoes, but
the proﬁt margin is still bigger if you shrink expenses, so that doesn't help (which is
why Fairtrade was a failure from the workers' perspective). You can't investigate the
manufacturing conditions of everything you buy-- it's just too time consuming. But if
you punish obvious enslavement and conduct no follow up studies, what you get is
obscured enslavement, not decent working conditions.

Moral Mazes describes the general phenomenon on page 21:
Moreover, pushing down details relieves superiors of the burden of too much
knowledge, particularly guilty knowledge. A superior will say to a subordinate, for
instance: "Give me your best thinking on the problem with [X]." When the
subordinate makes his report, he is often told: "I think you can do better than
that," until the subordinate has worked out all the details of the boss's
predetermined solution, without the boss being speciﬁcally aware of "all the eggs
that have to be broken." It is also not at all uncommon for very bald and
extremely general edicts to emerge from on high. For example, "Sell the plant in
[St. Louis]; let me know when you've struck a deal," or "We need to get higher
prices for [fabric X]; see what you can work out," or "Tom, I want you to go down
there and meet with those guys and make a deal and I don't want you to come
back until you've got one." This pushing down of details has important
consequences.
First, because they are unfamiliar with—indeed deliberately distance themselves
from—entangling details, corporate higher echelons tend to expect successful
results without messy complications. This is central to top executives' well-known
aversion to bad news and to the resulting tendency to kill the messenger who
bears the news.
Second, the pushing down of details creates great pressure on middle managers
not only to transmit good news but, precisely because they know the details, to
act to protect their corporations, their bosses, and themselves in the process.
They become the "point men" of a given strategy and the potential "fall guys"
when things go wrong. From an organizational standpoint, overly conscientious
managers are particularly useful at the middle levels of the structure. Upwardly
mobile men and women, especially those from working-class origins who ﬁnd
themselves in higher status milieux, seem to have the requisite level of anxiety,
and perhaps tightly controlled anger and hostility, that fuels an obsession with
detail. Of course, such conscientiousness is not necessarily, and is certainly not
systematically, rewarded; the real organizational premiums are placed on other,
more ﬂexible, behavior.
These examples diﬀer in an important way from tax structuring: structuring requires
seeking out advice and acting on it to achieve the goal. It's highly agentic. The Wells
Fargo and apparel-outsourcing cases required no such agency on the part of
executives. They vaguely wished for something (more revenue, fewer expenses), and
somehow it happened. An employee who tried to direct the executives' attention to
the fact that they were indirectly employing slaves would probably be ﬁred before
they ever reached the executives. Executives are not only outsourcing their dirty
work, they're outsourcing knowledge of their dirty work. 
[Details of personal anecdotes changed both intentionally and by the vagaries of
human memory]
Example/Exception 2.5: Corporate
Malfeasance Gone Wrong
The Wells Fargo account fraud scandal: in order to meet quotas, entry level Wells
Fargo employees created millions of unauthorized accounts (typically extra services
for existing customers). I originally included this as an example of "executives
incentivizing entry level employees to commit fraud on their behalf", but it turns out

Wells Fargo made almost no money oﬀ the fraud- $2m over ﬁve years, which hardly
seems worth the employees' time, much less the $185m ﬁne. I've left this in as an
example of how the incentives-not-orders system doesn't always work in powerful
people's favor.
Thanks to Larks for pointing this out.
Example 3: Foreign Medical Care
My cousin Angela broke her leg while traveling in Thailand, and was delighted by the
level of care she received at the Thai hospital-- not just medically, but socially. Nurses
brought her ﬂowers and were just generally nicer than their American counterparts.
Her interpretation was that Thailand was a place motivated by love and kindness, not
money, and Americans should aspire to this level of regard for their fellow human
being. My interpretation was that she had enough money to buy the goodwill of
everyone in the room without noticing, so what she should have learned is that being
rich is awesome, and that being an American who travels internationally is enough to
qualify you as rich.
This is mostly a success story for the free market: Angela got good medical care and
the nurses got money (I'm assuming). Any crime in this story were committed oﬀ-
screen. But Angela was certainly beneﬁting from the nurses' restrained choices in life.
And had she had actual power to aﬀect healthcare in US, trying to ﬁx it based on what
she learned in Thailand would have done a lot of damage.
Example 4: My Dating an Artist Experience
My starving-artist ex-boyfriend, Connor, stayed with me for two months after a little
bad luck and a lot of bad decisions cost him his job and then apartment (this was back
when I had a two bedroom apartment to myself-- I miss Seattle). During this time we
had one big ﬁght. My view on the ﬁght now is that I was locally in the right but
globally the disagreement was indicative of irreconcilable diﬀerences that should have
led us to break up. That was delayed by months when he capitulated.
One possibility is that he genuinely thought he could change and that I was worth the
attempt. Another is that he saw the incompatibility, or knew things that should have
led him to see it, but lied or blocked out the knowledge so that he could keep living
with me. This would be a shitty, manipulative thing for him to do. On the other hand,
what did I expect? If the punishment for breaking up with me was, best case scenario,
moving into a homeless shelter, of course he felt pressure to appease me. 
It wasn't my fault he felt that pressure, any more than it was Angela's fault her nurses
were born with fewer options than her. Time in my spare bedroom was a gift to him I
had no obligation to keep giving. But if I'd really valued a coercion free decision, I
would have committed to housing him independent of our relationship. Although if
that becomes common knowledge, it just means people can't make an uncoerced
decision to date me at all. And if helping Connor at all meant a commitment to do so
forever, he would get a lot less help.
This case is more like the Wells Fargo case than Amazon or Nike. I was getting only the
appearance of what I wanted (a genuine relationship with a compatible person), not

the real thing. Nonetheless, the universe was contorting itself to give me the
appearance of what I wanted.
Summary
What all of these stories have in common is that (relatively) powerful people's desires
were met by people less powerful than them, without them having to take
responsibility for the action or sometimes even the desire. Society conspired to give
them what they wanted (or in the case of Connor and Wells Fargo, a facsimile of what
they wanted) without them having to articulate the want, even to themselves. That's
what power means: ability to make the game come out like you want. Disempowered
people are forced to consciously notice things (e.g., this budget is unreachable) and
make plans (e.g., slavery) where a powerful person wouldn't. And it's unfair to judge
them for doing so while ignoring the morality of the powerful who never consider the
system that brings them such nice things. 
Take home message:
1. The most agentic person in a situation is not necessarily most morally
culpable. One of the things power buys you is distance from the crime.
2. Power obscures information ﬂow. If you are not proactively looking to
see how your wants and needs are being met, you are probably
beneﬁting from something immoral or being tricked.
This piece was inspired by a conversation with and beneﬁted from comments by Ben
Hoﬀman. I'd also like to thank several commenters on Facebook for comments on an
earlier draft and Justis Mills for copyediting.

Soft takeoﬀ can still lead to decisive
strategic advantage
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
[Epistemic status: Argument by analogy to historical cases. Best case scenario it's just
one argument among many. Edit: Also, thanks to feedback from others, especially
Paul, I intend to write a signiﬁcantly improved version of this post in the next two
weeks. Edit: I never did, because in the course of writing my response I realized the
original argument made a big mistake. See this review.]
I have on several occasions heard people say things like this:
The original Bostrom/Yudkowsky paradigm envisioned a single AI built by a single
AI project, undergoing intelligence explosion all by itself and attaining a decisive
strategic advantage as a result. However, this is very unrealistic. Discontinuous
jumps in technological capability are very rare, and it is very implausible that one
project could produce more innovations than the rest of the world combined.
Instead we should expect something more like the Industrial Revolution:
Continuous growth, spread among many projects and factions, shared via a
combination of trade and technology stealing. We should not expect any one
project or AI to attain a decisive strategic advantage, because there will always be
other projects and other AI that are only slightly less powerful, and coalitions will
act to counterbalance the technological advantage of the frontrunner.
(paraphrased)
Proponents of this view often cite Paul Christiano in support. Last week I heard him
say he thinks the future will be "like the Industrial Revolution but 10x-100x faster."
In this post, I assume that Paul's slogan for the future is correct and then nevertheless
push back against the view above. Basically, I will argue that even if the future is like
the industrial revolution only 10x-100x faster, there is a 30%+ chance that it will
involve a single AI project (or a single AI) with the ability to gain a decisive strategic
advantage, if they so choose. (Whether or not they exercise that ability is another
matter.)
Why am I interested in this? Do I expect some human group to take over the world?
No; instead what I think is that (1) an unaligned AI in the leading project might take
over the world, and (2) A human project that successfully aligns their AI might refrain
from taking over the world even if they have the ability to do so, and instead use their
capabilities to e.g. help the United Nations enforce a ban on unauthorized AGI
projects.
National ELO ratings during the industrial
revolution and the modern era
In chess (and some other games) ELO rankings are used to compare players. An
average club player might be rank 1500; the world chess champion might be 2800;

computer chess programs are even better. If one player has 400 points more than
another, it means the ﬁrst player would win with ~90% probability.
We could apply this system to compare the warmaking abilities of nation-states and
coalitions of nation-states. For example, in 1941 perhaps we could say that the ELO
rank of the Axis powers was ~300 points lower than the ELO rank of the rest of the
world combined (because what in fact happened was the rest of the world combining
to defeat them, but it wasn't a guaranteed victory). We could add that in 1939 the ELO
rank of Germany was ~400 points higher than that of Poland, and that the ELO rank of
Poland was probably 400+ points higher than that of Luxembourg.
We could make cross-temporal fantasy comparisons too. The ELO ranking of Germany
in 1939 was probably ~400 points greater than that of the entire world circa 1910, for
example. (Visualize the entirety of 1939 Germany teleporting back in time to 1910,
and then imagine the havoc it would wreak.)
Claim 1A: If we were to estimate the ELO rankings of all nation-states and sets of
nation-states (potential alliances) over the last 300 years, the rank of the most
powerful nation-state at at a given year would on several occasions be 400+ points
greater than the rank of the entire world combined 30 years prior.
Claim 1B: Over the last 300 years there have been several occasions in which one
nation-state had the capability to take over the entire world of 30 years prior.
I'm no historian, but I feel fairly conﬁdent in these claims.
In naval history, the best ﬂeets in the world in 1850 were obsolete by 1860
thanks to the introduction of iron-hulled steamships, and said steamships were
themselves obsolete a decade or so later, and then those ships were obsoleted
by the Dreadnought, and so on... This process continued into the modern era. By
"Obsoleted" I mean something like "A single ship of the new type could defeat
the entire combined ﬂeet of vessels of the old type."
A similar story could be told about air power. In a dogﬁght between planes of
year 19XX and year 19XX+30, the second group of planes will be limited only by
how much ammunition they can carry.
Small technologically advanced nations have regularly beaten huge sprawling
empires and coalitions. (See: Colonialism)
The entire world has been basically carved up between the small handful of
most-technologically advanced nations for two centuries now. For example, any
of the Great Powers of 1910 (plus the USA) could have taken over all of Africa,
Asia, South America, etc. if not for the resistance that the other great powers
would put up. The same was true 40 years later and 40 years earlier.
I conclude from this that if some great power in the era kicked oﬀ by the industrial
revolution had managed to "pull ahead" of the rest of the world more eﬀectively than
it actually did--30 years more eﬀectively, in particular--it really would have been able
to take over the world.
Claim 2: If the future is like the Industrial Revolution but 10x-100x faster, then
correspondingly the technological and economic power granted by being 3 - 0.3 years
ahead of the rest of the world should be enough to enable a decisive strategic
advantage.
The question is, how likely is it that one nation/project/AI could get that far ahead of
everyone else? After all, it didn't happen in the era of the Industrial Revolution. While

we did see a massive concentration of power into a few nations on the leading edge of
technological capability, there were always at least a few such nations and they kept
each other in check.
The "surely not faster than the rest of the
world combined" argument
Sometimes I have exchanges like this:
Me: Decisive strategic advantage is plausible!
Interlocutor: What? That means one entity must have more innovation power
than the rest of the world combined, to be able to take over the rest of the
world!
Me: Yeah, and that's possible after intelligence explosion. A superintelligence
would totally have that property.
Interlocutor: Well yeah, if we dropped a superintelligence into a world full of
humans. But realistically the rest of the world will be undergoing intelligence
explosion too. And indeed the world as a whole will undergo a faster intelligence
explosion than any particular project could; to think that one project could pull
ahead of everyone else is to think that, prior to intelligence explosion, there
would be a single project innovating faster than the rest of the world combined!
This section responds to that by way of sketching how one nation/project/AI might get
3 - 0.3 years ahead of everyone else.
Toy model: There are projects which research technology, each with their own
"innovation rate" at which they produce innovations from some latent tech tree. When
they produce innovations, they choose whether to make them public or private. They
have access to their private innovations + all the public innovations.
It follows from the above that the project with access to the most innovations at any
given time will be the project that has the most hoarded innovations, even though the
set of other projects has a higher combined innovation rate and also a larger
combined pool of accessible innovations. Moreover, the gap between the leading
project and the second-best project will increase over time, since the leading project
has a slightly higher rate of production of hoarded innovations, but both projects have
access to the same public innovations
This model leaves out several important things. First, it leaves out the whole
"intelligence explosion" idea: A project's innovation rate should increase as some
function of how many innovations they have access to. Adding this in will make the
situation more extreme and make the gap between the leading project and everyone
else grow even bigger very quickly.
Second, it leaves out reasons why innovations might be made public. Realistically
there are three reasons: Leaks, spies, and selling/using-in-a-way-that-makes-it-easy-
to-copy.
Claim 3: Leaks & Spies: I claim that the 10x-100x speedup Paul prophecies will not
come with an associated 10x-100x increase in the rate of leaks and successful spying.
Instead the rate of leaks and successful spying will be only a bit higher than it
currently is.

This is because humans are still humans even in this soft takeoﬀ future, still in human
institutions like companies and governments, still using more or less the same
internet infrastructure, etc. New AI-related technologies might make leaking and
spying easier than it currently is, but they also might make it harder. I'd love to see an
in-depth exploration of this question because I don't feel particularly conﬁdent.
But anyhow, if it doesn't get much easier than it currently is, then going 3 years to 0.3
years without a leak is possible, and more generally it's possible for the world's
leading project to build up a 0.3-3 year lead over the second-place project. For
example, the USSR had spies embedded in the Manhattan Project but it still took them
4 more years to make their ﬁrst bomb.
Claim 4: Selling etc. I claim that the 10x-100x speedup Paul prophecies will not
come with an associated 10x-100x increase in the budget pressure on projects to
make money fast. Again, today AI companies regularly go years without turning a
proﬁt -- DeepMind, for example, has never turned a proﬁt and is losing something like
a billion dollars a year for its parent company -- and I don't see any particularly good
reason to expect that to change much.
So yeah, it seems to me that it's totally possible for the leading AI project to survive
oﬀ investor money and parent company money (or government money, for that
matter!) for ﬁve years or so, while also keeping the rate of leaks and spies low enough
that the distance between them and their nearest competitor increases rather than
decreases. (Note how this doesn't involve them "innovating faster than the rest of the
world combined.")
Suppose they could get a 3-year lead this way, at the peak of their lead. Is that
enough?
Well, yes. A 3-year lead during a time 10x-100x faster than the Industrial Revolution
would be like a 30-300 year lead during the era of the Industrial Revolution. As I
argued in the previous section, even the low end of that range is probably enough to
get a decisive strategic advantage.
If this is so, why didn't nations during the Industrial Revolution try to hoard their
innovations and gain decisive strategic advantage?
England actually did, if I recall correctly. They passed laws and stuﬀ to prevent their
early Industrial Revolution technology from spreading outside their borders. They were
unsuccessful--spies and entrepreneurs dodged the customs oﬃcials and snuck
blueprints and expertise out of the country. It's not surprising that they weren't able to
successfully hoard innovations for 30+ years! Entire economies are a lot more leaky
than AI projects.
What a "Paul Slow" soft takeoﬀ might look
like according to me
At some point early in the transition to much faster innovation rates, the leading AI
companies "go quiet." Several of them either get huge investments or are
nationalized and given eﬀectively unlimited funding. The world as a whole continues
to innovate, and the leading companies beneﬁt from this public research, but they
hoard their own innovations to themselves. Meanwhile the beneﬁts of these AI
innovations are starting to be felt; all projects have signiﬁcantly increased (and

constantly increasing) rates of innovation. But the fastest increases go to the leading
project, which is one year ahead of the second-best project. (This sort of gap is normal
for tech projects today, especially the rare massively-funded ones, I think.) Perhaps
via a combination of spying, selling, and leaks, that lead narrows to six months
midway through the process. But by that time things are moving so quickly that a six
months' lead is like a 15-150 year lead during the era of the Industrial Revolution. It's
not guaranteed and perhaps still not probable, but at least it's reasonably likely that
the leading project will be able to take over the world if it chooses to.
Objection: What about coalitions? During the industrial revolution, if one country did
successfully avoid all leaks, the other countries could unite against them and make
the "public" technology inaccessible to them. (Trade does something like this
automatically, since refusing to sell your technology also lowers your income which
lowers your innovation rate as a nation.)
Reply: Coalitions to share AI research progress will be harder than free-trade /
embargo coalitions. This is because AI research progress is much more the result of
rare smart individuals talking face-to-face with each other and much less the result of
a zillion diﬀerent actions of millions of diﬀerent people, as the economy is. Besides, a
successful coalition can be thought of as just another project, and so it's still true that
one project could get a decisive strategic advantage. (Is it fair to call "The entire world
economy" a project with a decisive strategic advantage today? Well, maybe... but it
feels a lot less accurate since almost everyone is part of the economy but only a few
people would have control of even a broad coalition AI project.)
Anyhow, those are my thoughts. Not super conﬁdent in all this, but it does feel right to
me. Again, the conclusion is not that one project will take over the world even in Paul's
future, but rather that such a thing might still happen even in Paul's future.
Thanks to Magnus Vinding for helpful conversation.

The Commitment Races problem
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
[Epistemic status: Strong claims vaguely stated and weakly held. I expect that writing
this and digesting feedback on it will lead to a much better version in the future. EDIT:
So far this has stood the test of time. EDIT: As of September 2020 I think this is one of
the most important things to be thinking about.]
This post attempts to generalize and articulate a problem that people have been
thinking about since at least 2016. [Edit: 2009 in fact!] In short, here is the problem:
Consequentialists can get caught in commitment races, in which they want to make
commitments as soon as possible. When consequentialists make commitments too
soon, disastrous outcomes can sometimes result. The situation we are in (building AGI
and letting it self-modify) may be one of these times unless we think carefully about
this problem and how to avoid it.
For this post I use "consequentialists" to mean agents that choose actions entirely on
the basis of the expected consequences of those actions. For my purposes, this means
they don't care about historical facts such as whether the options and consequences
available now are the result of malicious past behavior. (I am trying to avoid trivial
deﬁnitions of consequentialism according to which everyone is a consequentialist
because e.g. "obeying the moral law" is a consequence.) This deﬁnition is somewhat
fuzzy and I look forward to searching for more precision some other day.
Consequentialists can get caught in
commitment races, in which they want to
make commitments as soon as possible
Consequentialists are bullies; a consequentialist will happily threaten someone insofar
as they think the victim might capitulate and won't retaliate.
Consequentialists are also cowards; they conform their behavior to the incentives set
up by others, regardless of the history of those incentives. For example, they
predictably give in to credible threats unless reputational eﬀects weigh heavily
enough in their minds to prevent this.
In most ordinary circumstances the stakes are suﬃciently low that reputational eﬀects
dominate: Even a consequentialist agent won't give up their lunch money to a
schoolyard bully if they think it will invite much more bullying later. But in some cases
the stakes are high enough, or the reputational eﬀects low enough, for this not to
matter.
So, amongst consequentialists, there is sometimes a huge advantage to "winning the
commitment race." If two consequentialists are playing a game of Chicken, the ﬁrst
one to throw out their steering wheel wins. If one consequentialist is in position to
seriously hurt another, it can extract concessions from the second by credibly
threatening to do so--unless the would-be victim credibly commits to not give in ﬁrst!

If two consequentialists are attempting to divide up a pie or select a game-theoretic
equilibrium to play in, the one that can "move ﬁrst" can get much more than the one
that "moves second." In general, because consequentialists are cowards and bullies,
the consequentialist who makes commitments ﬁrst will predictably be able to
massively control the behavior of the consequentialist who makes commitments later.
As the folk theorem shows, this can even be true in cases where games are iterated
and reputational eﬀects are signiﬁcant.
Note: "ﬁrst" and "later" in the above don't refer to clock time, though clock time is a
helpful metaphor for imagining what is going on. Really, what's going on is that agents
learn about each other, each on their own subjective timeline, while also making
choices (including the choice to commit to things) and the choices a consequentialist
makes at subjective time t are cravenly submissive to the commitments they've
learned about by t.
Logical updatelessness and acausal bargaining combine to create a particularly
important example of a dangerous commitment race. There are strong incentives for
consequentialist agents to self-modify to become updateless as soon as possible, and
going updateless is like making a bunch of commitments all at once. Since real agents
can't be logically omniscient, one needs to decide how much time to spend thinking
about things like game theory and what the outputs of various programs are before
making commitments. When we add acausal bargaining into the mix, things get even
more intense. Scott Garrabrant, Wei Dai, and Abram Demski have described this
problem already, so I won't say more about that here. Basically, in this context, there
are many other people observing your thoughts and making decisions on that basis.
So bluﬃng is impossible and there is constant pressure to make commitments quickly
before thinking longer. (That's my take on it anyway)
Anecdote: Playing a board game last week, my friend Lukas said (paraphrase) "I
commit to making you lose if you do that move." In rationalist gaming circles this
sort of thing is normal and fun. But I suspect his gambit would be considered
unsportsmanlike--and possibly outright bullying--by most people around the world,
and my compliance would be considered cowardly. (To be clear, I didn't comply.
Practice what you preach!)
When consequentialists make commitments
too soon, disastrous outcomes can sometimes
result. The situation we are in may be one of
these times.
This situation is already ridiculous: There is something very silly about two supposedly
rational agents racing to limit their own options before the other one limits theirs. But
it gets worse.
Sometimes commitments can be made "at the same time"--i.e. in ignorance of each
other--in such a way that they lock in an outcome that is disastrous for everyone.
(Think both players in Chicken throwing out their steering wheels simultaneously.)
Here is a somewhat concrete example: Two consequentialist AGI think for a little while
about game theory and commitment races and then self-modify to resist and heavily
punish anyone who bullies them. Alas, they had slightly diﬀerent ideas about what

counts as bullying and what counts as a reasonable request--perhaps one thinks that
demanding more than the Nash Bargaining Solution is bullying, and the other thinks
that demanding more than the Kalai-Smorodinsky Bargaining Solution is bullying--so
many years later they meet each other, learn about each other, and end up locked
into all-out war.
I'm not saying disastrous AGI commitments are the default outcome; I'm saying the
stakes are high enough that we should put a lot more thought into preventing them
than we have so far. It would really suck if we create a value-aligned AGI that ends up
getting into all sorts of ﬁghts across the multiverse with other value systems. We'd
wish we built a paperclip maximizer instead.
Objection: "Surely they wouldn't be so stupid as to make those commitments--even I
could see that bad outcome coming. A better commitment would be..."
Reply: The problem is that consequentialist agents are motivated to make
commitments as soon as possible, since that way they can inﬂuence the behavior of
other consequentialist agents who may be learning about them. Of course, they will
balance these motivations against the countervailing motive to learn more and think
more before doing drastic things. The problem is that the ﬁrst motivation will push
them to make commitments much sooner than would otherwise be optimal. So they
might not be as smart as us when they make their commitments, at least not in all the
relevant ways. Even if our baby AGIs are wiser than us, they might still make mistakes
that we haven't anticipated yet. The situation is like the centipede game: Collectively,
consequentialist agents beneﬁt from learning more about the world and each other
before committing to things. But because they are all bullies and cowards, they
individually beneﬁt from committing earlier, when they don't know so much.
Objection: "Threats, submission to threats, and costly ﬁghts are rather rare in human
society today. Why not expect this to hold in the future, for AGI, as well?"
Reply: Several points:
1. Devastating commitments (e.g. "Grim Trigger") are much more possible with AGI--
just alter the code! Inigo Montoya is a ﬁctional character and even he wasn't able to
summon lifelong commitment on a whim; it had to be triggered by the brutal murder
of his father.
2. Credibility is much easier also, especially in an acausal context (see above.)
3. Some AGI bullies may be harder to retaliate against than humans, lowering their
disincentive to make threats.
4. AGI may not have suﬃciently strong reputation eﬀects in the sense relevant to
consequentialists, partly because threats can be made more devastating (see above)
and partly because they may not believe they exist in a population of other powerful
agents who will bully them if they show weakness.
5. Finally, these terrible things (Brutal threats, costly ﬁghts) do happen to some extent
even among humans today--especially in situations of anarchy. We want the AGI we
built to be less likely to do that stuﬀ than humans, not merely as likely.
Objection: "Any AGI that falls for this commit-now-before-the-others-do argument will
also fall for many other silly do-X-now-before-it's-too-late arguments, and thus will be
incapable of hurting anyone."

Reply: That would be nice, wouldn't it? Let's hope so, but not count on it. Indeed
perhaps we should look into whether there are other arguments of this form that we
should worry about our AI falling for...
Anecdote: A friend of mine, when she was a toddler, would threaten her parents:
"I'll hold my breath until you give me the candy!" Imagine how badly things would
have gone if she was physically capable of making arbitrary credible
commitments. Meanwhile, a few years ago when I ﬁrst learned about the concept
of updatelessness, I resolved to be updateless from that point onwards. I am now
glad that I couldn't actually commit to anything then.
Conclusion
Overall, I'm not certain that this is a big problem. But it feels to me that it might be,
especially if acausal trade turns out to be a real thing. I would not be surprised if
"solving bargaining" turns out to be even more important than value alignment,
because the stakes are so high. I look forward to a better understanding of this
problem.
Many thanks to Abram Demski, Wei Dai, John Wentworth, and Romeo Stevens for
helpful conversations.

A Personal Rationality Wishlist
At one point I compiled a list of conundrums relating to rationality that come up in my
life. Instead of solving them, I thought I'd write up a selection of them, since that's
easier and maybe other people will solve them.
Punishing honesty vs no punishment
In some cases, you might want people to comply with some rule that they might
otherwise wish to break, but the only way to check if they have complied is to ask
them and hope that they're honest (or perhaps there's another, much more
expensive, way to check). Examples:
A sperm bank might only want donors without congenital abnormalities that
they might not be able to easily observe or test for.
I might not want my housemates to go into my room and look at all my stuﬀ
when I'm not there.
There's a dilemma: how should one enforce such a rule? If you just ask people, and
punish them if they say that they didn't comply, then you're incentivising people to lie
to you. But if you don't ask, the rule doesn't get enforced. Abstractly, it seems like you
just can't enforce such a rule at all, but it seems to me that often people are able to
be honest in the face of punishment, so not all hope is lost. How should I think about
these situations? In practice, how should I decide the enforcement mechanism?
According to David Friedman's recent book on legal systems, in saga-period Iceland,
there was a much larger penalty for killing somebody if you failed to confess as soon
as was practical. This suggests one solution: estimate the likelihood of discovery of
violation of a rule conditioned on the violater being dishonest, and set the punishment
of that high enough that it's worth it for rule violaters to be honest. But this leaves
open the question of how in practice to estimate this probability, calculate the
appropriate punishment level, and how much eﬀort to put into detection of rule
violations when nobody has confessed to a violation.
'The anime thing'
Once, a friend of mine observed that he couldn't talk about how he didn't like anime
without a bunch of people rushing in to tell him that anime was actually good and
recommending anime for him to watch, even when he explicitly asked them not to.
Similarly, another friend of mine went to a coding bootcamp, only to discover that she
intensely disliked coding, and would basically be unable to do it as a career, causing
her to decide to switch to her previous worse-paying job. When she talked about this,
often other people would suggest coding jobs for her to take, or remind her that
coding pays much better than her other options.
I think that the responses that my friends received are instances of the same
phenomenon, which I'll call 'the anime thing' (since I came across the anime example
ﬁrst, and don't have better name). Why does the anime thing happen? In what other
situations might it happen? If one wanted it to not happen, how would one go about
that?

When and how to increase neuroticism
Many people have advice on how to become more relaxed, calm, and happy. But
presumably it's possible to be too relaxed, calm, and/or happy, and one should
instead be anxious, angry, and/or sad. How can I tell when this is the case, and what
should I do to increase my neuroticism in-the-moment? Or could it really be true that
humans are universally biased towards feeling unpleasant emotions?
Virtue of bicycles
It seems to me that bicycles are an unusually wonderful device.
You can just look at them with your eyes, think a little, and then you'll know
basically how they work.
They are very eﬃcient in converting energy into forward motion.
By making transportation easier, they make people more free in one of the most
concrete ways possible.
They let you go very fast, while still being in full contact with the air and ground.
I want more of that in my life. How should I get it? Should I be deriving any deep
lessons from how great bicycles are?
Does my sleepy self know whether I should be
sleeping?
When I've just woken up from sleeping, often I'll have a strong impression that it
would be a good idea to go back to sleep, or at least stay in bed and daydream. It
seems plausible that this is a bad idea - as Marcus Aurelius reminded himself in his
journal:
At dawn, when you have trouble getting out of bed, tell yourself: "I have to go to
work—as a human being. What do I have to complain of, if I'm going to do what I
was born for—the things I was brought into the world to do? Or is this what I was
created for? To huddle under the blankets and stay warm?"
So you were born to feel "nice"? Instead of doing things and experiencing them?
Don't you see the plants, the birds, the ants and spiders and bees going about
their individual tasks, putting the world in order, as best they can? And you're not
willing to do your job as a human being? Why aren't you running to do what your
nature demands?
You don't love yourself enough. Or you'd love your nature too, and what it
demands of you.
On the other hand, I gather that sleep is in fact important for us biological humans.
And probably the way my body lets me know that is by making me sleepy.
On the third hand, I just woke up of my own accord (I rarely perceive my waking up as
being due to light or sound), which you'd think would be a sign that now would be a
good time to be awake. I know my waking self can be wrong about whether or not I
should be awake, why should my sleeping self be all that diﬀerent? Also, when I've

just woken up, I am in some important senses less intelligent than literally any other
waking moment.
Unfortunately, thinking hard about this problem in the moment makes sleep more
diﬃcult, meaning that a policy-level solution is necessary. The solution is likely 'try
both ways for a week, see how you do on a cognitive battery', but it would be nice to
reason the answer from ﬁrst principles.

Trauma, Meditation, and a Cool Scar
[Trigger Warning: I'll be discussing a physical injury, recovery, and panic attacks in
detail. The ﬁrst three pictures linked are gory. Again, they are linked, not directly
shown]
Trauma
One year ago today, I was in an accident with an industrial drone. It was spinning too
fast while arming (like how helicopters spin up before they took oﬀ), but nothing we
tried would ﬁx it. Eventually, I changed the PWM value back to the default value, and
it spun up even faster. Fast enough to take oﬀ right into me.
It tore up my arm. It tore up my face. After screaming, it didn't hurt that bad, so I
thought I overreacted. I told everyone "I think I'm okay". They didn't believe me, and I
was rushed to the hospital. The pain was horrible, but the nausea was worse. I had
made everyone apple pie that day, but I didn't get to keep my piece.
The doctor thought I needed facial reconstruction surgery, so they put me in an
ambulance and shipped me to another hospital. They stitched me up, said no facial
surgery was needed, but that my lens and iris were destroyed in my left eye. A couple
days later, my eyeball bruised. A week of checkups and eye drops 4 times a day, they
then put me under for surgery.
I woke up in so much pain, so confused. They told me to keep my head down. I asked
Why am I in so much pain? repeatedly. They put me in a wheelchair to take me
outside, and told me to keep my head down. But all I could do was feel terriﬁed
because I was in pain and no one was doing anything about it. I'm told to keep my
head down as they put me in my dad's car, so I kept my head down and hurt.
For a week, I had to keep my head down. When I ate, my head was down. When I
talked to someone, my head was down. When I slept, my head was down1. I couldn't
play piano like I used to because of my arm. I couldn't read like I used to because of
my eye. I couldn't even think like I used to because my working memory was shot. I
felt so powerless and isolated.
How am I supposed to program or learn new things when I could barely keep 3 things
on my mind, when I could barely read oﬀ a screen for 2 minutes before having to take
a break? How am I supposed to connect with someone when I could barely look them
in the eye, when I couldn't even give them my full attention?
On top of that, I was on eye drops to sooth my eye from all the other eye drops I was
taking. I was on laxatives to relieve constipation from all the pain medicine I was
taking. Even though I was on a tablet and two drops for eye pressure, I still got
glaucoma headaches. So another surgery, and more checkups. And of course, there
were the panic attacks.
Any unexpected loud noise would ﬁll me with distress, it felt like I was being attacked,
like it was happening again. A couple of months later, I was playing piano more like I
was used to. A picture frame on top of the piano fell, freaked me out, and I cried

because I thought I was over this. It was frustrating how scared I was, how easily I
could feel overwhelmed.
I've never been angrier in my life.
As a kid, I used to think "what doesn't kill you makes you stronger", that if I went
through horrible events, I would come out cooler, more mature. That I would be like
Sasuke from Naruto whose whole family died, but he came out so cool, and edgy, and
he got the girl! But really, horrible events mess you up, and I wouldn't wish that on
anyone. There's not a guarantee that things will be better, not even that things will be
as good as they were before.
But... things did get better.
Meditation
I read Hazard's post and took up meditating with the mind illuminated. I took Elo up
on talking about meditating, told him about my panic attacks, and we ﬁxed them! By
"ﬁxed" I mean they still happened, but drastically aﬀected me less and less. And then
they started happening less and less. Now, I really don't mind them more than an itch.
I was told:
1. Break down previous panic attacks into a sequence of events/sensations such as
physical sensations (jaws clenching, shoulders tensed, heart racing, breathing
change), and mental sensations (speciﬁc thoughts, movements of attention, loss
of awareness).
2. Be aware of the sensations you experience during the actual panic attack. From
Elo, "The piece of knowledge to maintain is that you are not these reactions, you
have them but they do not have you. You get to watch them happen."
For me, I could see "jerking back, elevated heart rate, cortisol/adrenaline feeling,
teary eyed because of how I reacted, eyes focus, shoulder tension, toes clenching",
but later, in the moment of actually having a panic attack, it was [noise]->[involuntary
yelp]->[chest tightness with stress]->[eyes widen]->[thinking that I'm ﬁne].
I would like to clarify that "chest tightness with stress" is a mental object in word
form, but I felt it as a physical sensation like a bad warmth spreading through my body
starting from my chest. But even that description fails to convey the reality of the
sensation! What's important is that I described it to myself in hard-to-convey physical
sensations. The same is true for the other links in the chain.
Doing this, I realized "Pain is inevitable; suﬀering is optional" with the next few panic
attacks. They happened. They sucked...but then they were over. Through meditating I
was building this skill even more, this skill of non-reacting, of accepting the reality of
sensations exactly as they were, of not ﬁghting it, of not getting trapped in a series of
thoughts, of not holding on to impulses. I used to think "Man, I'm so hungry". Now it's,
"Oh the sensation of hunger is there. Oh, now it's gone. What time is it? 11:00? I'll
work another hour and then eat". All that miserable anger that would keep me up at
nights, I've now let it all go.

I wish I would've had a consistent meditation practice before the accident. I predict
that I would've suﬀered much less. If you are going through a diﬃcult life trauma now,
I highly recommend getting professional help, and you're welcome to PM me about it
as well.
A Cool Scar
I can read and think like I used to (which were two of the most debilitating eﬀects). My
left eye rarely hurts anymore, though I still can't see out of it2. I'm not nauseous nor
do I have glaucoma headaches, though I am still on one eye drop indeﬁnitely. I have
most of the strength and ﬂexibility back in my left arm, though it will act up if I hit it
just right. I am technically bi-chromatic now because my iris was destroyed! Though,
that also means my left eye is a giant pupil, and I need shades to go outside when it's
sunny.
Just like in Valentine's Grieving Well, I was able to see what was important in my life. I
quit my job and started leveraging academia this Spring, I found a girl who kisses my
scars, and I've grown a lot closer to my family3.
Although I'm not as edgy as Sasuke (probably for the best) the scar does make me a
little bit cooler, and, well, I did get the girl.
1. I had an air bubble in my eye and had to keep my head down so that the bubble
would do something to my retina (keep pressure to it?). Pro tip: put pillows between
the bed and your chest when you sleep so you don't suﬀocate.
2. I can see a little actually. White is perfect vision, black is blind.

Do you notice the blind spot (black circle) in my right eye (on the left)? Notice how
that's most of my left eye?
3. My brother and I have such a good relationship that he made me this:
which is ripped from webcomicname
*Special thanks to Elo for reviewing the draft of this post

LW Team Updates - September 2019
To better communicate site updates, we're going to experiment with a once-monthly
updates post. We'll pin this post pin on the homepage for roughly a week and then
continue to post updates here throughout the month.
Please also feel free to use the comments section on this post as a Schelling point to
give feedback, ﬁle bug reports, or ask questions you have about the site. (You can also
email us, ask a question, or use Intercom.)
Recent Features
Shortform
The major feature announcement for August was the launch of the Shortform beta
(aka www.lesslong.com).
Shortform is for:
Writing that is short in length, or written in a short amount of time. Includes oﬀ-
the-cuﬀ thoughts and brainstorming.
Shortform content can be seen in Recent Discussion on the homepage, on
www.lesslong.com/shortform, and on the All Posts page using the Daily view.
Anyone can create their own shortform by clicking New Shortform in their dropdown
menu, clicking the New Shortform Post button on the homepage above Recent
Discussion, or via the text box on the Shortform page.
See the announcement post here with full explanation of the philosophy behind
Shortform and full instructions on how to use it. We'll also be having a party on
September 7 in Berkeley to celebrate the launch of Shortform. Details.
Upcoming Features
Heads up on new features which should arrive in September:
Subscriptions Overhaul
We have a planned overhaul of LessWrong's subscription system which will allow you
to subscribe to posts, comments, users, and private messages thereby receiving
notiﬁcations and/or emails.
Link Previews [DONE]
Soon, when you hover over a green link on a LessWrong page that leads to another
LessWrong page, you'll see a pop-up preview of what's on the other side plus extra
info like the author and karma of the post being linked to.
New Editor Option, including Collaborative Editing and In-Line Comments

Currently, you can write comments and posts using either the Draft.js or Markdown
editor. Soon, we'll enable a new experimental editor that should be more powerful and
include features like collaborative editing and in-line comments in the style of Google
Docs.
Convert Comments to Posts
Sometimes, you write a comment, and then realize it turned out to be a self-contained
essay. Soon comments will have a create draft post option that will create a copy of it
in draft form. You can make any additional edits you'd like, and then publish it like
normal.
If you do use this on your top-level shortform comments, you'll also have the option of
moving it's comments to the new post once you publish it.
Other Updates
MIRI Summer Fellows Program Writing Day
MSFP held a writing day on Thursday, 22nd August and produced several dozen posts.
You can use the All Posts page to easily rewind and see the great AI Safety content
they produced.
LessLong Launch Party in Berkeley
We'll be hosting a Shortform launch party in Berkeley on September 7. See the
LessWrong event or FB event for details.
How to reach the team for feedback or
support
Lastly, if you're looking to get in touch with the LessWrong team, the following are
good ways:
Comment on this post
Intercom (icon in the bottom right, you might have to edit your user settings)
Email us at hello@lesswrong.com or support@lesswrong.com
Ask a question on www.lesswrong.com/questions

Can we really prevent all warming for
less than 10B$ with the mostly side-
eﬀect free geoengineering technique
of Marine Cloud Brightening?
If we're to believe the Philosophical Transactions of the Royal Society, or the
Copenhagen Consensus Center, or apparently any of the individual geoengineering
researchers who've modelled it, it's possible to halt all warming by building a ﬂeet of
autonomous wind-powered platforms that do nothing more sinister than spraying
seawater into the air, in a place no more ecologically sensitive than the open ocean,
and for no greater cost than 10 billion USD
(edit: I'm not sure where the estimate of 10B came from. I saw the estimate of 9B in a
lot of news reports relating to CCC, and I rounded up to be conservative, but I couldn't
easily ﬁnd CCC materials conﬁrming this number)
If this works, no signiﬁcant warming will be allowed to occur after the political static
friction that opposes the use of geoengineering is broken.
Isn't any amount of mean warming bad? So shouldn't we deploy something like this as
soon as possible? Shouldn't we have started deploying it years ago?
Side eﬀects seem minimal. Relative to seeding clouds with sulpherous chemicals
(which will also be considered, once the heat becomes unbearable), it leaves no
ozone-eliminating residue, it produces no acid rain. It may disturb rainfall in some
regions, but it seems that it may be viable to just avoid seeding too close those
regions.
I want to make it clear how little support this needs in order to get done: 10 billion
USD is less than a quarter of the tax income generated by the nation of just New
Zealand one year. A single tiny oecd government could, in theory, do it alone. It wont
need the support of a majority of the US. It probably wont require any support from
the US at all.
What should we do with this information?
I do buy the claim that public support for any sort of emission control will evaporate
the moment geoengineering is realised as a tolerable alternative. Once the public
believe, there will never be a quorum of voters willing to sacriﬁce anything of their
own to reduce emissions. I think we may need to start talking about it anyway, at this
point. Major emitters have already signalled a clear lack of any real will to change. The
humans will not repent. Move on. Stop waiting for humanity to be punished for its sin,
act, do something that has some chance of solving the problem.
Could the taboos against discussing geoengineering delay the discovery of better, less
risky techniques?
Could failing to invest in geoengineering research ultimately lead to the deployment
of relatively crude approaches with costly side eﬀects? (Even if cloud brightening is
the ultimate solution to warming, we still need to address ocean acidiﬁcation and

carbon sequestration, and I'm not aware of any ideal solution to those problems yet,
but two weeks ago I wasn't aware of cloud brightening, so for all I know the problem
isn't a lack of investment, might just be a lack of policy discussion.)
Public funding for geoengineering research is currently non-existent in the US (All
research in the US is either privately funded or funded by universities), and weak in
China (3M USD. 14 members, no tech development, no outdoor experiments.)

Intentional Bucket Errors
I want to illustrate a research technique that I use sometimes. (My actual motivation
for writing this is to make it so that I don't feel as much like I need to defend myself
when I use this technique.) I am calling it intentional bucket errors after a CFAR
concept called bucket errors. Bucket errors is about noticing when multiple diﬀerent
concepts/questions are stored in your head as a single concept/question. Then, by
noticing this, you can think about the diﬀerent concepts/question separately.
What are Intentional Bucket Errors
Bucket errors are normally thought of as a bad thing. It has "errors" right in the name.
However, I want to argue that bucket errors can sometimes be useful, and you might
want to consider having some bucket errors on purpose. You can do this by taking
multiple diﬀerent concepts and just pretending that they are all the same. This usually
only works if the concepts started out suﬃciently close together.
Like many techniques that work by acting as though you believe something false, you
should use this technique responsibly. The goal is to pretend that the concepts are the
same to help you gain traction on thinking about them, but then to also be able to go
back to inhabiting the world where they are actually diﬀerent.
Why Use Intentional Bucket Errors
Why might you want to use intentional bucket errors? For one, maybe the concepts
actually are the same, but they look diﬀerent enough that you won't let yourself
consider the possibility. I think this is especially likely to happen if the concepts are
coming from very diﬀerent ﬁelds or areas of your life. Sometimes it feels silly to draw
strong connections between e.g. human rationality, AI alignment, evolution,
economics, etc. but such connections can be useful. 
Also I ﬁnd this useful for gaining traction. There is something useful about constrained
optimization for being able to start thinking about a problem. Sometimes it is harder
to say something true and useful about X than it is to say something true and useful
that simultaneously applies to X, Y, and Z. This is especially true when the concepts
you are conﬂating are imagined solutions to problems. 
For example, maybe I have an imagined solution to counterfactuals that has a hole in
it that looks like understanding multi-level world models. Then, maybe I also have
have an imagined solution to tiling that also has a hole in it that looks like
understanding multi-level world models. I could view this as two separate problems.
The desired properties of my MLWM theory for counterfactuals might be diﬀerent from
the desired properties for tiling. I have these two diﬀerent holes I want to ﬁll, and one
strategy I have, which superﬁcially looks like it makes the problem harder, is to try to
ﬁnd something that can ﬁll both holes simultaneously. However, this can sometimes
be easier because diﬀerent use cases can help you triangulate the simple theory from
which the speciﬁc solutions can be derived.
A lighter (maybe epistemically safer) version of intentional bucket errors is just to pay
a bunch of attention to the connections between the concepts. This has its own

advantages in that the relationships between the concepts might be interesting.
However, I personally prefer to just throw them all in together, since this way I only
have to work with one object, and it takes up fewer working memory slots while I'm
thinking about it.
Examples
Here are a some recent examples where I feel like I have used something like this, to
varying degrees. 
How the MtG Color Wheel Explains AI Safety is obviously the product of conﬂating
many things together without worrying too much about how all the clusters are
wrong. 
In How does Gradient Descent Interact with Goodhart, the question at the top about
rocket designs and human approval is really very diﬀerent from the experiments that I
suggested, but I feel like learning about one might help my intuitions about the other.
This was actually generated at the same time as I was thinking about Epistemic
Tenure, which for me what partially about the expectation that there is good research
and a correlated proxy of justiﬁable research, and even though our group idea
selection mechanism is going to optimize for justiﬁable research, it is better if the
inner optimization loops in the humans do not directly follow those incentives. The
connection is a bit of a stretch in hindsight, but believing the connection was
instrumental in giving me traction in thinking about all the problems.
Embedded Agency has a bunch of this, just because I was trying to factor a big
problem into a small number of subﬁelds, but the Robust Delegation section can sort
of be described as "Tiling and Corrigibility kind of look similar if you squint. What
happens when I just pretend they are two instantiations of the same problem?"

Troll Bridge
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
All of the results in this post, and most of the informal observations/interpretations, are
due to Sam Eisenstat. I think the Troll Bridge story, as a way to make the decision
problem understandable, is due to Tsvi; but I'm not sure.
Pure Logic Version
Troll Bridge is a decision problem which has been ﬂoating around for a while, but which
has lacked a good introductory post. The original post gives the essential example, but
it lacks the "troll bridge" story, which (1) makes it hard to understand, since it is just
stated in mathematical abstraction, and (2) makes it diﬃcult to ﬁnd if you search for
"troll bridge".
The basic idea is that you want to cross a bridge. However, there is a troll who will blow
up the bridge with you on it, if (and only if) you cross it "for a dumb reason" — for
example, due to unsound logic. You can get to where you want to go by a worse path
(through the stream). This path is better than being blown up, though.
We apply a Löbian proof to show not only that you choose not to cross, but
furthermore, that your counterfactual reasoning is conﬁdent that the bridge would
have blown up if you had crossed. This is supposed to be a counterexample to various
proposed notions of counterfactual, and for various proposed decision theories.

The pseudocode for the environment (more speciﬁcally, the utility gained from the
environment) is as follows:

IE, if the agent crosses the bridge and is inconsistent, then U=-10. (□⊥ means "PA
proves an inconsistency".) Otherwise, if the agent crosses the bridge, U=+10. If neither
of these (IE, the agent does not cross the bridge), U=0.
The pseudocode for the agent could be as follows:

This is a little more complicated, but the idea is supposed to be that you search for
every "action implies utility" pair, and take the action for which you can prove the
highest utility (with some tie-breaking procedure). Importantly, this is the kind of proof-
based decision theory which eliminates spurious counterfactuals in 5-and-10 type
problems. It isn't that easy to trip up with Löbian proofs. (Historical/terminological note:
This decision theory was initially called MUDT, and is still sometimes referred to in that
way. However, I now often call it proof-based decision theory, because it isn't centrally
a UDT. "Modal DT" (MDT) would be reasonable, but the modal operator involved is the
"provability" operator, so "proof-based DT" seems more direct.)
Now, the proof:

Reasoning within PA (ie, the logic of the agent):
Suppose the agent crosses.
Further suppose that the agent proves that crossing implies U=-10.
Examining the source code of the agent, because we're
assuming the agent crosses, either PA proved that crossing
implies U=+10, or it proved that crossing implies U=0.
So, either way, PA is inconsistent -- by way of 0=-10 or
+10=-10.
So the troll actually blows up the bridge, and really, U=-10.
Therefore (popping out of the second assumption), if the agent proves
that crossing implies U=-10, then in fact crossing implies U=-10.
By Löb's theorem, crossing really implies U=-10.
So (since we're still under the assumption that the agent crosses),
U=-10.
So (popping out of the assumption that the agent crosses), the agent
crossing implies U=-10.
Since we proved all of this in PA, the agent proves it, and proves no better utility
in addition (unless PA is truly inconsistent). On the other hand, it will prove that
not crossing gives it a safe U=0. So it will in fact not cross.
The paradoxical aspect of this example is not that the agent doesn't cross -- it makes
sense that a proof-based agent can't cross a bridge whose safety is dependent on the
agent's own logic being consistent, since proof-based agents can't know whether their
logic is consistent. Rather, the point is that the agent's "counterfactual" reasoning
looks crazy. (However, keep reading for a version of the argument where it does make
the agent take the wrong action.) Arguably, the agent should be uncertain of what
happens if it crosses the bridge, rather than certain that the bridge would blow up.
Furthermore, the agent is reasoning as if it can control whether PA is consistent, which
is arguably wrong.
In a comment, Stuart points out that this reasoning seems highly dependent on the
code of the agent; the "else" clause could be diﬀerent, and the argument falls apart. I

think the argument keeps its force:
On the one hand, it's still very concerning if the sensibility of the agent depends
greatly on which action it performs in the "else" case.
On the other hand, we can modify the troll's behavior to match the modiﬁed
agent. The general rule is that the troll blows up the bridge if the agent would
cross for a "dumb reason" -- the agent then concludes that the bridge would be
blown up if it crossed. I can no longer complain that the agent reasons as if it
were controlling the consistency of PA, but I can still complain that the agent
thinks an action is bad because that action indicates its own insanity, due to a
troublingly circular argument.
Analogy to Smoking Lesion
One interpretation of this thought-experiment is that it shows proof-based decision
theory to be essentially a version of EDT, in that it has EDT-like behavior for Smoking
Lesion. The analogy to Smoking Lesion is relatively strong:
An agent is at risk of having a signiﬁcant internal issue. (In Smoking Lesion, it's a
medical issue. In Troll Bridge, it is logical inconsistency.)
The internal issue would bias the agent toward a particular action. (In Smoking
Lesion, the agent smokes. In Troll Bridge, an inconsistent agent crosses the
bridge.)
The internal issue also causes some imagined practical problem for the agent. (In
Smoking Lesion, the lesion makes one more likely to get cancer. In Troll Bridge,
the inconsistency would make the troll blow up the bridge.)
There is a chain of reasoning which combines these facts to stop the agent from
taking the action. (In smoking lesion, EDT refuses to smoke due to the correlation
with cancer. In Troll Bridge, the proof-based agent refuses to cross the bridge
because of a Löbian proof that crossing the bridge leads to disaster.)
We intuitively ﬁnd the conclusion nonsensical. (It seems the EDT agent should
smoke; it seems the proof-based agent should not expect the bridge to explode.)
Indeed, the analogy to smoking lesion seems to strengthen the ﬁnal point -- that the
counterfactual reasoning is wrong.
I've come to think of Troll Bridge as "the real smoking lesion", since I'm generally not
satisﬁed with how smoking lesion is set up.
But is proof-based decision theory really a version of EDT? I think there's more to say
about the analogy, but a simple explanation is this: both EDT and proof-based decision
theory evaluate actions by adding them to the knowledge base and seeing what the
world looks like under that additional assumption. Or, to put it diﬀerently, proof-based
DT reasons about actions as if they're observations. That's the fundamental idea of
evidential decision theory.
Still, as good as the analogy might be, you still probably shouldn't be convinced until
you've seen the Troll Bridge defeat real EDT.
Probabilistic Version
For the purely logical version, I said that we can't fault the conclusion (because the
agent can't prove that it is safe to cross the bridge) - only the reasoning is being

critiqued. However, the probabilistic version better demonstrates the severity of the
reasoning error; it illustrates an EDT agent making what seems to be an outright
mistake.
It's reasonable to avoid crossing the bridge if you're only thinking about provable
results. But we might expect an agent who balances risks and rewards would decide to
cross, so long as it believes the probability of PA being inconsistent is low. Thus turns
out not to be the case.
Here's the setup.
Suppose that the agent uses a probability distribution which "respects logic" in the
sense that it assigns probability zero to anything which is logically refutable. (This
assumes logical omniscience. I'll drop that assumption later.)
The agent is as follows: if P(cross)=0, cross; if P(¬cross)=0, don't cross; else, take the
action with highest expected utility, breaking ties by not crossing. The idea is that we
are trying to take the highest-expected-utility action; however, we must handle cases
where this isn't well-deﬁned. Choosing to take an action when the probability of the
action is zero guarantees that the agent can never prove its own action; it is a version
of the chicken rule. Breaking ties by not crossing is arbitrary, but gives the simplest
version of the example, because this choice reduces the number of cases we need to
check.
The troll is as follows: if the agent crosses the bridge due to the P(cross)=0 clause,
blow up the bridge.
Reasoning in the logic of the agent:
Suppose □(A = cross →U = −10)
Suppose A = cross
Examining the agent, either crossing had higher expected utility, or
P(cross)=0. But we assumed □(A = cross →U = −10), so it must be
the latter. So the bridge gets blown up.
Therefore, A = cross →U = −10.
Therefore, □(A = cross →U = −10) →(A = cross →U = −10)
Thus, by Löb's theorem, we see that □(A = cross →U = −10).
Therefore, since the agent is certain of things which are provable, the agent assigns
expected value -10 to crossing. Since the expected value of the other action will always
be higher than this, and since we know the agent doesn't ever really enter into the P=0
conditions, the agent will choose not to cross. □
Notice that this reasoning did not depend much on the values 10, 0, and -10. The utility
of the bridge getting blown up could be -0.000001, and the agent still won't cross. It
isn't weighing the risk; it's decided that the worst outcome is inevitable. In the case of
proof-based agents, I said that the overall decision not to cross was understandable,

since proof-based agents are unable to weigh the risks. A probabilistic agent, however,
should intuitively be able to say "I don't know whether the bridge will get blown up,
because it involves reasoning about properties of myself which I'm fundamentally
uncertain about; but, the odds look decent." But that's not what happens: instead, it is
sure that crossing is unfavorable, no matter what overall probability it assigns to
P(A=cross)=0.
So, in this case we conclude that the Troll Bridge example results in a chicken-rule-
based agent taking the wrong action overall. The agent shouldn't be sure that it would
cross "for the right reason" (it should assign some probability to P(A=cross)=0, since it
can't know that its own logic is consistent). However, intuitively, it should be able to
assign some probability to this, and balance the risks. If the downside risk is
U=-0.000001, and the probability it assigns to its logic being consistent is not similarly
small, it should cross -- and in doing so, it would get +10.
As mentioned for the proof-based agent, the agent's code is a bit arbitrary, and it is
worth asking how important the details were. In particular, the default in the case of a
tie was to not cross. What if the default in case of a tie were to cross?
We then modify the troll's algorithm to blow up the bridge if and only if P(A=cross)=0
or there is a tie. The proof then goes through in the same way.
Perhaps you think that the problem with the above version is that I assumed logical
omniscience. It is unrealistic to suppose that agents have beliefs which perfectly
respect logic. (Un)Fortunately, the argument doesn't really depend on this; it only
requires that the agent respects proofs which it can see, and eventually sees the
Löbian proof referenced.
Random Exploration
The frustrating thing about Troll Bridge is that it seems like the agent could just cross
the bridge, and things would be ﬁne. The proof that things wouldn't be ﬁne relies on
the fact that the agent accepts that very proof as suﬃcient reason; so can't we just
ignore that kind of proof somehow?
One thing you might try is to consider a learning agent, and force random exploration
so the agent just crosses the bridge sometimes. If the agent crosses the bridge, it
should be able to see that it's safe, right?
However, we have to ask: what's the appropriate version of Troll Bridge for the
exploring agent? Remember I said that the basic idea of Troll Bridge is that the troll
blows up the bridge if the agent crosses "for a dumb reason" -- ie, for a reason other
than "crossing is a good idea". Random exploration falls into this category.
Furthermore, random exploration is really closely analogous to the chicken rule.
Consider:
(A) The ﬁrst chicken rule I mentioned: if you prove that you don't take an action,
you immediately take it.
(B) The second chicken rule mentioned: if P(A)=0, then do A.
(C) Epsilon-chicken: if P(A)<epsilon, then do A.
(D) Epsilon-exploration: randomly do A with probability epsilon.

(A) is very similar to (B) if the probability distribution respects proofs in the way I
mentioned previously; if the probability distribution has the further property of
assigning positive probability to anything not disproven, then they're exactly the same.
(C) is a natural generalization of (B). (B) tells us what to do when the EDT decision rule
isn't well-deﬁned (we can't condition on probability zero events). (C) generalizes this by
treating small-probability events in the same way (we might expect our conditional
beliefs to be bad for small-probability events, since we presumably don't have much
experience with them; so we treat it as if it's the undeﬁned case). (B) ensures there are
no zero-probability actions, which means we don't actually ever have to deal with the
EDT decision rule being non-well-deﬁned; similarly, (C) ensures no action is almost
surely never taken, which helps ensure that our conditional expectations for all actions
are empirically grounded.
(C) and (D) are very similar; (C) just supplies pseudorandomness vie the agent's own
probabilities. Whenever the agent is too conﬁdent about what it does, it does
something else. This makes the randomization unpredictable too the agent. (D)
supplies randomness from an external source instead.
So we have a chain connecting the chicken rule to random exploration.
(Another way of thinking about the similarity: The chicken rule ensures that there are
models of the agent's beliefs in which each action is taken, so that the agent can have
sensible logical reasoning about each alternative. Epsilon-exploration ensures that the
empirical experiences of the agent include all of the actions, so that the agent can
have reasonable empirical estimates of consequences of each action.)
OK. So Troll Bridge for an exploring agent should blow up the bridge on exploration, or
if PA is inconsistent. An agent could look like this:
With small probability, explore.
If exploring, ﬂip a coin to cross or not cross.
If P(cross)=0, cross.
If P(¬cross)=0, don't cross.
Otherwise, take the max-expected-value action, breaking ties randomly.
I'm still assuming that the agent's probability distribution respects proofs, as before.
I'm also assuming this agent is playing the game repeatedly, and learning. I also must
assume that the agent has found Now, the agent reasons:
Suppose □(cross → u=-10) for a particular round.
Further suppose I crossed on that round.
By the ﬁrst supposition, I knew the payout of crossing to be low; and I
must also have known that the payout of not crossing is higher, since I
can prove that. Since I can prove what both payouts are, the expected
values must equal those, unless PA is inconsistent (in which case
P(cross)=0 anyway, since my beliefs respect proofs). So I can only be
crossing the bridge for two reasons -- either this is an exploration
round, or P(cross)=0.
In either case, crossing the bridge yields payout u=-10.
Therefore, cross → u=-10 in fact.
So □(cross → u=-10) → (cross → u=-10).

Since the agent proves that a proof of crossing being bad implies crossing is actually
bad, the agent further must prove that crossing is bad in fact, by Löb.
I did this for the logically omniscient case again, but as before, I claim that you can
translate the above proof to work in the case that the agent's beliefs respect proofs it
can ﬁnd. That's maybe a bit weird, though, because it involves a Bayesian agent
updating on logical proofs; we know this isn't a particularly good way of handling
logical uncertainty.
We can use logical induction instead, using an epsilon-exploring version of LIDT. We
consider LIDT on a sequence of troll-bridge problems, and show that it eventually
notices the Löbian proof and starts refusing to cross. This is even more frustrating than
the previous examples, because LIDT might successfully cross for a long time,
apparently learning that crossing is safe, and reliably gets +10 payoﬀ. Then, one day, it
ﬁnds the Löbian proof and stops crossing the bridge!
That case is a little more complicated to work out than the Bayesian probability case,
and I omit the proof here.
Non-examples: RL
On the other hand, consider an agent which uses random exploration but doesn't do
any logical reasoning, like a typical RL agent. Such an agent doesn't need any chicken
rule, since it doesn't care about proofs of what it'll do. It still needs to explore, though.
So the troll can blow up the bridge whenever the RL agent crosses due to exploration.
This obviously messes with the RL agent's ability to learn to cross the bridge. The RL
agent might never learn to cross, since every time it tries it, it looks bad. So this is sort
of similar to Troll Bridge.
However, I think this isn't really the point of Troll Bridge. The key diﬀerence is this: the
RL agent can get past the bridge if its prior expectation that crossing is a good idea is
high enough. It just starts out crossing, and happily crosses all the time.
Troll Bridge is about the inevitable conﬁdence that crossing the bridge is bad. We would
be ﬁne if an agent decided not to cross because it assigned high probability to PA being
inconsistent. The RL example seems similar in that it depends on the agent's prior.
We could try to alter the example to get that kind of inevitability. Maybe we argue it's
still "dumb" to cross only because you start with a high prior probability of it being
good. Have the troll punish crossing unless the crossing is justiﬁed by an empirical
history of crossing being good. Then RL agents do poorly no matter what -- no one can
get the good outcome in order to build up the history, since getting the good outcome
requires the history.
But this still doesn't seem so interesting. You're just messing with these agents. It isn't
illustrating the degree of pathological reasoning which the Löbian proof illustrates -- of
course you don't put your hand in the ﬁre if you get burned every single time you try
it. There's nothing wrong with the way the RL agent is reacting!
So, Troll Bridge seems to be more exclusively about agents who do reason logically.
Conclusions

All of the examples have depended on a version of the chicken rule. This leaves us with
a fascinating catch-22:
We need the chicken rule to avoid spurious proofs. As a reminder: spurious proofs
are cases where an agent would reject an action if it could prove that it would not
take that action. These actions can then be rejected by an application of Löb's
theorem. The chicken rule avoids this problem by ensuring that agents cannot
know their own actions, since if they did then they'd take a diﬀerent action from
the one they know they'll take (and they know this, conditional on their logic
being consistent).
However, Troll Bridge shows that the chicken rule can lead to another kind of
problematic Löbian proof.
So, we might take Troll Bridge to show that the chicken rule does not achieve its goal,
and therefore reject the chicken rule. However, this conclusion is very severe. We
cannot simply drop the chicken rule and open the gates to the (much more common!)
spurious proofs. We would need an altogether diﬀerent way of rejecting the spurious
proofs; perhaps a full account of logical counterfactuals.
Furthermore, it is possible to come up with variants of Troll Bridge which counter some
such proposals. In particular, Troll Bridge was originally invented to counter proof-
length counterfactuals, which essentially generalize chicken rules, and therefore lead
to the same Troll Bridge problems).
Another possible conclusion could be that Troll Bridge is simply too hard, and we need
to accept that agents will be vulnerable to this kind of reasoning.

Inspection Paradox as a Driver of
Group Separation
Have you ever noticed that anybody driving slower than you is an idiot, and
anyone going faster than you is a maniac? -- George Carlin
The Inspection Paradox, where the reported results are heavily observer-dependent,
has been mentioned here a couple of times before:
https://www.lesswrong.com/posts/tqEAiScpz7g2xLHtE/the-inspection-paradox-is-
everywhere
https://www.lesswrong.com/posts/HW2fbbGM8B6y7pkDb/the-just-world-
hypothesis#8r2uaA6Mb25k2vF9Z
An example of it that is familiar to everyone is that, when driving with an average
speed, you see all other cars separating into two categories: slow drivers and fast
drivers, because you naturally encounter more cars who are faster or slower than you
are, and none that move at the same speed. So a normal distribution of speeds
becomes bimodal, from something like this:
to something like this:
Another familiar example, from the newer post by the original author, is that for an
average Facebook user,
Your friends have more Facebook friends than you do.
Because, naturally, members with more connections are more likely to have a
connection with you, among others.
(For the record, the inspection paradox can be classiﬁed as subset of the sampling
bias, which, in turn, is a form of the oft discussed selection bias.)
But back to the apparent multi-modality. You encounter is whenever unusual events
have a higher availability, sometimes properly measured like in the average driver
case above, and sometimes perceived, like in the availability heuristic. This doesn't

have to be about the frequency of the observations, it could be about their emotional
impact. If the more "out there" the observations are, the more they aﬀect you, then
their distribution would appear bimodal, and you might instinctively recoil from them,
and seek the comfort of the in-group. In this case the inspection-induced multi-
modality may turn into an actual one, a case of perception becoming reality. (I had
attempted to model it numerically some months ago, in two companion posts, sadly
not written well enough to attract much interest, but showing that the eﬀect described
may well be real.)

Subagents, trauma and rationality
[Content note: discussion of trauma, child and sexual abuse, sexual
violence, lack of self-worth, dissociation, PTSD, ﬂashbacks, DID, personality
disorders; some mildly graphic examples of abuse and trauma mentioned in
text form]
I have spent over two years doing emotional support for people who had survived
long-term childhood trauma, and in these cases spawning agents to deal with
unbearable suﬀering while having no escape from it is basically a standard
reaction that the brain/mind takes. The relevant psychiatric diagnosis is DID
(formerly MPD, multiple personality disorder). In these cases the multiple agents
often manifest very clearly and distinctly. It is tempting to write it oﬀ as a special
case that does not apply in the mainstream, yet I have seen more than once the
progression from someone suﬀering from CPTSD to a full-blown DID. The last thing
that happens is that the person recognizes that they "switch" between
personalities. Often way later than when others notice it, if they know what to look
for. After gaining some experience chatting with those who survived severe
prolonged trauma, I started recognizing subtler signs of "switching" in myself and
others. This switching between agents (I would not call them sub-agents, as they
are not necessarily less than the "main", and diﬀerent "mains" often take over
during diﬀerent parts of the person's life) while a normal way to operate, as far as
I can tell, almost never rises to the level of conscious awareness, as the brain
carefully constructs the lie of single identity for as long as it can.
-- shminux
As the above comment suggests, the appearance of something like distinct subagents
is particularly noticeable in people with heavy trauma, DID being the most extreme
example.
This post will interpret the appearance of subagents as emerging from unintegrated
memory networks, and argue that - as shminux suggests - the presence of these is a
matter of degree. There's a continuous progression of fragmented (dissociated)
memory networks giving arise to increasingly worse symptoms as the degree of
fragmentation grows. The continuum goes from everyday procrastination and akrasia
on the "normal" end, to disrupted and dysfunctional beliefs on the middle, and
conditions like clinical PTSD, borderline personality disorder, and dissociative identity
disorder on the severely traumatized end.
I will also argue that emotional work and exploring one's past traumas in order to heal
them, is necessary for eﬀective instrumental and epistemic rationality.
This post is largely based on what I understand to be relatively standard trauma
theory (e.g. van der Kolk, 2014; Shapiro, 2017; Baldwin, 2013; Schauer & Elbert, 2010;
Forgash & Copeley, 2007) and should not contain any particularly novel or original
claims, except maybe for drawing some connections to topics and framings which I
have been discussing previously in my sequence.

Emotional regulation as an approach-
avoid tradeoﬀ
In Building up to an Internal Family Systems model, I talked about "protector"
subagents (subdivided into managers and ﬁreﬁghters), whose purpose was to keep
negative emotions and memories ("exile" subagents) out of consciousness. If they
predicted that entering some situation would trigger a negative memory, then they
would try to prevent the person from going to that situation, because a situation
triggering a negative memory correlates with such situations being dangerous. Thus,
my explanation suggested that the only purpose of protectors was to keep a person
out of concrete danger.
However, something like protectors is also a necessary component for emotional
regulation. There are a number of diﬃcult tradeoﬀs implied by the following facts:
In detecting possible threats, it is usually better to err on the side of too many
false positives. Mistaking a tree branch for a snake is less costly than mistaking
a snake for a tree branch.
At the same time, false positives which trigger automatic ﬁght-or-ﬂight-or-freeze
responses do also have a cost; once an alarm has been shown to be false, there
need to be mechanisms for winding it down.
Sometimes a situation is dangerous and unpleasant and costly, and going into it
is what you need to do anyway. There has to be a mechanism for overriding the
alarms when it is necessary.
A special case of this is when trying to ﬂee a dangerous situation is by itself
dangerous. If you're wounded and there's an enemy nearby, you may have
better odds trying to play dead than running away. You may also be someone's
slave with poor chances of escaping. For those cases, you need a mechanism
speciﬁcally for shutting down the ﬁght-or-ﬂight response.
At the same time, an organism which had too easy of a time overriding its
alarms would not take them suﬃciently seriously, and would be killed or
otherwise hurt by constantly going into dangerous situations and not trying to
escape them.
In humans as well as other mammals, brain areas controlling evolutionarily ancient
defense state responses become active when danger is detected. While the "higher"
cognitive functions of the frontal cortex are to some extent capable of regulating
these emotional responses when they are mild, the emotional brain can and will
override the frontal cortex in dangerous situations. In situations of suﬃcient distress,
rational thinking and the ability to regulate emotional responses shut down entirely.
Furthermore, sensory inputs are constantly scanned by the amygdala for patterns
which have been associated with danger in the past. The time it takes for the
amygdala to process inputs is shorter than the time it takes for them to reach
consciousness, and the amygdala may trigger an emotional response before higher
cognitive systems even become aware of the situation.
Thus, if the emotional brain detects something it considers threatening enough, it will
react, regardless of whether or not the cognitive brain considers it a good idea. If
particular cues tend to co-occur with particularly serious danger states reliably
enough, then the brain will build up an associative network where detecting any of
those cues will automatically trigger threat responses, with no opportunity for

cognitive overrides. This can become a serious problem for normal functioning, as any
of the cues in the fear network may trigger an emergency response, even in
completely harmless situations.
Example trauma network (Schauer & Elbert 2010).
Many of the elements of the pictured trauma network - neighbors, motorbikes, the
smell of alcohol - are ones that would ordinarily have plenty of connections to other
concepts in daily life. In order to prevent extreme responses, the associative network
related to the trauma needs to be compartmentalized and isolated from the rest of a
person's memory networks, or risk anything activating the network and triggering an
emergency response.
Thus, the role of protectors is not just to avoid situations which would be actively
dangerous: they also need to manipulate the contents of consciousness so as to avoid
triggering the trauma network otherwise.
Even if someone's life circumstances have changed, and the original situation which
created the trauma is no longer an active issue (such as if someone is an adult and
has moved away from their abusive parents), the trauma network may remain too
strongly charged to allow its contents to be reprocessed. Reprocessing would require
the use of higher cognitive functions to put the experiences in a new context, but any
activation of the network will trigger an immediate emergency response, shutting
down those very cognitive functions. In such a situation, all that protectors can do is
to try to bury the network and suppress all memories related to it. To rephrase that in
less intentional language, the brain's Turing machine may come to learn that
suppressing particular memories produces beneﬁcial results, reinforcing the behavior.
Memory loss has been reported in people who have experienced natural disasters,
accidents, war trauma, kidnapping, torture, concentration camps, and physical

and sexual abuse. Total memory loss is most common in childhood sexual abuse,
with incidence ranging from 19 percent to 38 percent. This issue is not particularly
controversial: As early as 1980 the DSM-III recognized the existence of memory
loss for traumatic events in the diagnostic criteria for dissociative amnesia: "an
inability to recall important personal information, usually of a traumatic or
stressful nature, that is too extensive to be explained by normal forgetfulness."
Memory loss has been part of the criteria for PTSD since that diagnosis was ﬁrst
introduced.
One of the most interesting studies of repressed memory was conducted by Dr.
Linda Meyer Williams, which began when she was a graduate student in sociology
at the University of Pennsylvania in the early 1970s. Williams interviewed 206 girls
between the ages of ten and twelve who had been admitted to a hospital
emergency room following sexual abuse. Their laboratory tests, as well as the
interviews with the children and their parents, were kept in the hospital's medical
records. Seventeen years later Williams was able to track down 136 of the
children, now adults, with whom she conducted extensive follow-up interviews.
More than a third of the women (38 percent) did not recall the abuse that was
documented in their medical records, while only ﬁfteen women (12 percent) said
that they had never been abused as children. More than two-thirds (68 percent)
reported other incidents of childhood sexual abuse. Women who were younger at
the time of the incident and those who were molested by someone they knew
were more likely to have forgotten their abuse. (van der Kolk, 2014)
However, as I will soon discuss, a downside of this process is that the more that
associative networks get disconnected from each other, the less consistent a person's
responses to diﬀerent situations become. If a person is drawing on diﬀerent memories
in diﬀerent situations, then in some situations they may seem like an entirely diﬀerent
person than in others. Another way of framing is that, if we deﬁne a subagent as
decision-making entity that has access to its own set of beliefs and goals, then
diﬀerent subagents are in control at diﬀerent times.
A few words on the "memory wars"
This post discusses suppressing traumatic memories, drawing on the theories of
clinical practitioners, who have disagreements with clinical researchers about whether
memory suppression is a thing (Patihis, Ho, Tingen, Lilienfeld, & Loftus, 2014).
Much of the criticism about repressed memories is aimed at a speciﬁc concept from
Freudian theory, and/or on the question of how reliable therapeutically recovered
memories are. Several of the critics (e.g. (Rofé, 2008)) acknowledge that people may
suppress or intentionally forget painful memories, but argue that this is distinct from
the Freudian concept of repression. However, memory suppression in the sense
discussed in this post is not related to the Freudian concept, and also includes
intentional attempts to forget or avoid thinking about something, as the examples will
hopefully demonstrate.
In fact, the memories being hard to forget is exactly the problem, which is something
that many critics of the standard Freudian paradigm are keen to point out - traumatic
memories are often particularly powerful and long-lasting.
I do make the assumption that conscious attempts to forget something may
eventually become suﬃciently automated so as to become impossible for the person

themselves to notice; but this seems like a straightforward inference from the
observation that skills and habits in general can become automated enough so as to
happen without the person realizing what they are doing. A recent experiment
(unreplicated, but I have a reasonably high prior for cognitive psychology experiments
replicating) also showed that once people are trained to intentionally forget words
that are associated with a particular cue, the cue will reduce recall of words even
when it is paired with them in a form that is too short to consciously register (Salvador
et al. 2018).
I make no strong claims about the reliability of memories recovered in therapy. It has
been clearly demonstrated that it is possible for therapists to accidentally or
intentionally implant false memories, but there have also been cases of people
recovering memories which have then been conﬁrmed from other sources. Probably
some recovered memories are genuine (though possibly distorted) and some are not.
Mild disconnection: unintegrated
considerations and ugh ﬁelds
In my previous post, I mentioned the hypothesis (Shadlen & Shohamy, 2016) that in
choosing between several options, we sample memories related to our past
experiences with those options. This sampling may draw from several memory
networks, located in physically distinct areas in the brain. I also mentioned how
particularly negative memories or consideration may draw one's attention, so as to
make it hard to integrate those concerns with the concerns in other networks. And in
Integrating disagreeing subagents, I talked about how akrasia might be interpreted in
terms of conﬂicting and unintegrated beliefs pulling in opposite directions.
In the very mildest form, there isn't necessarily any trauma at all: just two diﬀerent
associative networks pointing in opposite directions, with concerns that have not been
integrated. This might cause diﬀerent kinds of behavior in diﬀerent situations,
depending on which network happens to become activated. However, once the
discrepancy is detected, integration may happen automatically or assisted in a
relatively straightforward fashion with techniques such as IDC.
The next level is when a concern is not serious, but still feels somewhat unpleasant to
think about - the territory of everyday ugh ﬁelds:
For example, suppose that you started oﬀ in life with a wandering mind and were
punished a few times for failing to respond to oﬃcial letters. Your TDL algorithm
began to propagate the pain back to the moment you looked at an oﬃcial letter or
bill. As a result, you would be less eﬀective than average at responding, so you
got punished a few more times. Henceforth, when you received a bill, you got the
pain before you even opened it, and it laid unpaid on the mantelpiece until a Big
Bad Red late payment notice with an $25 ﬁne arrived. More negative conditioning.
Now even thinking about a bill, form or letter invokes the ﬂinch response, and
your lizard brain has fully cut you out out. You ﬁnd yourself spending time on
internet time-wasters, comfort food, TV, computer games, etc. Your life may not
obviously be a disaster, but this is only because you can't see the alternative
paths that it could have taken if you had been able to take advantage of the
opportunities that came as letters and forms with deadlines.

The subtlety with the Ugh Field is that the ﬂinch occurs before you start to
consciously think about how to deal with the Unhappy Thing, meaning that you
never deal with it, and you don't even have the option of dealing with it in the
normal run of things.
As the post notes, your brain is automatically trying to avoid thinking about things
which would trigger the "ugh". (I've certainly noticed in myself a tendency to just
conveniently forget about lots of mildly unpleasant things I should get around doing.)
Plans which would involve engaging the ugh are ranked low in your preference
ordering so are never even generated as options. Except that some of your networks
do know that you need to engage with the ugh eventually, so they keep annoyingly
reminding you about it whenever something happens to activate them.
Assuming that the ughs are something that you do need to deal with, that is. They
could also be something else, such as past upsets and embarrassments that you
would prefer to forget, and then keep pushing away until nothing reminds you of them
anymore. In that case, the memory network might be successfully locked away and
compartmentalized - but still exerting a subtle inﬂuence in the form of the now-
automated mental motions aimed at making sure that it remains hidden. Those same
motions would also prevent the beliefs associated with it from coming up and being
re-evaluated.
A relevant question here is: if a negative memory is suppressed from consciousness,
to what extent does it inﬂuence decision-making? After all, one might think that if it
remains suppressed, then it should not have any eﬀect. But it seems likely that even if
the memory itself does not inﬂuence decision-making, the hoops that the brain has
learned to jump through to keep it suppressed do aﬀect one's decisions. (Under a
subagent framing, those hoops in question would be thought of as a protector.)
To take a minor example not involving memory suppression, for some reason I did not
learn to tie my shoelaces as a child. Instead I just used shoes which did not have
laces. After a while, it became kind of embarrassing to not know how to do that, so I
developed an identity and preference for using shoes without laces. This lasted well
into adulthood, despite the fact it would have been pretty trivial for me to just ﬁnally
learn how to do it. Yet the thought of asking someone to teach me would have felt
embarrassing, so my brain continued using the structures it had developed for
avoiding shoelaces. Even if I had somehow suppressed the knowledge of these
structures existing because of my embarrassment - something of which I was on some
level aware all the time - I very much doubt that that alone would have changed the
structures, or reduced my resistance to using shoelaces. It might plausibly even have
made things worse, as I would no longer even realize that my identity was a
justiﬁcation constructed to guard against embarrassment. (Eventually I looked up a
YouTube video called something like "easiest way of teaching your child to tie their
shoelaces" and ﬁgured it out.)
Eliezer says that "If you once tell a lie, the truth is ever after your enemy" -
maintaining one incorrect justiﬁcation for your behavior may require your brain to
contort itself to quite a lot of weird shapes. We all probably know examples of people
who seem reasonable and rational on most issues, but then on some they are oddly
forceful about their positions, or otherwise do not quite seem to be thinking clearly.
(But that's just other people being irrational, of course. We would never do such a
thing.)

A "big T" trauma (Criterion A event necessary to diagnose PTSD), such as rape,
sexual molestation, or combat experience, clearly has an impact on its victims in
terms of how they behave, think, and feel about themselves, and in their
susceptibility to pronounced symptoms, such as nightmares, ﬂashbacks, and
intrusive thoughts. These victims will have self-attributions such as "I'm
powerless," "I'm worthless," or "I'm not in control." Of course, clients who have
not experienced such traumas may also have dominant negative self-attributions,
such as "I'm worthless," "I'm powerless," or "I'm going to be abandoned." Many of
these clients seem to have derived their negative self-statements from early
childhood experiences. [...] Like "big T" trauma victims, they see the event, feel it,
and are profoundly aﬀected by it.
Such clients were not, of course, blown up in a mineﬁeld or molested by a parent.
Nevertheless, a memory of something that was said or that happened to them is
locked in their brain and seems to have an eﬀect similar to that of a traumatic
experience. In fact, by dictionary deﬁnition, any event that has had a lasting
negative eﬀect on the self or psyche is by its nature "traumatic." Consequently,
these ubiquitous adverse life events have long been referred to in EMDR practice
as "small t" traumas to keep in mind the nature of their impact [...]. A wide range
of adverse life experiences can be the basis of pathology, because of their
emotional impact. For instance, while being humiliated in grade school cannot be
designated a "trauma" for the diagnosis of PTSD, on an emotional level, such an
event can be considered the evolutionary equivalent of being cut out of the herd.
The impact can be aﬀectively devastating, with long-lasting eﬀects. (Shapiro,
2017)
Moderate disconnection: unintegrated
core beliefs
Sometimes a belief network is in some sense too central to just be pushed away: life
circumstances force it to be active despite being negatively laden, but that negativity
also prevents it from being integrated with other networks. Beliefs about ourselves are
a particularly common candidate for this category.
Imagine that a little girl is walking beside her father and reaches up for his hand.
At that moment the father deliberately or inadvertently swings his arm back and
hits the child in the face. The child experiences intense negative aﬀect, which
might be verbalized as "I can't get what I want; there is something wrong with
me." [...] The aﬀect, perhaps intense feelings of worthlessness and powerlessness,
and the images, sounds, and the pain of the blow are stored in the child's nervous
system. This experience becomes a touchstone, a primary self-deﬁning event in
her life; in the Adaptive Information Processing model we call it a node. [...] the
next event that represents a similar rejection is likely to link up with the node in
the ongoing creation of a neuro network that will be pivotal to the girl's deﬁnition
of her self-worth. Subsequent experiences of rejection by mother, siblings, friends,
and others may all link up with the node in channels of associated information.
Even before language is adequately developed, all the diﬀerent childhood
experiences containing similar feelings of powerlessness, despair, and inadequacy
are stored as information linking into a memory network organized around the
node of the earlier touchstone experience. Positive experiences are not
assimilated into the network because the node is deﬁned by the negative aﬀect.

When there is suﬃcient language to formulate a self-concept, such as "I can't get
what I want; there is something wrong with me," verbalization is linked
associatively with the network by the aﬀect that the meaning of those words
engenders. In essence, once the aﬀect-laden verbal conceptualization is
established in the neuro network, it can be viewed as generalizing to each of the
subsequent experiences stored as information in the network. The process
continues in adolescence, such as when, for instance, the girl in our example
experiences a rejection by a teacher or a boyfriend. Thus, all subsequent related
events may link to the same node point and take on the attributions of the initial
experience. Therefore, the assessment associated with such an event is not
limited to a function-speciﬁc statement (e.g., "I can't get what I want in this
instance"), but is linked to the dysfunctional generalized statement "I can't get
what I want; there is something wrong with me."
What happens when the girl reaches adulthood and something happens that
seems like—or even threatens to become—a rejection? This new information is
assimilated into the neuro network, and the concept "I can't get what I want;
there is something wrong with me" and its aﬀect generalize and become
associated with it. Over time, the accumulated related events produce a self-
fulﬁlling prophecy; thus, any hint or chance of rejection can trigger the neuro
network with its dominant cognition of "There is something wrong with me." This
person's consequent behavior and attributions in the present are dysfunctional
because what motivates and fuels them is the intense aﬀect, fear, pain, and
powerlessness of that ﬁrst experience, now compounded by all of the subsequent
experiences. (Shapiro, 2017)
Despite the prevalence of the negative aﬀect, there is still a strong need to push it
away, in order to be able to function normally and experience positive feelings. This
leads to something like unstable ﬂuctuation of at least two distinct memory networks
taking turns being active. One is loaded with all the negative examples, while another
might contain all the positive ones which have not been successfully integrated into
the negative ones.
Also, the person in question might understand on an intellectual level that there is
nothing actually wrong with them, that this is a negative cognition created by trauma,
and so on... but this belief also resides in its own network, separate from the one
which is causing the negative experience.
Dissociation
The examples so far have largely assumed that one is capable of somehow - in
principle at least - avoiding the unpleasant situation or trigger. But what if one is not?
While "ﬁght or ﬂight" are the most commonly known defensive reactions, there are
actually several defensive states. Exactly how many and how they should be classiﬁed
is somewhat disputed, but one model has the following in increasing order of threat
imminence: freeze-alert (stopping still and paying attention when noticing something
potentially threatening), ﬁght, ﬂight, freeze-fright (if the ﬁght and ﬂight options are
unviable), and collapse (feigning death):
A simple thought experiment illustrates these ﬁve defensive options. Imagine that
you surprise a large bear while alone in the wilderness. Your immediate stillness is
the freeze-alert state. If the bear moves oﬀ, you can return home with an exciting

story for your family. If the bear approaches, your danger deepens. Neither ﬂight
nor ﬁght oﬀers a viable option in this and many other cases of extreme threat,
where active defenses increase the risk of death. The best option here is freeze-
fright, although your chances are slim unless a hunter is nearby. Finally, when the
bear has you in its mouth, you are out of options. You go limp in a state of collapse
[...] Collapse reduces the likelihood of continued violence, while preparing the
individual for injury or death (release of endogenous opioids decreases pain; [...]).
Immobility is the most eﬀective response during attack because quiescence
eliminates auditory and visual cues that elicit or maintain aggression. All of these
defense states survive in us from our evolutionary past because each has
enhanced the odds of survival. (Baldwin 2013)
"Dissociation" is a broad term with several meanings; in reference to states of
consciousness in particular, it refers to states such as ones in which a person feels
detached from the world, up to the point of their experiences feeling unreal, them
being unable to see or hear anything, or feeling like they are someone else and
watching events which are happening to some stranger.
Schauer & Elbert (2010) conceptualize dissociative states of consciousness as ones
which are related to the freeze and collapse stages of the defensive progression. In
global workspace terms, one might frame it as a workspace state which helps - as a
part of a wider physiological shutdown - prevent the kinds of more active defensive
responses which would put the person in more danger. Many rape and abuse victims
describe having had dissociative episodes during their experience.
The milder, non-pathological forms of dissociative states include e.g. daydreaming,
which may help keep the person still in situations such as boring lectures with
obligatory attendance, suppressing desires to move elsewhere. The more serious
forms may kick in during e.g. repeated abuse a person does not have a chance to
escape from, and to protect against memories of such incidents. (Schauer & Elbert
also hypothesize that forms of self-harm, such as cutting, may act as ways to self-
regulate by escalating one's defensive state to one of shutdown, calming down
stressful memories and responses.)
If someone is forced to live with their abuser, dissociation may help them remain
functional in the abuser's presence rather than trying to uselessly ﬂee; as dissociative
states frequently involve memory deﬁcits, this may further lead to fragmented
memory networks.
Extreme disconnection: PTSD,
personality disorders, DID
An extreme case of incomplete memory suppression is PTSD, where the trauma
network may be so intense as to completely overwhelm the person whenever it is
activated. As a result, cognitive analysis may shut down whenever the network is
triggered. The person may become completely ﬂooded with the memory, to the point
of reliving it as if they were experiencing the event again.
Because the overwhelm is so strong, they may afterwards have little memory of
what even happened - the cognitive shutdown may suppress the ability to form
new memories of the event or to express it in language, leaving the experience
completely in a world of its own.

The overwhelming experience is split oﬀ and fragmented, so that the emotions,
sounds, images, thoughts, and physical sensations related to the trauma take on a
life of their own. The sensory fragments of memory intrude into the present,
where they are literally relived. As long as the trauma is not resolved, the stress
hormones that the body secretes to protect itself keep circulating, and the
defensive movements and emotional responses keep getting replayed. [...]
... many people may not be aware of the connection between their "crazy"
feelings and reactions and the traumatic events that are being replayed. They
have no idea why they respond to some minor irritation as if they were about to
be annihilated. Flashbacks and reliving are in some ways worse that the trauma
itself. A traumatic event has a beginning and an end—at some point it is over. But
for people with PTSD a ﬂashback can occur at any time, whether they are awake
or asleep. There is no way of knowing when it's going to occur again or how long it
will last. People who suﬀer from ﬂashbacks often organize their lives around trying
to protect against them. They may compulsively go to the gym to pump iron (but
ﬁnding that they are never strong enough), numb themselves with drugs, or try to
cultivate an illusory sense of control in highly dangerous situations (like
motorcycle racing, bungee jumping, or working as an ambulance driver).
Constantly ﬁghting unseen dangers is exhausting and leaves them fatigued,
depressed, and weary. (van der Kolk 2014)
There is debate about whether or not DID is a real phenomenon. I do not have the
expertise to have a strong opinion on this, but to the extent that it is, it could be
considered a more extreme version of the dissociative responses described in the
previous sections. Forced to live for an extended time in extreme circumstances with
contradictory demands - such as being required to be happy and obedient while also
being the subject of regular extreme abuse - a child may develop extreme amnesiac
walls between diﬀerent memory networks, to the point of having an entirely diﬀerent
personality depending on which network happens to be active. This allows for feelings
of fear or panic that would otherwise arise in the company of an abusive parent, to be
kept away as the memories associated with the abuse cannot be accessed.
Several clinicians also consider borderline personality disorder to be a PTSD-like
response to trauma, with symptoms such as extreme fears of abandonment,
alternation between idealization and devaluation, and poor emotional regulation being
caused by the kinds of mechanisms that I have been describing.
In cases of suﬃcient trauma, an extreme form of dissociation seems to happen, where
protectors extensively shut down access to bodily sensations and emotional
awareness, turning oﬀ any systems which might cause emotional reactions that would
be too strong for the system to handle:
While Sherry dutifully came to every appointment and answered my questions
with great sincerity, I did not feel we were making the sort of vital connection that
is necessary for therapy to work. Struck by how frozen and uptight she was, I
suggested that she see Liz, a massage therapist I had worked with previously.
During their ﬁrst meeting Liz positioned Sherry on the massage table, then moved
to the end of the table and gently held Sherry's feet. Lying there with her eyes
closed, Sherry suddenly yelled in a panic: "Where are you?" Somehow Sherry had
lost track of Liz, even though Liz was right there, with her hands on Sherry's feet.
Sherry was one of the ﬁrst patients who taught me about the extreme
disconnection from the body that so many people with histories of trauma and

neglect experience. [...] Once I was alerted to this, I was amazed to discover how
many of my patients told me they could not feel whole areas of their bodies.
Sometimes I'd ask them to close their eyes and tell me what I had put into their
outstretched hands. Whether it was a car key, a quarter, or a can opener, they
often could not even guess what they were holding—their sensory perceptions
simply weren't working. [...]
In response to the trauma itself, and in coping with the dread that persisted long
afterward, these patients had learned to shut down the brain areas that transmit
the visceral feelings and emotions that accompany and deﬁne terror. Yet in
everyday life, those same brain areas are responsible for registering the entire
range of emotions and sensations that form the foundation of our self-awareness,
our sense of who we are. What we witnessed here was a tragic adaptation: In an
eﬀort to shut oﬀ terrifying sensations, they also deadened their capacity to feel
fully alive. (van der Kolk, 2014)
Takeaway: emotional healing as a
prerequisite for rationality
In this post, I have covered ways in which painful experiences - anything from "big-T
Trauma" to mildly unpleasant thoughts - seem to shape our thinking. Everyone has a
built-in desire to avoid painful thoughts and experiences, which reinforces cognitive
patterns aimed at keeping those kinds of thoughts hidden and buried. Often this is
functional, as keeping them suppressed allows us to remain more functional in
situations where it would not be useful for old and non-relevant memories to come up,
causing fear and avoidance responses when we need to be doing something else.
At the same, these kinds of processes control that which we can think; and as they
become automated, they nudge our reasoning to take weird contortions, keeping our
belief networks fragmented and our behavior less than coherent, operating in ways
that keep themselves hidden. They also limit us with regard to instrumental
rationality, as options which we could otherwise have taken - anything from wearing
particular kinds of shoes to taking up new careers which challenge our chosen
identities - are judged as categorically unacceptable.
To actually change your mind, you need to be able to dig up and address your past
traumas. Fortunately, there are various tools available for that purpose.
References
Baldwin, D. V. (2013). Primitive mechanisms of trauma response: an evolutionary
perspective on trauma-related disorders. Neuroscience and Biobehavioral Reviews,
37(8), 1549-1566.
Forgash, C., & Copeley, M. (Eds.). (2007). Healing the Heart of Trauma and
Dissociation with EMDR and Ego State Therapy (1 edition). Springer Publishing
Company.
Patihis, L., Ho, L. Y., Tingen, I. W., Lilienfeld, S. O., & Loftus, E. F. (2014). Are the
"Memory Wars" Over? A Scientist-Practitioner Gap in Beliefs About Repressed Memory.

Psychological Science, 25(2), 519-530.
Rofé, Y. (2008). Does Repression Exist? Memory, Pathogenic, Unconscious and Clinical
Evidence. Review of General Psychology: Journal of Division 1, of the American
Psychological Association, 12(1), 63-85.
Salvador, A., Berkovitch, L., Vinckier, F., Cohen, L., Naccache, L., Dehaene, S., &
Gaillard, R. (2018). Unconscious memory suppression. Cognition, 180, 191-199.
Schauer, M., & Elbert, T. (2010). Dissociation Following Traumatic Stress. Swiss Journal
of Psychology: Oﬃcial Publication of the Swiss Psychological Society Schweizerische
Zeitschrift Fur Psychologie = Revue Suisse de Psychologie, 218(2), 109-127.
Shadlen, M. N., & Shohamy, D. (2016). Decision Making and Sequential Sampling from
Memory. Neuron, 90(5), 927-939.
Shapiro, F. (2017). Eye Movement Desensitization and Reprocessing (EMDR) Therapy,
Third Edition: Basic Principles, Protocols, and Procedures (Third edition). The Guilford
Press.
van der Kolk, B. (2014). The Body Keeps the Score: Brain, Mind, and Body in the
Healing of Trauma (1 edition). Viking.

Algorithmic Similarity
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
The Problem
If someone asked you and me to each write a python program to solves some novel
problem then it wouldn't be surprising if our solutions looked quite diﬀerent. If we then
tried explaining to the other person the way our own program worked, one thing that
might happen is that we would realize that in spite of surface diﬀerences in the way
that we wrote it the solutions were actually quite similar. We might have called
variables diﬀerent names, done some steps in a diﬀerent order and so on, but both
programs worked for the same reasons.
Another thing that might happen is that even after really understanding how the other
persons program worked the solutions still seemed diﬀerent. Maybe each program
would seem to rely on diﬀerent mathematical facts. Maybe you would realize that the
way I saw the problem was fundamentally diﬀerent from the way you saw it, you'd
never thought about it like that before, and so the approaches I considered were in a
totally diﬀerent regime from the ones you considered. Yours might run asymptotically
faster than mine, in a way such that even if I tried to cut out redundancies and
optimize my algorithm I could never make it run as fast as yours without
fundamentally changing how it worked. Mine might generalize better in some
direction, whereas yours might not generalize as well or maybe generalize in a
diﬀerent direction.
This is the kind of intuition that a formal theory of algorithmic similarity should seek to
capture. Given two Turing machines that compute the same function it should be able
to tell us to what degree they use the same or diﬀerent algorithms. Exactly how
course or ﬁne-grained the classiﬁcation should be is unclear, and maybe diﬀerent
versions could work with diﬀerent levels of detail, but it should capture some of our
intuitive notions of algorithmic similarity, e.g. that changing the name of a variable
generally doesn't change the algorithm, whereas there are fundamental diﬀerences
between the sieve of Erathostenes and the AKS primality test.
A diﬀerent problem that might be the same problem or might be a diﬀerent but
related problem is the question of whether one program uses/contains another
program.
A Variant
Consider a simple program that outputs 1 if x>y and else outputs 0. Call this program
F1. And lets suppose that I know how to compute x-y when x>y or x=y. Then I might
write a program that computes the absolute diﬀerence of two natural numbers x,y in
the following way:
def P1(x,y):
if F1(x,y)==1

output x-y
if F1(x,y)==0
output y-x
This speciﬁc program clearly makes use of F1. But lets say we had a diﬀerent program
F2 that computed the same function as F1 but in a fundamentally diﬀerent way. Then
we could deﬁne:
def P2(x,y):
if F2(x,y)==1
output x-y
if F2(x,y)==0
output y-x
Now, in some ways P1 and P2 look the same, since they both follow the following
general outline:
if x>y
output x-y
else
output y-x
Yet P1 contains F1 whereas P2 does not. Intuitively, if it turned out that the algorithm
in F1 didn't actually work as we thought and so didn't compute the function we
thought it did, then P2 would still work the same but P1 would be broken.
The general question we would like the theory to answer is thus: given two
algorithms, determine whether (and maybe to what degree) one contains the other.
Notice that even if an algorithm contains F1 this might be much harder to spot than in
the above example of P1, especially if we allow it to use trivial variations of F1 and still
have it count. When humans write programs they explicitly write them in a modular
form so that it is easy to see how they work and what components they contain, but in
cases such as algorithms created by gradient descent, ﬁguring out how they work and
which components they contain can be much harder.
Notice also that this is a question at the level of algorithms not functions. You can't
just say that absolute diﬀerence contains F1 whereas addition does not, even if the
most natural way to make an absolute diﬀerence algorithm contains F1 and most
addition algorithms really shouldn't have need of F1.
Finally, though P1 and P2 used diﬀerent algorithms to compute the greater-than-
function they both still used some algorithm to compute the greater-than-function,
and so we might be interested in the more general question of whether a given
program contains the greater-than-function at all, irrespective of whether it uses F1,
F2 or something else entirely.
In a similar vein, and more related to the ﬁrst question of algorithmic similarity, even
though at some level of resolution P1 and P2 are diﬀerent since one uses F1 and the
other F2, we would still like our theory to recognize that the overall strategy of P1 and
P2 were the same, and to diﬀerentiate that strategy with other programs that might
use completely diﬀerent strategies, even ones that also compute the absolute
diﬀerence .

So this is all well and good and technical. But of course the real problem is about
philosophy and metaphysics.
The Real Problem
Mathematics is uncannily useful at describing the world. In fact a lot of the great
successes of science has been to ﬁnd good ways of describing parts of the world using
math, whether it be general relativity describing gravity, information theory giving the
woolly concept of information a precise meaning or encoding complicated real world
planning problems into number crunching problems that we can just feed into a
computer to get the optimal answer out.
And even if you wont go as far as proclaiming that the world simply is math, you
might still believe that there is nothing in the world that can't be explained by math,
that there exists such a complete mathematical description of the universe such that
for every property of the real world there is a corresponding mathematical property
concerning this description of the world.
What does this have to do with algorithmic similarity? Assume the strong Church-
Turing thesis i.e. that the whole universe is computable. Then a natural question to
ask is, which Turing machines simulate the universe and which doesn't. This question
seems like it should have an answer, some Turing machines just run to the left as fast
as possible, those don't seem to simulate the universe, whereas others might encode
the state of the universe on their tape and then gradually change the tape content
according to the laws of physics (it would probably have been more natural to think in
terms of cellular automaton). And yet, this raises the question of algorithmic similarity.
Two Turing machines might use very diﬀerent encodings of the world state, how do I
tell that they are really computing the same universe? And on the other hand, one
Turing machine might just be doing some arbitrary thing that looks complicated but is
actually meaningless, how do i rule out the claim that it is just simulating the universe
in an non-obvious way?
Consider now the question of whether there is diamond in the universe. For a given
Turing machine simulating a universe similar to ours this question should have an
answer (an answer for every time step or something). Yet in general, this seems very
hard to answer. Though diamond is a natural category in the world, it might not be a
very natural category in a speciﬁc encoding of the world, so even though a Turing
machine simulating the universe must contain a piece that is simulating diamond,
having a theory that identiﬁes that subcompontent and diﬀerentiates it from
simulations without that component look like quite the challenge.
But even now that you know the real question is about cool philosophy stuﬀ, you
might still want to ask; why should I care?
Why You Should Care
Reason 1: Annoying waterfall apologists
The one comes to you, shows you a random waterfall and says: this waterfall is
implementing a conscious human being.

What do you say to this person? They are clearly crazy, but how do you convince them
of their error or at least make their position look undefendable to bystanders watching
the argument?
I can imagine someone understanding minds so well on a technical level that they
could build a computer that completely emulates a human mind. And maybe someone
skilled enough in the art of hydrocomputation could even do the same on a waterfall.
But there is just no way that a arbitrary naturally occurring waterfall is running a
human mind, and yet here we are, with waterfall apologists taking over key positions
in government and academia, all because we don't have the theory of algorithmic
similarity that could have prevented this tragedy.
Reason 2: What is a world model/What does it mean to have true beliefs
True belief is when your map matches the territory. These are words we all hear every
morning when we recite our morning prayer to rationality. But what does it actually
mean? Intuitively, there is some thing in the world, and there is some thing in your
brain, and then there is some relation between the thing in the world and the thing in
your brain such that the thing in your brain is "like" the thing in the world and you can
make predictions about how the thing in the world would react merely by doing stuﬀ
to the thing in your brain. To explain what that relation is and what "like" means we
need a theory of algorithmic similarity. (I know the point of The Simple Truth was not
to ask too many questions, but honestly, how are the pebbles "like" the sheep??)
Reason 3: We need it for this very unrealistic AI design
Consider this very unrealistic AI design. It's got at great model of the world inside its
head. It considers sequences of actions, for each sequence computes the diﬀerent
states of the world that could follow from that sequence and how likely they each are.
Then it runs a pre-programmed function on the hypothetical world states to compute
the amount of value, e.g. diamonds, in each state of the world, weights each world by
its likelihood and chooses the action sequence that would in expectation lead to the
most value.
Forgetting the complexity of human values for the moment, how does the function
count the amount of value in the representation of a world state in the AI's head?
Algorithmic similarity. But wait, you might say, what if we ﬁx the way the AI represents
world states such that it's easy to look inside? This might ﬁx part of the problem (see
Ontological Identiﬁcation), but we might not want to ﬁx the AI's representation like
that, maybe we want it to learn the best world model it can without constraints, but in
that case we probably need a better understanding of how to look inside of
computational objects.
Even disregarding values, when the AI has found a world model that yields good
predictions we want it to be able to look inside that model and do stuﬀ like count the
amount of diamonds in it, or ﬁnd out other physical facts about it, even if it found that
model just by looking for computational objects that predicted its sensory input.
Reason 4: It could give us a better agent/optimizer criterion
In Risks From Learned Optimization the authors write: "whether a system is an
optimizer is a property of its internal structure—what algorithm it is physically
implementing—and not a property of its input-output behavior ". In this case the
usefulness of a theory of algorithmic similarity is obvious. Algorithmic similarity could
give us a framework with which to talk about classiﬁcations of exactly this nature.

Reason 5: Because FDT needs it
FDT reasons by imagining that the mathematical function making its decisions have
diﬀerent outcomes. But the question then arises of how FDT determine what things in
the universe are implementing her decision algorithm. This seems like an ideal
situation for a theory of algorithmic similarity to bring some clarity. We need a theory
of how similar two algorithms should be for FDT to change both of them, when one
algorithm contains FDT's decision algorithm so that changing her own means
changing part of the other and what it means for part of the universe to implement a
decision algorithm at all.
Reason 6: Deep insights into the nature of reality/Giving computational functionalism
the comeback it deserves!
Self-explanatory. (see Computational Functionalism, these guys need help)
Now that you understand the problem and care deeply about solving it the only
reasonable thing to do would be to end this blog post and get to work, right?...
Why The Question Doesn't Make
Sense And We Shouldn't Expect There
To Be A Satisfying Answer
I've been telling you tales about all the nice properties this theory will have and all the
confusing situations that it will explain in a perfectly satisfying manner but all this
time I was really selling you a mirage, we don't have the thing yet, and so we run the
risk of the question itself just not making sense to ask. I think this is a reasonable
thing to be afraid of. Intuitions can be misleading and when the thing you are looking
for is this great the reason might be because it is just a magical dream.
I don't believe so, but other smart people do. And even if you think the criticisms in
the end will turn out to be invalid, they're still important to be aware of now so we
know the extent of the challenges that a theory like this would somehow have to
overcome.
This will not be en exhaustive list of the problems you could have with the views
presented so far. In fact, there will only be one entry to the list and I will deﬁnitely not
present the strongest case for it. Treat it as an appetizer. I wont even respond to the
criticism, hopefully I will get back to that some time, maybe someone else will as well.
I don't believe I have any knock-down arguments, that might require the full theory,
but I still think there is something to be said in response.
Criticism 1: It's Trivial
Lets say you have some system A, might be an algorithm, some casual graph, some
mathematical description of the human mind or maybe the whole universe. I will give
an intuitive argument that basically any other physical system can be seen as
implementing this system.
First we need to identify the states that A can be in. Lets suppose A only has ﬁnitely
many states, this should normally be enough since real things are generally thought

to be ﬁnite, but much of the same argument would work with inﬁnitely many states.
Since this is a deterministic system, for each state there will be exactly one state that
follows. You can imagine drawing this out as a huge diagram. If you also did this with
something like a bucket of water then because of the astronomical amount of
molecules in the bucket there would be an almost inconceivable amount of states it
could be in, with all sorts of complicated connections between them.
Now the heuristic argument is that there would almost certainly be a way to identify
sets of states in the bucket system such that the resulting graph were identical to the
graph of system A. This might be a very unnatural coarse-graining, you might end up
associating very diﬀerent physical states in the bucket system and saying that they
are the same, but on the other hand, when you say that a physical calculator is
implementing addition you are also bundling together a bunch of distinct physical
states together to make that argument. And in any case, there should deﬁnitely be
something disturbing about the statement that if you just look at a bucket of water
the right way it can be seen to implement pretty much anything, including a human
brain.
You could try to make rules for which ways of associating states are allowed and which
aren't, but this seems somewhat unnatural, and in any case it seems hard to make up
a set of rules such that you could never game the deﬁnition with something like this.
There are better versions of this arguments, versions where you actually construct a
trivial physical system that implements A in this way without resorting to "almost
certainly", but I wont go into them here. For now I just want to make the remark that
triviality results like these will have to be dealt with in some way for any good theory
of algorithmic similarity to work, especially if we want it to have all the nice
philosophical implications.
Conclusion
If you only remember one thing from this post, let it be this: algorithmic similarity is a
cool problem and by thinking about it you can be cool too.
In terms of grand strategies for tackling the problem, well, those will be in the next
post, I swear. For now, I think the main thing is to focus on the technical problem. It's
interesting, it's pretty concrete and hopefully the philosophical stuﬀ will just fall out of
it when we've got the formal theory down.
In terms of work to build of, there should be plenty. For one thing, it seems like some
people in proof theory are interested in similar stuﬀ, the way they frame it is "when
are diﬀerent proofs really the same", but there are reasons to believe that the
questions might be intimately related (see Curry-Howard correspondence).
There is the debate about computational functionalism in the philosophy literature,
which doesn't look at the technical side of the problem as much, at least not in the
way I presented it here (or so I believe), but have considered a lot of the philosophical
implications.
Finally, computational complexity theory has obviously done a lot of work that seems
to be in this area, since the complexity classes are examples of classiﬁcations of
programs that distinguish between diﬀerent programs which compute the same
function.

My impression is that a surprising amount of people and areas of thought have been
in contact with pieces of this problem, and I think it would be great to try and gather
as much as possible of their work in one place. I would like to compile a giant reading
list/archive of stuﬀ related to algorithmic similarity, and I hope that some of you
reading this post will help by contributing links to work or ideas that you have
stumbled upon and think might be relevant.
Another thing that might be useful is to compile a list of problems where for each
problem there are multiple algorithms that solve the same problem in what, at least
initially, seems to be diﬀerent ways. I don't have many concrete examples yet but it
should be relatively easy to ﬁnd some. Whether or not people can agree on whether
two algorithms are "actually" diﬀerent or "really" just doing the same thing is another
question, I predict that there might be quite a lot of disagreement on that, but that
discussion might itself be useful for clarifying some intuitions. Especially interesting
would be examples of algorithms having the same asymptotic run-time that
nevertheless ran clearly diﬀerent algorithms.
Here is the link to the reading list, I have got some stuﬀ already, and if you post a
suggestion in the comments for something that you think should be in there, then I
will add it in:
https://docs.google.com/document/d/1cC9H2sBd09OUYroo9YMFg2Gc438PEvyxnBO1P
NxUZkA/edit?usp=sharing
Related documents:
https://drive.google.com/drive/folders/1wdxLcb7nTLYl1byycXIE9tNm3G5jNaPG?
usp=sharing
Thanks for reading!

What are the reasons to *not*
consider reducing AI-Xrisk the highest
priority cause?
ETA: I'll be adding things to the list that I think belong there.
I'm assuming a high level of credence in classic utilitarianism, and that AI-Xrisk is
signiﬁcant (e.g. roughly >10%), and timelines are not long (e.g. >50% ASI in
<100years). ETA: For the purpose of this list, I don't care about questioning those
assumptions.
Here's my current list (oﬀ the top of my head):
not your comparitive advantage
consider other Xrisks more threatening (top contenders: bio / nuclear)
inﬁnite ethics (and maybe other fundamental ethical questions, e.g. to do with
moral uncertainty)
S-risks
simulation hypothesis
ETA: AI has high moral value in expectation / by default
ETA: low tractability (either at present or in general)
ETA: Doomsday Argument as overwhelming evidence against futures with large
number of minds
Also, does anyone want to say why they think none of these should change the
picture? Or point to a good reference discussing this question? (etc.)

What explanatory power does
Kahneman's System 2 possess?
In the 70s and 80s, Kahneman and Tversky did a bunch of pioneering research on
heuristics and biases in human thought. Then, in Thinking Fast and Slow, Kahneman
divided human cognition into System 1 and System 2 - basically, System 1 applies
quick heuristics which are prone to biases, and System 2 does the slow, eﬀortful
thinking.
But what does System 2 actually add to the theory in terms of explanatory power?
Consider an alternative version of Thinking Fast and Slow in which Kahneman wrote
something like "Here are the conditions in which humans use this mode of reasoning
I'm calling System 1, which is fast and approximate and eﬀortless and uses heuristics
and demonstrates biases which can be detected in certain ways. The rest of the time,
I have no idea what's going on, except that it doesn't display the traits that would
qualify it as System 1 inference." In what ways would this be less informative than his
actual claims?

How Can People Evaluate Complex
Questions Consistently?
I'm doing a project on how humans can evaluate messy problems and come up with
consistent answers (consistent with both themselves over time and with other
people), and what the trade oﬀ is with accuracy. This isn't a single uniﬁed ﬁeld, so I
need to go poking for bits of it in lots of places. Where would people suggest I look?
I'm especially interested in information on consistently evaluating novel questions that
don't have enough data to make statistical models ("When will [country] develop the
nuclear bomb?") as opposed to questions for which we have enough data that we're
pretty much just looking for similarities ("Does this biopsy reveal cancer?").
An incomplete list of places I have looked or plan on looking at:
interrater reliability
test-retest reliability
educational rubrics (for both student and teacher evaluations)
medical decision making/standard of care
Daniel Kahneman's work
Philip Tetlock's work
The Handbook of Inter-Rater Reliability

[AN #62] Are adversarial examples
caused by real but imperceptible
features?
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Find all Alignment Newsletter resources here. In particular, you can sign up, or look
through this spreadsheet of all summaries that have ever been in the newsletter. I'm
always happy to hear feedback; you can send it to me by commenting on this post.
Audio version here (may not be up yet).
Highlights
Call for contributors to the Alignment Newsletter (Rohin Shah): I'm looking for content
creators and a publisher for this newsletter! Apply by September 6.
Adversarial Examples Are Not Bugs, They Are Features (Andrew Ilyas, Shibani
Santurkar, Dimitris Tsipras, Logan Engstrom et al) (summarized by Rohin and Cody):
Distill published a discussion of this paper. This highlights section will cover the full
discussion; all of these summaries and opinions are meant to be read together.
Consider two possible explanations of adversarial examples. First, they could be
caused because the model "hallucinates" a signal that is not useful for classiﬁcation,
and it becomes very sensitive to this feature. We could call these "bugs", since they
don't generalize well. Second, they could be caused by features that do generalize to
the test set, but can be modiﬁed by an adversarial perturbation. We could call these
"non-robust features" (as opposed to "robust features", which can't be changed by an
adversarial perturbation). The authors argue that at least some adversarial
perturbations fall into the second category of being informative but sensitive features,
based on two experiments.
If the "hallucination" explanation were true, the hallucinations would presumably be
caused by the training process, the choice of architecture, the size of the dataset, but
not by the type of data. So one thing to do would be to see if we can construct a
dataset such that a model trained on that dataset is already robust, without
adversarial training. The authors do this in the ﬁrst experiment. They take an
adversarially trained robust classiﬁer, and create images whose features (ﬁnal-layer
activations of the robust classiﬁer) match the features of some unmodiﬁed input. The
generated images only have robust features because the original classiﬁer was robust,
and in fact models trained on this dataset are automatically robust.
If the "non-robust features" explanation were true, then it should be possible for a
model to learn on a dataset containing only non-robust features (which will look
nonsensical to humans) and still generalize to a normal-looking test set. In the
second experiment (henceforth WrongLabels), the authors construct such a dataset.
Their hypothesis is that adversarial perturbations work by introducing non-robust
features of the target class. So, to construct their dataset, they take an image x with

original label y, adversarially perturb it towards some class y' to get image x', and
then add (x', y') to their dataset (even though to a human x' looks like class y). They
have two versions of this: in RandLabels, the target class y' is chosen randomly,
whereas in DetLabels, y' is chosen to be y + 1. For both datasets, if you train a new
model on the dataset, you get good performance on the original test set, showing
that the "non-robust features" do generalize.
Rohin's opinion: I buy this hypothesis. It's a plausible explanation for brittleness
towards adversarial noise ("because non-robust features are useful to reduce loss"),
and why adversarial examples transfer across models ("because diﬀerent models can
learn the same non-robust features"). In fact, the paper shows that architectures that
did worse in ExpWrongLabels (and so presumably are bad at learning non-robust
features) are also the ones to which adversarial examples transfer the least. I'll leave
the rest of my opinion to the opinions on the responses.
Read more: Paper and Author response
Response: Learning from Incorrectly Labeled Data (Eric Wallace): This response notes
that all of the experiments are of the form: create a dataset D that is consistent with a
model M; then, when you train a new model M' on D you get the same properties as
M. Thus, we can interpret these experiments as showing that model distillation can
work even with data points that we would naively think of "incorrectly labeled". This is
a more general phenomenon: we can take an MNIST model, select only the examples
for which the top prediction is incorrect (labeled with these incorrect top predictions),
and train a new model on that -- and get nontrivial performance on the original test
set, even though the new model has never seen a "correctly labeled" example.
Rohin's opinion: I deﬁnitely agree that these results can be thought of as a form of
model distillation. I don't think this detracts from the main point of the paper: the
reason model distillation works even with incorrectly labeled data is probably because
the data is labeled in such a way that it incentivizes the new model to pick out the
same features that the old model was paying attention to.
Response: Robust Feature Leakage (Gabriel Goh): This response investigates whether
the datasets in WrongLabels could have had robust features. Speciﬁcally, it checks
whether a linear classiﬁer over provably robust features trained on the WrongLabels
dataset can get good accuracy on the original test set. This shouldn't be possible
since WrongLabels is meant to correlate only non-robust features with labels. It ﬁnds
that you can get some accuracy with RandLabels, but you don't get much accuracy
with DetLabels.
The original authors can actually explain this: intuitively, you get accuracy with
RandLabels because it's less harmful to choose labels randomly than to choose them
explicitly incorrectly. With random labels on unmodiﬁed inputs, robust features should
be completely uncorrelated with accuracy. However, with random labels followed by
an adversarial perturbation towards the label, there can be some correlation, because
the adversarial perturbation can add "a small amount" of the robust feature. However,
in DetLabels, the labels are wrong, and so the robust features are negatively
correlated with the true label, and while this can be reduced by an adversarial
perturbation, it can't be reversed (otherwise it wouldn't be robust).
Rohin's opinion: The original authors' explanation of these results is quite
compelling; it seems correct to me.

Response: Adversarial Examples are Just Bugs, Too (Preetum Nakkiran): The main
point of this response is that adversarial examples can be bugs too. In particular, if
you construct adversarial examples that explicitly don't transfer between models, and
then run ExpWrongLabels with such adversarial perturbations, then the resulting
model doesn't perform well on the original test set (and so it must not have learned
non-robust features).
It also constructs a data distribution where every useful feature of the optimal
classifer is guaranteed to be robust, and shows that we can still get adversarial
examples with a typical model, showing that it is not just non-robust features that
cause adversarial examples.
In their response, the authors clarify that they didn't intend to claim that adversarial
examples could not arise due to "bugs", just that "bugs" were not the only
explanation. In particular, they say that their main thesis is "adversarial examples will
not just go away as we ﬁx bugs in our models", which is consistent with the point in
this response.
Rohin's opinion: Amusingly, I think I'm more bullish on the original paper's claims
than the authors themselves. It's certainly true that adversarial examples can arise
from "bugs": if your model overﬁts to your data, then you should expect adversarial
examples along the overﬁtted decision boundary. The dataset constructed in this
response is a particularly clean example: the optimal classiﬁer would have an
accuracy of 90%, but the model is trained to accuracy 99.9%, which means it must be
overﬁtting.
However, I claim that with large and varied datasets with neural nets, we are typically
not in the regime where models overﬁt to the data, and the presence of "bugs" in the
model will decrease. (You certainly can get a neural net to be "buggy", e.g. by
randomly labeling the data, but if you're using real data with a natural task then I
don't expect it to happen to a signiﬁcant degree.) Nonetheless, adversarial examples
persist, because the features that models use are not the ones that humans use.
It's also worth noting that this experiment strongly supports the hypothesis that
adversarial examples transfer because they are real features that generalize to the
test set.
Response: Adversarial Example Researchers Need to Expand What is Meant by
'Robustness' (Justin Gilmer et al): This response argues that the results in the original
paper are simply a consequence of a generally accepted principle: "models lack
robustness to distribution shift because they latch onto superﬁcial correlations in the
data". This isn't just about L_p norm ball adversarial perturbations: for example, one
recent paper shows that if the model is only given access to high frequency features
of images (which look uniformly grey to humans), it can still get above 50% accuracy.
In fact, when we do adversarial training to become robust to L_p perturbations, then
the model pays attention to diﬀerent non-robust features and becomes more
vulnerable to e.g. low-frequency fog corruption. The authors call for adversarial
examples researchers to move beyond L_p perturbations and think about the many
diﬀerent ways models can be fragile, and to make them more robust to distributional
shift.
Rohin's opinion: I strongly agree with the worldview behind this response, and
especially the principle they identiﬁed. I didn't know this was a generally accepted
principle, though of course I am not an expert on distributional robustness.

One thing to note is what is meant by "superﬁcial correlation" here. It means a
correlation that really does exist in the dataset, that really does generalize to the test
set, but that doesn't generalize out of distribution. A better term might be "fragile
correlation". All of the experiments so far have been looking at within-distribution
generalization (aka generalization to the test set), and are showing that non-robust
features do generalize within-distribution. This response is arguing that there are
many such non-robust features that will generalize within-distribution but will not
generalize under distributional shift, and we need to make our models robust to all of
them, not just L_p adversarial perturbations.
Response: Two Examples of Useful, Non-Robust Features (Gabriel Goh): This response
studies linear features, since we can analytically compute their usefulness and
robustness. It plots the singular vectors of the data as features, and ﬁnds that such
features are either robust and useful, or non-robust and not useful. However, you can
get useful, non-robust features by ensembling or contamination (see response for
details).
Response: Adversarially Robust Neural Style Transfer (Reiichiro Nakano): The original
paper showed that adversarial examples don't transfer well to VGG, and that VGG
doesn't tend to learn similar non-robust features as a ResNet. Separately, VGG works
particularly well for style transfer. Perhaps since VGG doesn't capture non-robust
features as well, the results of style transfer look better to humans? This response and
the author's response investigate this hypothesis in more detail and ﬁnd that it seems
broadly supported, but there are still ﬁnnicky details to be worked out.
Rohin's opinion: This is an intriguing empirical fact. However, I don't really buy the
theoretical argument that style transfer works because it doesn't use non-robust
features, since I would typically expect that a model that doesn't use L_p-fragile
features would instead use features that are fragile or non-robust in some other way.
Technical AI alignment
Problems
Problems in AI Alignment that philosophers could potentially contribute to (Wei Dai):
Exactly what it says. The post is short enough that I'm not going to summarize it -- it
would be as long as the original.
Iterated ampliﬁcation
Delegating open-ended cognitive work (Andreas Stuhlmüller): This is the latest
explanation of the approach Ought is experimenting with: Factored Evaluation (in
contrast to Factored Cognition (AN #36)). With Factored Cognition, the idea was to
recursively decompose a high-level task until you reach subtasks that can be directly
solved. Factored Evaluation still does recursive decomposition, but now it is aimed at
evaluating the work of experts, along the same lines as recursive reward modeling
(AN #34).
This shift means that Ought is attacking a very natural problem: how to eﬀectively
delegate work to experts while avoiding principal-agent problems. In particular, we
want to design incentives such that untrusted experts under the incentives will be as

helpful as experts intrinsically motivated to help. The experts could be human experts
or advanced ML systems; ideally our incentive design would work for both.
Currently, Ought is running experiments with reading comprehension on Wikipedia
articles. The experts get access to the article while the judge does not, but the judge
can check whether particular quotes come from the article. They would like to move
to tasks that have a greater gap between the experts and the judge (e.g. allowing the
experts to use Google), and to tasks that are more subjective (e.g. whether the judge
should get Lasik surgery).
Rohin's opinion: The switch from Factored Cognition to Factored Evaluation is
interesting. While it does make it more relevant outside the context of AI alignment
(since principal-agent problems abound outside of AI), it still seems like the major
impact of Ought is on AI alignment, and I'm not sure what the diﬀerence is there. In
iterated ampliﬁcation (AN #30), when decomposing tasks in the Factored Cognition
sense, you would use imitation learning during the distillation step, whereas with
Factored Evaluation, you would use reinforcement learning to optimize the evaluation
signal. The switch would be useful if you expect the reinforcement learning to work
signiﬁcantly better than imitation learning.
However, with Factored Evaluation, the agent that you train iteratively is one that
must be good at evaluating tasks, and then you'd need another agent that actually
performs the task (or you could train the same agent to do both). In contrast, with
Factored Cognition you only need an agent that is performing the task. If the
decompositions needed to perform the task are diﬀerent from the decompositions
needed to evaluate the task, then Factored Cognition would presumably have an
advantage.
Miscellaneous (Alignment)
Clarifying some key hypotheses in AI alignment (Ben Cottier et al): This post (that I
contributed to) introduces a diagram that maps out important and controversial
hypotheses for AI alignment. The goal is to help researchers identify and more
productively discuss their disagreements.
Near-term concerns
Privacy and security
Evaluating and Testing Unintended Memorization in Neural Networks (Nicholas Carlini
et al)
Read more: The Secret Sharer: Evaluating and Testing Unintended Memorization in
Neural Networks
Machine ethics
Towards Empathic Deep Q-Learning (Bart Bussmann et al): This paper introduces the
empathic DQN, which is inspired by the golden rule: "Do unto others as you would
have them do unto you". Given a speciﬁed reward, the empathic DQN optimizes for a
weighted combination of the speciﬁed reward, and the reward that other agents in the

environment would get if they were a copy of the agent. They show that this results in
resource sharing (when there are diminishing returns to resources) and avoiding
conﬂict in two toy gridworlds.
Rohin's opinion: This seems similar in spirit to impact regularization methods: the
hope is that this is a simple rule that prevents catastrophic outcomes without having
to solve all of human values.
AI strategy and policy
AI Algorithms Need FDA-Style Drug Trials (Olaf J. Groth et al)
Other progress in AI
Critiques (AI)
Evidence against current methods leading to human level artiﬁcial intelligence (Asya
Bergal and Robert Long): This post brieﬂy lists arguments that current AI techniques
will not lead to high-level machine intelligence (HLMI), without taking a stance on how
strong these arguments are.
News
Ought: why it matters and ways to help (Paul Christiano): This post discusses the work
that Ought is doing, and makes a case that it is important for AI alignment (see the
summary for Delegating open-ended cognitive work above). Readers can help Ought
by applying for their web developer role, by participating in their experiments, and by
donating.
Project Proposal: Considerations for trading oﬀ capabilities and safety impacts of AI
research (David Krueger): This post calls for a thorough and systematic evaluation of
whether AI safety researchers should worry about the impact of their work on
capabilities.

AI Forecasting Resolution Council
(Forecasting infrastructure, part 2)
UPDATE October 18, 2020: The AI Resolution Forecasting Council has been retired due
to lack of demand for its services. I'm keeping this post up for historical value.
---------------
This post introduces the AI Forecasting Resolution Council, a group of researchers with
technical expertise in AI who will allow us to expand the space of eﬀectively
forecastable questions. It is the second part in a series of blog posts which motivate
and introduce pieces of infrastructure intended to improve our ability to forecast novel
and uncertain domains like AI.
The Council is currently in beta, and we're launching early to get feedback from the
community and quickly ﬁgure out how useful it is.
Background and motivation
A key challenge in (AI) forecasting is to write good questions. This is tricky because we
want questions which both capture important uncertainties, and are suﬃciently
concrete that we can resolve them and award points to forecasters in hindsight.
Here are some example questions within AI that make this especially diﬃcult:
Counterfactual questions
Suppose in 2000 you use "superhuman Othello from self-play" as a benchmark of a
certain kind of impressive AI progress, and forecast it to be possible by 2020. It seems
you were correct -- very plausibly the AlphaZero architecture should work for this.
However, in a strict sense your forecast was wrong -- because no one has actually
bothered to build a powerful Othello agent without relying on handcrafted evaluation
functions (EDIT: thanks to Vanessa Kosoy for pointing out that otherwise superhuman
Othello systems exist).
So if a calibrated forecaster faces this question in 2000, considerations regarding who
will bother to pursue what project "screen oﬀ" considerations regarding fundamental
drivers of AI progress and their gradients. Yet the latter concern is arguably more
interesting.
This problem could be solved if we instead forecasted the question "If someone were
to run an experiment using the AI technology available in 2020, given certain resource
constraints, would it seem with >95% conﬁdence, that they'd be able to create a
superhuman Othello agent that learnt only from self-play?"
Doing so requires a way of evaluating the truth value of that counterfactual, such as
by asking a group of experts.
Similarity questions

Suppose we try to capture performance by appealing to a particular benchmark.
There's a risk that the community will change its focus to another benchmark. We
don't want forecasters to spend their eﬀort thinking about whether this change will
occur, as opposed to fundamental question about the speed of progress (even if we
would want to track such sociological facts about which benchmarks were prominent,
that should be handled by a diﬀerent question where it's clear that this is the intent).
So to avoid this we need a suﬃciently formal way of doing things like comparing
performance of algorithms across multiple benchmarks (for example, if RL agents are
trained on a new version of Dota, can we compare performance to OpenAI Five's on
Dota 2?).
Deﬁnition-of-terms questions
This is more straightforward and related to the AI Forecasting Dictionary. For example,
how do we suﬃciently clearly deﬁne what counts as "hard-coded domain knowledge",
and how much reward shaping you can add before the system no longer learns from
"ﬁrst principles"?
Valuation questions
Not all important uncertainties we care about might be able to be turned into a
concretely operationalised future event. For example, instead of trying to
operationalise how plausible the IDA agenda will seem in 3 years by making a long,
detailed speciﬁcation of the outcome of various experiments, we might just ask "How
plausible will IDA seem to this evaluator in 3 years?" and then try to forecast that
claim.
Making this work will require carefully choosing the evaluators such that, for example,
it is generally easier and less costly to forecast the underlying event than the opinions
of the evaluator, and that we trust that the evaluation actually tracks some important,
natural, hard-to-deﬁne measure.
Prediction-driven evaluation is a deep topic, yet if we could make it work it is
potentially very powerful. See e.g. this post for more details.
AI Forecasting Resolution Council
As a step towards solving the above problems, we're setting up the AI Forecasting
Resolution Council, a group of researchers with technical expertise in AI, who are
volunteering their judgement to resolve questions like the above.
The services of the council are available to any forecasting project, and all operations
for the council will be managed by Parallel Forecast. In case there is more demand for
resolutions than can be ﬁlled, Parallel will decide which requests to meet.
We think that this Council will create streamlined, standardised procedures for dealing
with tricky cases like the above, thereby greatly expanding the space of eﬀectively
forecastable questions.
There are still many questions to be ﬁgured out regarding incentives, mechanism
design, and question operationalisation, and we think that by setting up the
Resolution Council, we are laying some groundwork to begin experimenting in this
direction; and discover best practices and ideas for new, exciting experiments.

The initial members of the council are:
Daniel Filan (CHAI)
Chris Cundy (Stanford)
Gavin Leech (Bristol)
William Saunders (Ought)
We expect to be adding several more members over the coming months.
The database of previous verdicts and upcoming resolution requests can be found
here.
How to use the council if you run a forecasting project
If you're attempting to forecast AI and have a problem that could be solved by
querying the expert council at a future state, let us know by ﬁlling in this resolution
request form.
How to join the council
If you have technical expertise in AI and would be interested in contributing to help
expand the space of forecastable questions, let us know using this form.
There is no limit on the number of judges, since we can always randomise who will
vote on each distinct verdict.

Problems in AI Alignment that
philosophers could potentially
contribute to
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
(This was originally a comment that I wrote as a follow up to my question for William
MacAskill's AMA. I'm moving it since it's perhaps more on-topic here.)
It occurs to me that another reason for the lack of engagement by people with
philosophy backgrounds may be that philosophers aren't aware of the many
philosophical problems in AI alignment that they could potentially contribute to. So
here's a list of philosophical problems that have come up just in my own thinking
about AI alignment.
Decision theory for AI / AI designers
How to resolve standard debates in decision theory?
Logical counterfactuals
Open source game theory
Acausal game theory / reasoning about distant superintelligences
Inﬁnite/multiversal/astronomical ethics
Should we (or our AI) care much more about a universe that is capable of
doing a lot more computations?
What kinds of (e.g. spatial-temporal) discounting is necessary and/or
desirable?
Fair distribution of beneﬁts
How should beneﬁts from AGI be distributed?
For example, would it be fair to distribute it equally over all humans who
currently exist, or according to how much AI services they can aﬀord to
buy?
What about people who existed or will exist at other times and in other
places or universes?
Need for "metaphilosophical paternalism"?
However we distribute the beneﬁts, if we let the beneﬁciaries decide what
to do with their windfall using their own philosophical faculties, is that
likely to lead to a good outcome?
Metaphilosophy
What is the nature of philosophy?
What constitutes correct philosophical reasoning?
How to specify this into an AI design?
Philosophical forecasting
How are various AI technologies and AI safety proposals likely to aﬀect
future philosophical progress (relative to other kinds of progress)?
Preference aggregation between AIs and between users
How should two AIs that want to merge with each other aggregate their
preferences?
How should an AI aggregate preferences between its users?
Normativity for AI / AI designers

What is the nature of normativity? Do we need to make sure an AGI has a
suﬃcient understanding of this?
Metaethical policing
What are the implicit metaethical assumptions in a given AI alignment
proposal (in case the authors didn't spell them out)?
What are the implications of an AI design or alignment proposal under
diﬀerent metaethical assumptions?
Encouraging designs that make minimal metaethical assumptions or is
likely to lead to good outcomes regardless of which metaethical theory
turns out to be true.
(Nowadays AI alignment researchers seem to be generally good about not
placing too much conﬁdence in their own moral theories, but the same
can't always be said to be true with regard to their metaethical ideas.)

Permissions in Governance
Compliance Costs
The burden of a rule can be separated into (at least) two components.
First, there's the direct opportunity cost of not being allowed to do the things the rule
forbids. (We can include here the penalties for violating the rule.)
Second, there's the "cost of compliance", the eﬀort spent on ﬁnding out what is
permitted vs. forbidden and demonstrating that you are only doing the permitted
things.
Separating these is useful. You can, at least in principle, aim to reduce the compliance
costs of a rule without making it less stringent.
For instance, you could aim to simplify the documentation requirements for
environmental impact assessments, without relaxing standards for pollution or safety. 
"Streamlining" or "simplifying" regulations aims to reduce compliance costs, without
necessarily lowering standards or softening penalties.
If your goal in making a rule is to avoid or reduce some unwanted behavior — for
instance, to reduce the amount of toxic pollution people and animals are exposed to
— then shifting up or down your pollution standards is a zero-sum tradeoﬀ between
your environmental goals and the convenience of polluters.
Reducing the costs of compliance, on the other hand, is positive-sum: it saves money
for developers, without increasing pollution levels.  Everybody wins. Where possible,
you'd intuitively think rulemakers would always want to do this.
Of course, this assumes an idealized world where the only goal of a prohibition is to
reduce the total amount of prohibited behavior.
You might want compliance costs to be high if you're using the rule, not to reduce
incidence of the forbidden behavior, but to produce distinctions between people — i.e.
to separate the extremely committed from the casual, so you can reward them
relative to others.  Costly signals are good if you're playing a competitive zero-sum
game; they induce variance because not everyone is able or willing to pay the cost.
For instance, some theories of sexual selection (such as the handicap principle) argue
that we evolved traits which are not beneﬁcial in themselves but are sensitive
indicators of whether or not we have other ﬁtness-enhancing traits. E.g. a peacock's
tail is so heavy and showy that only the strongest and healthiest and best-fed birds
can aﬀord to maintain it. The tail magniﬁes variance, making it easier for peahens to
distinguish otherwise small variations in the health of potential mates.
Such "magnifying glasses for small ﬂaws" are useful in situations where you need to
pick "winners" and can inherently only choose a few. Sexual selection is an example of
such a a situation, as females have biological limits on how many children they can
bear per lifetime; there is a ﬁxed number of males they can reproduce with.  So it's a
zero-sum situation, as males are competing for a ﬁxed number of breeding slots. 
Other competitions for ﬁxed prizes are similar in structure, and likewise tend to evolve

expensive signals of commitment or quality.  A test that's so easy anyone can pass it,
is useless for identifying the top 1%.
On a regulatory-capture or spoils-based account of politics, where politics (including
regulation) is seen as a negotiation to divide up a ﬁxed pool of resources, and
loyalty/trust is important in repeated negotiations, high compliance costs are easy to
explain. They prevent diluting the spoils among too many people, and create variance
in people's ability to comply, which allows you to be selective along whatever
dimension you care about.
Competitive (selective, zero-sum) processes work better when there's wide variance
among people. A rule (or boundary, or incentive) that's meant to minimize an
undesired behavior is, by contrast, looking at aggregate outcomes. If you can make it
easier for people to do the desired behavior and refrain from the undesired, you'll get
better aggregate behavior, all else being equal.  These goals are, in a sense,
"democratic" or "anti-elitist"; if you just care about total aggregate outcomes, then
you want good behavior to be broadly accessible.
Requiring Permission Raises Compliance Costs 
A straightforward way of avoiding undesired behavior is to require people to ask an
authority's permission before acting.
This has advantages: sometimes "undesired behavior" is a complex, situational thing
that's hard to codify into a rule, so the discretional judgment of a human can do better
than a rigid rule.
One disadvantage that I think people underestimate, however, is the chilling eﬀect it
has on desired behavior.
For instance:
If you have to ask the boss's permission individually for each purchase, no
matter how cheap, not only will you waste a lot of your employees' time, but
you'll disincentivize them from asking for even cost-eﬀective purchases, which
can be more costly in the long run.
If you require a doctor's appointment for giving pain medication every time, to
guard against drug abuse, you're going to see a lot of people who really do have
chronic pain doing without medication because they don't want the anxiety of
going to a doctor and being suspected of "drug-seeking".
If you have to get permission before cleaning or contributing supplies for a
shared space, then that space will be chronically under-cleaned and under-
supplied.
If you have to get permission from a superior in order to stop the production line
to ﬁx a problem, then safety risks and defective products will get overlooked.
(This is why Toyota mandated that any worker can unilaterally stop the
production line.)
The inhibition against asking for permission is going to be strongest for shy people
who "don't want to be a bother" — i.e. those who are most conscious of the eﬀects of
their actions on others, and perhaps those who you'd most want to encourage to act. 
Those who don't care about bothering you are going to be undaunted, and will ﬂood
you with unreasonable requests.  A system where you have to ask a human's
permission before doing anything is an asshole ﬁlter, in Siderea's terminology; it
empowers assholes and disadvantages everyone else.

The direct costs of a rule fall only on those who violate it (or wish they could); the
compliance costs fall on everyone.  A system of enforcement
that preferentially inhibits desired behavior (while not being that reliable in restricting
undesired behavior) is even worse from an eﬃciency perspective than a high
compliance cost on everyone.
Impersonal Boundaries
An alternative is to instantiate your boundaries in an inanimate object — something
that can't intimidate shy people or cave to pressure from entitled jerks.  For instance:
a lock on a door is an inanimate boundary on space
a set of password-protected permissions on a ﬁlesystem is an inanimate
boundary on information access
a departmental budget and a credit card with a ﬁxed spending limit is an
inanimate boundary on spending
an electricity source that shuts oﬀ automatically when you don't pay your bill is
an inanimate boundary against theft
The key element here isn't information-theoretic simplicity, as in the debate over
simple rules vs. discretion.  Inanimate boundaries can be complex and opaque.  They
can be a black box to the user.
The key elements are that, unlike humans, inanimate boundaries do not punish
requests that are refused (even socially, by wearing a disappointed facial expression),
and they do not give in to repeated or more forceful requests.
An inanimate boundary is, rather, like the ideal version of a human maintaining a
boundary in an "assertive" fashion; it enforces the boundary reliably and patiently and
without emotion.
This way, it produces less inhibition in shy or empathetic people (who hate to make
requests that could make someone unhappy) and is less vulnerable to pushy people
(who browbeat others into compromising on boundaries.)
In fact, you can get some of the beneﬁts of an inanimate boundary
without actually taking a human out of the loop, but just by reducing the bandwidth
for social signals. By using email instead of in-person communication, for instance, or
by using formalized scripts and impersonal terminology.  Distancing tactics make it
easier to refuse requests and easier to make requests; if these eﬀects are roughly the
same in magnitude, you get a system that selects more eﬀectively for enabling
desired behavior and preventing undesired behavior. (Of course, when you have one
permission-granter and many permission-seekers, the eﬀects are not the same in
aggregate magnitude; the permission-granter can get spammed by tons of
unreasonable requests.)
Of course, if you're trying to select for transgressiveness — if you want to reward
people who are too savvy to follow the oﬃcial rules and too stubborn to take no for an
answer — you'd want to do the opposite; have an automated, impersonal ﬁlter to
block or intimidate the dutiful, and an extremely personal, intimate, psychologically
grueling test for the exceptional. But in this case, what you've set up is a
competitive test to diﬀerentiate between people, not a rule or boundary which you'd
like followed as widely as possible.
Consensus and Do-Ocracy

So far, the systems we've talked about are centralized, and described from the
perspective of an authority ﬁgure. Given that you, the authority, want to achieve
some goal, how should you most eﬀectively enforce or incentivize desired activity?
But, of course, that's not the only perspective one might take. You could instead take
the perspective that everybody has goals, with no a priori reason to prefer one
person's goals to anyone else's (without knowing  what the goals are), and model the
situation as a group deliberating on how to make decisions.
Consensus represents the egalitarian-group version of permission-asking. Before an
action is taken, the group must discuss it, and must agree (by majority vote, or
unanimous consent, or some other aggregation mechanism) that it's suﬃciently
widely accepted.
This has all of the typical ﬂaws of asking permission from an authority ﬁgure, with the
added problem that groups can take longer to come to consensus than a single
authority takes to make a go/no-go decision. Consensus decision processes inhibit
action.
(Of course, sometimes that's exactly what you want. We have jury trials to prevent
giving criminal penalties lightly or without deliberation.)
An alternative, equally egalitarian structure is what some hackerspaces call do-ocracy.
In a do-ocracy, everyone has authority to act, unilaterally. If you think something
should be done, like rearranging the tables in a shared space, you do it. No need to
ask for permission.
There might be disputes when someone objects to your actions, which have to be
resolved in some way.  But this is basically the only situation where governance enters
into a do-ocracy. Consensus decisionmaking is an informal version of a legislative or
executive body; do-ocracy is an informal version of a judicial system.  Instead of
needing governance every time someone acts, in a judicial-only system you only need
governance every time someone acts (or states an intention to act) AND someone
else objects.
The primary advantage of do-ocracy is that it doesn't slow down actions in the
majority of cases where nobody minds.  There's no friction, no barrier to taking
initiative.  You don't have tasks lying undone because nobody knows "whose job" they
are.  Additionally, it grants the most power to the most active participants, which
intuitively has a kind of fairness to it, especially in voluntary clubs that have a lot of
passive members who barely engage at all.
The disadvantages of do-ocracy are exactly the same as its advantages.  First of all,
any action which is potentially harmful and hard to reverse (including, of course,
dangerous accidents and violence) can be unilaterally initiated, and do-ocracy cannot
prevent it, only remediate it after the fact (or penalize the agent.)  Do-ocracies don't
deal well with very severe, irreversible risks. When they have to, they evolve
permission-based functions; for instance, the rules ﬁrms or insurance companies
institute to prevent risky activities that could lead to lawsuits.
Secondly, do-ocracies grant the most power to the most active participants, which
often means those who have the most time on their hands, or who are closest to the
action, at the expense of absent stakeholders. This means, for instance, it favors a
ﬁrm's executives (who engage in day-to-day activity) over investors or donors or the

general public; in volunteer and political organizations it favors those who have more
free time to participate (retirees, students, the unemployed, the independently
wealthy) over those who have less (working adults, parents).  The general
phenomenon here is principal-agent problems — theft, self-dealing, negligence, all
cases where the people who are physically there and acting take unfair advantage of
the people who are absent and not in the loop, but depend on things remaining okay.
A judicial system doesn't help those who don't know they've been wronged.
Consensus systems, in fact, are designed to force governance to include or represent
all the stakeholders — even those who would, by default, not take the initiative to
participate.
Consumer-product companies mostly have do-ocratic power over their users. It's
possible to quit Facebook, with the touch of a button. Facebook changes its
algorithms, often in ways users don't like — but, in most cases, people don't hate the
changes enough to quit.  Facebook makes use of personal data — after putting up a
dialog box requesting permission to use it. Yet, some people are dissatisﬁed and feel
like Facebook is too powerful, like it's hacking into their baser instincts, like this wasn't
what they'd wanted. But Facebook hasn't harmed them in any way they didn't, in a
sense, consent to. The issue is that Facebook was doing things they didn't reﬂectively
approve of while they weren't paying attention. Not secretly — none of this was
secret, it just wasn't on their minds, until suddenly a big media ﬁrestorm put it there.
You can get a lot of power to shape human behavior just by showing up, knowing what
you want, and enacting it before anyone else has thought about it enough to object.
That's the side of do-ocracy that freaks people out.  Wherever in your life you're
running on autopilot, an adversarial innovator can take a bite out of you and get away
with it long before you notice something's wrong.  
This is another part of the appeal of permission-based systems, whether egalitarian or
authoritarian; if you have to make a high-touch, human connection with me and get
my permission before acting, I'm more likely to notice changes that are bad in ways I
didn't have any prior model of. If I'm suﬃciently cautious or pessimistic, I might even
be ok with the costs in terms of causing a chilling eﬀect on harmless actions, so long
as I make sure I'm sensitive to new kinds of shenanigans that can't be captured in
pre-existing rules.  If I don't know what I want exactly, but I expect change is bad, I'm
going to be much more drawn to permssion-based systems than if I know exactly what
I want or if I expect typical actions to be improvements.

Book Review: Secular Cycles
I.
There is a tide in the aﬀairs of men. It cycles with a period of about three hundred
years. During its ﬂood, farms and businesses prosper, and great empires enjoy golden
ages. During its ebb, war and famine stalk the land, and states collapse into
barbarism.
Chinese population over time
At least this is the thesis of Peter Turchin and Sergey Nefedov, authors of Secular
Cycles. They start oﬀ Malthusian: due to natural reproduction, population will keep
increasing until it reaches the limits of what the land can support. At that point,
everyone will be stuck at subsistence level. If any group ever enjoys a standard of
living above subsistence level, they will keep reproducing until they are back down at
subsistence.
Standard Malthusian theory evokes images of a population stable at subsistence level
forever. But Turchin and Nefedov argues this isn't how it works. A population at
subsistence will always be one meal away from starving. When a famine hits, many of
them will starve. When a plague hits, they will already be too sickly to ﬁght it oﬀ.
When conﬂict arrives, they will be desperate enough to enlist in the armies of
whichever warlord can oﬀer them a warm meal.
These are not piecemeal events, picking oﬀ just enough of the population to bring it
back to subsistence. They are great cataclysms. The Black Plague killed 30% - 60% of
Europeans; the Antonine Plague of Rome was almost as deadly. The Thirty Years War
killed 25% - 40% of Germans; the Time of Troubles may have killed half the population
of medieval Russia.
Thus the secular cycle. When population is low, everyone has more than enough land.
People grow rich and reproduce. As time goes on, the same amount of farmland gets
split among more and more people. Wages are driven down to subsistence. War,
Famine, and Pestilence ravage the land, with Death not far behind. The killings
continue until population is low again, at which point the cycle starts over.
This applies mostly to peasants, who are most at risk of starving. But nobles go
through a related process. As a cycle begins, their numbers are low. As time goes on,
their population expands, both through natural reproduction and through upward
mobility. Eventually, there are more nobles than there are good positions...
(this part confused me a little. Shouldn't number of good positions scale with
population? IE if one baron rules 1,000 peasants, the number of baronial positions
should scale with the size of a society. I think T&N hint at a few answers. First, some
positions are absolute rather than relative, eg "King" or "Minister of the Economy".
Second, noble numbers may sometimes increase faster than peasant numbers, since
nobles have more food and better chances to reproduce. Third, during boom times,
the ranks of nobles are swelled through upward mobility. Fourth, conspicuous
consumption is a ratchet eﬀect: during boom times, the expectations of nobility
should gradually rise. Fifth, sometimes the relevant denominator is not peasants but
land: if a noble only has one acre of land, it doesn't matter how many peasants he

controls. Sixth nobles usually survive famines and plagues pretty well, so after those
have done their work, there are far fewer peasants but basically the same number of
nobles. All of these factors contribute to excess noble population - or as T&N call it,
"elite overproduction")
...and the nobles form "rival patronage networks" to ﬁght for the few remaining good
spots. The state goes from united (or at least all nobles united against the peasants)
to divided, with coalitions of nobles duking it out (no pun intended). This can lead
either to successful peasant rebellion, as some nobles support the peasants as part of
inter-noble power plays, or just to civil war. Although famine and plague barely aﬀect
nobles, war aﬀects them disproportionately - both because they are often knights or
other front-line soldiers, and because killing the other side's nobles was a major
strategic goal (think Game of Thrones). So a civil war usually further depletes the
already-depleted peasant population, and ﬁnally depletes noble populations, leading
to a general underpopulation and the beginning of the next cycle.
Combine these two processes, and you get the basic structure of a secular cycle.
There are about a hundred years of unalloyed growth, as peasant and noble
populations rebound from the last disaster. During this period, the economy is strong,
the people are optimistic and patriotic, and the state is strong and united.
After this come about ﬁfty years of "stagﬂation". There is no more room for easy
growth, but the system is able to absorb the surplus population without cracking.
Peasants may not have enough land, but they go to the city in search of jobs. Nobles
may not have enough of the positions they want, but they go to college in order to
become bureaucrats, or join the retinues of stronger nobles. The price of labor reaches
its lowest point, and the haves are able to exploit the desperation of the have-nots to
reach the zenith of their power. From the outside, this period can look like a golden
age: huge cities buzzing with people, universities crammed with students, ultra-rich
nobles throwing money at the arts and sciences. From the inside, for most people it
will look like a narrowing of opportunity and a hard-to-explain but growing sense that
something is wrong.
After this comes a crisis. The mechanisms that have previously absorbed surplus
population fail. Famine and disease ravage the peasantry. State ﬁnances fall apart.
Social trust and patriotism disappear as it becomes increasingly obvious that it's
every man for himself and that people with scruples will be defeated or exploited by
people without.
After this comes the depression period (marked "intercycle" on this graph, but I'm
going to stick with the book's term). This graph makes it look puny, but it can last 100
to 150 years. During this period, the peasant population is low, but the noble
population is still high. This is most likely a period of very weak or even absent state
power, frequent barbarian invasions, and frequent civil war. The peasant population is
in a good position to expand, but cannot do so because wars keep killing people oﬀ or
forcing them into walled towns where they can't do any farming. Usually it takes a
couple more wars and disasters before the noble population has decreased enough to
reverse elite overproduction. At this point the remaining nobles look around, decide
that there is more than enough for all of them, and feel incentivized to cooperate with
the formation of a strong centralized state.

This cycle is interwoven with a second 40-60 year process that T&N call the "fathers-
and-sons cycle" or "bigenerational cycle". The data tend to show waves of disorder
about every 40-60 years. During the "integrative trend" (T&N's term for the optimistic
growth and stagﬂation phases), these can just be minor protests or a small rebellion
that is easily crushed. During the "disintegrative trend" (crisis + depression), they
usually represent individual outbreaks of civil war. For example, during the Roman
Republic, the violence around the death of Tiberius Gracchus in 133 BC was relatively
limited, because Rome had not yet entered its crisis phase. 40 years later, in the
depths of the crisis phase, there was a second outbreak of violence (91 - 82 BC)
including the Social War and Sulla's wars, which escalated to full-scale (though
limited) civil war. 40 years later there was a third outbreak (49 - 27 BC) including
Caesar and Augustus's very large civil wars. After that the new integrative trend
started and further violence was suppressed.
I'm not sure this even makes sense in the Secular Cycle system, since it should apply
only to individual countries and not to the entire world, but these sure are some
interesting data.
In Secular Cycles, T&N mostly just identiﬁes this pattern from the data and doesn't
talk a lot about what causes it. But in some of Turchin's other work, he applies some of
the math used to model epidemics in public health. His model imagines three kinds of
people: naives, radicals, and moderates. At the start of a cycle, most people are naive,
with a few radicals. Radicals gradually spread radicalism, either by converting their
friends or provoking their enemies (eg a terrorist attack by one side convinces
previously disengaged people to join the other side). This spreads like any other
epidemic. But as violence gets worse, some people convert to "moderates", here
meaning not "wishy-washy people who don't care" but something more like "people
disenchanted with the cycle of violence, determined to get peace at any price".
Moderates suppress radicals, but as they die oﬀ most people are naive and the cycle
begins again. Using various parameters for his model Turchin claims this predicts the
forty-to-sixty year cycle of violence observed in the data.
So this is the basic thesis of Secular Cycles. Pre-industrial history operates on two
cycles: ﬁrst, a three-hundred year cycle of the rise-and-fall of civilizations. And
second, a 40-60 year cycle of violent disorder that only becomes relevant during the
lowest parts of the ﬁrst cycle.
II.
This is all in the ﬁrst chapter of the book! The next eight chapters are case studies of
eight diﬀerent historical periods and how they followed the secular cycle model.
For example, Chapter 7 is on the Roman Empire. It starts with Augustus in 27 BC. The
Roman Republic has just undergone a hundred years of civil war, from the Gracchi to
Marius to Sulla to Pompey to Caesar to Antony. All of this decreased its population by
30% from its second-century peak. That means things are set to get a lot better very
quickly.
The expansion phase of the Empire lasted from Augustus (27 BC) to Nerva (96 AD),
followed by a stagﬂation phase from Nerva to Antonius Pius (165 AD). Throughout
both phases, the population grew - from about 40 million in Augustus' day to 65
million in Antonius'. Wheat prices stayed stable until Nerva, then doubled from the
beginning of the second century to its end. Legionary pay followed the inverse

pattern, staying stable until Nerva and then decreasing by a third before 200. The
ﬁnances of the state were the same - pretty good until the late second century
(despite occasional crazy people becoming Emperor and spending the entire treasury
building statues of themselves), but cratering during the time of Marcus Aurelius and
Commodus (who debased the denarius down to only 2 g silver).
Throughout expansion and stagﬂation, the Empire was relatively peaceful (the "Pax
Romana"). Sure, occasionally a crazy person would become Emperor and they would
have to kill him. There was even one small civil war which lasted all of a year (69 AD).
But in general, these were isolated incidents.
Throughout the expansion phase, upward mobility was high and income inequality
relatively low. T&N measure this as how many consuls (the highest position in the
Roman governmental hierarchy) had fathers who were also consuls. This decreased
throughout the ﬁrst century - from 46% to 18% - then started creeping back up during
the stagﬂation phase to reach 32% at the end of the second century.
The crisis phase began in 165 AD at the peak of Rome's population and wealth. The
Antonine Plague ravaged the Empire, killing up to 30% of the population. Fifteen years
later, the century-long dominance of the Good Emperors ended, and Commodus took
the throne. Then he was murdered and Pertinax took the throne. Then he was
murdered and Didius Julianus took the throne. Then he was murdered and Septimius
Severus took the throne.
Now we are well into the disintegrative trend, and the shorter 40-60 year cycle comes
into play. Septimus Severius founds a dynasty that lasts 41 years, until Septimius
Alexander (the cousin of the grandson of Septimius Severus' sister-in-law; it's
complicated) was assassinated by his own soldiers in Germany. This begins the Crisis
Of The Third Century, a time of constant civil war, mass depopulation, and economic
collapse. The Five Good Emperors of the second century ruled 84 years between them
(average of 17 years per emperor). The ﬁfty year Crisis included 27 emperors, for an
average of less than 2 years per emperor.
Finally, in 284, Emperor Diocletian ended the civil wars, re-established centralized
authority, and essentially refounded the Roman Empire - a nice round 310 years after
Augustus did the same. T&N mark this as the end of a secular cycle and the beginning
of a new integrative trend.
T&N are able to tell this story. But they don't just tell the story. They are able to cite
various statistics to back themselves up. The Roman population statistics. The price of
wheat and other foodstuﬀs. The average wages for laborers. They especially like coin
hoards - the amount of buried treasure from a given period discovered by treasure-
hunters - because they argue you only bury your money during times of instability, so
this forms a semi-objective way of measuring how unstable things are.
They are at their best when presenting very broad summary statistics. For example,
Roman industry produced vast amounts of lead, which entered the atmosphere and
settled into the Greenland ice sheet. Here is Roman lead output per year as measured
in ice cores:
This shows four peaks for the four cycles T&N identify in Rome: the Kingdom, the
Republic, the Early Empire of Augustus (Principate, the one described above), and the

Late Empire of Diocletian (Dominate). It even shows a sawtooth-y pattern
corresponding to the shorter bigenerational cycles.
Or here is building activity in Rome, measured by how many buildings archaeologists
have found from a given time:
This is a little less perfect (why is there a big gap in the middle of the Principate? I
guess Augustus is a hard act to follow, building-wise) but it still looks good for the
cycle theory.
And here is an Index Of Political Instability, which "combines measures of duration,
intensity, and scale of political instability events, coded by a team of professional
historians":
Rome is the one on top. Instability clearly peaks during the crisis-depression phases
between T&N's secular cycles - again with a sawtooth pattern representing the
bigenerational cycles.
III.
Seeing patterns in random noise is one of the basic human failure modes. Secular
Cycles is so prima facie crackpottish that it should require mountains of data before
we even start wondering if it might be true. I want to make it clear that the book -
plus Turchin individually in some of his other books and papers - provides these
mountains. I can't show every single case study, graph, and table in this book review.
But the chapter above on the Roman Principate included 25 named ﬁgures and
graphs, plus countless more informal presentations of data series, from "real wages of
agricultural laborers in Roman Egypt during the second century" to "mean annual real
land rents for wheat ﬁelds in artabas per aroura, 27 BC to 268 CE" to "imperial
handouts per reign-year" to "importation of African red slip ware into the Albegna
Valley of Etruria, 100 - 600". And this is just one chapter, randomly chosen. There are
seven others just like this. This book understands the burden of proof it is under, and
does everything it can to meet it.
Still, we should be skeptical. How many degrees of freedom do T&N have, and is it
enough to undermine their case?
First, they get some freedom in the civilizations they use as case studies. They could
have searched through every region and period and cherry-picked eight civilizations
that rose and fell over a periods of three hundred years. Did they? I don't think so. The
case studies are England, France, Rome, and Russia. These are some of the
civilizations of greatest interest to the English-speaking world (except Russia, which
makes sense in context because the authors are both Russian). They're also some of
the civilizations best-studied by Anglophone historians and with the most data
available (the authors' methodology requires having good time-series of populations,
budgets, food production, etc).
Also, it's not too hard to look at the civilizations they didn't study and ﬁll in the gaps.
The book barely mentions China, but it seems to ﬁt the model pretty well ("the empire
united longs to divide; divided longs to unite"). In fact, taking the quotation
completely seriously - the empire was ﬁrst united during the Qin Dynasty starting in

221 BC, which lasted only 20 years before seguing into the Han Dynasty in 202 BC.
The Han expanded and prospered for about a century, had another century of
complicated intrigue and frequently revolt, and then ended in disaster in the ﬁrst part
of the ﬁrst century, with a set of failed reforms, civil war, the sack of the capital, some
more civil war, peasant revolt, and even more civil war. The separate period of the
Eastern Han Dynasty began in 25 AD, about 240 years after the beginning of the Qin-
Han cycle. The Eastern Han also grew and prospered for about a hundred years, then
had another ﬁfty years of simmering discontent, then fell apart in about 184 AD, with
another series of civil wars, peasant rebellions, etc. This was the Three Kingdoms
Period during which "the empire united longs to divide, divided longs to unite" was
written to describe. It lasted another eighty years until 266 AD, after which the Jin
Dynasty began. The Jin Dynasty was kind of crap, but it lasted another 180 years until
420, followed by 160 years of division, followed by the Sui and Tang dynasties, which
were not crap. So I don't think it takes too much pattern-matching to identify a
Western-Han-to-Eastern-Han Cycle of 240 years, followed by an Eastern-Han-to-Jin
Cycle of 241 years, followed by a Jin-to-Sui/Tang-Cycle of 324 years.
One could make a more hostile analysis. Is it really fair to lump the Western Jin and
Eastern Jin conveniently together, but separate the Western Han and Eastern Han
conveniently apart? Is it really fair to call the crappy and revolt-prone Jin Dynasty an
"integrative trend" rather than a disintegrative trend that lasted much longer than the
theory should predict? Is it really fair to round oﬀ cycles of 240 and 320 years to
"basically 300 years"?
I think the answer to all of these is "T&N aren't making predictions about the length of
Chinese dynasties, they're making predictions about the nature of secular cycles,
which are correlated with dynasties but not identical to them". If I had the equivalent
to lead core readings for China, or an "instability index", or time series data for wages
or health or pottery importation or so on, maybe it would be perfectly obvious that the
Eastern and Western Han deﬁned two diﬀerent periods, but the Eastern and Western
Jin were part of the same period - the same way one look at the lead core data for
Rome shows that the Julio-Claudian dynasty vs. the Flavian Dynasty is not an
interesting transition.
A secondary answer might be that T&N admit all sorts of things can alter the length of
secular cycles. They tragically devote only a few pages to "Ibn Khaldun cycles", the
theory of 14th century Arabic historian Ibn Khaldun that civilizations in the Maghreb
rise and fall on a one hundred year period. But they discuss it just enough to say their
data conﬁrm Ibn Khaldun's observations. The accelerated timescale (100 vs. 300
years) is because the Maghreb is massively polygynous, with successful leaders
having harems of hundreds of concubines. This speeds up the elite overproduction
process and makes everything happen in fast-forward. T&N also admit that their
theory only describes civilizations insofar as they are self-contained. This
approximately holds for hegemons like Rome at its height, but fails for eg Poland,
whose history is going to be much more inﬂuenced by when Russia or Germany
decides to invade than by the internal mechanisms of Polish society. Insofar as
external shocks - whether climatic, foreign military, or whatever else - aﬀect a
civilization, secular cycles will be stretched out, compressed, or just totally absent.
This sort of thing must obviously be true, and it's good T&N say it, but it's also a free
pass to add as many epicycles as you need to explain failure to match data. All I can
say looking at China is that, if you give it some wiggle room, it seems to ﬁt T&N's
theories okay. The same is true of a bunch of other civilizations I plugged in to see if
they would work.

Second, T&N get some degrees of freedom based on what statistics they use. In every
case, they present statistics that support the presence of secular cycles, but they're
not the same statistics in every case. On the one hand, this is unavoidable; we may
not have good wage data for every civilization, and levels of pottery importation might
be more relevant to ancient Rome than to 19th-century Russia. On the other hand, I'm
not sure what prevents them from just never mentioning the Instability Index if the
Instability Index doesn't show what they want it to show.
Here are some random Rome-related indicators I found online:
None of them show the same four-peaked Kingdom-Republic-Principate-Dominate
pattern as the ones Secular Cycles cites, or the ones Turchin has online.
Third, a lot of the statistics themselves have some degrees of freedom. A lot of them
are things like "Instability Index" or "Index of Social Well-Being" or "General Badness
Index". These seem like the kind of scores you can ﬁddle with to get the results you
want. Turchin claims he hasn't ﬁddled with them - his instability index is taken from a
1937 paper I haven't been able to ﬁnd. But how many papers like that are there? Am I
getting too conspiratorial now?
Likewise, we don't have direct access to the budget of the Roman Empire (or
Plantagenet England, or...). Historians have tried to reconstruct it based on
archaeology and the few records that have survived. T&N cite these people, and the
people they cite are at the top of their ﬁelds and say what T&N say they say. But how
much ﬂexibility did they have in deciding which estimate of the Roman budget to cite?
Is there enough disagreement that they could cite the high estimate for one period
and the low estimate for another, then prove it had gone down? I don't know (though
a few hours' work ought to be enough to establish this).
I wish I could ﬁnd commentary by other academics and historians on Secular Cycles,
or Turchin's work more generally. I feel like somebody should either be violently
debunking this, or else throwing the authors a ticker-tape parade for having solved
history. Neither is happening. The few comments I can ﬁnd are mostly limited to naval
gazing about whether history should be quantitative or qualitative. The few exceptions
I can ﬁnd are blog posts by people I know and respect urging me to read Turchin ﬁve
years ago, advice I am sorry for not taking. If you know of any good criticism, please
tell me where to ﬁnd it.
Until then, my very quick double-checking suggests T&N are pretty much on the level.
But there could still be subtler forms of overﬁtting going on that I don't know enough
about history to detect.
IV.
If this is true, does it have any implications for people today?
First, a very weak implication: it makes history easier to learn. I was shocked how
much more I remembered about the Plantagenets, Tudors, Capetians after reading this
book than after reading any normal history book about them. I think the secret
ingredient is structure. If history is just "one damn thing after another", there's no
framework for ﬁguring out what matters, what's worth learning, what follows what
else. The secular cycle idea creates a structure that everything ﬁts into neatly. I know
that the Plantagenet Dynasty lasted from 1154 - 1485, because it had to, because

that's a 331 year secular cycle. I know that the important events to remember include
the Anarchy of 1135 - 1153 and the War of the Roses from 1455 - 1487, because
those are the two crisis-depression periods that frame the cycle. I know that after
1485 Henry Tudor took the throne and began a new age of English history, because
that's the beginning of the integrative phase of the next cycle. All of this is a lot easier
than trying to remember these names and dates absent any context. I would
recommend this book for that reason alone.
Second, I think this might give new context to Piketty on inequality. T&N describe
inequality as starting out very low during the growth phase of a secular cycle, rising to
a peak during the stagﬂation phase, then dropping precipitously during the crisis.
Piketty describes the same: inequality rising through the peaceful period of 1800 to
1900, dropping precipitously during the two World Wars, then gradually rising again
since then. This doesn't make a huge amount of sense, since I'm not sure you can ﬁt
the post industrial world into secular cycles. But I notice Piketty seems to think of this
as a once-oﬀ event - inequality has been rising forever, broken only by the freak crisis
of the two World Wars - and it's interesting to read T&N talk about the exact same
process recurring again and again throughout history.
Finally, and most important: is there any sense in which this is still going on?
The easiest answer would be no, there isn't. The secular cycles are based around
Malthusian population growth, but we are now in a post-Malthusian regime where land
is no longer the limiting resource. And the cycles seem to assume huge crises killing
oﬀ 30% to 50% of the population, but those don't happen anymore in First World
countries; the Civil War was the bloodiest period of US history, and even it only killed
2% of Americans. Even Germany only lost about 15% of its population in World Wars I
+ II.
But Turchin has another book, Ages Of Discord, arguing that they do. I have bought it
and started it and will report back when I'm done.
Even without a framework, this is just interesting to think about. In popular
understanding of American history, you can trace out optimistic and pessimistic
periods. The national narrative seems to include a story of the 1950s as a golden age
of optimism. Then everyone got angry and violent in the early 1970s (the Status 451
review of Days Of Rage is pretty great here, and reminds us that "people have
completely forgotten that in 1972 we had over nineteen hundred domestic bombings
in the United States"). Then everything suddenly got better once Reagan declared
"morning in America" in the 1980s, with an era of optimism and good feelings lasting
through the Clinton administration. Then things starting to turn bad sometime around
Bush II. And now everybody hates each other, and fascists and antifa are ﬁghting in
the streets, and people are talking about how "civility" and "bipartisanship" are evil
tools of oppression, and PredictIt says an avowed socialist has a 10% chance of
becoming president of the US. To what extent is this narrative true? I don't know, but
it's deﬁnitely the narrative.
One thing that strikes me about T&N's cycles is the ideological component. They
describe how, during a growth phase, everyone is optimistic and patriotic, secure in
the knowledge that there is enough for everybody. During the stagﬂation phase,
inequality increases, but concern about inequality increases even more, zero-sum
thinking predominates, and social trust craters (both because people are actually
defecting, and because it's in lots of people's interest to play up the degree to which

people are defecting). By the crisis phase, partisanship is much stronger than
patriotism and radicals are talking openly about how violence is ethically obligatory.
And then, eventually, things get better. There is a new Augustan Age of virtue and the
reestablishment of all good things. This is a really interesting claim. Western
philosophy tends to think in terms of trends, not cycles. We see everything going on
around us, and we think this is some endless trend towards more partisanship, more
inequality, more hatred, and more state dysfunction.
Secular Cycles oﬀers a narrative where endless trends can end, and things can get
better after all. Of course, it also oﬀers a narrative where sometimes this process
involves the death of 30% - 50% of the population. Maybe I should read Turchin's
other books before speculating any further.

Call for contributors to the Alignment
Newsletter
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
TL;DR: I am looking for (possibly paid) contributors to write summaries and opinions
for the Alignment Newsletter. This is currently experimental, but I estimate ~80%
chance that it will become long-term, and so I'm looking for people who are likely to
contribute at least 20 summaries over the course of their tenure at the newsletter
(see caveats in the post). To apply, read this doc, write an example summary +
opinion, and ﬁll out this form by Friday, September 6. I am also looking for
someone to take over the work of publishing the newsletter (~1-3 hours per week);
please send me an email if you'd be interested in this.
ETA: I now have enough interest in the publisher role that I would be shocked if none
of them worked out. Feel free to continue expressing interest if you think you'd
particularly beneﬁt from doing the work, or if you think you'd be particularly good at
it.
Roles I am looking for
Publisher: Once all of the summaries and opinions are written, you would turn them
into an actual newsletter, send it out for proofreading, ﬁx any typos found, update the
database, etc. This currently takes me around half an hour per newsletter. Ideally, you
would also take on some tasks that I haven't found the time for: improving the visual
design of the newsletter, A/B testing diﬀerent versions to see what people engage
with, publicity, and so on, for a total of ~1-3 hours per week.
Since I don't yet have the setup to pay people to help with the newsletter, I am only
looking for expressions of interest. If you think you'd be interested in this role, click
this link to email me at rohinmshah@berkeley.edu with the subject line "Interested in
publisher role for Alignment Newsletter EOM". If I do end up hiring for the publisher
role I'll reach out to you with more details.
The rest of this doc will be focused on the more substantial role:
Content creator: You would choose articles that you're interested in, and write
summaries and opinions for them, that would then be published in the newsletter.
Why am I looking for content creators?
In the past few months, I haven't been allocating as much time to the newsletter (you
may have noticed they're coming out every other week now). There have been many
other things that seem more important to do. This is both because I'm more optimistic
about the other work I'm doing, and because I no longer ﬁnd it as useful to read
papers as I did when I started the newsletter. As a result, I now have over 100 articles
that I would probably want to send out, but haven't gotten around to yet. This is also
partly because there's just more stuﬀ coming out now. (I mentioned some of these
points in the retrospective.)

Another reason for more content creators is that as I have learned more since starting
the newsletter, I have developed my own idiosyncratic beliefs, and I think I have
become worse at intuitively interpreting other posts from the author's perspective
rather than my own. (In other words, I would perform worse at an Ideological Turing
Test of their position than I would have in the past, unless I put in a lot of eﬀort into it.)
I expect that with more writers the newsletter will better reﬂect a diversity of opinions.
Why should you do it?
It's impactful. See the retrospective for more on this point. I'm not currently able to
get a (normal length) newsletter out every week; you'd likely be causally responsible
for getting back to weekly newsletters.
You will improve your analytical writing skills. Hopefully clear.
You'll learn more about safety by reading papers. You could do this by yourself,
but by summarizing the papers, you're also providing a valuable service for everyone
else.
You might learn more about AI safety by getting feedback from me. This is a
"might" because I don't know how much feedback I will end up giving to you about
your summaries and opinions that's actually about key ideas in AI safety (as opposed
to feedback about the writing itself).
You might build career capital. I certainly have built career capital by creating this
newsletter -- it has made me well known in some communities. I don't know to what
extent this will transfer to you.
You might be paid. Currently this is experimental, so I haven't actually thought
much about payment. I expect that I could get a grant to pay you if I ended up
deciding that it would be worth it. However, it might be that dealing with all of the
paperwork + tax implications cancels out any time savings, though I think this is
unlikely. If this is an important factor to you, please do let me know when you apply.
Qualiﬁcations
Likely to contribute at least 20 summaries to the newsletter over time, at least 4
of which are in the ﬁrst month (for onboarding purposes). Alternatively, if you
have deep expertise in a topic that the newsletter covers infrequently, such as
formal veriﬁcation, you should be likely to summarize relevant papers for at
least the next 6 months.
Basic familiarity with AI safety arguments
Medium familiarity with the topic that you want to write summaries about
Good writing skills (though I recommend just applying regardless and letting me
evaluate based on your example summary)
Application process
Fill out this form. The main part of the application is to write an example summary and
opinion for an article (which I may send out in the newsletter, if you give me
permission to). Ideally you would write a summary on one of the articles from the list
below, but if there isn't an article in the subarea you'd like to write on, you can choose

some other article (that hasn't already been summarized in the newsletter) and
summarize that. The whole process should take 1-4 hours, depending on how much
time you put into the summary and opinion.
List of articles:
Aligning a toy model of optimization
Four Ways An Impact Measure Could Help Alignment
AI-GAs: AI-generating algorithms, an alternate paradigm for producing general
artiﬁcial intelligence
Learning to Interactively Learn and Assist
Natural Adversarial Examples
On Inductive Biases in Deep Reinforcement Learning

September Bragging Thread
Thought I'd try reviving an old LessWrong experiment: the Bragging Thread. (See this
old one for some context)
LessWrong mostly rewards people for writing blogposts. That's cool and all, but is not
the only thing worth doing. What's an awesome thing you've done lately?
You are encouraged to be as blatantly proud as you inwardly feel. :)
(Note that this is not a thread for progress, or for things you might do. This is for
celebrating things you have actually done lately)

Schelling Categories, and Simple
Membership Tests
Followup to: Where to Draw the Boundaries?
Or there might be social or psychological forces anchoring word usages on identiﬁable
Schelling points that are easy for diﬀerent people to agree upon, even at the cost of some
statistical "ﬁt" ...
The one comes to you and says, "That paragraph about Schelling points sounded
interesting. What did you mean by that? Can you give an example?"
Sure. Previously on Less Wrong, in "The Univariate Fallacy", we studied points sampled from
two multivariate probability distributions PA and PB, and showed that it was possible to infer
with very high probability which distribution a given point was sampled from, despite
signiﬁcant overlap in the marginal distributions for any one variable considered individually.
From the standpoint of "the way to carve reality at its joints, is to draw your boundaries
around concentrations of unusually high probability density in Thingspace", the correct
categorization of the points in that example is clear. We have two clearly distinguishable
clusters. The conditional independence property is satisﬁed: given a point's cluster-
membership, knowing one of the xi doesn't tell you anything about xj for j ≠ i. So we should
draw a category boundary around each cluster. Obviously. We might ask hypophorically:
what could possibly change this moral?
More constraints on the problem, that's what!
Suppose you needed to coordinate with someone else to make decisions about these points
—that is, it's important not just that you and your partner make good decisions, but also
that you make the same decision—but that each of you only got to observe one coordinate
from each point. As we saw, the predictive work we get from category-membership in this
scenario is spread across many variables: if you only get to observe a few dimensions, you
have a lot of uncertainty about cluster-membership (which carries over into additional
uncertainty about the other dimensions that you haven't observed, but which aﬀect the ex
post quality of your decision).
If you and your partner were both ideal Bayesian calculators who could communicate
costlessly, you would share your observations, work out the correct probability, and use
that to make optimal decisions. But suppose you couldn't do that—either because
communication is expensive, or your partner was bad at math, or any other reason. Then it
would be sad if you happened to see x9 = 2 and said "It's an A (probably)!", and your
partner happened to see x27 = 3 and said "It's a B (I think)!", and the two of you made
inconsistent decisions.
Okay, now suppose that there's actually a forty-ﬁrst, binary, variable that I didn't tell you
about earlier, distributed like so:
PA(x41) = {
3/4
x41 = 0
1/4
x41 = 1

PB(x41) = {
1/4
x41 = 0
3/4
x41 = 1
Observing x41 gives you log2 3 ≈ 1.585 bits of evidence about cluster-membership, which is
more than the
⋅| log2(4)| +
⋅| log2(7/4)| +
⋅| log2(4/7)| +
⋅| log2(4)|
≈ 1.18 bits you can get from any one observation of one of the xi for i ∈ {1...40}.
If you and your partner can both observe x41, you might end up wanting to base your
shared categories and language on that—calling a point an "A" if it has x41 = 0, even
though such points actually came from PB a full quarter of the time—even if x41 itself has
no eﬀect on the quality of your decisions, and what you actually care about is wholely
determined by the values of x1 through x40! It's not the intension you would pick if you
could make (and share) more observations—but ex hypothesi, you can't.
If you and your partner only get to observe one variable, x41 is your best choice—the single
variable that gives you the most information about the "natural" cluster-membership. That
also makes it a Schelling point—if you and your partner didn't get to commmunicate in
advance about how you want to draw your shared category boundaries, you could pick x41
as your deﬁning observation and be pretty conﬁdent your partner would make the same
choice. We could imagine an even more pessimistic scenario in which the Schelling point
category deﬁnition (a set of variables that "stuck out" from all the others) was less
predictive than some other candidates—but if you couldn't coordinate to pick one of the
more predictive category systems, you might be stuck with the Schelling point.
In conclusion, the right categories to use given constraints on communication and
observation, might be diﬀerent from the category boundaries you would draw from a "God's
eye view", in part because consideration of which categories are easy for diﬀerent agents
to coordinate on is relevant, not just raw information-theoretic expressive power. Thus,
"Schelling categories."
Thanks for reading!
The one says, "No, I meant, like, a real world example, not some dumb math thing for
nerds. What is this post really about?"
It's about ... math? Or like, the relationship between math and human natural language?
Like, I was wondering what "second-order" caveats or complications there might be to the
basic "carve reality at the joints" moral of our standard Bayesian philosophy of language,
and some of the people I've been collaborating with lately had been talking a lot about the
importance of intersubjective epistemology—that is, shared mapmaking, so—
"But where's the actionable takeaway? What's your real agenda here, huh?"
Oh. One of those readers, I see. Fine, I can probably think of some—how do you say?
—"applications."
1/4 + 1/16
2
7/16 + 1/4
2
1/4 + 7/16
2
1/16 + 1/4
2

Ummmm ...
Let's see ...
Okay, here's something, maybe. What's the deal with the age of majority?
Society needs to decide who it wants to be allowed to vote, stand trial, sign contracts,
serve in the military, &c. Whether it's a good idea for a particular person to have these
privileges presumably depends on various relevant features of that person: things like
cognitive ability, foresight, wisdom, relevant life experiences, &c. In particular, it would be
pretty weird for someone's ﬁtness to vote to directly depend on how many times the Earth
has gone around the sun since they were born. What does that number have to do with
anything?
It doesn't! But if Society isn't well-coordinated enough to agree on the exact prerequisites
for voting and how to measure them, but can agree that most twenty-ﬁve-year-olds have
them and most eleven-year-olds don't, then we end up choosing some arbitrary age cutoﬀ
as the criterion for our "legal adulthood" social construct. It works, but it's just a legal
ﬁction—and not necessarily a particularly good ﬁction, as any bright teenagers reading this
will doubtlessly attest.
If I told you that a particular fourteen-year-old was very "mature", that's a contentful
statement: we have shared meaning attached to the word mature, such that my describing
someone that way constrains your anticipations. But it's a really complicated meaning, a
statistical signal in behavior that your brain can pick up on, but which isn't particularly
veriﬁable to others who might have reasons to doubt my character assessment. In contrast,
age is easy for everyone to agree on. We could imagine some hypothetical science-ﬁctional
Society that used brain scans and some sophisticated machine-learning classifer to
determine which citizens get which privileges—but in our dumber, poorer world, calendars
and subtraction will have to do.
In terms of Scott Garrabrant's taxonomy of applications of Goodhart's law, this is
regressional Goodhart: Society wants to select for maturity, chooses age as a proxy, and in
the process, ends up granting or withholding privileges that a more discriminating Society
maybe wouldn't.
The age of majority is a case of replacing a complicated, illegible category ("maturity", the
kind of abstract thing you might want to model as a cluster in a forty- or forty-one-
dimensional space) with a simple membership test (an age cutoﬀ that everyone knows how
to compute). Diﬀerent people might make make diﬀerent subjective (but not arbitrary)
judgements of the complicated, illegible category, so in order to get a more
intersubjectively robust verdict on category-membership, we rely on an objective
measurement that everyone can agree on.
If no convenient objective measurement is available, another strategy is possible: we can
delegate to some canonical trusted authority, whose opinion of the complicated category
will take precdence over everyone else's. An example of this is commodity grading
standards. What is a "Grade AA" egg? Well, there's a complicated deﬁnition written down in
a manual somewhere that you could try applying yourself—but for most people, Grade AA
eggs are simply "those which have been certiﬁed as Grade AA by the USDA."[1]
It's even possible for the "simple objective measurement" and "delegate to an authority's
subjective judgement" strategies to be combined. In "The Ideology Is Not the Movement",
the immortal Scott Alexander writes about his model of the genesis of social groups—
Pre-existing diﬀerences are the raw materials out of which tribes are made. A good
tribe combines people who have similar interests and styles of interaction even before
the ethnogenesis event. Any description of these diﬀerences will necessarily involve

stereotypes, but a lot of them should be hard to argue. [...] There are subtle habits of
thought, not yet described by any word or sentence, which atheists are more likely to
have than other people. [...]
The rallying ﬂag is the explicit purpose of the tribe. It's usually a belief, event, or
activity that get people with that speciﬁc pre-existing diﬀerence together and excited.
Often it brings previously latent diﬀerences into sharp relief. People meet around the
rallying ﬂag, encounter each other, and say "You seem like a kindred soul!" or "I
thought I was the only one!" Usually it suggests some course of action, which provides
the tribe with a purpose.
Eliezer Yudkowsky's "A Fable of Science and Politics" depicts a ﬁctional underground society
split between two such tribes: an predominantly urban tribe that believes that the unseen
sky is blue (and favors an income tax, strong marriage laws, and an Earth-centric
cosmology), and predominanty rural one that believes that the sky is green (and favors
merchant taxes, no-fault divorce, and a heliocentric cosmology). In this story, beliefs about
the color of the sky are functioning as the "rallying ﬂag" for tribe-formation in Alexander's
model—and as a Schelling point for category deﬁnition.
We don't know how to talk about the preëxisting undeﬁnable habits of thought that make
social groups work—it's hard to explicitly articulate what exact statistical regularity our
brains have detected in ﬁve-and-more-dimensional locale/sky-belief/tax-belief/divorce-
belief/cosmology/&c.-space. (Although we could imagine some hypothetical science-
ﬁctional Society that did know how to articulate it, and consequently had richer forms of
social and political organization than our own.) It's a lot simpler to talk about whether
someone has pledged allegiance to the rallying ﬂag: just ask someone, "What color do you
believe the sky is?" (using sky-beliefs as as an "objective" simple membership test), or
simply, "Are you a Blue or a Green?" (delegating the classiﬁcation problem to the person
themselves as the authority whose discernment is to be trusted)—and whatever they say,
that's what they are.
Well, probably. We've seen that objective measurements like age are subject to
regressional Goodhart, but the delegation-to-authority strategy is furthermore subject to
adversarial Goodhart: once a category-membership test has been established, some agents
might have an incentive to create examples that pass the test, but don't have the
complicated, illegible properties than made the test a useful proxy in the ﬁrst place.
We've seen this, for example, with title inﬂation: we expect the "job title" (the words that
get printed on business cards or immigration sponsorship forms) to be the canonical
description of what someone "does", even if the vagaries of the workday encompass many
tasks,[2] and an alien anthropologist tasked with observing the worksite and summarizing
what each of the humans did might slice up her observations into categories with little
resemblance to the company's formal org chart. But since we don't know how to do the
obvious thing and average over all possible alien anthropologists weighted by simplicity,
we can only rely on the org chart—which people have political incentives to manipulate,
with the result that everyone in the ﬁnance industry is a "vice president" of some sort or
another.
But "Vice President" has a literal meaning. Or it used to. Vice, "in place of; subordinate to."
President, one who presides over some deliberative body. The adversarial-Goodhart
pressures on language "exploit[ ] the trust we have in a functioning piece of language until
it's lost all meaning".
So for readers who demand a takeaway beyond just an edge case in the math, perhaps
take away this: coordination is costly. From the standpoint of language as an AI capability,
the social constructions that feeble humans need in order to work together may be

unavoidably dumbed-down for mass consumption, but that's no reason to not aspire to the
true precision of the Bayes-structure to whatever extent possible.
(Thanks to Ben Hoﬀman for the etymology of "Vice President.")
1. Or the analogous agency in your country. ↩ 
2. When I worked in a supermarket, two days a week I did Tracy's
bookkeeping/customer-service job while Tracy had her weekend, which entailed
counting the money from last night's tills and swapping in new coinmags and
completing the FSM report and answering the phone and selling money orders and
covering the ﬂoral stand when the ﬂoral lady was on lunch, &c. I'm actually not sure
what oﬃcial name this role had in Safeway's oﬃcial org chart. We just called it "the
booth." ↩ 

Understanding understanding
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
How does 'understanding' a problem work, and when do we feel like we understood an
explanation or proof? Having an opaque long formal proof often feels insuﬃcient,
similarly some arguments feel unsatisfying because they contain many subjectively
arbitrary choices.
An explanation is a sequence of claims corroborating a statement. A reasoner
understands an explanation (or rather has more understanding of an explanation)
when each claim has low surprisal given its mental state after reading all of the
previous statements, starting with the problem statement. The aggregate of the
surprise over the entire explanation indicates how poorly understood it is. The
measure of surprisal is essentially about the reconstructability of the explanation
using something like Monte Carlo Tree Search over all possible explanations informed
by the mental states of the reasoner.
The surprisal of claims is roughly how long it would take for the reasoner to come up
with the claim given its current mental state. Since the space of such claims is
exponential in the length of the claim the reasoner has to use some form of attention
to guide its search. We can model this attention mechanism by an ontological graph.
Such a graph encodes the collection of mental concepts and associative links between
them. The mental state of the reasoner is an activation of an ensemble of concepts
and the associative links make available other concepts, lowering their surprisal when
invoked in the next step of the explanation.
When a step in an explanation is highly surprising some understanding is needed. The
reasoner does this by modifying its ontology, creating new concepts or creating
associations that make the step more obvious. I call such modiﬁcations insights, a
good insight gives clarity and makes steps obvious and subjectively trivial.
To examine this situation consider the mutilated chessboard problem[1] and
subsequent explanation:
Suppose a standard 8×8 chessboard has two diagonally opposite corners
removed, leaving 62 squares. Is it possible to place 31 dominoes of size 2×1 so as
to cover all of these squares?
The answer that it is impossible. Opposite corner squares always have the same
color so that there are 30 and 32 white or black squares left. A domino placed on
the chessboard will always cover one white square and one black square. So there
will always be either two white or two black squares left which can not be covered
by a domino.
The problem becomes trivial after making available the idea of attaching the invariant
to the problem. We can imagine that the color invariant is activated by the joint
activation of the domino and chess nodes in the reasoners ontology.
In this scheme there is nothing stopping a reasoner from encoding solutions to all

problems directly into its ontology. This leads to 1) poor generalization/transfer; and
2) a large ontology.
Compression deals with both of these issues, indeed compression is closely related to
generalization. General compression works by recognizing and exploiting all
detectable patterns with certain computational bounds. Once these patterns are
detected they can be extract and recognized in diﬀerent contexts, which is
generalization.[2]
Compressing the ontology has the same general shape as understanding
explanations, except this time we want to understand all elements in our ontology
given the rest of the ontology. This is related to drop-out: how surprised would I be of
this ontological node given that it was not in my ontology. Compressing of the
ontology has the nice feature that the ontology becomes more fault tolerant: things
that are forgotten but once well understood can be reconstructed from context.
In the example of the mutilated chessboard we can explain the insight by noting that
is a general instance of attaching invariant to problems.
I believe there is both pressure to prune nodes with low surprisal (beliefs that do not
pay rent) and low activation rates. Very low surprisal nodes can be safely pruned as
they are reconstructible from context. On the other hand, nodes with very high
surprisal are hard to recall (i.e. get activated) and will also be pruned. This explains
why opaque formal proofs don't feel like satisfying explanations, it's hard to ﬁt them
into our information retrieval indices.
[1]: https://en.wikipedia.org/wiki/Mutilated_chessboard_problem
[2]: Compression in the colloquial sense only exploits patterns with very low
computational complexity

Subagents, neural Turing machines,
thought selection, and blindspots
In my summary of Consciousness and the Brain (Dehaene, 2014), I brieﬂy mentioned
that one of the functions of consciousness is to carry out artiﬁcial serial operations; or
in other words, implement a production system (equivalent to a Turing machine) in the
brain.
While I did not go into very much detail about this model in the post, I've used it in
later articles. For instance, in Building up to an Internal Family Systems model, I used
a toy model where diﬀerent subagents cast votes to modify the contents of
consciousness. One may conceptualize this as equivalent to the production system
model, where diﬀerent subagents implement diﬀerent production rules which compete
to modify the contents of consciousness.
In this post, I will ﬂesh out the model a bit more, as well as applying it to a few other
examples, such as emotion suppression, internal conﬂict, and blind spots.
Evidence accumulation
Dehaene has outlined his model in a pair of papers (Zylberberg, Dehaene, Roelfsema,
& Sigman, 2011; Dehaene & Sigman, 2012), though he is not the ﬁrst one to propose
this kind of a model. Daniel Dennett's Consciousness Explained (1991) also discusses
consciousness as implementing a virtual Turing machine; both cite as examples earlier
computational models of the mind, such as Soar and ACT, which work on the same
principles.
An important building block in Dehane's model is based on what we know about
evidence accumulation and decision-making in the brain, so let's start by taking a look
at that.
Sequential sampling models (SSMs) are a family of models from mathematical
psychology that have been developed since the 1960s (Forstmann, Ratcliﬀ, &
Wagenmakers, 2016). A particularly common SSM is the diﬀusion decision model
(DDM) of decision-making, in which a decision-maker is assumed to noisily accumulate
evidence towards a particular choice. Once the evidence in favor of a particular choice
meets a decision threshold, that choice is taken.
For example, someone might be shown dots on a screen, some of which are moving in
a certain direction. The task is to tell which direction the dots are moving in. After the
person has seen enough dot movements, they will have suﬃcient conﬁdence to make
their judgment. The diﬃculty of the task can be precisely varied by changing the
proportion of moving dots and their speed, making the movement easier or harder to
detect. One can then measure how such changes aﬀect the time needed for people to
make a judgment. 

A DDM is a simple model with just four parameters:
decision threshold: a threshold for the amount of evidence in favor of one option
which causes that option to be chosen
starting point bias: a person may start biased towards one particular alternative,
which can be modeled by them having some initial evidence putting them closer
to one threshold than the other
drift rate: the average amount of evidence accumulated per time unit
non-decision time: when measuring e.g. reaction times, a delay introduced by
factors such as perceptual processing which take time but are not involved in
the decision process itself
These parameters can be measured from behavioral experiments, and the model
manages to ﬁt a wide variety of behavioral experiments and intuitive phenomena well
(Forstmann et al., 2016; Ratcliﬀ, Smith, Brown, & McKoon, 2016; Roberts &
Hutcherson, 2019). For example, easier-to-perceive evidence in favor of a particular
option is reﬂected in a faster drift rate towards the decision threshold, causing faster
decisions. On the other hand, making mistakes or being falsely told that one's
performance on a trial is below that of most other participants prompts caution,
increasing people's decision thresholds and slowing down response times (Roberts &
Hutcherson, 2019).
While the models have been studied the most in the context of binary decisions, one
can easily extend the model to a choice between n alternatives by assuming the
existence of multiple accumulators, each accumulating decision towards their own
choice, possibly inhibiting the others in the process. Neuroscience studies have
identiﬁed structures which seem to correspond to various parts of SSMs. For example,
in random dot motion tasks, where participants have to indicate the direction that
dots on a screen are moving in,
the ﬁring rates of direction selective neurons in the visual cortex (area MT/V5)
exhibit a roughly linear increase (or decrease) as a function of the strength of
motion in their preferred (or anti-preferred) direction. The average ﬁring rate from
a pool of neurons sharing similar direction preferences provides a time varying
signal that can be compared to an average of another, opposing pool. This

diﬀerence can be positive or negative, reﬂecting the momentary evidence in favor
of one direction and against the other. (Shadlen & Shohamy, 2016)
Shadlen & Shohamy (2016) note that experiments on more "real-world" decisions,
such as decisions on which stock to pick or which snack to choose, also seem to be
compatible with an SSM framework. However, this raises a few questions. For
instance, it makes intuitive sense why people would take more time on a random
motion task when they lose conﬁdence: watching the movements for a longer time
accumulates more evidence for the right answer, until the decision threshold is met.
But what is the additional evidence that is being accumulated in the case of making a
decision based on subjective value?
The authors make an analogy to a symbol task which has been studied in rhesus
monkeys. The monkeys need to decide between two choices, one of which is correct.
For this task, they are shown a series of symbols, each of which predicts one of the
choices as being correct with some probability. Through experience, the monkeys
come to learn the weight of evidence carried by each symbol. In eﬀect, they are
accumulating evidence not by motion discrimination but memory retrieval: retrieving
some pre-learned association between a symbol and its assigned weight. This "leads
to an incremental change in the ﬁring rate of LIP neurons that represent the
cumulative [likelihood ratio] in favor of the target".
The proposal is that humans make choices based on subjective value using a similar
process: by perceiving a possible option and then retrieving memories which carry
information about the value of that option. For instance, when deciding between an
apple and a chocolate bar, someone might recall how apples and chocolate bars have
tasted in the past, how they felt after eating them, what kinds of associations they
have about the healthiness of apples vs. chocolate, any other emotional associations
they might have (such as fond memories of their grandmother's apple pie) and so on.
Shadlen & Shohamy further hypothesize that the reason why the decision process
seems to take time is that diﬀerent pieces of relevant information are found in
physically disparate memory networks and neuronal sites. Access from the memory
networks to the evidence accumulator neurons is physically bottlenecked by a limited
number of "pipes". Thus, a number of diﬀerent memory networks need to take turns
in accessing the pipe, causing a serial delay in the evidence accumulation process.

The biological Turing machine
In Consciousness and the Brain, Dehaene considers the example of doing arithmetic.
Someone who is calculating something like 12 * 13 in their head, might ﬁrst multiply
10 by 12, keep the result in memory, multiply 3 by 12, and then add the results
together. Thus, if a circuit in the brain has learned to do multiplication, consciousness
can be used to route its results to a temporary memory storage, with those results
then being routed from the storage to a circuit that does addition.
Production systems in AI are composed of if-then rules (production rules) which modify
the contents of memory: one might work by detecting the presence of an item like "10
* 12" and rewriting it as "120". On a conceptual level, the brain is proposed to do
something similar: various contents of consciousness activate neurons storing
something like production rules, which compete to ﬁre. The ﬁrst one to ﬁre gets to
apply its production, changing the contents of consciousness.
If I understand Deheane's model correctly, he proposes to apply the neural
mechanisms discussed in the previous sections - such as neuron groups which
accumulate evidence towards some kind of decision - at a slightly lower level. In the
behavioral experiments, there are mechanisms which accumulate evidence towards
which particular physical actions to take, but a person might still be distracted by
unrelated thoughts while performing that task. Dehaene's papers look at the kinds of
mechanisms choosing what thoughts to think. That is, there are accumulator neurons
which take "actions" to modify the contents of consciousness and working memory.
We can think of this as a two-stage process:

1. A process involving subconscious "decisions" about what thoughts to think, and
what kind of content to maintain in consciousness. Evidence indicating the kind
of conscious content is most suited for the situation is in part based on
hardwired priorities, and in part stored associations about the kinds of thoughts
that previously produced beneﬁcial results.
2. A higher-level process involving decisions about what physical actions to take.
While the inputs to this process do not necessarily need to go through
consciousness, consciously perceived evidence has a much higher weight. Thus,
the lower-level process has signiﬁcant inﬂuence on which evidence gets to the
accumulators on this level.
To be clear, this does not necessarily correspond to two clearly distinct levels:
Zylberberg, Dehaene, Roelfsema, & Sigman (2011) do not talk about there being any
levels, and they suggest that "triggering motor actions" is one of the possible
decisions involved. But their paper seems to mostly be focused on actions - or, in their
language, production rules - which manipulate the contents of consciousness.
There seems to me to be a conceptual diﬀerence between the kinds of actions that
change the contents of consciousness, and the kinds of actions which accumulate
evidence over many items in consciousness (such as iterative memories of snacks).
Zylberberg et al. talk about a "winner-take-all race" to trigger a production rule, which
to me implies that the evidence accumulated in favor of each production rule is
cleared each time that the contents of consciousness is changed. This is seemingly
incompatible with accumulating evidence over many consciousness-moments, so
postulating a two-level distinction between accumulators seems like a straightforward
way of resolving the issue.
[EDIT: Hazard suggests that the two-level split is implemented by the basal ganglia
carrying out evidence accumulation across changes in conscious content.]
As an aside, I am, as Dehaene is, treating consciousness and working memory as
basically synonymous for the purposes of this discussion. This is not strictly correct;
e.g. there may be items in working memory which are not currently conscious.
However, since it's generally thought that items in working memory need to be
actively rehearsed through consciousness in order to avoid be maintained, I think that
this equivocation is okay for these purposes.
Here's a conceptual overview of the stages in the "biological Turing machine's"
operation (as Zylberberg et al. note, a production ﬁring "is essentially equivalent to
the action performed by a Turing machine in a single step"):
1. The production selection stage

At the beginning of a cognitive cycle, a person's working memory contains a number
of diﬀerent items, some internally generated (e.g. memories, thoughts) and some
external (e.g. the sight or sound of something in the environment). Each item in
memory may activate (contribute evidence to) neurons which accumulate weight
towards triggering a particular kind of production rule. When some accumulator
neurons reach their decision threshold, they apply their associated production rule.
In the above image, the blue circles at the bottom represent active items in working
memory. Two items are activating the same group of accumulator neurons (shown red)
and one is activating an unrelated one (shown brown).
2. Production rule ignition
Once a group of accumulator neurons reach their decision threshold and ﬁre a
production rule, the model suggests that there are a number of things that the rule
can do. In the above image, an active rule is modifying the contents of working
memory: taking one of the blue circles, deleting it, and creating a new blue circle
nearby. Hypothetically, this might be something like taking the mental objects holding
"120" and "36", adding them together, and storing the output of "156" in memory.
Obviously, since we are talking about brains, expressions like "writing into memory" or
"deleting from memory" need to be understood in somewhat diﬀerent terms than in

computers; something being "deleted from working memory" mostly just means that
a neuronal group which was storing the item in its ﬁring pattern stops doing so.
The authors suggest that among other things, production rules can:
trigger motor actions (e.g. saying or doing something)
change the contents of working memory to trigger a new processing step (e.g.
saving the intermediate stage of an arithmetic operation, together with the
intention to proceed with the next step)
activate and broadcast information that is in a "latent" state (e.g. retrieving a
memory and sending it to consciousness)
activate peripheral processors capable of performing speciﬁc functions (e.g.
changing the focus of attention)
3. New production selection
After the winning production rule has been applied, the production selection phase
begins anew. At this stage or a future one, some kind of a credit assignment process
likely modiﬁes the decision weights involved in choosing production rules: if a
particular rule was activated in particular circumstances and seemed to produce
positive consequences, then the connections which caused those circumstances to be
considered evidence for that rule are strengthened.
Practical relevance
Okay, so why do we care? What is the practical relevance of this model?
First, this helps make some of my previous posts more concrete. In Building up to an
Internal Family Systems model, I proposed some sort of a process where diﬀerent
subagents were competing to change the contents of consciousness. For instance,
"manager" subagents might be trying to manipulate the contents of consciousness so
as to avoid unpleasant thoughts and to keep the person out of dangerous
circumstances.
People who do IFS, or other kinds of "parts work", will notice that diﬀerent subagents
are associated with diﬀerent kinds of bodily sensations and ﬂavors of consciousness. A

priori, there shouldn't be any particular reason for this... except, perhaps, if the
strength of such sensations correlated with the activation of a particular subagent,
with those sensations then being internally used for credit assignment to identify and
reward subagents which had been active in a given cognitive cycle. (This is mostly
pure speculation, but supported by some observations to which I hope to return in a
future post.)
In my original post, I mostly talked about exiles - neural patterns blocked from
consciousness by other subagents - as being subagents related to a painful memory.
But while it is not emphasized as much, IFS holds that other subagents can in principle
be exiled too. For example, a subagent which tends to react using anger may
frequently lead to harmful consequences, and then be blocked by other subagents.
This can easily be modeled using the neural Turing machine framework: over time, the
system learns decisions which modify consciousness so as to prevent the activation of
a production rule that gives power to the angry subagent. As this helps avoid harmful
consequences, this begins to happen more and more often.
Hazard has a nice recent post of this kind of a thing happening with emotions in
general:
So young me is upset that the grub master for our camping trip forgot half the
food on the menu, and all we have for breakfast is milk. I couldn't "ﬁx it" given
that we were in the woods, so my next option was "stop feeling upset about it." So
I reached around in the dark of my mind, and Oops, the "healthily process
feelings" lever is right next to the "stop listening to my emotions" lever.
The end result? "Wow, I decided to stop feeling upset, and then I stopped feeling
upset. I'm so fucking good at emotional regulation!!!!!"
My model now is that I substituted "is there a monologue of upsetness in my
conscious mental loop?" for "am I feeling upset?". So from my perspective, it just
felt like I was very in control of my feelings. Whenever I wanted to stop feeling
something, I could. When I thought of ignoring/repressing emotions, I imagined
trying to cover up something that was there, maybe with a story. Or I thought if
you poked around ignored emotions there would be a response of anger or
annoyance. I at least expected that if I was ignoring my emotions, that if I got very
calm and then asked myself, "Is there anything that you're feeling?" I would get
an answer.
Again, the assumption was, "If it's in my mind, I should be able to notice if I look."
This ignored what was actually happening, which was that I was cutting the phone
lines so my emotions couldn't talk to me in the ﬁrst place.
Feeling upset feels bad, ceasing to feel upset feels good. Brain notices that there is
some operation which causes the feeling of upset to disappear from consciousness:
carrying out this operation also produces a feeling of satisfaction in the form of "yay,
I'm good at emotional regulation!". As a result of being rewarded, it eventually
becomes so automatic as to block even hints of undesired emotions, making the block
in question impossible to notice.
Another observation is that in IFS as well as in Internal Double Crux, an important
mental move seems to be "giving subagents a chance to ﬁnish talking". For instance,
subagent A might hold a consideration pointing in a particular direction, while
subagent B holds a consideration in the opposite direction. When A starts presenting
its points, B interrupts with its own point; in response, A interrupts with its point. It

seems to be possible to commit to not taking a decision before having heard both
subagents, and having done that, ask them to take turns presenting their points and
not interrupt each other. What exactly is going on here?
Suppose that a person is contemplating the decision, "should I trust my friend to have
my back in a particular risky venture". Subagent A holds the consideration "allies are
important, and we don't have any, we should really trust our friend so that we would
have more allies". Subagent B holds the consideration "being betrayed would be
really bad, and our friend seems untrustworthy, it's important that we don't sign up
for this". Subagent A considers it really important to go on this venture together;
subagent B considers it really important not to.
Recall that human decision-making happens by accumulating evidence towards
diﬀerent choices, until a decision threshold is met. If A were allowed to present its
evidence in favor of signing up on the venture, that might sway the decision over the
threshold before B was able to present the evidence against. Thus, there is a
mechanism which allows B to "interrupt" A, in order to present its own evidence.
Unfortunately, it is now A which risks B's evidence being suﬃcient to meet a decision
threshold prematurely unless B is prevented from presenting its evidence, so A must
interrupt.
Subjectively, this is experienced as intense internal conﬂict, with two extreme
considerations pushing in opposite directions, allowing no decision to be made -
unless there is a plausible commitment to not making a decision until both have been
heard out. (To me, this feels like my attention being caught in a tug-of-war between
one set of considerations versus another. Roberts & Hutcherson (2019) note that A
large body of work suggests that negative information draws focus through rapid
detection [64-68] and attentional capture [69-71]. [....] Several studies now show that
attending to a choice alternative or attribute increases its weighting in the evidence
accumulation process [72-75]. To the extent that negative aﬀect draws attention to a
choice-relevant attribute or object, it should thus increase the weight it receives.)
There's one more important consideration. Eliezer has written about cached thoughts
- beliefs which we have once acquired, then never re-evaluated and just acted on
them from that onwards. But this model suggests that things may be worse: it's not
just that we are running on cached thoughts. Instead, even the pre-conscious
mechanisms deciding which thoughts are worth re-evaluating are running on cached
values.
Sometimes external evidence may be suﬃcient to force an update, but there can also
be self-fulﬁlling blind spots. For instance, you may note that negative emotions never
even surface into your consciousness. This observation then triggers a sense of
satisfaction about being good at emotional regulation, so that thoughts about
alternative - and less pleasant - hypotheses are never selected for consideration. In
fact, evidence to the contrary may feel actively unpleasant to consider, triggering
subagents which use feelings such as annoyance - or if annoyance would be too
suspicious, just plain indiﬀerence - to push that evidence out of consciousness, before
it can contribute to a decision.
And the older those ﬂawed assumptions are, the more time there is for additional
structures to build on top of them.
This post is part of research funded by the Foundational Research Institute. Thanks to
Maija Haavisto for line editing an initial draft.

References
Dehaene, S. (2014). Consciousness and the Brain: Deciphering How the Brain Codes
Our Thoughts. New York, New York: Viking.
Dehaene, S., & Sigman, M. (2012). From a single decision to a multi-step algorithm.
Current Opinion in Neurobiology, 22(6), 937-945.
Dennett, D. C. (1991). Consciousness Explained (1st edition). Boston: Little Brown &
Co.
Forstmann, B. U., Ratcliﬀ, R., & Wagenmakers, E.-J. (2016). Sequential Sampling
Models in Cognitive Neuroscience: Advantages, Applications, and Extensions. Annual
Review of Psychology, 67, 641-666.
Ratcliﬀ, R., Smith, P. L., Brown, S. D., & McKoon, G. (2016). Diﬀusion Decision Model:
Current Issues and History. Trends in Cognitive Sciences, 20(4), 260-281.
Roberts, I. D., & Hutcherson, C. A. (2019). Aﬀect and Decision Making: Insights and
Predictions from Computational Models. Trends in Cognitive Sciences, 23(7), 602-614.
Shadlen, M. N., & Shohamy, D. (2016). Decision Making and Sequential Sampling from
Memory. Neuron, 90(5), 927-939.
Zylberberg, A., Dehaene, S., Roelfsema, P. R., & Sigman, M. (2011). The human Turing
machine: a neural framework for mental programs. Trends in Cognitive Sciences,
15(7), 293-300.

[Link] Book Review: Reframing
Superintelligence (SSC)
https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/
(archive)
Drexler asks: what if future AI looks a lot like current AI, but better?
For example, take Google Translate. A future superintelligent Google Translate
would be able to translate texts faster and better than any human translator,
capturing subtleties of language beyond what even a native speaker could pick
up. It might be able to understand hundreds of languages, handle complicated
multilingual puns with ease, do all sorts of amazing things. But in the end, it would
just be a translation app. It wouldn't want to take over the world. It wouldn't even
"want" to become better at translating than it was already. It would just translate
stuﬀ really well.
...
In this future, our AI technology would have taken the same path as our physical
technology. The human body can run fast, lift weights, and ﬁght oﬀ enemies. But
the automobile, crane, and gun are three diﬀerent machines. Evolution had to
cram running-ability, lifting-ability, and ﬁghting-ability into the same body, but
humans had more options and were able to do better by separating them out. In
the same way, evolution had to cram book-writing, technology-inventing, and
strategic-planning into the same kind of intelligence - an intelligence that also has
associated goals and drives. But humans don't have to do that, and we probably
won't. We're not doing it today in 2019, when Google Translate and AlphaGo are
two diﬀerent AIs; there's no reason to write a single AI that both translates
languages and plays Go. And we probably won't do it in the superintelligent future
either. Any assumption that we will is based more on anthropomorphism than on a
true understanding of intelligence.
These superintelligent services would be safer than general-purpose
superintelligent agents. General-purpose superintelligent agents (from here on:
agents) would need a human-like structure of goals and desires to operate
independently in the world; Bostrom has explained ways this is likely to go wrong.
AI services would just sit around algorithmically mapping inputs to outputs in a
speciﬁc domain.
A takeaway:
I think Drexler's basic insight is that Bostromian agents need to be really diﬀerent
from our current paradigm to do any of the things Bostrom predicts. A paperclip
maximizer built on current technology would have to eat gigabytes of training

data about various ways people have tried to get paperclips in the past so it can
build a model that lets it predict what works. It would build the model on its
actually-existing hardware (not an agent that could adapt to much better
hardware or change its hardware whenever convenient). The model would have a
superintelligent understanding of the principles that had guided some things to
succeed or fail in the training data, but wouldn't be able to go far beyond them
into completely new out-of-the-box strategies. It would then output some of those
plans to a human, who would look them over and make paperclips 10% more
eﬀectively.
The very fact that this is less eﬀective than the Bostromian agent suggests there
will be pressure to build the Bostromian agent eventually (Drexler disagrees with
this, but I don't understand why). But this will be a very diﬀerent project from AI
the way it currently exists, and if AI the way it currently exists can be extended all
the way to superintelligence, that would give us a way to deal with hostile
superintelligences in the future.

Distance Functions are Hard
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
[Epistemic status: Describes a failed research approach I had a while ago, and my
only purpose here is to warn people oﬀ from that way of thinking. Every now and then
I see someone working on an AIS subproblem say "if only we had a distance function
for things in domain X", and my intuition is that they are probably doing a wrong-way
reduction. But I only mean this as a soft guideline, and I'm only somewhat conﬁdent in
my current thinking on this.]
~~~
Terminology: We use the terms distance or distance function to denote any function 
d : X × X →R≥0 that intuitively tells us how "dissimilar" any two members of a set X
are (regardless of the whether d is a metric).
Counterfactual Worlds
Consider the counterfactual "If Lincoln were not assassinated, he would not have been
impeached". If we would like to say this has a truth value, we need to imagine what
such a counterfactual world would have looked like: was it because Lincoln (somehow)
survived his wounds, John Wilkes Booth (somehow) missed, that the plot was
(somehow) discovered the day before, etc. Somehow, we must pick out the world that
is in some sense "closest" to our actual world, but it seems very diﬃcult to compare
any two such worlds in a principled way.
To formalize Functional Decision Theory (FDT), we likely need to have a better
understanding of counterfactuals, although even in restricted mathematical contexts,
we don't have a satisfactory understanding of why "If 0 = 1..." simply returns
incoherence, yet "If the Modularity Theorem were false..." seemingly conjures up a
possible world that we feel we can reason about.
(Also, in terms of corrigibility, we are often interested in formalizing the notion of "low-
impact" agents, and the naive idea one often has is to deﬁne a distance metric on
counterfactual world-states, as in p. 5 of Concrete Problems in AI Safety).
Algorithmic Similarity
In the FDT framework, we do not view ourselves as a solitary agent, but as a function
(or algorithm) that can be copied, modiﬁed, and read, and we wish to maximize the
utility achieved by our algorithm. Minor details of our implementation that don't aﬀect
our behavior (such as whether we are written in Java or Python) should not be
decision-relevant, and if some algorithm does the same thing as us "most" of the
time, then we would probably (e.g.) want to cooperate with it in a Prisoner's Dilemma.
Deﬁning what it means for two algorithms to be similar remains an outstanding open
problem.

At MSFP 2018, a small group (4-5) of us tried tackling this for a couple hours, had a
few ideas that "felt" promising, but gradually realized that none of these made any
sense, until ultimately we gave up with the feeling that we hadn't made any
intellectual advances. I only say this to give outside-view evidence of intractability,
but it's diﬃcult for me to concisely communicate why its hard (I could say "try it
yourself for an hour and you'll see", but part of my point is that hour is better spent).
For those who insist on inside-view evidence, here's an outline of one of the ideas we
had and why it turned out to be unworkable:
We attempted to partition algorithm-space into equivalence classes that represent
"conceptual similarity", which should not be harder than deﬁning a distance function
on the space. By the Curry-Howard correspondence, we can rephrase this as asking
when two proofs are similar (this felt easier for us to think about, but that's entirely
subjective). Suppose we have some proof A of size n, and we want to ﬁnd proofs that
"don't use any fundamentally diﬀerent ideas". The obvious approach is to think of
which proofs we can get to with minor edits. If we make some edit of size ϵ⋅n for some
small ϵ and the result is still a valid proof, it should be more or less the same. If we
take the closure under minor edits that preserve validity, it would seem superﬁcially
plausible that this would result in proofs that are similar. However, suppose we
discover a one-line proof B that's totally diﬀerent from A: then we can append it to A
as a minor edit, then gradually delete A with minor edits, until we have a drastically
diﬀerent proof (among other complications).
Adversarial Examples
Given some data point x correctly classiﬁed by an ML model, a new point x′:=x+ϵ is
an adversarial example if it is now misclassiﬁed, despite only diﬀering from x by a tiny
amount ϵ (i.e. making relatively small RGB changes to a few pixels). For every state-
of-the-art image classiﬁer tested, if you give me:
Any image classiﬁed correctly by that model
Any target class you would like to have the model misclassify the image as
Then one can usually ﬁnd some small perturbation of that image that the model
believes is in the target class with high probability.
In the classic example we can have GoogLeNet classify a panda as a gibbon with 99%
conﬁdence. Moreover, these have been found to generalize very well across diﬀerent
models, even with very diﬀerent architectures. Last year, a paper came out taking this
further, by obtaining adversarial examples with the best cross-generalization, and
giving these to humans who had only a few seconds to classify the image.
Interestingly, the humans were "fooled" in the sense that their snap judgments--those
formed by their pure visual system--diﬀered from how they classiﬁed the images when
given more time for reﬂection. In terms of robustness to these examples, it seems, our
perceptual system by itself is not qualitatively better than today's classiﬁers, but our
lens can see its own ﬂaws.
The paper was popularized in various places under a bolder headline, namely that
there now existed full-blown adversarial examples for humans (reﬂection or not). This
was showcased with a picture from a diﬀerent part of the paper showing an image of a
(somewhat dog-like) cat being given a tiny amount of noise, and subsequently looking
like a dog to a human with any amount of visual processing and top-down feedback.
This sparked controversy, with many pointing out that a small change (in RGB values)

to some visual concept does not necessarily correspond to a small change in concept-
space. The paper itself punted on this:
it is philosophically diﬃcult to deﬁne the real object class for an image that is not
a picture of a real object. In this work, we assume that an adversarial image is
misclassiﬁed if the output label diﬀers from the human-provided label of the clean
image that was used as the starting point for the adversarial image. We make
small adversarial perturbations and we assume that these small perturbations are
insuﬃcient to change the true class.
And in response to comments, co-author Ian Goodfellow acknowledged on Twitter:
While everyone else was scrambling to ﬁnish running experiments for ICML, my
co-authors and I were having intense debates about philosophy and semantics
and how to write the paper. Some of our open oﬃce colleagues were entertained
by how surreal this sounded.
Making models robust against adversarial examples remains an outstanding and
diﬃcult topic with a considerable paper trail. The problem of merely verifying that a
given model has no local adversarial examples (e.g. within a few RGB values of a
given data point) has been the subject of some interesting formal veriﬁcation work in
the past couple years. But to even do this veriﬁcation work, one needs a formal
speciﬁcation of what an adversarial example is, which in turn requires a formal
speciﬁcation of what a "small change" between (e.g.) images is, that somehow
captures something about conceptual distance. It seems to me that even this smaller
problem will be hard to solve in a philosophically satisfying way because of the
inherent subjectivity/fuzziness in deﬁning "distance in concept-space" or anything that
even comes close.
Distance Functions are Hard: The Evidence
What we are asking for, in all these instances, is some distance function precise
enough to be mathematizable in some form, but robust enough to include many very
fuzzy desiderata we have in mind. It seems natural to ask what distance functions of
this form have been successfully developed before. The Encyclopedia of Distances
comes out to over 700 pages, split roughly in half between those distances used in
pure math (especially, as one would expect, topology, geometry, and functional
analysis), and those used in applied math, computing disciplines, and the natural
sciences.
Of the distance functions listed in the latter half, most were simply "the obvious thing
one would do" given the preexisting mathematical structure around the topic in
question (e.g. Levenshtein distance on strings). Others were less obvious, but usually
because they used nontrivial mathematical machinery to answer speciﬁc
mathematical questions, not to actually shed light on fuzzy philosophical questions
one would have about it.
Getting to the social science section, where no existing mathematical formalism
existed on most of the topics in the ﬁrst place, virtually none of the distances
particularly helped to remedy this fuzziness by themselves. Though I do not claim to
have spent that much time ﬂipping through this tome, never did I see a distance
notion that struck me as a profound non-mathematical insight, or that even gestured
at an "art of coming up with distance functions".

Conclusions
I conclude, with medium conﬁdence, that each of the questions posed in the ﬁrst 3
sections will be particularly hard to answer in a satisfying way, and if they are, then
probably this won't be by thinking about distance functions directly.
As a general heuristic, I feel like if you've reduced a philosophical problem to "deﬁning
the appropriate distance function", then it's worth pausing to consider if you've made
a wrong-way reduction. Chances are, the distance function you want is inherently
value-laden, and so the problem of deﬁning it inherits the diﬃculty of the value
alignment problem itself.
I also think this heuristic is especially salient if you're trying to capture something like
"conceptual similarity/distance": if you could do this, then you'd have an objective
map/taxonomy of (a large fraction of) concept-space.

Status 451 on Diagnosis: Russell
Aphasia
This is a linkpost for https://status451.com/2019/06/28/diagnosis-russell-aphasia/
Samuel the Fifth at Status 451 considers the implications of Russell conjugation, or
emotive conjugation (example: she lied; you distorted the truth; I honestly reported
unconsciously biased beliefs). June 2019, ~1200 words.
Samuel emphasizes that this phenomenon isn't merely a matter of perceiving the
outgroup in a negative light. Even if the diﬀerent "forms" in a Russell-conjugation
triplet are synonyms, they're not exact synonyms: "lie" and "rationalize" actually
mean diﬀerent things. The inability to separate the denotative content of language
from an imputed enactive "side-channel attack" about who is to be blamed is a critical
threat to our collective ability to construct shared maps that reﬂect the territory! Key
quote:
A culture that cannot tell the diﬀerence between "reporting" and "doxing" and
merely considers it "doxing" when they do it, is a culture that cannot accurately
talk about behavior anymore. It's a culture where the words have been spun so
much, that they have lost their objective meaning, and are now instead used more
interchangeably to deliver more subjective messages. It's also a culture where not
enough people feel the need to deﬁne their words, and to actually ﬁgure out what
counts as reporting, as gossiping, as doxing, because this way the words can be
used strategically in the moment, without having to be fully consistent with next
time.

2-D Robustness
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
This is a short note on a framing that was developed in collaboration with Joar Skalse,
Chris van Merwijk and Evan Hubinger while working on Risks from Learned
Optimization, but which did not ﬁnd a natural place in the report.
Mesa-optimisation is a kind of robustness problem, in the following sense:
Since the mesa-optimiser is selected based on performance on the base objective, we
expect it (once trained) to have a good policy on the training distribution. That is, we
can expect the mesa-optimiser to act in a way that results in outcomes that we want,
and to do so competently.
The place where we expect trouble is oﬀ-distribution. When the mesa-optimiser is
placed in a new situation, I want to highlight two distinct failure modes; that is,
outcomes which score poorly on the base objective:
The mesa-optimiser fails to generalise in any way, and simply breaks, scoring
poorly on the base objective.
The mesa-optimiser robustly and competently achieves an objective that is
diﬀerent from the base objective, thereby scoring poorly on it.
Both of these are failures of robustness, but there is an important distinction to be
made between them. In the ﬁrst failure mode, the agent's capabilities fail to
generalise. In the second, its capabilities generalise, but its objective does not. This
second failure mode seems in general more dangerous: if an agent is suﬃciently
capable, it might, for example, hinder human attempts to shut it down (if its
capabilities are robust enough to generalise to situations involving human attempts to
shut it down). These failure modes map to what Paul Christiano calls benign and malign
failures in Techniques for optimizing worst-case performance.
This distinction suggests a framing of robustness that we have found useful while
writing our report: instead of treating robustness as a scalar quantity that measures
the degree to which the system continues working oﬀ-distribution, we can view
robustness as a 2-dimensional quantity. Its two axes are something like "capabilities"
and "alignment", and the failure modes at diﬀerent points in the space look diﬀerent.

Unlike the 1-d picture, the 2-d picture suggests that more robustness is not always a
good thing. In particular, robustness in capabilities is only good insofar is it is matched
by robust alignment between the mesa-objective and the base objective. It may be the
case that for some systems, we'd rather the system get totally confused in new
situations than remain competent while pursuing the wrong objective.
Of course, there is a reason why we usually think of robustness as a scalar: one can
deﬁne clear metrics for how well the system generalises, in terms of the diﬀerence
between performance on the base objective on- and oﬀ-distribution. In contrast, 2-d
robustness does not yet have an obvious way to ground its two axes in measurable
quantities. Nevertheless, as an intuitive framing I ﬁnd it quite compelling, and invite
you to also think in these terms.

Negative "eeny meeny miny moe"
As a kid, I learned the rhyme as:
Eeny, meeny, miny, moe,
Catch a tiger by the toe.
If he hollers, let him go,
Out goes Y, O, U!
Since kids can't predict where it will end, and adults are not supposed to try, it's a
reasonably fair way of drawing lots.
At times I've heard versions where the selected person wins instead of loses, and
while with two kids it doesn't matter, with three or more it matters a lot!
Let's model each kid having a choice at each stage between "accept" and "protest".
While protesting probably doesn't work, if enough of you protest it might. If you do the
positive version, where the selected kid wins, the winner accepts but the others may
choose to protest. This isn't good: everyone has reason to protest except the single
winner.
On the other hand, with the negative version, where one kid is eliminated at once, it's
the other way around. When the ﬁrst kid is eliminated they may protest, but the other
kids all accept because then they retain their chance to win. With each successive
round the dynamic is the same, plus the already-eliminated kids all choose accept out
of a desire for fairness. Even with the last elimination there's still only one person
choosing protest.
The iterative process is O(n) instead of O(1), but it also works much better because it
keeps a majority for "accept" at each stage.
(If you have a very large group of kids, then I could imagine a O(log(n)) version being
worth the added complexity. Divide the kids into three groups, and do negative eeny
meeny miny moe on the groups. A third of the kids may protest, but you've still got
two thirds accepting. Then redivide those remaining two thirds into three groups, and
keep going.)
cross-posted from https://www.jeﬀtk.com/p/negative-eeny-meeny-miny-moe

When do utility functions constrain?
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
The Problem
This post is an exploration of a very simple worry about the concept of utility
maximisers - that they seem capable of explaining any exhibited behaviour. It is one
that has, in diﬀerent ways, has been brought up many times before. Rohin Shah, for
example, complained that the behaviour of everything from robots to rocks can be
described by utility functions. The conclusion seems to be that being an expected
utility maximiser tells us nothing at all about the way a decision maker acts in the
world - the utility function does not constrain. This clashes with arguments that
suggest, for example, that a future humanity or AI would wish to self-modify its
preferences to be representable as a utility function.
According to Wikipedia's deﬁnition, a decision maker can be modelled as maximising
expected utility if its preferences over mutually exclusive outcomes satisfy
completeness, transitivity, continuity and independence. This is neat. However, it still
leaves us with free choice in ordering over our set of mutually exclusive outcomes.
The more outcomes we have, the less constrained we are by utility maximisation and
when we look closely there are often a LOT of mutually exclusive outcomes, if we give
ourselves truly free range over the variables in question.
In the context of thought around AI safety, introductions to these axioms have not
denied this but added a gloss of something like 'if you have a consistent direction in
which you are trying to steer the future, you must be an expected utility maximizer',
in this case from Benya's post in 2012. Here the work done by 'consistent direction'
seems to imply that you have reduced the world to a suﬃciently low dimensional
space of outcomes. Though I agree with this sentiment, it seems fuzzy. I hope to add
some clarity to this kind of qualiﬁcation. To put it another way, I want to see what
restrictions we need to add to regain something like the intuitive notion of the
maximiser and see if it is still suﬃciently general as to be worth applying to
theoretical discussions of agents.
Pareto Frontier of Concepts
A quick aside: Why do we want to use the idea of a utility maximiser? The explanation
that comes to mind is that it is felt to be a concept that lies on the Pareto frontier of a
trade-oﬀ between the explanatory power of a concept and the lack of troubling
assumptions. I doubt this framing is new but I drew up a little diagram anyway.

Explaining Too Much
What I want to do is move from considering utility functions in general to particular
classes of utility functions. To begin, let's look at the ﬁrst example, let's look at a
canonical example of utilities being constraining - the Allais paradox. In the paradox it
is shown that people have inconsistent preferences over money where the utilities
involved are samples of the underlying utility function U : R →R where the input is
money, for concreteness, at a particular future time t′. In this case most humans can
be shown to make decisions not compatible with any preference ordering.
When we construct this scenario we are imagining that people are in identical states
when they are oﬀered the various choices. However, we could instead construct these
lotteries as being oﬀered at distinct moments in time, and allow preferences over
money to vary freely as time goes on. Now we are assuming instead the class of utility
functions U : R2 →R where the input is the pair (gain in money at current time t, time).
Importantly, a person can still make irrational decisions with respect to their individual
utility function, but now there is no behaviour that can be ruled out by this
abstraction.
The basic point is the obvious one that the class of potential utility functions decides
the extent to which the utility function constrains behaviour. Note that even the ﬁrst
class of utility functions over money is extremely permissive - the Allais paradox only
works because the possible outcomes in terms of money are identical, but in any
scenario where the quantities involved in the lottery are diﬀerent, one could make any
set of decisions and remain free of inconsistency. I may for example have wildly
diﬀerent preferences for having £1, £1.01, £1.02 etc.. It is also important to note that
we only need to have one such 'free' variable in our utility function to strip it of its
predictive power.
Restricting Function Classes
The next question is whether it is possible to tighten up this theory so that it accords
with my intuitive notion of a maximiser, and whether too much is lost in doing so.
One simple way is to reduce possible trajectories to a discrete space of outcomes - I
want an aligned AI, I want to be alive in 20 years etc - and view all possible decisions
as lotteries over this space. No arguments from me, but we certainly want a more
expressive theory.
The example given in Eliezer's Stanford talk, which opens with an argument as to why
advanced AIs will be utility maximisers, is that of a hospital trying to maximise the
number of lives saved. Here we ﬁnd the rather unsurprising result that one's
preferences can be gainfully modelled as maximising expected utility... if they were
already a linear function of some persistent real world variable.
What if we go beyond linear functions though? There certainly seems to be a space for
'near to a goal' functions, like radial basis functions, both in theoretical and practical
uses of utility functions. What about cubics? Any polynomials? My instinct is to reach
for some kind of notion of smooth preferences over a ﬁnite number of 'resources'
which persist and remain similarly valued over time. This certainly does a lot of work
to recover what we are assuming, or at least imagining, when we posit that something
is a utility maximiser.

However, there seem to be nearly maximal examples of non-smoothness that still
allow for a class of utility functions that constrain behaviour. For example, take the
class of preferences over a rational number in which all I care about is the
denominator when the number is in its simplest form. I could for example desire the
greatest denominator: 
<
∼
. In most situations, were these preferences over my
store of dollars for example, this would seem to be outside the class of utility functions
that would meaningfully constrain my action, since this function is not at all smooth
over the resource in question. However, the are imaginable cases, such as when my
action space is to multiply my bank account by a lottery of other rationals, in which a
class of utility functions incorporating this sort of information represents a meaningful
restriction on my behaviour.
A contrived example? Completely, but what this implies is that the real problem is not
strictly in the way in which our function class is able to map 'real' variables to utility
but in the way in which our decisions represent a restriction of our future outcome
space.
Connecting Function Classes and Decision Spaces
The result is the perhaps obvious point that whether a class of utility functions
constrains behaviour in a given situation is function not just of either the class in
question or the type of actions available but a question of the extent to which our
available actions are correlated with the possible. When this is true for all utility
functions in the class we are considering, our utility function becomes capable of
constraining behaviour (though there may still be cases where no course of action
could not result from a utility function). This is more likely to happen when our
objective must be a simple function of persistent real world states but it does not
require this simplicity, as the above example shows.
To take an example of how this works in practice: in economics textbooks we will
encounter a wide range of utility functions. These will of course determine the way
that the agent behaves in the given scenario. More than this though, the very fact that
these agents are utility maximisers feels as if it constrains their behaviour, not just
because of the speciﬁc utility function but in the kind of calculating, maximising
reasoning that it enforces. Under this analysis, the reason that it seems this way is
that all utility functions that we might expect in such a scenario, whether logarithmic,
quadratic, lexical, all are such that any given utility function, our actions will change
the amount of utility we expect to be available. This is not because of the class of
potential utility functions exactly, but because the action space/utility function class
combination are such that the actions and changes in magnitude of available utility
are always linked. (In this pedagogical case of course we come to expect this with
good reason because the utility function/action space pair are selected to produce
interesting interaction.)
What we need to ﬁnd, for a given agent to be constrained by being a 'utility
maximiser' is to consider it as having a member of a class of utility functions where
the actions that are available to it systematically alter the expected utility available to
it - for all utility functions within this class. This is a necessary condition for utility
functions to restrict behaviour, not a suﬃcient one. Note that within almost any
natural class there will be the degenerate utility function in which all results result in
equal utility and therefore all actions are permissible - this must be deliberately
excluded to make predictions. It is this notion of classes rather than individual utility
functions, which saves my post (I hope) from total triviality.
34
59
19

What remains?
All this talk of restricting function classes is ﬁne, and there is much more to say, but
we're talking about selecting the class of possible utility functions as a modelling tool!
We can restrict the class of functions that their utility function is allowed to be a part
of, but if decision makers can employ a function from outside our allowed class and
laugh at our Dutch books then what use is all this talk, and these abstractions?
Well, a few things. Firstly I hope this leads some people to clearer understanding of
what is going on with utility functions - I feel I am now thinking more clearly about
them, though only time will tell if this feeling lasts.
Second, when we make statements about utility functions being meaningful for an
agent which wants to steer the future in a consistent direction, we have a more direct
way of talking about this.
Third, there may be some ways in which such restrictions are natural for interesting
classes of agents.
One thing to consider about a system, with respect to how we should model it, is
whether we should expect a system to systematically acquire the capacity to control
variables that are of relevance to it.
If I understand them correctly, systems which run according to active
inference/predictive processing (of which humans may be a member) can be
interpreted as maximising the power to predict observed variation (in this case
entropy), presumably where variation is measured across a ﬁnite set of information
streams. This may suggest that these systems naturally converge to methods of
behaviour that are well modelled by utility functions of a particular class and so the
abstraction of utility functions may regain meaning while still accurately describing
such systems.
Sweeping the Floor
So what becomes of our rock that maximises its rock-like actions? Firstly, we can say
that the class of utility functions needed to represent these 'decisions' is simply too
broad to be consistently correlated with the minimal freedom of 'action' that a rock
has, and thus our abstraction has no predictive power. Second, of course, is that
thinking about utility in this way emphasizes that utility is a way of representing
decisions. and a rock does not make decisions. How do we know? I'm not entirely sure,
but it's not a question that can be oﬄoaded to utility theory.
And what of our humans, and AIs, that may wish to modify themselves, in order to
become truly consistent utility maximisers? Does this represent a likely happening? I
think this may well still be true, for humans at least, but what is going on is not purely
the result of utility functions but comes from the fact that humans seem to (a)
organize their way of evaluating to situations to simpler outcome spaces so that there
is a restriction of the space of utility functions that contains all of the possible ways in
which humans value trajectories and (b) is such that all possible functions in this class
require humans to take decisions which are consequential the evaluation of this utility
function. In some ways this is very similar to the 'steering the future' hypothesis
presented at the beginning but hopefully with some further clariﬁcation and
operationalization of what this means.

My conclusions are getting shakier by the paragraph but it's posting day here at MSFP
and so the ﬂaws are left as an exercise to the reader.

Emotions are not beliefs
This lesson took me a long time to learn. Consider the following questions:
Should I be conﬁdent? Do you regret that decision? Are you happy about what
happened?
In each case, there are two things that are being asked:
Do I have a good chance of success? AND Should I feel conﬁdent?
Would you change the decision if you had the opportunity? AND Are you feeling
negative emotions about having made that decision?
Was the outcome in line with your preferences? AND Is the outcome creating
positive emotional aﬀect for you?
Because these are two seperate questions, they can have two separate answers. It's
not common, but you can believe that you are unlikely to succeed, whilst also having
the internal emotional experience associated with conﬁdence. You could wish that you
had made a diﬀerent decision without necessarily feeling negative emotions because
of it. You can have an outcome that isn't in line with your preferences, but experience
positive emotional aﬀect by looking on the bright side.
Truth
Properties like true or false only strictly apply to beliefs. Whether or not you are likely
to succeed is either true or false, but this doesn't apply to the emotive aﬀect of
conﬁdence itself. Someone can aﬀectively feel very "conﬁdent" without being factually
wrong.
Calibration
Nonetheless, explicit knowledge isn't the only knowledge contained in our brains. We
also possess implicit knowledge and this can be calibrated or uncalibrated. Since
these are heuristics, they can't literally be true or false or it would no longer be a
heuristic. But we can create diﬀerent measures of how accurate these heuristics are.
Emotions are linked to our heuristics, so we don't always want to shift ourselves
towards feeling positive aﬀect as this might bias us. However, emotions are seperate
from implicit knowledge. For example, someone can have implicit knowledge that
something is a bad idea, without experiencing negative aﬀect like a sense of dread.
Alternatively, someone can have a sense of dread, yet also have a strong intuition
that that it is the right decision. Since they are separate, they'll be times when we can
experience positive aﬀect instead of the "appropriate" emotion without having a
signiﬁcantly detrimental impact on our calibration.
Unfortunately, it is very hard to predict to what extent emotions aﬀect our implicit
understanding. This means it is very hard to ﬁgure out the appropriate trade-oﬀ in
terms of feeling positive aﬀect vs. avoiding bias. Nonetheless, I would be astonished if
the best trade-oﬀ involved taking no risk at all (see Barbarians vs. Bayesians for the
opposite stance).
Final thoughts

So far, I've mainly experimented with cognitive defusion as described by Kaj Solata. In
many cases, this has allowed me to cognitively think that an outcome is bad, without
having any signiﬁcant emotive aﬀect. Note that I limit my use of this, because
sometimes I see advantages of experiencing negative aﬀect. I mean, from an
evolutionary psychology perspective, experiencing negative emotions makes a
mistake stick in our brain for longer and has a greater impact on our implicit beliefs.
Unfortunately, I'm not similarly successful in creating positive eﬀect. All I can
recommend at the moment is allowing yourself to feel positive aﬀect, even if you are
aware the the reason you are feeling it is quite "silly".
This approach is somewhat inﬂuenced by Buddhism, but is also distinct from it. I've
heard people inﬂuenced by Buddhism say things like, "there is no such thing as good
or bad, it's all in your mind" and I ﬁnd that really frustrating. I suppose that if I was a
perfect, equanimous enlightened being than there really wouldn't be a diﬀerence, but
given that I am not, some things really are bad for me and some really are good. It
resonates with me much more to think, "this situation is bad, but I don't have to feel
bad". I won't claim that this works for everyone, but that's just what I've found useful
so far.

Clarifying some key hypotheses in AI
alignment
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
We've created a diagram mapping out important and controversial hypotheses for AI
alignment. We hope that this will help researchers identify and more productively
discuss their disagreements.
Diagram
A part of the diagram. Click through to see the full version.
Caveats
1. This does not decompose arguments exhaustively. It does not include every
reason to favour or disfavour ideas. Rather, it is a set of key hypotheses and
relationships with other hypotheses, problems, solutions, models, etc. Some
examples of important but apparently uncontroversial premises within the AI
safety community: orthogonality, complexity of value, Goodhart's Curse, AI being
deployed in a catastrophe-sensitive context.

2. This is not a comprehensive collection of key hypotheses across the whole space
of AI alignment. It focuses on a subspace that we ﬁnd interesting and is relevant
to more recent discussions we have encountered, but where key hypotheses
seem relatively less illuminated. This includes rational agency and goal-
directedness, CAIS, corrigibility, and the rationale of foundational and practical
research. In hindsight, the selection criteria was something like:
1. The idea is closely connected to the problem of artiﬁcial systems optimizing
adversarially against humans.
2. The idea must be explained suﬃciently well that we believe it is plausible.
3. Arrows in the diagram indicate ﬂows of evidence or soft relations, not absolute
logical implications — please read the "interpretation" box in the diagram. Also
pay attention to any reasoning written next to a Yes/No/Defer arrow — you may
disagree with it, so don't blindly follow the arrow!
Background
Much has been written in the way of arguments for AI risk. Recently there have been
some talks and posts that clarify diﬀerent arguments, point to open questions, and
highlight the need for further clariﬁcation and analysis. We largely share their
assessments and echo their recommendations.
One aspect of the discourse that seems to be lacking clariﬁcation and analysis is the
reasons to favour one argument over another — in particular, the key hypotheses or
cruxes that underlie the diﬀerent arguments. Understanding this better will make
discourse more productive and help people reason about their beliefs.
This work aims to collate and clarify hypotheses that seem key to AI alignment in
particular (by "alignment" we mean the problem of getting an AI system to reliably do
what an overseer intends, or try to do so, depending on which part of the diagram you
are in). We point to which hypotheses, arguments, approaches, and scenarios are
favoured and disfavoured by each other. It is neither comprehensive nor suﬃciently
nuanced to capture everyone's views, but we expect it to reduce confusion and
encourage further analysis.
You can digest this post through the diagram or the supplementary information, which
have their respective strengths and limitations. However, we recommend starting
with the diagram, then if you are interested in related reading or our comments
about a particular hypothesis, you can click the link on the box title in the diagram, or
look it up below.
Supplementary information
The sections here list the hypotheses in the diagram, along with related readings and
our more opinion-based comments, for lack of software to neatly embed this
information (however, boxes in the diagram do link back to the headings here). Note
that the diagram is the best way to understand relationships and high-level meaning,
while this oﬀers more depth and resources for each hypothesis. Phrases in italics with
the ﬁrst letter capitalised are referring to a box in the diagram.
Deﬁnitions

AGI: a system (not necessarily agentive) that, for almost all economically relevant
cognitive tasks, at least matches any human's ability at the task. Here, "agentive
AGI" is essentially what people in the AI safety community usually mean when
they say AGI. References to before and after AGI are to be interpreted as fuzzy,
since this deﬁnition is fuzzy.
CAIS: comprehensive AI services. See Reframing Superintelligence.
Goal-directed: describes a type of behaviour, currently not formalised, but
characterised by generalisation to novel circumstances and the acquisition of
power and resources. See Intuitions about goal-directed behaviour.
Agentive AGI?
Will the ﬁrst AGI be most eﬀectively modelled like a unitary, unbounded, goal-directed
agent?
Related reading: Reframing Superintelligence, Comments on CAIS, Summary and
opinions on CAIS, embedded agency sequence, Intuitions about goal-directed
behaviour
Comment: This is consistent with some of classical AI theory, and agency
continues to be a relevant concept in capability-focused research, e.g.
reinforcement learning. However, it has been argued that the way AI systems are
taking shape today, and the way humans historically do engineering, are cause to
believe superintelligent capabilities will be achieved by diﬀerent means. Some
grant that a CAIS-like scenario is probable, but maintain that there will still be
Incentive for agentive AGI. Others argue that the current understanding of
agency is problematic (perhaps just for being vague, or speciﬁcally in relation to
embeddedness), so we should defer on this hypothesis until we better
understand what we are talking about. It appears that this is a strong crux for the
problem of Incorrigible goal-directed superintelligence and the general aim of
(Near) proof-level assurance of alignment, versus other approaches that reject
alignment being such a hard, one-false-move kind of problem. However, to
advance this debate it does seem important to clarify notions of goal-
directedness and agency.
Incentive for agentive AGI?
Are there features of systems built like unitary goal-directed agents that oﬀer a
worthwhile advantage over other broadly superintelligent systems?
Related reading: Reframing Superintelligence, Comments on CAIS, Summary and
opinions on CAIS, Will humans build goal-directed agents?, AGI will drastically
increase economies of scale
Comment: Some basic points argued in favour are that agentive AGI is
signiﬁcantly more eﬃcient, or humans ﬁnd agents easier to think about, or
humans just want to build human-like agents for its own sake. However, even if
agentive AGI oﬀers greater eﬃciency, one could argue it is too risky or diﬃcult to
build, so we are better oﬀ settling for something like CAIS.
Modularity over integration?
In general and holding resources constant, is a collection of modular AI systems with
distinct interfaces more competent than a single integrated AI system?

Related reading: Reframing Superintelligence Ch. 12, 13, AGI will drastically
increase economies of scale
Comment: an almost equivalent trade-oﬀ here is generality vs. specialisation.
Modular systems would beneﬁt from specialisation, but likely bear greater cost in
principal-agent problems and sharing information (see this comment thread). One
case that might be relevant to think about is human roles in the economy —
although humans have a general learning capacity, they have tended towards
specialising their competencies as part of the economy, with almost no one being
truly self-suﬃcient. However, this may be explained merely by limited brain size.
The recent success of end-to-end learning systems has been argued in favour of
integration, as has the evolutionary precedent of humans (since human minds
appear to be more integrated than modular).
Current AI R&D extrapolates to AI services?
AI systems so far generally lack some key qualities that are traditionally supposed of
AGI, namely: pursuing cross-domain long-term goals, having broad capabilities, and
being persistent and unitary. Does this lacking extrapolate, with increasing automation
of AI R&D and the rise of a broad collection of superintelligent services?
Related reading: Reframing Superintelligence Ch. I
Incidental agentive AGI?
Will systems built like unitary goal-directed agents develop incidentally from something
humans or other AI systems build?
Related reading: Subsystem Alignment, Risks from Learned Optimization, Let's
talk about "Convergent Rationality"
Convergent rationality?
Given suﬃcient capacity, does an AI system converge on rational agency and
consequentialism to achieve its objective?
Related reading: Let's talk about "Convergent Rationality"
Comment: As far as we know, "convergent rationality" has only been named
recently by David Krueger, and while it is not well ﬂeshed out yet, it seems to
point at an important and commonly-held assumption. There is some confusion
about whether the convergence could be a theoretical property, or is merely a
matter of human framing, or merely a matter of Incentive for agentive AGI.
Mesa-optimisation?
Will there be optimisation processes that, in turn, develop considerably powerful
optimisers to achieve their objective? A historical example is natural selection
optimising for reproductive ﬁtness to make humans. Humans may have good
reproductive ﬁtness, but optimise for other things such as pleasure even when this
diverges from ﬁtness.
Related reading: Subsystem alignment, Risks from Learned Optimization

Discontinuity to AGI?
Will there be discontinuous, explosive growth in AI capabilities to reach the ﬁrst
agentive AGI? A discontinuity reduces the opportunity to correct course. Before AGI it
seems most likely to result from a qualitative change in learning curve, due to an
algorithmic insight, architectural change or scale-up in resource utilisation.
Related reading: Intelligence Explosion Microeconomics, The Hanson-Yudkowsky
AI-Foom Debate, A Contra AI FOOM Reading List, Any rebuttals of Christiano and
AI Impacts on takeoﬀ speeds?, A shift in arguments for AI risk
Comment: Discontinuity or fast takeoﬀ is a central assumption of early arguments
for AI risk and seems to have the greatest quantity of debate. A large proportion
of the community supports it in some form, but this proportion has apparently
decreased signiﬁcantly in the last few years (beliefs changing or new people, it's
unclear), with Paul Christiano's and Katja Grace's writing being a key inﬂuence.
Note that we distinguish to AGI and from AGI because of strategic and
developmental considerations around human-level. In published works the
distinction has not been very clear — we would like to see more discussion about
it. Thanks to Ben Garﬁnkel for pointing out how the distinction can be important.
Recursive self improvement?
Is an AI system that improves through its own AI R&D and self-modiﬁcation capabilities
more likely than distributed AI R&D automation? Recursive improvement would give
some form of explosive growth, and so could result in unprecedented gains in
intelligence.
Related reading: Intelligence Explosion Microeconomics, Reframing
Superintelligence Ch. 1
Discontinuity from AGI?
Will there be discontinuous, explosive growth in AI capabilities after agentive AGI? A
discontinuity reduces the opportunity to correct course. After AGI it seems most likely
to result from a recursive improvement capability.
Related reading: see Discontinuity to AGI
Comment: see Discontinuity to AGI
ML scales to AGI?
Do contemporary machine learning techniques scale to general human level (and
beyond)? The state-of-the-art experimental research aiming towards AGI is
characterised by a set of theoretical assumptions, such as reinforcement learning and
probabilistic inference. Does this paradigm readily scale to general human-level
capabilities without fundamental changes in the assumptions or methods?
Related reading: Prosaic AI alignment, A possible stance for alignment research,
Conceptual issues in AI safety: the paradigmatic gap, Discussion on the machine
learning approach to AI safety
Comment: One might wonder how much change in assumptions or methods
constitutes a paradigm shift, but the more important question is how relevant

current ML safety work can be to the most high-stakes problems, and that seems
to depend strongly on this hypothesis. Proponents of the ML safety approach
admit that much of the work could turn out to be irrelevant, especially with a
paradigm shift, but argue that there is nonetheless a worthwhile chance. ML is a
fairly broad ﬁeld, so people taking this approach should think more speciﬁcally
about what aspects are relevant and scalable. If one proposes to build safe AGI by
scaling up contemporary ML techniques, clearly they should believe the
hypothesis — but there is also a feedback loop: the more feasible approaches one
comes up with, the more evidence there is for the hypothesis. You may opt for
Foundational or "deconfusion" research if (1) you don't feel conﬁdent enough
about this to commit to working on ML, or (2) you think that, whether or not ML
scales in terms of capability, we need deep insights about intelligence to get a
satisfactory solution to alignment. This implies Alignment is much harder than, or
does not overlap much with, capability gain.
Deep insights needed?
Do we need a much deeper understanding of intelligence to build an aligned AI?
Related reading: The Rocket Alignment Problem
Broad basin for corrigibility?
Do corrigible AI systems have a broad basin of attraction to intent alignment?
Corrigible AI tries to help an overseer. It acts to improve its model of the overseer's
preferences, and is incentivised to make sure any subsystems it creates are aligned —
perhaps even more so than itself. In this way, perturbations or errors in alignment tend
to be corrected, and it takes a large perturbation to move out of this "basin" of
corrigibility.
Related reading: Corrigibility, discussion on the need for a grounded deﬁnition of
preferences (comment thread), Two Neglected Problems in Human-AI Safety
(problem 1 poses a challenge for corrigibility)
Comment: this deﬁnition of corrigibility is still vague, and although it can be
explained to work in a desirable way, it is not clear how practically feasible it is. It
seems that proponents of corrigible AI accept that greater theoretical
understanding and clariﬁcation is needed: how much is a key source of
disagreement. On a practical extreme, one would iterate experiments with tight
feedback loops to ﬁgure it out, and correct errors on the go. This assumes ample
opportunity for trial and error, rejecting Discontinuity to/from AGI. On a
theoretical extreme, some argue that one would need to develop a new
mathematical theory of preferences to be conﬁdent enough that this approach
will work, or such a theory would provide the necessary insights to make it work
at all. If you ﬁnd this hypothesis weak, you probably put more weight on threat
models based on Goodhart's Curse, e.g. Incorrigible goal-directed
superintelligence, and the general aim of (Near) proof-level assurance of
alignment.
Inconspicuous failure?
Will a concrete, catastrophic AI failure be overwhelmingly hard to recognise or
anticipate? For certain kinds of advanced AI systems (namely the goal-directed type), it
seems that short of near proof-level assurances, all safeguards are thwarted by the

nearest unblocked strategy. Such AI may also be incentivised for deception and
manipulation towards a treacherous turn. Or, in a machine learning framing, it would
be very diﬃcult to make such AI robust to distributional shift.
Related reading: Importance of new mathematical foundations to avoid
inconspicuous failure (comment thread)
Comment: This seems to be a key part of many people's models for AI risk, which
we associate most with MIRI. We think it signiﬁcantly depends on whether there is
Agentive AGI, and it supports the general aim of (Near) proof-level assurance of
alignment. If we can get away from that kind of AI, it is more likely that we can
relax our approach and Use feedback loops to correct course as we go.
Creeping failure?
Would gradual gains in the inﬂuence of AI allow small problems to accumulate to
catastrophe? The gradual aspect aﬀords opportunity to recognise failures and think
about solutions. Yet for any given incremental change in the use of AI, the economic
incentives could outweigh the problems, such that we become more entangled in, and
reliant on, a complex system that can collapse suddenly or drift from our values.
Related reading: What failure looks like, A shift in arguments for AI risk > The
alignment problem without a discontinuity > Questions about this argument
Thanks to Stuart Armstrong, Wei Dai, Daniel Dewey, Eric Drexler, Scott Emmons, Ben
Garﬁnkel, Richard Ngo and Cody Wild for helpful feedback on drafts of this work. Ben
especially thanks Rohin for his generous feedback and assistance throughout its
development.

Calibrating With Cards
In this post, I'll try to bring together two things I enjoy: rationality and magic. Like
Hazard, I've also practiced close-up magic for a good amount of time now. After
recently seeing Tyler Alterman make a Facebook post about estimations and System 1,
it occurred to me that there are a few calibration exercises you can do with a deck of
playing cards. The three exercises below are all variants of cutting/manipulating a
deck of cards, and then trying to intuit something about the deck.
This serves three purposes:
1. Get a feel for your System 1:
1. The goal of the following three exercises is to see how good your gut is at
estimating uncertainty (hint: probably better than you think!)
2. Improve calibration:
1. These exercises all allow for some room for error. You can set your
conﬁdence intervals and see how quickly you can get calibrated using ﬁrst
principles.
3. Practice cool party tricks:
1. While I don't intend for this to be a full-on magic tutorial, the exercises I
outline are building blocks for magic tricks, and even demonstrating your
super-calibration (after getting good) might be impressive.
Below are the three exercises. If you have a deck of cards handy, you can tag along!
Cut Estimation
The simplest exercise is as follows:
1. Lift up a packet of cards.
2. Estimate how many cards you've picked up.
3. Count to check how many you actually picked up.
You have a few obvious reference points. The entire deck is 52 cards, and you can
easily tell if you've lifted up more or less than half (and this recurses). With a little

practice. I've found that my gut is pretty good at this sort of thing. I'll ask myself how
many cards, and there will be a number that feels right. It's usually quite close.
Things to pay attention to:
When your System 2 estimate conﬂicts with your System 1 gut answer of how
many cards you cut oﬀ.
Whether you are systematically over or underestimating the amount.
Repeating The Cut
This is similar to the ﬁrst one:
1. Cut to a card.
2. Replace the pack on the deck.
3. Cut to the same card again.
Things to pay attention to:
When you try to cut to the card a second time, how quickly does your gut know
that you got it right or wrong?
What does it feel like to "know" that the pack of cards in your hand is not the
same size as the ﬁrst time?
If you know you got it wrong, do you know if you cut too much or too little? And
by how much?
Riﬄe Peek
This one is something I've just started playing with recently, and it's a mildly
superhuman feat to get down right.

1. Name a card. Any playing card.
2. Riﬄe through the cards, watching the corners (where the number and suits are)
ﬂip towards you, and look for the card you named.
3. Using the information in 2, cut to where you saw the card.
This is diﬃcult. It's partially an estimation task because you need to know
approximately where through the deck you saw the card, i.e. half-way, at the end, etc.
To start, you can go slow, such that you can see each card as it slips oﬀ your thumb.
This gets harder the faster you riﬄe through the cards. To ramp up the diﬃculty, riﬄe
faster, such that you can only get a fractional peek at each card's corner.
Things to pay attention to:
How does your visual experience of watching the cards diﬀer when you aren't
looking for a particular card vs when you are? Does anything jump out at you?
Are there false positives?
How many riﬄes through the deck does it take for you to glimpse the card? (I
don't always see it on the ﬁrst pass through the deck myself.)
What is the visual experience of trying to diﬀerentiate between two similar cards
(e.g. Ace of Hearts vs Ace of Diamonds)?
I think the above three exercises are fun learning experiences and a way to check in
with your gut feelings through a medium many people may not have tried before.
With enough practice, you can hit very levels of accuracy on these tasks, despite
them seeming just a little impossible.
If you do decide to give these a go, let me know how it turns out!

Predicted AI alignment event/meeting
calendar
Update 2020-06-21: Linda Linsefors made the AI Safety Google Calendar. I published
How to make a predicted AI alignment event/meeting calendar, in case you want to
make your own.
Update 2021-11-21: The AI Safety Google Calendar is not in a state (anymore?) where
it would be a replacement for the kind of calendar I maintained.
I kept this calendar up-to-date between August 2019 and April 2020. As I've
left AI alignment (see also I'm leaving AI alignment - you better stay) and
nobody has told me that they want me to keep this calendar up-to-date, I
stop keeping it up-to-date.
If you ﬁnd this calendar useful and would like it to continue to be kept up-to-date,
please tell me in the comments or via PM. If you want to contribute to the community
by maintaining this calendar, let me know and I'll share my process with you.
I update this every month. If you know of more events, please comment, PM or email
me. The same goes for events that I listed, but which won't actually take place.
See also (I won't repeat here what is listed there):
MIRI's AI Risk for Computer Scientists workshops
Last updated (search for 'UPDATED'): 2020-04-18
Next update will be published by: 2020-05-18 - If someone tells me that they ﬁnd this
calendar useful. See also the paragraph at the top.
2020
Many of the events went virtual because of COVID-19.
(Apparently no SafeML workshop at ICLR 2020.)
April, cyberspace workshop Towards Trustworthy ML: Rethinking Security and Privacy
for ML at ICLR 2020 (only somewhat related to AIA)
April proposal submission deadline for the AISafety workshop at IJCAI
May, cyberspace research retreat of the AI Safety Camp Toronto - The in-person event
is cancelled. The organizers are doing their best putting together a virtual camp.
UPDATED May paper submission deadline for the Third International Workshop on
Artiﬁcial Intelligence Safety Engineering
UPDATED May, cyberspace Web Technical AI Safety Unconference

May, cyberspace Workshop on Assured Autonomous Systems at the 41st IEEE
Symposium on Security and Privacy
June application deadline for the Human-aligned AI Summer School (my guess based
on 2019)
July/August, Prague CZE Human-aligned AI Summer School (my guess based on 2018,
2019)
July, Yokohama JPN AISafety workshop at IJCAI 2020
August, Bodega Bay USA MIRI Summer Fellows Program (my guess based on 2019 -
There were also MSFPs/AISFPs in 2015-2018.)
September, Lisbon PRT International Workshop on Artiﬁcial Intelligence Safety
Engineering at SafeComp 2020
October, Prague CZE International Congress for the Governance of AI

Alleviating Bipolar with meditation
Original post: Alleviating Bipolar with meditation
I was asked on the slack, about bipolar and what might help from a meditation
standpoint.  I have my own experiences to share. (standard non-medical advice
disclaimer applies here, i'm not qualiﬁed to give professional advice and you
should probably conﬁrm with a professional if you have doubts about trying
any of this.)
Here's a list of things that might help with the subjective mood swinging of bipolar
experience.
1. A broadening of awareness and contexts. 
For about 6 months of time when I was really focused on moods (and 10 years before
that), I felt like I didn't have moods, moods had me (moods distinct from emotions
which can be had from moment to moment, moods are more like background, the
colour of the day). I would wake up and ﬁnd out today was "miserable" or "excited".
I worked on a speciﬁc type of meditation practice that is called broadening of
awareness (there are 2 diﬀerent instructions for methods).  I got lucky that this helped
me and I wasn't expecting it. When moods had me, it felt like things "just are"
miserable. Now my awareness is broader than the moods and "I"* contain them. 
(*meditative "I" and "self" are a rabbit hole)
Instructions: Most people have their sense of their self boundary in line with their
skin barrier. "I" end at my skin. But it's possible to expand that boundary, and shift it
to larger. Particularly the "kinetic sphere", the area where one might be able to reach
outside the body, and then further to the whole room size. Holding this "barrier" thing
at the size of the room means that I'm "anchored" metaphorically to more solid things
than my own body. Obviously "I'm" still the same but my ground is the actual
stationary room. Which does not feel moods like my body does. (*explanation of why
it helps may be entirely irrelevant, fact is, anecdata: it helped me)
There's space in my new expanded "me" to ﬁnd the body being a certain mood but
also to ﬁnd stillness out there in the room which doesn't get dragged around like the
moods do.  I felt the pull of daily moods dry up. Obviously my body is still in grump
but "I'm not" mentally trapped in that experience. From there, there's a new, deeper
breathing pattern that supports the broader awareness practice and that's to be
discovered and also hinted at.  I would encourage trying it for a few minutes a day and
then going for a permanent shift into what is sometimes described as "spaciousness".
Instructions 2: awareness speciﬁcally in the visual ﬁeld can be expanded out the
peripheral. Start by picking an object straight ahead to look at and focus on. Now
expand the awareness to the peripheral of the visual ﬁeld. Hold there for 30 seconds,
then push on towards expanding the peripheral. this works well looking up at the sky,
or the ocean because of the broadness of the visual object in the visual ﬁeld. push the
"awareness" beyond the visual ﬁeld until there's a sense of spidey-sense tingling to
what's outside the visual ﬁeld. Hold a broadness of awareness to the visual area and
the spidey sense. Try to engage this broad sense regularly and through the day, try to

live in this broad-sense of the world around you. Notice that a "mood" is within this
sense, not fully covering the whole space. If you work at the broadness, that sense
comes.
2. Stages of insight
At the same time as trying that practice, I was cycling through (technical meditation
term - can be read about in MCTB2 book) "the stages of insight". As I would cycle I
would hit sensation like fear, and it would call up involuntary intrusive memories
about things I feared, then I would the next day have a "when will it end" feeling and
wrestle with that one.
For 2, what became important is forming a relationship with the memories that I didn't
like. Due to lots of meditation, I was pretty clear what was normal and what was an
intrusive visit from my past. I started asking the question, "why is this here?" and that
question eventually turned into, "how is this here to help?" or "what do I need to still
learn from this memory?" and that was a huge shift.
After those questions were hard ingrained into my attitude, within a week, shitty
memories stopped showing up. Possibly because I got so good at relating to them that
I was never calling them, "shitty memories", and possibly because I never felt shit
again about them, I'd just appreciate the lesson that I was to learn.  And from that I
stopped cycling nearly as hard. I still notice bits of cycling but I'm above the cycle, not
in it.
3 Greater bodily awareness.
a few days ago I wanted a photo of myself, so I put on a fancy shirt and got out of bed
to take the photo.  3 minutes later I found myself eating things. When I asked myself
what's going on, because I wasn't hungry, I noticed that I was cold and I was using
food to stop feeling cold. An interesting discovery. I made my way back to warm
things.
It's bodily awareness that helps with the moods and actions. I can feel where in my
body (or not) I'm feeling depressed or angry and I can alleviate it via movement or
internal sensation and not by outwardly being moody or suﬀering mood swings.
For this I've done a lot of meditation and body scan attention work. Any sensation is
relevant, itching the head, the knot in the stomach, the tingle in the toes. It's all
relevant to the way I think.
It's a rat rationality thing to assume that these sensation experiences are noise but
they are not. All sensation is relevant.
Some combination of the 3 have helped me to the point where I doubt I have bipolar
any more.  I was fairly conﬁdent at one point and now it seems unlikely to be a useful
diagnosis.
And if there's a 4 and 5 it's, watch sleep and social life and make sure to get enough
of both, as well as being aware of instability in both which can start a cycle of
instability.  This is from Interpersonal Social Rhythm Therapy IPSRT - the only therapy

designed for bipolar. Fixing my sleep made a big diﬀerence, and ﬁxing my mood ﬁrst
thing in the morning did too.
Shoutout to Bipolar Awakenings for being more on the odd-strange-spiritual side of
meditative practice towards progress on alleviating bipolar.

Could we solve this email mess if we
all moved to paid emails?
Have you ever...
Sent an email to someone in rationality and not heard back for many weeks (or
more)?
Avoided sending an email to someone because you wanted to spare their
attention, despite thinking there was a fair chance they'd be genuinely
interested?
Wanted some way to signal that you actually cared more than usual about this
email, but without having to burn social capital (such as by saying "urgent" or
"please read")?
Had to ignore an email because, even though it might have been interesting,
ﬁguring that out would simply have been too eﬀortful?
I think that 1) problems like these are prevalent, 2) they have pretty bad
consequences, and 3) they could be partly solved by using services where you can
pay to send someone an email (N.B. payment is conditional on reply).
I'm considering running a coordination campaign to move the community to using
paid emails (in addition to their ordinary inbox), but before launching that unilaterally I
want more conﬁdence it is a good idea.
It would be very helpful data if people who'd use this is if >=50 other people
also did would post just saying "I'd use this is >=50 particular other people
did".
Background
Email seems broken. This is not that surprising: your email is basically a to-do list
where other people (and companies) can add items for free, without asking; and
where you're the only one who can remove them. We should do something about this.
More broadly, the attention economy seems broken. Recognising this, many
rationalists use various software tools to protect themselves from apps that are
engineered to be addictive. This helps at an individual level, but it doesn't help solve
the collective action problem of how to allocate our attention as a community. We
should do something about this.
Costly signalling and avoiding information asymmetries
An "information asymmetry" is situation where someone has true information which
they are unable to communicate. For example, suppose 10 economists are trying to
inﬂuence government policy on issue X, and one of them actually, really knows what
the most eﬀective thing is. Yet, they might not be able to communicate this to the
decision-makers, since the remaining 9 have degrees from equally prestigious
institutions and arguments that sound equally rigorous to someone without formal
training in economics. Information asymmetries are a key mechanism that generate
bad equilibria.

When it comes to email, this might look as follows: Lots of people write to senior
researchers asking for feedback on papers or ideas, yet they're mostly crackpots or
uninteresting, so most stuﬀ is not worth reading. A promising young researcher
without many connections would want their feedback (and the senior researcher
would want to give it!), but it simply takes too much eﬀort to ﬁgure out that the paper
is promising, so it never gets read. In fact, expecting this, the junior researcher might
not even send it in the ﬁrst place
This could be avoided if people who genuinely believed their stuﬀ was important could
pay some money as a costly signal of this fact. Actual crackpots could of course also
pay up, but 1) they might be less likely to, and 2) the payment would oﬀset some of
the cost of the recipient ﬁguring out whether the email is important or not.
How the signalling problem is currently solved, and why that's bad
Currently, the signalling problem is solved by things like:
Spending lots of eﬀort crafting interesting-sounding intros which signal that the
thing is worth reading, instead of just getting to the point
Burning social capital -- adding tags like "[Urgent]" or "[Important]" to the
subject line
This is bad, because:
1) It's a slippery slope to a really bad equilibrium. I've gotten emails with titles like
"Jacob, is everything alright between us?" because I didn't buy a water bottle from
some company. This is what we should expect when companies ﬁght for my attention
without any way to just directly pay for it. Even within the rationality community, if
our only way of allocating importance is by drawing upon very serious vocabulary,
we'll create an incentive for exaggeration, diﬀerentially favouring those less
scrupulous about this practice, and chip away at our ability to use shared-cues-of-
importance when it really matters.
2) The main thing protecting us from this inside a smaller community is that people
want to preserve their reputations. But if you're unsure how important your thing is,
and mislabeling it means potentially crying-wolf and risking your reputation, this
usually makes it more worth it to just avoid the tag. Which means that we lose out on
all those times when your thing actually was important and using the tag would have
communicated that.
3) It puts the recipient between a rock and a hard place, and they're not being
compensated for it. If you mark something as "[Urgent]" that actually is urgent, and
the person responds and does what you want, you've still presented them with the
choice between sacriﬁcing some ability to freely prioritise their tasks, and sacriﬁcing
some part of the quality of your relationship. There should be some easy way for you
to compensate them for that.
4) It's way too coarse-grained. There's not really any way of saying:
"This is kinda important, but not that urgent, though it would probably be good if you
read it at some point, though that depends on what else is on your plate"
apart from writing exactly that -- but then you're making a complicated cognitive
demand, which has already burnt lots of attention for the recipient.

Brief FAQ
What if replacing email with paid emails puts us in another equilibrium
that's bad for unexpected reasons?
At the moment, it doesn't seem feasible for us to use this to replace email. There isn't
even software available for doing that completely. Rather, people would consent to
receiving paid messages (for example via earn.com, see below) in addition to having
their regular inbox.
What if people don't have enough money?
As mentioned above, sending standard emails are still an option. Yet this becomes a
problem in the world where we move to the equilibrium where a standard email is
taken to signal "I didn't pay for this, so it's not that important". Then I can imagine
grants for "email costs" being a thing, or that the beneﬁts of the new equilibrium
outweigh this cost, or that they don't. I'm uncertain.
Wouldn't this waste a lot of money?
Not really, assuming that the people who you send money to are at least as eﬀective
at spending it as you are, which seems likely if this gets used within the rationality
community.
If this is basically right: then what do we do?
Earn.com is a site which oﬀers paid emails. For example, you can pay to message me
at earn.com/jacobjacob/ EDIT: note that payment is conditional on actually getting a
reply.
If this seems like something that could solve the current email mess, we should
coordinate to get a critical mass of the community to sign-up, and make their proﬁle
url:s available. (Compare this to how we've previously started using things
reciprocity.io and Calendly.)
I'd be happy to coordinate such a campaign, but I don't want to do it until I'm more
conﬁdent it would be a good thing.
(For the record, I have no relation to earn.com and would not beneﬁt personally by
others joining, beyond the obvious positive eﬀects on the community. They simply
seem like the best available option for doing this. They have a pretty solid team, and
are used by some very senior VCs like Marc Andreessen and Keith Rabois.)

How to Make Billions of Dollars
Reducing Loneliness
Loneliness Is a Big Problem
On Facebook, my friend Tyler writes:
Lately, I've been having an alarming amount of conversations arise about the
burdens of loneliness, alienation, rootlessness, and a lack of belonging that many
of my peers feel, especially in the Bay Area. I feel it too. Everyone has a gazillion
friends and events to attend. But there's a palpable lack of social fabric. I worry
that this atomization is becoming a world-wide phenomenon - that we might be
some of the ﬁrst generations without the sort of community that it's in human
nature to rely on.
And that the result is a worsening epidemic of mental illness...
Without the framework of a uniting religion, ethnicity, or purpose, it's hard to get
people to truly commit to a given community. Especially when it's so easy to swipe
left and opt for things that oﬀer the ﬂeeting feeling of community without being
the real thing: the parties, the once-a-month lecture series, the Facebook threads,
the workshops, the New Age ceremonies. We often use these as "community
porn" - they're easier than the real thing and they satisfy enough of the craving.
But they don't make you whole.
I've had some thoughts about experiments to try. But then I think about how hard
it is (especially in this geographic area) to get people to show up to something on
at least a weekly basis. Even if it's for something really great. I see many great
attempts at community slowly peter out.
Young people are lonely. Old people are lonely. Loneliness is bad for your health. It's
bad for society's health.
Having a smartphone that keeps you entertained all day, and enough money to live
by yourself, might sound like ﬁrst world problems. But they are likely contributors to
loneliness. And as developing countries get richer, they'll start having ﬁrst world
problems too. So I think addressing loneliness could be very high-leverage for the
world.
People are starting businesses to address loneliness: you can pay someone to call you
periodically or take you for a walk. But I'd argue these services are a band-aid in the
same sense that parties, workshops, and ceremonies are. They don't solve the
underlying problem: You're still alone by default instead of together by default.
Roommates Could Be a Great Solution
Sociologists think there are three conditions necessary for making friends: proximity;
repeated, unplanned interactions; and a setting that encourages people to let their

guard down and conﬁde in each other. These conditions tend to be present during
college for many people, but not afterwards.
Why do people ﬁnd it easier to make friends in college? Maybe it's because college
students don't usually live alone.
Going to events doesn't work because (a) you don't typically get repeated interactions
with the same person and (b) events take place at a scheduled time. Which may or
may not be a time you're feeling lonely.
If you have a lot of roommates, all you have to do is step outside your room and ﬁnd
someone to chat with. No transportation CO2 emissions needed. But more important,
you know your roommates are always gonna be around.
But I Already Have Roommates
Even if you already have roommates, I think there's a good chance your roommate
situation is under-optimized. Given that you spend so much time with them, there's a
lot of value in living with people you really connect with. (Finding great coworkers
makes sense for similar reasons.)
The layout of your house and the number of roommates you have can also make a big
diﬀerence. I used to have friends living in a 4-bedroom place where all the bedrooms
opened directly into a single large common area. If anyone else was outside their
room, you'd immediately know it and have an opportunity for interaction. Later I lived
in an 8-bedroom place which felt far lonelier, even with every room occupied. The
house was laid out so it was easy to go about your day without ever running into a
fellow roommate. I also lived in a house with over 50 bedrooms for a while, which was
wild & a lot of fun.
But I Don't Want Roommates
One reason you might not want roommates is because you're worried you might have
conﬂicting preferences for what living together should be like. For example, my
philosophy towards dirty dishes is to let them pile up on the counter and periodically
stuﬀ them all in the dishwasher, to be as time-eﬃcient as possible. Surprisingly, some
people dislike this approach.
RoomieMatch.com is a website which tries to solve the roommate compatibility
problem. You create a proﬁle by answering questions about dishes, food in the fridge,
housecleaning, social events, noise, overnight guests, shared household items,
walking around in your underwear, TV, etc. In addition, there are questions to help
predict how you well you will connect as people.
You Could Make a Lot of Money
RoomieMatch has two search options: free and cheap. Cheap costs $20/year.
The problem with RoomieMatch is they're leaving a massive amount of money on the
table.

A few years ago, a friend of mine was jobless & struggling ﬁnancially. He was living in
a 4-bedroom house at the time, and he was the primary contact with the landlord. My
friend took responsibility for vetting folks from Craigslist in order to ﬁll the remaining
rooms in the house. He found that folks from Craigslist were willing to pay enough rent
for the remaining 3 rooms that he was able to live rent-free until he found a job.
I acknowledge this is murky ethical territory, and I'm not condoning my friend's
actions. (I don't believe anyone ever found out or got upset, for whatever that's
worth.) The point I'm trying to make is that property management is way more
lucrative than roommate matching. RoomieMatch makes $20 per user per year at
best. My friend was making $100+ per user per month.
What I'm suggesting is that you take the full-stack startup playbook which has been
successful in Silicon Valley recently, and apply it to online roommate matching +
property management.
The extreme full-stack approach is to own your own properties. Apparently the US has
a surplus of big houses right now.
There are already players in this space such as Roam which are proving that people
will pay for community. (As if people paying extra to live in hip cities like SF & NYC
didn't prove that already. BTW, I found that the awesome community at the Athena
Hotel more than made up for the fact that it's in a non-hip city.) Anyway, I think
existing players are mostly pursuing the extreme full-stack option. I actually think this
is the wrong play. You want to be a marketplace, like Airbnb (valued at over $30
billion). The more people who are using your tool, the ﬁner-grained roommate
matching services you can provide. It's hard to achieve massive scale if you have to
own every property. You want to be playing matchmaker for individuals with common
interests who all happen to be looking for rooms around the same time, plus landlords
with empty houses. Maybe you'll want to undercut RoomieMatch, and provide free
matching services for people who live in their properties, in order to achieve the
necessary scale. (RoomieMatch's existing scale is impressive by the way--I quickly got
100+ active, vetted matches in a midsize US city when I tried the tool. If you have the
money you might want to just buy it.)
So instead of buying properties, maybe you just want to contact people selling large
homes & see if you can convince them to let you manage their property.
Note that this is a good company to start if a recession happens, since people who
currently live alone will be thinking about how to save on rent.
This Could Be Really Great
Most roommate search tools, like Craigslist, don't make it easy to ﬁgure out if a future
roommate is someone you'd actually want to live with. Imagine reaching a scale
where you could match people based on factors like:
They love to play board games, or pool, or Super Smash Bros.
They want a compost pile and a garden in their backyard.
One has a pet, and the other likes animals but isn't yet ready to make a lifetime
commitment.

They want a squat rack in the basement to save time & money going to the
gym.
They want to continue partying like college students after graduation.
They want to be part of an intentional community devoted to mutual
improvement and life optimization, or spirituality, or whatever.
They want to share childcare responsibilities.
They're all fans of the same sports team.
They enjoy reading and discussing the same genre of novels, or watching the
same movies.
They're musicians looking for people to jam with.
They want to live near hiking trails and go on group hikes together.
They want to do independent study of the same topic.
They're trying to eat a healthier diet.
They just moved to a new city and want friends they can explore the city with.
They have the same unusual work schedule.
One needs a caretaker, and the other wants to make extra money.
They like the idea of having a couch or two listed on CouchSurﬁng.
One knows a language the other wants to learn.
They work close together in the same expensive metropolitan area and want
save on housing. So they live in the outskirts of the city and commute together
every day using the diamond lane. One drives and the other pays for gas.
I also see opportunities to reduce friction in the current roommate matching process:
Automatically ﬁnd times when everyone is available for a meet & greet video
call.
Let people take virtual tours of the houses on oﬀer to minimize driving.
No need to worry about breaking a lease if someone moves to a diﬀerent house
in your company's network. Let people try out a few communities & see what
works for them. Use machine learning to improve your matching as you gather
more data.
Provide external mediation in the event of roommate disputes, and have a
reputation system to encourage good behavior.
You aren't providing housing as a service (like Airbnb), or companionship as a service
(like the people-walking startup). You're providing community as a service. You could
even organize mixers across your houses.

Conclusion
Technology has been blamed for the loneliness epidemic, but I think we can use
technology to cure the loneliness epidemic as well.
I'm too busy being obsessed with machine learning to start any company which isn't
mostly about that. But I think this is a product the world needs, and I want you to build
it. I encourage you to sign the Founders Pledge and donate the money to eﬀective
charities in case you actually end up making billions of dollars as a result of reading
this.
I apologize if you found the tone of this post overly sales-y. My goal was to light a
spark in the right person. (Feel free to steal phrases from this post when pitching
investors!)
Some folks in the rationalist community might be a little underwhelmed by this idea,
since people in the rationalist community have been living together in group houses
for a long time. The thing is, ﬁnding roommates by connecting based on mutual
interests via the internet is still kind of weird in the eyes of the general public. As Paul
Graham put it: "Live in the future, then build what's missing." The existence of so
many lonely people proves that this option is still missing for most people.
Anyway, if you're interested in building/investing in this, please comment below, or
send me a private message via my user page with the country you're in and I'll put
you in contact with others who message me. (Edit: I might be slow to reply, sorry)
Cross-posted from the Eﬀective Altruism Forum. See also discussion on Hacker News.

Which of these ﬁve AI alignment
research projects ideas are no good?
I'll post ﬁve AI alignment research project ideas as comments. It would be great if you
could approval-vote on them by using upvotes. Ie. when you think the project idea
isn't good, you leave the comment as is; otherwise you give it a single upvote.
The project ideas follow this format (cf. The Craft of Research):
I'm studying <topic>, 
    because I want to <question that guides the search>, 
        in order to help my reader understand <more significant 
        question that would be informed by an answer to the 
        previous question>. 
The project ideas are ﬁxed-width in order to preserve the indentation. If they get
formatted strangely, you might be able to ﬁx it by increasing the width of your
browser window or zooming out.

[Site Update] Behind the scenes data-
layer and caching improvements
We just pushed a major behind the scenes update we've been working on for a while.
This was primarily about reducing technical debt and upgrading a bunch of our
outdated dependencies, so you shouldn't see much happening on your side besides
some small performance improvements and a general reduction in weird data-loading
jankiness.
This was however a pretty big refactor for our codebase, making this a very high risk
update. We've spent a lot of time testing things, but please let us know if you notice
anything broken or weird in the next few days.

Very diﬀerent, very adequate
outcomes
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Let Up be the utility function that - somehow - expresses your preferences[1]. Let Uh be
the utility function expresses your hedonistic pleasure.
Now imagine an AI is programmed to maximise U(q) = qUp + (1 −q)Uh. If we vary q in
the range of 5% to 95%, then we will get very diﬀerent outcomes. At 5%, we will
generally be hedonically satisﬁed, and our preferences will be followed if they don't
cause us to be unhappy. At 95%, we will accomplish any preference that doesn't cause
us huge amounts of misery.
It's clear that, extrapolated over the whole future of the universe, these could lead to
very diﬀerent outcomes[2]. But - and this is the crucial point - none of these outcomes
are really that bad. None of them are the disasters that could happen if we picked a
random utility U. So, for all their diﬀerences, they reside in the same nebulous
category of "yeah, that's an ok outcome." Of course, we would have preferences as to
where q lies exactly, but few of us would risk the survival of the universe to yank q
around within that range.
What happens when we push q towards the edges? Pushing q towards 0 seems a clear
disaster: we're happy, but none of our preferences are respected; we basically don't
matter as agents interacting with the universe any more. Pushing q towards 1 might
be a disaster: we could end up always miserable, even as our preferences are fully
followed. The only thing protecting us from that fate is the fact that our preferences
include hedonistic pleasure; but this might not be the case in all circumstances. So
moving q to the edges is risky in the way that moving around in the middle is not.
In my research agenda, I talk about adequate outcomes, given a choice of
parameters, or acceptable approximations. I mean these terms in the sense of the
example above: the outcomes may vary tremendously from one another, given the
parameters or the approximation. Nevertheless, all the outcomes avoid disasters and
are clearly better than maximising a random utility function.
1. This being a somewhat naive form of preference utilitarianism, along the lines of
"if the human choose it, then its ok". In particular, you can end up in
equilibriums where you are miserable, but unwilling to choose not to be (see for
example, some forms of depression). ↩ 

2. This fails to be true if preference and hedonism can be maximised
independently; eg if we could take an eﬀective happy pill and still follow all our
preferences. I'll focus on the situation where there are true tradeoﬀs between
preference and hedonism. ↩ 

"Can We Survive Technology" by von
Neumann
This is a linkpost for http://geosci.uchicago.edu/~kite/doc/von_Neumann_1955.pdf
"The great globe itself" is in a rapidly maturing crisis —a crisis attributable to the
fact that the environment in which technological progress must occur has become
both undersized and underorganized. To deﬁne the crisis with any accuracy, and
to explore possibilities of dealing with it, we must not only look at relevant facts,
but also engage in some speculation. The process will illuminate some potential
technological developments of the next quarter-century.
Added: see orthonormal's comment below for a summary.

Vaniver's View on Factored Cognition
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
The View from 2018
In April of last year, I wrote up my confusions with Paul's agenda, focusing mostly on
approval directed agents. I mostly have similar opinions now; the main thing I noticed
on rereading it was I talked about 'human-sized' consciences, when now I would
describe them as larger than human size (since moral reasoning depends on cultural
accumulation which is larger than human size). But on the meta level, I think they're
less relevant to Paul's agenda than I thought then; I was confused about how Paul's
argument for alignment worked. (I do think my objections were correct objections to
the thing I was hallucinating Paul meant.) So let's see if I can explain it to
Vaniver_2018, which includes pointing out the obstacles that Vaniver_2019 still sees.
It wouldn't surprise me if I was similarly confused now, tho hopefully I am less so, and
you shouldn't take this post as me speaking for Paul.
Factored Cognition
One core idea that Paul's approach rests on is that thoughts, even the big thoughts
necessary to solve big problems, can be broken up into smaller chunks, and this can
be done until the smallest chunk is digestible. That is, problems can be 'factored' into
parts, and the factoring itself is a task (that may need to be factored). Vaniver_2018
will object that it seems like 'big thoughts' require 'big contexts', and Vaniver_2019
has the same intuition, but this does seem to be an empirical question that
experiments can give actual traction on (more on that later).
The hope behind Paul's approach is not that the small chunks are all aligned, and
chaining together small aligned things leads to a big aligned thing, which is what
Vaniver_2018 thinks Paul is trying to do. A hope behind Paul's approach is that the
small chunks are incentivized to be honest. This is possibly useful for transparency
and avoiding inner optimizers. A separate hope with small chunks is that they're
cheap; mimicking the sort of things that human personal assistants can do in 10
minutes only requires lots of 10 minute chunks of human time (each of which only
costs a few dollars) and doesn't require ﬁguring out how intelligence works; that's the
machine learning algorithm's problem.
So how does it work? You put in an English string, a human-like thing processes it, and
it passes out English strings--subquestions downwards if necessary, and answers
upwards. The answers can be "I don't know" or "Recursion depth exceeded" or
whatever. The human-like thing comes preloaded (or pre-trained) with some idea of
how to do this correctly; obviously incorrect strategies like "just pass the question
downward for someone else to answer" get ruled out, and the humans we've trained
on have been taught things like how to do good Fermi estimation and some of the
alignment basics. This is general, and lets you do anything humans can do in a short
amount of time (and when skillfully chained, anything humans can do in a long
amount of time, given the large assumption that you can serialize the relevant state
and subdivide problems in the relevant ways).
Now schemes diverge a bit on how they use factored cognition, but in at least some
we begin by training the system to simply imitate humans, and then switch to training

the system to be good at answering questions or to distill long computations into
cached answers or quicker computations. One of the tricks we can use here is that
'self-play' of a sort is possible, where we can just ask the system whether a
decomposition was the right move, and this is an English question like any other.
Honesty Criterion
Originally, I viewed the frequent reserialization as a solution to a security concern. If
you do arbitrary thought for arbitrary lengths of time, then you risk running into inner
optimizers or other sorts of unaligned cognition. Now it seems that the real goal is
closer to an 'honesty criterion'; if you ask a question, all the computation in that unit
will be devoted to answering the question, and all messages between units are passed
where the operator can see them, in plain English.[1]
Even if one succeeds at honesty, it still seems diﬃcult to maintain both generality and
safety. That is, I can easily see how factored cognition allows you to stick to cognitive
strategies that deﬁnitely solve a problem in a safe way, but don't see how it does that
and allows you to develop new cognitive strategies to solve a problem that doesn't
result in an opening for inner optimizers--not within units, but within assemblages of
units. Or, conversely, one could become more general while giving up on safety. In
order to get both it seems like we're resting a lot on the Overseer's Manual or way
that we trained the humans that we used as training data.
Serialized State is Inadequate or Ineﬃcient
In my mind, the primary reason to build advanced AI (as opposed to simple AI) is to
accomplish megaprojects instead of projects. Curing cancer (in a way that potentially
involves novel research) seems like a megaproject, whereas determining how a
particular protein folds (which might be part of curing cancer) is more like a project. To
the extent that Factored Cognition relies on the serialized state (of questions and
answers) to enforce honesty on the units of computation, it seems like that will be
ineﬃcient for problems whose state are large enough that they impose signiﬁcant
serialization costs, and inadequate for problems whose state are too large to serialize.
If we allow answers that are a page long at most, or that a human could write out in
10 minutes, then we're not going to get a 300-page report of detailed instructions. (Of
course, allowing them to collate reports written by subprocesses gets around this
diﬃculty, but means that we won't have 'holistic oversight' and will allow for garbage
to be moved around without being caught if the system doesn't have the ability to
read what it's passing.)
The factored cognition approach also has a tree structure of computation, as opposed
to a graph structure, which leads to lots of duplicated eﬀort and the impossibility of
horizontal communication. If I'm designing a car, I might consider each part
separately, but then also modify the parts as I learn more about the requirements of
the other parts. This sort of sketch-then-reﬁnement seems quite diﬃcult to do under
the factored cognition approach, even though it involves reductionism and
factorization.
Shared memory partially solves this (because, among other things, it introduces the
graph structure of computation), but now reduces the guarantee of our honesty
criterion because we allow arbitrary side eﬀects. It seems to me like this is a
necessary component for most of human reasoning, however. James Maxwell, the
pioneer behind electromagnetism, lost most of his memory with age, in a way that

seriously reduced his scientiﬁc productivity. And factored cognition doesn't even allow
the external notes and record-keeping he used to partially compensate.
There's Actually a Training Procedure
The previous section described what seems to me to be a bug; from Paul's perspective
this might be a necessary feature because his approaches are designed around taking
advantage of arbitrary machine learning, which means only the barest of constraints
can be imposed. IDA presents a simple training procedure that, if used with an
extremely powerful model-ﬁnding machine learning system, allows us to recursively
surpass the human level in a smooth way. (Amusingly to me, this is like Paul enforcing
slow takeoﬀ.)
Training The Factoring Problem is Ungrounded
From my vantage point, the trick that we can improve the system by asking it
questions like "was X a good way to factor question Y?", where X was the attempt it
had at factoring Y, is one of the core reasons to think this approach is workable, and
also seems like it won't work (or will preserve blind spots in dangerous ways). This is
because while we could actually ﬁnd the ground truth on how many golf balls ﬁt in a
737, it is much harder to ﬁnd the ground truth on what cognitive style most accurately
estimates how many golf balls ﬁt in a 737.
It seems like there are a few ways to go about this:
1. Check how similar it is to what you would do. A master artist might watch the
brushstrokes made by a novice artist, and then point out wherever the novice
artist made questionable choices. Similarly, if we get the question "if you're
trying to estimate how many golf balls ﬁt in a 737, is 'length of 737 * height of
737 * width of 737 / volume of golf ball' a good method?" we just compute what
we would have done and estimate if the approach will have a better or worse
error.
2. Check whether or not it accords with principles (or violates them). Checking the
validity of a mathematical proof normally is done by making sure that all steps
are locally valid according to the relevant rules of inference. In a verbal
argument, one might just check for the presence of fallacies of reasoning.
3. Search over a wide range of possible solutions, and see how it compares to the
distribution. But how broadly in question-answer policy space are we searching?
We now face some tradeoﬀs between exploration (in a monstrously huge search
space, which may be highly computationally costly to meaningfully explore) and
rubber-stamping, where I use my cognitive style to evaluate whether or not my
cognitive style is any good. Even if we have a good resolution to that tradeoﬀ, we
have to deal with the cognitive credit-assignment problem.
That is, in reinforcement learning one has to ﬁgure out which actions taken (or not
taken) before a reward led to receiving the reward so that it can properly assign
credit; similarly the system that's training the Q&A policy needs to understand well
enough how the policy is leading to correct answers such that it can apply the right
gradients in the right places (or use a tremendous amount of compute doing this by
blind search).
This is complicated by the fact that there may be multiple approaches to problem-
solving that are internally coherent, but mixtures of those approaches fail. If we only
use methods like gradient-descent that smoothly traverse the solution space, this

won't be a problem (because gradient descent won't sharply jump from one to
another), but it's an open empirical question as to whether future ML techniques will
be based on gradient descent. It's not obvious how we can extricate ourselves from
the dependence on our learned question-answer policy. If I normally split a model into
submodels based on a lexicographical ordering, and now I'm considering a
hypothetical split into submodels based on statistical clustering, I would likely want to
consider the hypothetical split all the way down the tree (as updates to my beliefs on
'what strategy should I use to A this Q?' will impact more than just this question),
especially if there are two coherent strategies but a mixture of the strategies is
incoherent. But how to implement this is nonobvious; am I not just passing questions
to the alternate branch, but also a complete description of the new cognitive strategy
they should employ? It seems like a tremendous security hole to have 'blindly follow
whatever advice you get in the plaintext of questions' as part of my Q->A policy, and
so it seems more like I should be spinning up a new hypothetical agent (where the
advice is baked into their policy instead of their joint memory) in a way that may
cause some of my other guarantees that relied on smoothness to fail.
Also note that because updates to my policy impact other questions, I might actually
want to consider the impact on other questions as well, further complicating the
search space. (Ideally, if I had been handling two questions the same way and
discover that I should handle them separately, my policy will adjust to recognize the
two types and split accordingly.) While this is mostly done by the machine learning
algorithm that's trying to massage the Q->A policy to maximize reward, it seems like
making the reward signal (from the answer to this meta-question) attuned to how it
will be used will probably make it better (consider the answer "it should be answered
like these questions, instead of those," though generally we assume yes/no answers
are used for reward signals).
When we have an update procedure to a system, we can think of that update
procedure as the system's "grounding", or the source of gravity that it becomes
arranged around. I don't yet see a satisfying source of grounding for proposals like
HCH that are built on factored cognition. Empiricism doesn't allow us to make good
use of samples or computation, in a way that may render the systems uncompetitive,
and alternatives to empiricism seem like they allow the system to go oﬀ in a crazy
direction in a way that's possibly unrecoverable. It seems like the hope is that we have
a good human seed that then is gradually ampliﬁed, in a way that seems like it might
work but relies on more luck than I would like: the system is rolling the dice whenever
it makes a signiﬁcant transition in its cognitive style, as it can no longer fully trust
oversight from previous systems in the ampliﬁcation tree as they may misunderstand
what's going on in the contemporary system, and it can no longer fully trust oversight
from itself, because it's using the potentially corrupted reasoning process to evaluate
itself.
1. Of course some messages could be hidden through codes, but this behavior is
generally discouraged by the optimization procedure, as whenever you compare
to a human baseline they will not do the necessary decoding and will behave in
a diﬀerent way, costing you points. ↩ 

How has rationalism helped you?
This November, I will be participating in NaNoWriMo, an online event where
participants have one month to write a 50,000-word manuscript for a novel. I'm fairly
settled on the idea that I'm going to write about a person who is fairly smart, but who
has no rationalist training, discovering rationalism and developing into a fully-ﬂedged
rationalist.
I'm looking for inspiration for what kind of problems they might learn to solve. How
has rationalism helped you? There is no answer too big or too small. If rationalism
helped you realize that you needed to divorce your spouse and change careers, that's
a good answer; if rationalism changed the way you tie your shoelaces, I'm all ears. In
particular, I'd like to hear:
The thing you used to do, which was lacking in some way.
The rationalist concept that challenged your habit.
What you do now.

I'm interested in a sub-ﬁeld of AI but
don't know what to call it.
A colleague of mine and I have recently decided to tackle a project which we think has
important ramiﬁcations for the ﬁeld of artiﬁcial intelligence. We're vaguely aware that
relevant work has been done at places like Carnegie Mellon and are hoping to get
some guidance.
Our two overarching goals are to discover how models are constructed and to develop
a model 'calculus' describing how an agent searches the space of possible
hypotheses. We want to examine these questions from a number of diﬀerent
perspectives, studying the means by which both human and artiﬁcial agents actually
arrive at models that allow them to understand the world, as well as the ways in which
they would do so were they performing optimally.
Our motivation is to probe the subtle interchange between inductive and deductive
reasoning. When is it that an agent moves from merely noticing patterns in data to
reasoning on the basis of an axiomatic, deductive, predictive theory? Does it work
diﬀerently in diﬀerent domains? Do these inﬂection points have common
characteristics, and, if so, can we use them to form a general theory of theories?
Does this framing even make any sense?
Along the way we hope to address a number of ancillary questions. Perhaps we can
arrive at a formal theory of models, or probe the similarities and diﬀerences in how
artiﬁcial intelligences and humans approach the task of compressing data down into
simpler representations. There are obvious connections to Thomas Kuhn, to David
Hume's is/ought problem, and to vast swathes of the philosophy of science.
We'd like help locating important books, seminal papers, good overview materials,
words or phrases we can Google, theorists that have pushed the ﬁeld forward,
anything like that.
Thank you.

Six AI Risk/Strategy Ideas
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
AI risk ideas are piling up in my head (and in my notebook) faster than I can write
them down as full posts, so I'm going to condense multiple posts into one again. I may
expand some or all of these into full posts in the future. References to prior art are
also welcome as I haven't done an extensive search myself yet.
The "search engine" model of AGI
development
The current OpenAI/DeepMind model of AGI development (i.e., fund research using
only investor / parent company money, without making signiﬁcant proﬁts) isn't likely
to be sustainable, assuming a soft takeoﬀ, but the "search engine" model very well
could be. In the "search engine" model, a company (and eventually the AGI itself)
funds AGI research and development by selling AI services, while keeping its
technology secret. At some point it achieves DSA either by accumulating a big enough
lead in AGI technology and other resources to win an open war against the rest of the
world, or by being able to simultaneously subvert a large fraction of all cognition done
on Earth (i.e., all the AI services that it is oﬀering), causing that cognition to suddenly
optimize for its own interests. (This was inspired by / a reply to Daniel Kokotajlo's Soft
takeoﬀ can still lead to decisive strategic advantage.)
Coordination as an AGI service
As a reﬁnement of the above, to build a more impregnable monopoly via network
eﬀects, the AGI company could oﬀer "coordination as a service", where it promises
that any company that hires its AGI as CEO will eﬃciently coordinate in some fair way
with all other companies that also hire its AGI as CEO. See also my AGI will drastically
increase economies of scale.
Multiple simultaneous DSAs under
CAIS
Suppose CAIS turns out to be a better model than AGI. Many AI services may be
natural monopolies and have a large market share for its niche. Suppose many high
level AI services all use one particular low level AI service, that lower level AI service
(or rather the humans or higher level AI services that have write access to it) could
achieve a decisive strategic advantage by subverting the service in a way that causes
a large fraction of all cognition on Earth (i.e., all the higher level services that depend
on it) to start optimizing for its own interests. Multiple diﬀerent lower level services
could simultaneously have this option. (This was inspired by a comment from ryan_b.)

Logical vs physical risk aversion
Some types of risks may be more concerning than others because they are "logical
risks" or highly correlated between Everett branches. Suppose Omega appears and
says he is appearing in all Everett branches where some version of you exists and
oﬀering you the same choice: If you choose option A he will destroy the universe if the
trillionth digit of pi equals the trillionth digit of e, and if you choose option B he will
destroy the universe if a quantum RNG returns 0 when generating a random digit. It
seems to me that option B is better because it ensures that there's no risk of all
Everett branches being wiped out. See The Moral Status of Independent Identical
Copies for my intuitions behind this. (How much more risk should we accept under
option B until we're indiﬀerent between the two options?)
More realistic examples of logical risks:
1. AI safety requires solving metaphilosophy.
2. AI safety requires very diﬃcult global coordination.
3. Dangerous synthetic biology is easy.
Examples of physical risk:
1. global nuclear war
2. natural pandemic
3. asteroid strike
4. AI safety doesn't require very diﬃcult global coordination but we fail to achieve
suﬃcient coordination anyway for idiosyncratic reasons.
Combining oracles with human
imitations
It seems very plausible that oracles/predictors and human imitations (which can be
thought of as a speciﬁc kind of predictor) are safer (or more easily made safe) than
utility maximizers or other kinds of artiﬁcial agents. Each of them has disadvantages
though: oracles need a human in the loop to perform actions, which is slow and costly,
leading to a competitive disadvantage versus AGI agents, and human imitations can
be faster and cheaper than humans but not smarter, also leading to a competitive
disadvantage versus AGI agents. Combining the two ideas can result in a more
competitive (and still relatively easy to make safe) agent. (See this comment for an
example.) This is not a particularly novel idea, since arguably quantilizers and IDA
already combine oracles/predictors and human imitations to achieve superintelligent
agency, but it still seems worth writing down explicitly.
"Generate evidence of diﬃculty" as a
research purpose
How to handle the problem of AI risk is one of, if not the most important and
consequential strategic decisions facing humanity. If we err in the direction of too
much caution, in the short run resources are diverted into AI safety projects that could

instead go to other x-risk eﬀorts, and in the long run, billions of people could
unnecessarily die while we hold oﬀ on building "dangerous" AGI and wait for "safe"
algorithms to come along. If we err in the opposite direction, well presumably
everyone here already knows the downside there.
A crucial input into this decision is the diﬃculty of AI safety, and the obvious place for
decision makers to obtain evidence about the diﬃculty of AI safety is from technical AI
safety researchers (and AI researchers in general), but it seems that not many people
have given much thought on how to optimize for the production and communication
of such evidence (leading to communication gaps like this one). (As another example,
many people do not seem to consider that doing research on a seemingly intractably
diﬃcult problem can be valuable because it can at least generate evidence of
diﬃculty of that particular line of research.)
The evidence can be in the form of:
1. Oﬃcial or semi-oﬃcial consensus of the ﬁeld
2. Technical arguments about the diﬃculty of AI safety
3. "AI Safety Experts" who can state or explain the diﬃculty of AI safety to a wider
audience
4. Amount of visible progress in AI safety per unit of resources expended
5. How optimistic or pessimistic safety researchers seem when they talk to each
other or to outside audiences
Bias about the diﬃculty of AI safety is costly/dangerous, so we should think about how
to minimize this bias while producing evidence of diﬃculty. Some possible sources of
bias:
1. Personal bias (due to genetics, background, etc.)
2. Selection eﬀects (people who think AI safety is intractable because it's too easy
or too hard tend to go into other ﬁelds)
3. Incentives (e.g., your job or social status depends on AI safety not being too
easy or too hard)

Towards an Intentional Research
Agenda
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
This post is motivated by research intuitions that better formalisms in consciousness
research contribute to agent foundations in more ways than just the value loading
problem. Epistemic status: speculative.
David Marr's levels of analysis is the idea that any analysis of a system involves
analyzing it at multiple, distinct levels of abstraction. These levels are the
computational, which describes what it is the system is trying to do, the algorithmic,
which describes which algorithms the system instantiates in order to accomplish that
goal, and the implementation level, describing the hardware or substrate on which the
system is running. Each level underdetermines the other levels. You can choose lots of
diﬀerent algorithms for a given goal, and algorithms don't restrict which goals can use
them. A concrete example Marr uses, is that you'd have a very hard time ﬁguring out
what a feather was for if you'd never seen a bird ﬂying, and if you only saw a bird
ﬂying you might have a very diﬃcult time coming up with something like the design of
a feather.
Imagine a world that had recently invented computers. The early examples are very
primitive, but people can extrapolate and see that these things will be very powerful,
likely transformative to society. They're pretty concerned about the potential for these
changes to be harmful, maybe even catastrophic. Although people have done a bit of
theoretical work on algorithms, it isn't all that sophisticated. But since the stakes are
high, they try their best to start ﬁguring out what it would mean for there to be such a
thing as harmful algorithms, or how to bound general use algorithms such that they
can only be used for certain things. They even make some good progress, coming up
with the concept of ASICs so that they can maybe hard code the good algorithms and
make it impossible to run the bad. They're still concerned that a suﬃciently clever or
suﬃciently incentivized agent could use ASICs for bad ends somehow.
If this situation seems a bit absurd to you, it's because you intuitively recognize that
the hardware level underdetermines the algorithmic level. I argue the possibility that
we're making the same error now. The algorithmic level underdetermines the
computational level, and no matter how many combinations of cleverly constructed
algorithms you stack on themselves, you won't be able to bound the space of possible
goals in a way that gets you much more than weak guarantees. In particular, a system
constructed with the right intentional formalism should actively want to avoid being
goodharted just like a human does. Such an agent should have knightian uncertainty
and therefore also (potentially) avoid maximizing.
In physics (or the implementation level) there are notions of smallest units, and
counting up the diﬀerent ways these units can be combined creates the notion of
thermodynamic entropy, we can also easily deﬁne distance functions. In information
theory (or the algorithmic level) there are notions of bits, and counting up the
diﬀerent ways these bits could be creates the notion of information theoretic entropy,
we can also deﬁne distance functions. I think we need to build a notion of units of

intentionality (on the computation level), and measures of permutations of ways these
units can be to give a notion of intentional (computational) entropy, along with getting
what could turn out to be a key insight for aligning AI, a distance function between
intentions. 
In the same way that trying to build complex information processing systems without
a concrete notion of information would be quite confusing, I claim that trying to build
complex intentional systems without a concrete notion of intention is confusing. This
may sound a bit far fetched, but I claim that it is exactly as hard to think about as
information theory was before Shannon found a formalism that worked.
I think there are already several beachheads for this problem that are suggestive:
Predictive processing (relation to smallest units of intention).
In particular, one candidate for smallest unit is the smallest unit that a given feedback
circuit (like a thermostat) can actually distinguish. We humans get around this by
translating from systems in which we can make fewer distinctions (like say heat) into
systems in which we can make more (like say our symbolic processing of visual
information in the form of numbers).
Convergent instrumental goals (structural invariants in goal systems).
In particular I think it would be worth investigating diﬀering intuitions about just how
much a forcing function convergent instrumental goals are. Do we expect a universe
optimized by a capability boosted Gandhi and Clippy to be 10% similar, 50%, 90% or
perhaps 99.9999+% similar?
Modal Logic (relation to counterfactuals and as semantics for the intentionality of
beliefs).
Goodhart's taxonomy begins to parameterize, and therefore deﬁne distance functions
for divergence of intent.
Some other questions:
How do simple intentions get combined to form more complex intentions? I think this
is tractable via experimentation with simple circuits. This could also suggest
approaches to pre-rationality via explaining (rigorously) how complex priors arise from
homeostatic priors.
In Buddhism, intention is considered synonymous with consciousness, while in the
west this is considered a contentious claim. What simple facts, if known, would
collapse the seeming complexity here?
Can we consider intentions as a query language? If so, what useful ideas or results
can we port over from database science? Is the apparent complexity of human values
a side eﬀect of the dimensionality of the space more so than the degree of resolution
on any particular dimension?
Note:
When I read vague posts like this myself, I sometimes have vague objections but don't
write them up due to the eﬀort to bridge the inferential distance to the author and
also the sense that the author will interpret attempts to bridge that distance as
harsher criticism than I intend. Please feel free to give half formed criticism and leave

me to ﬁll in the blanks. It might poke my own half formed thoughts in this area in an
interesting way.

Cartographic Processes
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
As a general rule, street maps of New York City do not form spontaneously - they
involve some cause-and-eﬀect process which takes in data from the territory (NYC's
streets) and produces the map from that data. Let's call these "cartographic
processes": causal processes which produce a map from some territory.
Formalizing a bit:
We have a territory and a map. I'm mostly interested in the case where both of
these are causal models (possibly with symmetry), but other models are
certainly possible.
Both the territory and the map are embedded in a larger causal model, the
cartographic process, in which the map is generated from the territory. Note that
the "territory" may be the entire cartographic process, including the map.
There is some class of queries on the territory which can be "translated" into
queries on the map, yielding answers which reliably predict the answers to the
corresponding territory-queries - this is what it means for the map to "match"
the territory. I'm mostly interested in counterfactual queries on the map, and
some preimage of those queries in the territory.
In our NYC streetmap example, the physical streets are the territory, the paper with
lines on it is the map, the cartographic process encompasses the map and territory
and all the people and equipment and computations which produced the map from
the territory, and the class of queries includes things like distance and street
connectivity. Note that, in this example, neither the territory nor the map is a causal
model, although the cartographic process is a causal model. In general, the
cartographic process itself will always be a causal model - accurate maps do not form
spontaneously, there is always a cause-and-eﬀect process which creates the map. Part
of the reason I'm speciﬁcally interested in causal models for the map and territory is
because I ultimately want maps of cartographic processes themselves.
For purposes of embedded agency via abstraction, we want to answer questions like:
Given a cartographic process, characterize the queries which the map can
reliably answer
Given a cartographic process, translate queries on the map into queries on the
territory, and vice-versa
Given a class of queries and territories, construct a cartographic process whose
output map reliably answers the queries on the territories
Given multiple cartographic processes on the same territory, how can we
integrate their output maps, i.e. translate queries between the two maps?
More generally, since we're talking about processes which make maps "match" their
territories, we'd like to know what role probabilistic models play. It seems like it should
be possible to talk about the role probabilistic models play in map - territory
correspondence without introducing limiting behavior (i.e. frequentism) or Cartesian
agents (i.e. Bayesianism). I suspect that there is some inherently embedded
interpretation of probability which would make answers to many of our questions

obvious. But that's still speculation; I do not yet know what such an interpretation
might be.
The rest of this post will build up to an informal conjecture on the correspondence
between counterfactual queries on causal maps and causal territories. First, though,
we'll take a brief detour to talk about controllers.
Controller <-> Cartographer Duality
A simple example of a cartographic process is a digital thermometer in a room. The
temperature readout is the map, the average kinetic energy of the air molecules is the
territory, and the interactions between air molecules, electrons, various circuit
components, and ultimately the LCD display together comprise the cartographic
process.
More surprisingly, we can also model a thermostat as a cartographic process: it's like
the thermometer's cartographic process, but with map and territory switched. For a
thermostat, the digital readout is the "territory", and the average kinetic energy of the
air molecules in the room is the "map". The cartographic process consists of the
temperature sensor, feedback control circuitry, heater/air conditioner, and vents - all
of which act together to make the "map" match the "territory", i.e. to make the room
temperature match the temperature setting on the thermostat's interface.
This suggests a general principle: take a control process, call the controller's target
point a "territory" and the environment a "map", and the control process looks like a
cartographic process.
(Thankyou to romeostevensit for hashing out the controller-cartographer duality idea
with me at MSFP 2019.)
Abstract Causality
The ﬁrst challenge of abstract causality - i.e. causal maps of causal territories - is to
translate counterfactuals on the map into counterfactuals on the territory, in such a
way that the causal structure "works" - i.e. the causal structure of the map is implied
by the causal structure of the territory. This should be straightforward to formalize, but
for now we're just going to run with the idea intuitively. (As Wheeler put it: "never
make a calculation until you know the answer.")
The obvious way for causal structure on the map to correspond to causal structure on
the territory is coarse-graining: take a set of nodes whose combined parents/ancestors
(mostly) don't overlap with their combined children/descendents, and combine them
into a single node. The thermometer in a room is an example: the kinetic energy of all
the diﬀerent air molecules mostly comes from the same places (walls, other air
molecules) over time, so we can glom all those air molecules together into a single
abstract "gas" with a "temperature", then talk about how things aﬀect the aggregate
temperature rather than how things aﬀect the individual molecules.
But coarse-graining is not the only way for causal structure to line up. As a
counterexample, consider a thermostat. At the abstract level, a great causal model for
a thermostat is (temperature setting in interface) -> (kinetic energy of air molecules).
But in the physical world, there's a complicated feedback controller in the middle, with

causal arrows going back-and-forth (though always forward in time). The net eﬀect of
that complicated controller is that we can replace it with a simple causal arrow in an
abstract map.
More generally, cartographic processes make the world behave-as-if there were a
simple causal arrow (territory) -> (map), even when the underlying causality is more
complicated.
This suggests an informal conjecture: every correspondence between counterfactual
structures on a map and a territory, for which the causal structure of the map is
implied by the causal structure of the territory, can be described by some combination
of coarse-graining and embedded cartographic processes. Intuitively, either the causal
arrows line up already, or we need some kind of controller to make the system
behave-as-if the causal arrows line up.

Markets are Universal for Logical
Induction
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Background
Logical Induction is the best framework currently available for thinking about logical
uncertainty - i.e. the "probability" that the twin primes conjecture is true, or that the 
(10101010
)th digit of pi is 3. This is important for lots of reasons, and you should read
the introduction of the paper (or the abridged version) for a much more detailed
background.
The general idea of logical induction is to assign probability-like numbers to logical
statements like "the (10101010
)th digit of pi is 3", and to reﬁne these "probabilities"
over time as the system thinks more. To create these "probabilities", each statement
is associated with an asset in a prediction market, which eventually pays $1 if the
statement is proven true, or $0 if it is proven false. The "probabilities" are then the
prices of these assets.
(It's also possible that a statement is never proven or disproven, and one of the many
interesting results of the paper is that logical inductors assign useful prices in that
case too.)
The logical induction paper has two main pieces. First, it introduces the logical
induction criterion: a system which assigns prices to statements over time is called a
"logical inductor" if the prices cannot be exploited by any polynomial-time trading
algorithm. The paper then shows that this criterion implies that the prices have a
whole slew of useful, intuitive, probability-like properties.
The second main piece of the paper proves that at least one logical inductor is
computable: the paper constructs an (extremely slow) algorithm to compute
inexploitable prices for logical statements. The algorithm works by running a
prediction market in which every possible polynomial-time trader is a participant.
Naturally, the prices in this market turn out to be inexploitable by any polynomial-time
trader - so, this giant simulated prediction market is a logical inductor.
Our Goal
An analogy: one could imagine a decision theorist making a list of cool properties they
want their decision theory to have, then saying "well, here's one possible decision
algorithm which satisﬁes these properties: maximize expected utility". That would be
cool and useful, but what we really want is a theorem saying that any possible
decision algorithm which satisﬁes the cool properties can be represented as
maximizing expected utility.

This is analogous to the situation in the logical induction paper: there's this cool
criterion for handling logical uncertainty, and it implies a bunch of cool properties. The
paper then says "well, here's one possible algorithm which satisﬁes these properties:
simulate a prediction market containing every possible polynomial-time trader".
That's super cool and useful, but what we really want is a theorem saying that any
possible algorithm which satisﬁes the cool properties can represented as a prediction
market containing every possible polynomial-time trader.
That's our goal. We want to show that any possible logical inductor can be
represented by a market of traders - i.e. there is some market of traders which
produces exactly the same prices.
The Proof
We'll start with a slightly weaker theorem: any prices which are inexploitable by a
particular trader T can be represented by a market in which T is a participant.
Conceptually, we can sketch out the proof as:
The "trader" T is a function which takes in prices P, and returns a portfolio T(P)
specifying how much of each asset it wants to hold.
The rest of the market is represented by an aggregate trader M (for "market"),
which takes in prices P and returns a portfolio M(P) specifying how much of each
asset the rest of the market wants to hold.
The "market maker" mechanism chooses prices so that the total portfolio held
by everyone is zero - i.e. T(P) + M(P) = 0. This means that any long position held
by T must be balanced by a short position held by M, and vice versa, at the
market prices.
We know the trader's function T, and we know the prices P which we want to
represent, so we solve for M: M(P) = -T(P)
... so the market containing M and T reproduces the prices P, as desired. One problem:
this conceptual "proof" works even if the prices are exploitable. What gives?
The main trick here is budgeting: the real setup gives the traders limited budget, and
they can't keep trading if they go broke. Since M is doing exactly the opposite of T, M
will go broke when T has unbounded gains - i.e. when T exploits the prices (this is
basically the deﬁnition of exploitation used in the paper). But if the prices are
inexploitable, then T's possible gains are bounded, therefore M's possible losses are
bounded, and M can keep counterbalancing T's trades indeﬁnitely.
Let's formalize that a bit. I'll re-use notation from the logical induction paper without
redeﬁning everything, so check the paper for full deﬁnitions.
First, let's write out the correct version of "T(P) + M(P) = 0". The missing piece is
budgeting: rather than letting traders trade directly, the logical induction algorithm
builds "budgeted" traders BbT (T) and BbM(M), where bT and bM are the two traders'
starting budgets. At each time t, the market maker mechanism then ﬁnds prices P_t
for which
BbT (T)(Pt, t) + BbM(M)(Pt, t) = 0

The budgeting function is a bit involved; see the paper for more. The important points
are that:
BbT (T) will exploit the prices as long as T does
Budgeting doesn't change anything at all as long as the traders don't put more
money on the line than they have available; otherwise it scales down the
trader's investments to match their budget.
Enforcing the second piece involves ﬁnding the worst-possible world for each trader's
portfolio. M's worst-possible world is BbT (T)'s best-possible world, so we reason:
Since T cannot exploit the prices, neither can BbT (T)
Since BbT (T) cannot exploit the prices, its best-case gain is bounded
Since BbT (T)'s best-case gain is bounded, the opposite strategy −BbT (T)'s
maximum loss is bounded
We can set M's budget bM higher than this maximum loss, and set M = −BbT (T),
so that BbM(M) = −BbT (T), as desired.
To recap: given a series of prices over time Pt inexploitable by trader T, we
constructed a market containing T which reproduces the prices Pt.
The proof approach generalizes easily to more traders: simply replace "BbT (T)" with 
∑i BbTi(Ti) to sum over the contribution of each individual trader, then select M to
balance them out, as before. Since the prices are inexploitable by every trader, the
traders' aggregate best-case gains are bounded above, so M's worst-case loss is
bounded below. This works even with inﬁnite traders, as long as the total budget of all
traders remains ﬁnite.
In particular, if we consider all polynomial-time traders (with budgeting and other
details handled as in the paper), we ﬁnd that any prices satisfying the logical
induction criterion can be represented by a market containing all of the polynomial-
time traders.
The Bigger Picture
Why does this matter?
First and foremost, it neatly characterizes the class of logical inductors: there are
degrees of freedom in budgeting, and a huge degree of freedom in choosing the
trader M which shares the market with our polynomial-time traders, but that's it -
that's all we need to represent all possible logical inductors. (Note that this does not
mean that simulating a prediction market containing all possible polynomial-time

traders is the only way to implement a logical inductor - just that there is always a
prediction market which produces the same prices as the logical inductor.)
Second, the proof is short and simple enough to generalize well. We should expect a
similar technique to work under other notions of exploitability, and in scenarios
beyond logical induction. This ties in to a general research path I've been playing with:
many conditions of "inexploitability" or "eﬃciency" which we typically associate with
utility functions instead produce markets when we relax the assumptions somewhat.
Since markets are strictly more general than utility functions, and typically satisfy the
same inexploitability/ineﬃciency criteria under more general assumptions, these kinds
of results suggest that we should use markets of subagents in many models where we
currently use utilities - see "Why Subagents?" for more along these lines.

