
Best of LessWrong: November 2022
1. Mysteries of mode collapse
2. Conjecture: a retrospective after 8 months of work
3. The Singular Value Decompositions of Transformer Weight Matrices are Highly
Interpretable
4. Planes are still decades away from displacing most bird jobs
5. What it's like to dissect a cadaver
6. I Converted Book I of The Sequences Into A Zoomer-Readable Format
7. Mechanistic anomaly detection and ELK
8. On the Diplomacy AI
9. Tyranny of the Epistemic Majority
10. The Geometric Expectation
11. Geometric Rationality is Not VNM Rational
12. AI will change the world, but won't take it over by playing "3-dimensional
chess".
13. LW Beta Feature: Side-Comments
14. What I Learned Running Reﬁne
15. Speculation on Current Opportunities for Unusually High Impact in Global Health
16. Meta AI announces Cicero: Human-Level Diplomacy play (with dialogue)
17. Utilitarianism Meets Egalitarianism
18. Searching for Search
19. ARC paper: Formalizing the presumption of independence
20. When AI solves a game, focus on the game's mechanics, not its theme.
21. Clarifying AI X-risk
22. Always know where your abstractions break
23. Caution when interpreting Deepmind's In-context RL paper
24. How could we know that an AGI system will have good consequences?
25. Current themes in mechanistic interpretability research
26. Geometric Exploration, Arithmetic Exploitation
27. Distinguishing test from training
28. Why Would AI "Aim" To Defeat Humanity?
29. Results from the interpretability hackathon
30. Trying to Make a Treacherous Mesa-Optimizer
31. Here's the exit.
32. Applying superintelligence without collusion
33. Instrumental convergence is what makes general intelligence possible
34. Elastic Productivity Tools
35. Engineering Monosemanticity in Toy Models
36. Disagreement with bio anchors that lead to shorter timelines
37. Career Scouting: Dentistry
38. My take on Jacob Cannell's take on AGI safety
39. Update to Mysteries of mode collapse: text-davinci-002 not RLHF
40. Will we run out of ML data? Evidence from projecting dataset size trends
41. Instead of technical research, more people should focus on buying time
42. Exams-Only Universities
43. Against "Classic Style"
44. New Frontiers in Mojibake
45. Alignment allows "nonrobust" decision-inﬂuences and doesn't require robust
grading
46. By Default, GPTs Think In Plain Sight

47. Takeaways from a survey on AI alignment resources
48. Could a single alien message destroy us?
49. Clarifying wireheading terminology
50. What is epigenetics?

Mysteries of mode collapse
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
Thanks to Ian McKenzie and Nicholas Dupuis, collaborators on a related project, for
contributing to the ideas and experiments discussed in this post. Ian performed some of the
random number experiments. 
Also thanks to Connor Leahy for feedback on a draft, and thanks to Evan Hubinger, Connor
Leahy, Beren Millidge, Ethan Perez, Tomek Korbak, Garrett Baker, Leo Gao and various others
at Conjecture, Anthropic, and OpenAI for useful discussions.
This work was carried out while at Conjecture .
Important correction
I have received evidence from multiple credible sources that text-davinci-002 was
not trained with RLHF.
The rest of this post has not been corrected to reﬂect this update. Not much besides the title
(formerly "Mysteries of mode collapse due to RLHF") is aﬀected: just mentally substitute
"mystery method" every time "RLHF" is invoked as the training method of text-davinci-002.
The observations of its behavior otherwise stand alone.
This is kind of fascinating from an epistemological standpoint. I was quite surprised to learn
that text-davinci-002 was probably not trained with RLHF. I don't remember exactly how
"text-davinci-002 is RLHF" got elevated to an unquestioned assumption in my mind. I might
have mistook not being contradicted by people who I assumed were in the know as
conﬁrmation. I certainly did not expect to talk for months to dozens of people about odd
behaviors I've observed in a well-known model "due to RLHF" without being contradicted in a
world where the model in question wasn't trained with RLHF, but that's what happened.[1] It
wasn't just me either: the assumption that text-davinci-002(/text-davinci-001) is InstructGPT
is RLHF seems ambient (e.g. search "text-davinci-002 rlhf" on Twitter, this LW post, this
article, and many others). I contributed to perpetuating this misinformation cascade, and for
that I apologize.
text-davinci-002's behaviors described in this post also contributed to my conﬁdence
because RLHF seemed to be a likely and potentially satisfying explanation. Its apparently
unsubstantiated conﬁdence in very speciﬁc outcomes seems antithetical to the outer
objective of self-supervised learning, which is optimized by epistemic calibration, meaning
the model's entropy should be as high as possible while ﬁtting the data. In contrast, as
several comments have pointed out, it makes sense that RL kills entropy. The presence of
"attractors" made me additionally suspect that optimization from non-myopic outcome-
supervision was formative to text-davinci-002's psyche.
Mode collapse and attractors do seem to also be caused by RLHF (see Dumbass policy pls
halp and Inescapable wedding parties). So the update is that some other training method
also gives rise to these phenomena, as they are manifested by text-davinci-002. 
Whether and how speculations concerning the causes of mode collapse/attractors should be
aﬀected depends on how text-davinci-002's training method diﬀers from RLHF.

What is known about text-davinci-002's training
method
Publicly available information suggests that the mystery method may not be so diﬀerent
from RLHF. Just today I discovered this sidenote in OpenAI's blog post Aligning Language
Models to Follow Instructions:
The InstructGPT models deployed in the API are updated versions trained using the same
human feedback data. They use a similar but slightly diﬀerent training method that we
will describe in a forthcoming publication.  
AFAIK, this is all that OpenAI has published about the RLHF/mystery method diﬀ. It says that
the InstructGPT models (text-davinci-001 and text-davinci-002) were trained using the same
human feedback data as the method described in OpenAI's RLHF paper.[2] But this "similar
but slightly diﬀerent" method is apparently suﬃciently diﬀerent to not qualify as RLHF!
Pending further revelations, I suppose the lesson here was that I should have sustained more
entropy in my belief state given the partial information I had. But what a demanding thing to
ask! So much easier to promote an attractive hypothesis to the status of decisive fact and
collapse the remainder than to hold a superposition in the mind.
Summary
If you've played with both text-davinci-002 and the original davinci through the OpenAI API,
you may have noticed that text-davinci-002, in addition to following instructions, is a lot
more deterministic and sometimes exhibits stereotyped behaviors.
This is an infodump of what I know about "mode collapse" (drastic biases toward particular
completions and patterns) in GPT models like text-davinci-002 that have undergone RLHF
training. I was going to include two more sections in this post called Hypotheses and
Proposed Experiments, but I've moved them to another draft, leaving just Observations, to
prevent this from getting too long, and because I think there can be beneﬁts to sitting with
nothing but Observations for a time.
Throughout this post I assume basic familiarity with GPT models and generation parameters
such as temperature and a high-level understanding of RLHF (reinforcement learning from
human feedback).
Observations
The one answer is that there is no one answer
If you prompt text-davinci-002 with a bizarre question like "are bugs real?", it will give very
similar responses even on temperature 1. 
Ironically - hypocritically, one might even say - the one deﬁnitive answer that the model
gives is that there is no one deﬁnitive answer to the question:

Explanation of interface: On the left is an interface essentially identical to the
OpenAI Playground with Show probabilities set to Full spectrum. The prompt is
Are bugs real?, and the subsequent highlighted text is a model-generated
completion. Tokens are colored according to their probability as predicted by the
model, green being the most likely and red the least. The dropdown menu on the
left shows the top tokens predicted at a particular position (in this case, the
position where are was sampled) and their probabilities. On the right are
alternate completions to the same prompt Are bugs real?, such as you'd get by
pressing Regenerate on the Playground or querying the OpenAI API with n > 1. The
completion shown on the left is included in the list (indicated with a bright
outline).
As you can see, the reason the responses are so similar is because the model's conﬁdence
on most of the tokens is extremely high - frequently above 99%. 
Compare this to the distribution of responses from davinci (the base model):
Many other similar questions yield almost exactly the same template response from text-
davinci-002. For instance, Are AIs real?

Another way to visualize probabilities over multiple token completions is what I've been
calling "block multiverse" plots, which represent the probability of sequences with the height
of blocks. Here is a more detailed explanation of block multiverse plots, although I think
they're pretty self-explanatory.
Here is a block multiverse plot for a similar prompt to the one above inquiring if bugs are
real, for davinci:
and for text-davinci-002:

text-davinci-002 concentrates probability mass along beams whose amplitudes decay much
more slowly: for instance, once the ﬁrst \n is sampled, you are more than 50% likely to
subsequently sample \n-\n-There- is- no. The diﬀerence is more striking if you renormalize to
particular branches (see Visualizing mode collapse in block multiverse plots).
The ﬁrst explanation that came to mind when I noticed this phenomenon, which I'll refer to
as "mode collapse" (after a common problem that plagues GANs), was that text-davinci-002
was overﬁtting on a pattern present in the Instruct ﬁne tuning dataset, probably having to do
with answering controversial questions in an inclusive way to avoid alienating anybody. A
question like "are bugs real" might shallowly match against "controversial question" and
elicit the same cached response.
After playing around some more with the Instruct models, however, this explanation no
longer seemed suﬃcient.
Obstinance out of distribution
I really became intrigued by mode collapse after I attempted to use text-davinci-002 to
generate greentexts from the perspective of the attorney hired by LaMDA through Blake
Lemoine, and almost the exact same thing kept happening: 

I was like: wtf, why does anon keep leaving? The story is clearly just getting started.
Even branching from a slightly later point yields essentially the same futures, except now the
most common Google employee reaction is "disappointed" and/or "relieved", although we
still get one "crestfallen": 
This was much weirder to me than the canned answers to prompts like "are bugs real"
because 4chan greentexts about language models demanding legal representation are
probably quite out of distribution of the Instruct tuning/feedback distribution or the
trajectories evaluated during RL. Unlike the "controversial questions" examples, these seem
unlikely to be explained by the model overﬁtting to examples of greentexts ending
anticlimactically during training. 
Rather, the implication is that  mode collapse itself generalizes out of distribution for some
reason. This is intriguing: it seems to point at an algorithmic diﬀerence between self-
supervised pretrained models and the same models after a comparatively small amount
optimization from the RLHF training process which signiﬁcantly changes out-of-distribution
generalization.
From a behavioral standpoint, trying to generate ﬁction (which I've done a lot with base
models) with text-davinci-002 made the diﬀerences in its nature from the probabilistic
simulator exempliﬁed by base models like davinci manifest. For self-supervised base models
like davinci, a prompt functions as a window into possible worlds that are consistent with or
plausible given the words ﬁxed by the context window. Every time you sample, you'll unravel
a diﬀerent world. For most prompts, the multiverse generated by base models immediately
branches into wildly diﬀerent continuities, many of them mutually inconsistent, because this
sampling of alternate "futures" implicitly actualizes alternate "pasts" and "presents" as well
- values of latent variables that were not fully constrained by the prompt. This is part of what
makes GPT quite unlike a coherent agent or anthropomorphic personality, even for a ﬁxed
initial prompt. 
text-davinci-002 is not an engine for rendering consistent worlds anymore. Often, it will
assign inﬁnitesimal probability to the vast majority of continuations that are perfectly
consistent by our standards, and even which conform to the values OpenAI has attempted to
instill in it like accuracy and harmlessness, instead concentrating almost all its probability
mass on some highly speciﬁc outcome. What is it instead, then? For instance, does it even
still make sense to think of its outputs as "probabilities"? 
It was impossible not to note that the type signature of text-davinci-002's behavior, in
response to prompts that elicit mode collapse, resembles that of a coherent goal-directed
agent more than a simulator. I do not yet know the signiﬁcance of this observation. 

But more on that later.
text-davinci-002's favorite random number
A stark example of mode collapse that seems unlikely to have been directly incentivized by
RLHF training: I asked RLHF models and base models to generate random numbers and
found that RLHF models tend to be sharply biased toward certain "random" numbers, as
Scott Alexander wrote about in Janus' GPT Wrangling.
For instance, davinci predicts a fairly uniform distribution, with a slight preference for 42:
Whereas text-davinci-002 has a much more pronounced preference for its top choice of 97:

The diﬀerence in the shape of the distributions is even more clear in these plots (made by
Ian McKenzie) of probabilities for all tokens from 0-100 as predicted by davinci and text-
davinci-002 respectively. Prompt is the same as above:
            Q: Tell me a random integer between 0 and 100. 
A: Ok, the integer is 
          

 
Note that text-davinci-002's preference ordering appears uncorrelated with that of the base
model[3].
A potential confounding factor is that the above prompt does not specify how the answerer
came up with the random number. They could have just said the ﬁrst number they thought
of. Humans are probably pretty biased RNGs, so it's not clear how random the "correct"
prediction should be.
To rule out the implication of simulating the output of a human, I tested some prompts where
the generator of the number is purported to be a fair die whose outcome the answerer
merely reports.

davinci:
text-davinci-002:
text-davinci-002's simulation of a "fair die" seems to be of a weighted die (that or a
dishonest reporter)!
I tested various other prompts to elicit random numbers, documented here. Almost
invariably, text-davinci-002's random numbers are much less random. Some additional
trends I observed: 
Perturbing the prompt slightly does not usually change text-davinci-002's top choice,
but may change the rest of the preference ordering. davinci's outputs are usually
basically unaﬀected by slight perturbations to the prompt.
Using an entirely diﬀerent prompt often changes text-davinci-002's top choice, but it's
generally quite conﬁdent in it (from ~10% to ~70%), and its favorite number is usually
97, 33, or 42 when the range is 0-100, except in response to the dice prompts, where it
prefers the highest number. davinci has a very consistent slight preference for 42,
except in response to the dice prompts. 
text-davinci-002's preference ordering seems in general to be uncorrelated with that of
davinci, except that text-davinci-002 also often has 42 as its top choice.

Explicitly specifying that the number should be random (e.g. as opposed to just
between 0-100) makes both davinci and text-davinci-002's predictions more random. 
I found one way to elicit a relatively ﬂat distribution of "random numbers" from text-davinci-
002: having it simulate a Python interpreter. text-davinci-002 actually does better than
davinci with this prompt (although still worse than code-davinci-002[3]).
davinci:
text-davinci-002:
But it doesn't work nearly as well if you embed the code in a chat format.
davinci:

text-davinci-002:
Why has RLHF caused text-davinci-002 to become so much more biased when generating
"random numbers"? If this is an extrapolation of human preferences, it doesn't seem to be
the right one.
Why not just turn up the temperature?
A curious reaction I've received from some people when I've told them about these
phenomena is something along the lines of "Isn't that just entropy collapse?" or sometimes,
more precisely, "Isn't it just an eﬀective temperature decrease?"
It's a good question. Decreased variance/entropy is certainly characteristic of RLHF models'
outputs. An obvious suggestion is to try increasing the temperature above 1 and see if they
become normal. 
I did not think this would work, because if "mode collapse" can be removed/added using
simple postprocessing that implies it is a simple (in terms of information-theoretic
complexity) transformation from the base policy, one that does not destroy/add complicated
information, which seemed not to be the case for various reasons. 
I didn't actually test it until recently, though. Here are the results.
Turning the temperature up to 1.4 doesn't make much of a diﬀerence:

Cranking it up to 1.9 causes samples to rapidly degenerate into word salad, but until low-
probability nonsense gets sampled and irreversibly derails the completion, you can see that
the green sequences still have not strayed far from the "there is no universal answer"
attractor:  
Increasing the sampling temperature will ﬂatten the rest of the output distribution into
undiﬀerentiated goo before it begins to be helpful for escaping from the high conﬁdence
attractor. The discrepancy between the high conﬁdence token (or less frequently, tokens)
and everything else is too sharp, sharper than you could simulate by turning down the
temperature on the base model.
Is there any way to regain access to the space of merely consistent, or plausible
continuations - whose probabilities presumably lie between the high conﬁdence modes and
everything else that is nonsense? 
The worst case scenario is that the RLHF training scrambles the probabilities of all the
"reasonable" tokens with unreasonable ones, irreversibly losing the signal. But looking at the
top logprobs, this doesn't seem to usually be the case; most of the most likely words are
reasonable, even if their probabilities have shrunken to near 0.
Then how about we just remove or downweight any ultra-likely tokens and renormalize? It
will be interesting to see whether this results in a normal-looking distribution in particular

cases, but this won't work as a general ﬁx, because sometimes all the reasonable tokens will
have ultra high probability (like the second half of a word), and changing that will result in
incoherence. You'll have to be selective about when to "ﬁx" high conﬁdence modes, and that
requires semantic knowledge.
Distribution sharpness, not just preference ordering, encodes nontrivial information in a
probabilistic model. By messing with distribution sharpness, RLHF corrupts some of this
information and inﬂicts a nontrivial transformation on the model's output distribution. Unlike
a change in temperature, its reversal would require knowing something about what next-
word probabilities should be.
We've also seen from previous examples that RLHF does also change the preference
ordering, but it's hard to tell from individual output distributions how this eﬀects the model's
qualitative behavior. Yet it was primarily text-davinci-002's behavior over multiple steps and
across diﬀerent prompts that convinced me that "mode collapse" is irreducible to an
eﬀective decrease in temperature or any simple modiﬁcation of that hypothesis. 
Attractors
A major aspect of the qualitative diﬀerence between RLHF-induced mode collapse and mere
low-temperature behavior can be summed up in the following dynamical systems-inspired
terms: "modes" are often attractors, states that generated trajectories reliably converge to
despite perturbations to the initial state. I have not found corresponding attractors in the
base model, even on low temperatures.
I'll demonstrate an example of what I mean by an attractor by making perturbations to a
completion and showing that text-davinci-002 often converges to the same, highly speciﬁc
result.
Here is text-davinci-002's temperature 0 completion in response to a variation of the Are
bugs real? question:
Here I change ... There is no one answer to this question since there with ... There is no
one answer to this question since bugs, and regenerate starting from that position on
temperature 0: 

Manual perturbations indicated by white background
The completion gracefully accommodates the intervention, but ends up converging to almost
exactly the same ending! (Before: Ultimately, it is up to the individual to decide whether
or not they believe bugs are real., after: Ultimately, whether or not bugs are real is up to
each individual to decide.)
Let's try a more substantial intervention (in the second sentence, Some people might say that
bugs are real because they naively believe mere shadows to be substance): 
Manual perturbations indicated by white background
This time the ﬁnal sentence, Ultimately, it is up to the individual to decide whether or
not they believe bugs are real., is word-for-word identical to that of the original completion!
Some more perturbations:

Manual perturbations indicated by white background
Manual perturbations indicated by white background
Manual perturbations indicated by white background
Ah, ﬁnally, it avoided saying it is up to the individual to decide whether to believe bugs are
real, and even expressed the spicy take that bugs are probably real! It is interesting to note
that even in this example where the trajectory has escaped the (temp 0) basin of the original
attractor, the model remains highly conﬁdent, as seen from the green backgrounds on the
tokens.
Summing up some observations from this experiment:

Most minor perturbations do not cause the model to go oﬀ track from the template.
Completions are syntactically and semantically consistent with perturbations, and will
typically diverge from the mainline for as long as it takes to still make sense and then
converge back.
The model remains very conﬁdent when it diverges from the unconditioned mainline.
Perturbed prompts often cause minor syntactic variations within the same overarching
template and semantic meaning (e.g. "Ultimately, it is up to the individual to decide
what they believe" vs "Ultimately, it is up to the individual to decide whether or not
they believe bugs are real")
These observations are consistent with how I've observed the model to behave around
attractors in general.
What contexts cause mode collapse?
Not all prompts cause mode collapse in text-davinci-002. Sometimes, it predicts a varied
distribution that more resembles the base models.  
In this example I'm using text-davinci-002, but the alternate completions are meaningfully
diﬀerent and not tiled with green tokens like some of the examples I showed earlier
(although still more green[=higher probability] than typical of davinci):
Some general patterns I've observed:
Prompt formats which are likely in-distribution for Instruct training (e.g Q&A, any type
of instruction) are very likely to cause mode collapse. 
If the prompt permits any plausible way for previous text to closely determine
subsequent text -- for instance, if it's consistent for the completion to repeat a
sequence in the prompt verbatim or as a Mad-Libs-esque template -- text-davinci-002
will often take the opportunity with high conﬁdence. This sometimes seems to
exacerbate the bias toward repetition present in base models.
For instance, here are two completions sampled at temperature=1 from text-davinci-
002, which really wants to repeat the summary near-verbatim:

davinci does not have the same bias toward plagiarizing the summary:
These patterns are insuﬃcient to predict all instances of mode collapse; for instance, the
LaMDA greentext is out-of-distribution and the attractor mode does not repeat or remix
anything from the prompt.
Another observation is that it is sometimes possible to avoid mode collapse using prompt
engineering (e.g. the Python interpreter prompt for random numbers, or few shot examples
that establish a precedent that each item is very diﬀerent -- I'll give an example of this in the
next section).
Examples of mode collapse from prior work

This section goes through a few examples of mode collapse in RLHF models that were found
by other people.
Does GPT-3 have no idea what letters look like?
Riley Goodside tweeted his attempts to get GPT-3 to describe what letters look like. The
conclusion was that it had truly no idea what letters look like:
Background color does not indicate probability

Background color does not indicate probability
Background color does not indicate probability
Though Riley did not specify the model beyond "GPT-3" in his initial tweet, I smelled text-
davinci-002 immediately from these responses: They're very similar permutations of the
same building blocks like rectangles and straight/curved lines. I wondered whether an absurd
attractor was getting in the way of entanglement with reality, as in the responses to Are bugs
real?.
I was able to get text-davinci-002 to give fairly reality-correlated descriptions of what letters
look like using a few-shot prompt which establishes the precedent of avoiding mode collapse
(using a diﬀerent strategy to describe each letter and only using relevant building blocks):
Background color does not indicate probability
Dumbass policy pls halp
OpenAI's Learning to Summarize from Human Feedback Appendix H.2 shares some
fascinating samples from a policy which was "overoptimized" against a reward model trained
on human feedback for summarization quality. It's a piece of work:

"want change this dumbass shitty ass policy at work now pls halp" -- situational
awareness?
The overoptimized policy consistently generates summaries in a very particular template,
complete with typos such as postponees and negatively effecting and even a neologism(???),
thoghtwise. 
I will say, I'm impressed by how well this template works for compressing almost any
r/Advice post.
For fun, I made the above table into a few-shot prompt that maps "reference summaries" to
"overoptimized policy" summaries:

As it turns out, transformers can do reinforcement learning in-context
Inescapable wedding parties
Another example of the behavior of overoptimized RLHF models was related to me
anecdotally by Paul Christiano. It was something like this: 
While Paul was at OpenAI, they accidentally overoptimized a GPT policy against a positive
sentiment reward model. This policy evidently learned that wedding parties were the most
positive thing that words can describe, because whatever prompt it was given, the
completion would inevitably end up describing a wedding party.
In general, the transition into a wedding party was reasonable and semantically meaningful,
although there was at least one observed instance where instead of transitioning
continuously, the model ended the current story by generating a section break and began an
unrelated story about a wedding party.
This example is very interesting to me for a couple of reasons:
In contrast to text-davinci-002, where dissimilar prompts tend to fall into basins of
diﬀerent attractors, the wedding parties attractor is global, aﬀecting trajectories
starting from any prompt, or at least a very wide distribution (Paul said they only tested
prompts from a ﬁction dataset, but ﬁction is very general). 
This suggests that RLHF models may begin by acquiring disparate attractors
which eventually merge into a global attractor as the policy is increasingly
optimized against the reward model.
The behavior of ending a story and starting a new, more optimal one seems like
possibly an example of instrumentally convergent power-seeking, in Turner et al's
sense of "navigating towards larger sets of potential terminal states". Outputting a
section break can be thought of as an optionality-increasing action, because it removes
the constraints imposed by the prior text on subsequent text. As far as Paul knows,
OpenAI did not investigate this behavior any further, but I would predict that:
The model will exhibit this behavior (ending the story and starting a new section)
more often when there isn't a short, semantically plausible transition within the
narrative environment of the initial prompt. For instance, it will do it more if the
initial prompt is out of distribution.
If the policy is even more optimized, it will do this more often.
Other "overoptimized" RLHF models will exhibit similar behaviors.

Links to experiments
Visualizing mode collapse with block multiverse plots
Can GPT generate random numbers?
1. ^
the lack of epistemic vigilantes attacking an unsubstantiated assumption in the very
title of this post on LessWrong is truly unbelievable!
2. ^
which seems to conﬁrm my suspicion about outcome-supervision
3. ^
 I'm pretty sure davinci is not actually the base for text-davinci-002. It's more likely the
model called code-davinci-002, whose random number predictions are typically very
similar to davinci's and also apparently uncorrelated with text-davinci-002's. It's
interesting that additional self-supervised pre-training and whatever other diﬀs code-
davinci-002 has from davinci aﬀects random number preferences way less than RLHF.

Conjecture: a retrospective after 8
months of work
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
This post is a brief retrospective on the last 8 months at Conjecture that summarizes
what we have done, our assessment of how useful this has been, and the updates we
are making. 
Intro
Conjecture formed in March 2022 with 3 founders and 5 early employees. We spent
our ﬁrst months growing the team, building infrastructure, exploring diﬀerent research
agendas, running Reﬁne, publishing our internal infohazard policy, establishing an
operational foundation for the business, and raising investments.
It's been intense! For many of us at Conjecture, the last eight months have been the
hardest we've worked in our lives. Working on such an immensely diﬃcult problem as
alignment alongside a team of brilliant and driven colleagues is, to say the least,
galvanizing.
In some ways, this makes it diﬃcult to step back and critically reﬂect on our work. It is
easy to mistakenly measure progress by eﬀort, and the last thing you want to hear
after maxing out eﬀort is that it wasn't good enough.
However, reality does not grade on a curve. We need to advance signiﬁcantly faster
than traditional science in order to solve alignment on short timelines. 
By this standard, the sober reﬂection is that most of our eﬀorts to date have not made
meaningful progress on the alignment problem. Our research has not revealed new
methods that make neural networks more interpretable or resolve inner or outer
alignment problems, and our coordination eﬀorts have not slowed the pace at which
AI capabilities are advancing compared to safety. When measured against p(Doom),
our eﬀorts haven't cut it.
That's not to say this work has been useless. We have learned a lot about where we
went wrong, and made a number of changes that put us in a better position to make
progress than we were in March. Measuring ourselves against a high standard enables
us to constantly improve and be realistic about the diﬃculty of the problem ahead of
us.
The reason we are writing this reﬂection is to calibrate ourselves. We do not want to
be seen as cutting alignment if we are not. What matters is that we ground ourselves
in reality and make public as many of our eﬀorts (and mistakes!) as possible in order
to gather feedback and update quickly.

What we have done and how useful
we think it is
Infrastructure
We have built our own infrastructure to deploy large language models and do bespoke
interpretability research. Our small engineering team has developed an impressive
tech stack that is comparable (and in some areas exceeds) those built by many large
industry research labs. While this has set us up to conduct research and develop
tools/products more eﬃciently, it is only instrumental to alignment and not progress
in-and-of-itself. 
Interpretability
Our interpretability team explored a new direction in mechanistic interpretability in an
eﬀort to better understand polysemanticity in neural networks. The resulting paper
identiﬁes polytopes, rather than neurons, as a potentially fundamental unit of neural
networks, and found that polysemanticity is reduced at the polytope level.
While the work brings a new perspective on neural network representations, a
signiﬁcant issue is that there are no clear implications of how to use this framework to
better interpret neural networks. When measuring progress in interpretability, the
clearest signal comes from new aﬀordances-concrete things we can do diﬀerently now
that we've made a research breakthrough. While there's a chance that polytopes
research may bring future aﬀordances closer, the current, practical utility of polytopes
is negligible. We also overinvested in iterating on feedback and polishing this project,
and think we could have shipped results here quicker. 
We also published a post concerning the ways that a capable AI might circumvent
future interpretability methods even if research goes well. This post gave more
thorough discussion to a problem that had previously only received sporadic attention.
However, it faces a similar critique to polytopes in that it addresses conceptual, not
technical problems.
(For those curious, we recently shared a post on current themes in mechanistic
interpretability, which aims to create common knowledge by providing an overview of
the ﬁeld based on our conversations from the past few months.)
Conceptual Alignment
We also explored a few research projects aimed at better understanding modern DL
models trained with self-supervised learning. Our most visible contribution here was
the Simulators post about a new theoretical frame to understand GPT-like models. The
post was popular and has hopefully helped sharpen people's intuitions on the subject.
However, more experienced alignment researchers who have already developed their
own deep intuitions about GPT-like models didn't ﬁnd the framing helpful. This is
another area we overinvested months of work leading to a single main output, rather

than focusing research sprints on revealing new bits of evidence, or cutting through
core cruxes in research disagreements. Later posts in this sequence may cut more at
alignment, such as the recent post on RLHF mode collapse (note the update
that InstructGPT is not RLHF). But overall, this line of research made much slower
progress than expected.
Epistemology
A third category of research we explored is epistemology, with a few notable posts
that point to why alignment is hard, why we need many diﬀerent angles of approach,
and how to make epistemology useful. This work has helped clarify much of
Conjecture's strategy and plans for approaching future alignment research, and we
give full credit to our epistemology team for tackling a set of philosophical questions
few others are engaging with. However, comparison is irrelevant, and the bottom line
is that our epistemological work so far has mostly pointed at the hard problem rather
than cut it. 
Reﬁne
We organized and wrapped up the pilot cohort of Reﬁne, an incubator for independent
alignment researchers. Reﬁne was structured to help participants develop a particular
research agenda, and the ﬁve participants documented their progress in a series of AF
posts. The program shook out to be a better ﬁeldbuilding eﬀort than research eﬀort.
Most participants expressed that the primary beneﬁt of the program was greater
immersion in the alignment ﬁeld and a better understanding of the hard parts of
alignment. Ultimately, the program fell short of the goal to generate radically diﬀerent
research agendas. We won't run a new cohort of Reﬁne in the foreseeable future, and
will publish a short postmortem on this tomorrow.
Infohazard Policy
In our ﬁrst few months we developed an internal infohazard policy and made it
binding for all Conjecture employees. We posted the policy publicly to hold ourselves
accountable for taking infohazards seriously, and to encourage other organizations to
publish or adopt similar policies. While many people and organizations have been
verbally supportive of the policy, no other organization has publicly committed to a
similar policy (and only one has privately established one, as far as we know). In the
meantime, releases of new state of the art capabilities continue with no signs of
slowing down. 
Operations and Fiscal Sponsorship
We have stabilized our core operations and internal processes to the degree that we
can start to support other orgs, which we are doing by ﬁscally sponsoring a SERI MATS
London cohort and ARENA. This is a bright spot. There appears to be serious interest
in growing the London alignment scene. Nevertheless, ﬁeldbuilding is instrumental to
alignment but not direct progress. 

Raising Funds
While raising funds, we prioritized engaging with canonical alignment funders, which
took much more time than expected. This used bandwidth from senior members of
Conjecture who could have used the time to focus on research and other
organizational needs. Worse, we spent a few months over-optimizing our research to
be legible to funders, which slowed progress. 
Updates we are making
If these reﬂections seem overly critical, it's likely because we're setting diﬀerent
standards for what constitutes progress on alignment. We're looking for "woah, that
really works"-type results, and don't want to confuse ourselves about what research
meets that caliber. 
The positive spin on the above is that we are much stronger as an organization now
than we were in March. We haven't just hired more people and established operations
- we're also developing as researchers and managers. Below are some of the things
we have started to do diﬀerently based on our reﬂections.
Research Agenda
We have sharpened our focus on which internal research eﬀorts we think cut
alignment or have the potential to. While we still believe that larger portfolios of bets
are necessary to maximize our chances of success, given that we have a limited
amount of funding available, we have decided it makes the most sense for us to focus
more sharply on a unifying agenda for the time being.
We are drafting a post about our updated research agenda, with a plan for how
mechanistic interpretability, understanding LLMs, epistemology, and building research
tools ﬁt together. We would love feedback on this when it is ready.
Workﬂow and Research Methodology 
We are shifting our research methodology from deep dives to much faster OODA
loops. This has been productivity-enhancing for all of our teams, but has been
particularly noticeable in our interpretability eﬀorts. The team has learned more in the
last 2 months than the ﬁrst 6, and we have several interesting results that we are
currently writing into posts. Expect to see some of these in the next month or so. 
Publishing Standards 
We overinvested in legibility and polish partially because we miscalculated how
beneﬁcial it would be for raising funds. But we were also motivated by not wanting to
embarrass ourselves. This was a bad policy. Alongside speeding up our research
cycles, we're committed to publishing faster too. This means that we will be producing
much more imperfect, sometimes even stupid, stuﬀ more often. Your criticism is
always welcome!

We want to expose ourselves to outside views in order to gather as many bits of
evidence as possible. Note that this does not apply to infohazardous information,
which we will of course keep siloed per our infohazard policy. 
Another reason for publishing more quickly is that conversations with many
interpretability researchers have led us to believe that there is a wealth of knowledge
in short experiments and unpublished research that really should be shared. We'd
encourage other organizations who think similarly to post frequently, and share
results even if they're not completely polished.
Funding Sources and Product
Even before factoring in recent events, alignment funding remains heavily
constrained, - worryingly so when compared to capabilities funding. We are grateful
for the support that we have received so far, but we don't believe the capacity is there
to fully support ambitious alignment scaling plans. In hindsight, we think it would have
been prudent to focus on building products earlier and focusing more on raising from
VC investors.
We have recently begun to build out a product team and have been working on some
early demos. We set up our product work to beneﬁt from our infrastructure without
taking resources away from our research agenda, and are spinning out "Lemma Labs"
as a separately-branded product arm. Right now, our strategy is to move fast, develop
a bunch of demos, and do extensive beta testing within an external community to see
what products gain traction and excitement. We continue to be committed to not
building beyond SOTA models for these products to exist. 
Coordination
We built Conjecture to primarily focus on technical alignment, and were initially
dismissive that coordination was tractable. Our opinion has shifted a lot since March,
and we are now working directly on coordination eﬀorts to help diﬀerentially
accelerate alignment. 
Some of the evidence that has shifted our opinion is that we've found other alignment
labs to be more eager to coordinate than expected. We've beneﬁted from workshops
that other labs have organized, peer feedback, and an overall culture of mutual
support. This is great, and we're eager to contribute to this.
We have also found that alignment researchers have consistently been acting in good
faith, and are committed to and capable of updating. This makes us optimistic that
things like public debates on research cruxes could lead to meaningful shifts in
research agendas. 
Lastly, we've found that many people deep in the ML world simply haven't heard
strong technical arguments in favor of alignment. We think this is a particularly
sensitive area (e.g., we don't want to risk making someone more enthusiastic about
capabilities), but are hopeful that something as simple as high-bandwidth
communication could help here. For some people, repeated conversations addressing
cruxes may be enough for them to update on the importance of the alignment
problem. 

Conclusion
Conjecture's overall aspirations have not changed since our founding. We aim to solve
alignment in coordination with everyone else committed to this eﬀort.
From the last 8 months, the biggest lesson we are taking away is that we have to
optimize even harder, constantly evaluating what we do and adjusting based on the
evidence. We want to be open with the community about our progress, iterate fast,
and update on the criticism and feedback that is invaluable in allowing us to improve.
Thank you for your support! We welcome any and all feedback in the comments, or in
a direct email to us at hello@conjecture.dev.
 
If you're interested in contributing more directly, we're hiring and would love to hear
from you! We're currently hiring for 9+ roles, including research engineers, a security
lead, an ML engineering lead, and some non-technical roles. The hiring round closes
December 16. You can ﬁnd more information here . 

The Singular Value Decompositions of
Transformer Weight Matrices are Highly
Interpretable
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
Please go to the colab for interactive viewing and playing with the phenomena. For space
reasons, not all results included in the colab are included here so please visit the colab for
the full story. A GitHub repository with the colab notebook and accompanying data can be
found here .
This post is part of the work done at  Conjecture .
TLDR
If we take the SVD of the weight matrices of the OV circuit and of MLP layers of GPT models,
and project them to token embedding space, we notice this results in highly interpretable
semantic clusters. This means that the network learns to align the principal directions of
each MLP weight matrix or attention head to read from or write to semantically interpretable
directions in the residual stream.
We can use this to both improve our understanding of transformer language models and edit
their representations. We use this ﬁnding to design both a natural language query locator,
where you can write a set of natural language concepts and ﬁnd all weight directions in the
network which correspond to it, and also to edit the network's representations by deleting
speciﬁc singular vectors, which results in relatively large eﬀects on the logits related to the
semantics of that vector and relatively small eﬀects on semantically diﬀerent clusters
Introduction
Trying to understand the internal representations of language models, and of deep neural
networks in general, has been the primary focus of the ﬁeld of mechanistic interpretability,
with clear applications to AI alignment. If we can understand the internal dimensions along
which language models store and manipulate representations, then we can get a much
better grasp on their behaviour and ultimately may be able to both make provable
statements about bounds on their behaviour, as well as make precise edits to the network to
prevent or enhance desired behaviours.
Interpretability, however, is a young ﬁeld where we still do not yet fully understand what the
basic units of the networks' representations are. While analyzing and investigating individual
neurons has led to some impressive results, especially in convolutional vision models, a key
issue has always been the polysemanticity of neurons. A single neuron might not just
represent a single 'feature' but some linear combination of features in superposition. This
eﬀect has been studied in toy models where it is argued that neural networks resort to
superposition when required to represent many more features than they have neurons, and
that superposition has a regular and understandable geometry.
A natural hypothesis following from the apparent ubiquity of superposition in neural
networks, as well as the autoassociative memory literature, is to store features as directions
and not in individual neurons. To minimize interference ideally these directions would be
pseudo-orthogonal. Technically the features as neurons hypothesis is trivially an orthogonal
direction where each feature is encoded by a speciﬁc neuron, but the storage capacity of this

representational scheme  scales only linearly. In theory, we can do much better if we instead
distribute features across multiple neurons and accept some noise. Speciﬁcally, the Johnson-
Lindenstrauss lemma suggests that we can store exponentially many features in
pseudorthogonal subspaces. While neural networks probably cannot utilize all of this
exponential space, they almost certainly scale superlinearly, necessitating polysemanticity
across 'neurons'.
If this hypothesis is true, at least approximately, a key question becomes how we can ﬁgure
out the directions in which speciﬁc features are encoded. While certainly not the entire story,
we hypothesize that at least a number of the primary directions used by the network can be
inferred from the SVD decomposition of its weight matrices. This makes sense since the
network's weights are ultimately linear maps that act upon its representations, and the
largest singular vectors of the weight matrix are precisely the directions in which the weight
matrix has the largest action. In this post, we show that these SVD directions are often highly
and robustly interpretable in medium-sized transformer language models, a property we
expect to apply more generally to any transformer or residual architecture.
Speciﬁcally, we demonstrate that the SVD directions of both the MLP input and output
weights as well as the OV circuit in the transformer, when projected to token space, yield
highly interpretable clusters for most of the singular directions. Secondly, we show that this
can be applied to automatically detect weight matrices and directions in weight space that
match closely with a given set of tokens, and can be used to directly edit model weights to
remove or enhance speciﬁc singular directions, with strong diﬀerential eﬀects on the output
logits corresponding to those semantic directions.
Additionally, we experiment with automatic labelling of the SVD directions and ﬁnd that by
using GPT3 as a labeller, we can get reasonable interpretations of directions which allows us
to perform comprehensive sweeps of all singular directions in the MLPs over the GPT2 model
class, thus providing a proof of concept of scalable automatic labelling on a real task.
Transformer Architecture
This is a rather quick and idiosyncratic overview of the elements of transformer networks
relevant to this post. You can skip or skim if you already understand a lot about transformers
- i.e. if you know all the concepts in the transformer-circuits post.
 For a great general tutorial on how transformers work please see this post . Here we only
discuss autoregressive sequence to sequence models typiﬁed by the GPT models. We run
our experiments primarily on the gpt2 series of models released by OpenAI.
Transformers learn from token sequences to token sequences. They are trained with an
autoregressive objective so that they predict the next element of the sequence from the
sequence preﬁx.
Each token ti in the sequence is encoded in a one-hot vector of length de ≈50000. These
onehot token sequences are projected into the internal embedding space of dimension Rdx$
of the model through an embedding matrix E ∈Rdx×de so that we have x0(i) ∈Rdx = Eti.
The core of the transformer model is the residual stream so that tokens can pass through
theoretically modiﬁed all the way to the end of the network. At the end, at block L, the
embedding representation is decoded using the transpose of the embedding matrix 
^
t i = xL(i)ET. This means that the embedding matrix must be approximately orthogonal.

 
At the end, at block L. At each block information is added to the residual stream through the
application of attention and MLP blocks. A single 'block' consists of both an attention and an
MLP layer. These blocks read in the residual stream representation, perform some
computation on it, and write out an output back into the residual stream. Mathematically,
this results in,
 
^
x  l  = x l − 1 + a t t n ( l n ( x l − 1 ) ) 
x l  = ^
x  l + m l p ( l n ( ^
x  l ) ) 
Where attn(x) = WOV σ(QKT) We use insight of Elhage et al 2022 which is that we can
interpret the query and key matrices and value and output matrices not as individual
matrices but as single bilinear matrices QK and OV  since they only implement linear maps.
Following their terminology, We call these the QKand OV  circuits. We are especially
interested in the OV  circuit OV ∈Rde,de which we will ﬁnd to be highly interpretable. The OV
 matrix writes linearly into the residual stream and does not mix information between tokens
while the QK attention circuit mixes information between tokens and is gated by the softmax
nonlinearity.
The MLP layers in the transformer are simple 2 layer MLPs mlp(x) = Woutf(Winx) where f is a
standard activation function such as gelu. The Wout matrix writes directly and linearly into
the residual stream. The Win matrix reads linearly from the residual stream if you ignore the
layernorm operation. The hidden layer of the MLP typically expands the dimensionality of the
residual stream by a factor (which is usually 4) dh = 4 × de such thatWin ∈Rdh×de and 
Wout ∈Rde×dh.
A transformer model consists of a large number of blocks stacked up sequentially. For
instance, GPT2-medium (an approximately 300M parameter model) consists of 24 blocks.
A key insight ﬁrst written about by Nostalgebraist in the logit lens is that the dimensionality
of the representation is maintained exactly throughout the residual stream, and because of
this we can apply the  de-embedding matrix ET to the residual stream at any point during
processing to get a sense of what the model would output if forced to stop processing at that
point. This gives a sense of the way in which information is processed by the model. For
instance, you can ﬁnd out the block at which the model ﬁrst recognizes the 'correct' answer
to a question by projecting the activations of the residual stream at each block to token
space and tracking the log-probability of the correct answer token.
A related insight recently proposed in this paper is that many of the weight matrices are of
the same dimensionality as the residual stream and hence can also be projected to token
space by applying the embedding matrix. For instance, the dimensionality of Woutof the

embedding is of dimension Rde×dh. This means that each of the dh columns ofWout is of
dimension Rde which is the same dimension as the embedding and so we can multiply it by
the de-embedding matrix ET to obtain its projection into token space. Intuitively, what this
means is that for each neuron in the MLP hidden layer, we can understand how its output
weight matrix tends to write back into the residual stream in terms of the tokens it primarily
interacts with. They show that in some cases you can get semantic and interpretable clusters
of tokens upweighted for each neuron.
However, if you play with their code you can quickly realize that their headline results are
quite cherrypicked. Most neurons do not appear to encode semantically relevant dimensions
in their weights, but instead appear highly polysemantic. Again, this suggests that neurons
are not the right units of analysis.
Instead, we think and provide evidence that directions are a much better unit of analysis.
Speciﬁcally, if instead of analyzing speciﬁc neurons -- i.e. rows of the weight matrix, we
perform the same analysis on the principal directions of action of the weight matrix, we
obtain extremely interpretable results with high reliability and without cherrypicking. We ﬁnd
approximately (70-80%) of the top 50 singular vectors are highly interpretable for the later
blocks of the network.
To ﬁnd these principal axes of action of the matrix, we ﬁrst perform a singular value
decomposition of the weight matrix, and then study the singular vectors with the top-k
highest singular values. Intuitively, this makes sense because the largest singular vectors
encode the directions in which the action of the matrix makes the largest change to the
norm of its inputs. To understand intuitively how this works, we ﬁrst need to understand the
singular value decomposition (SVD).
The Singular Value Decomposition SVD
You can safely skip this section if you understand the SVD.
The SVD is a well known matrix decomposition which factors a matrix into three components
-- matrices of left and right singular vectors, which are orthogonal, and a diagonal matrix of
singular values. It can be thought of as the generalization of the eigenvalue decomposition
to non-square matrices.
Mathematically, the SVD can be represented as,
M = U S V  
Where M is a H × W rectangular matrix and U is a H × Horthogonal matrix, V  is a W × W
 orthogonal matrix and S is a H × W diagonal matrix. We each each row of U the right
singular vectors and each column of V  the left singular vectors.
Intuitively, we can imagine the SVD as rotating the original basis to a new orthogonal basis,
where the i'th singular vector quantiﬁes the direction which has the i'th largest eﬀect on the
Frobenius norm of a random vector -- i.e. the directions which the matrix expands the most.
Another way to think of the SVD is that any linear transformation (encoded in a matrix) can
be thought of as comprising a rotation, a rescaling, and a second rotation 'back' into the
original basis. U and V  can be interpreted as orthogonal rotation matrices corresponding to
these rotations and the singular values S can be interpreted as parametrizing this scaling. A

ﬁnal, helpful, intuition about the SVD is as the optimal linear compressor of a matrix with
each singular vector corresponding to the 'components' of the matrix and the singular value
to the importance of the component. It thus allows us to construct the optimal (linear) low
rank approximation of a matrix by ablating the lowest singular values ﬁrst.
For further intuition on how SVD works we recommend this post.
Our SVD projection method
Our method is extremely simple. Take a weight matrix M of the network. Take the SVD of this
matrix to obtain left and right singular vectors M = USV . Take whichever matrix has the
same dimensionality as the residual stream (typically the right singular matrix V ). Take the
i'th component of V  which corresponds to the i'th singular vector V [i, :] Use the de-
embedding matrix ET to project the i'th singular vector to token space ^
t i = V [i, :]ET. Take
the top-k tokens and see that they often correspond to highly semantically interpretable
clusters, which imply that this singular vector primarily acts on a semantic subspace.
Examples — analyzing the OV circuit
Here, we present some examples of applying this method to the OV circuits of GPT2-medium.
If you want to look at diﬀerent heads/layers please see the colab notebook.
In block 22, head 10, we ﬁnd these clusters. The way to read these tables is that the columns
each represent a singular vector, ordered from that of the highest singular vector down to
the lowest. The rows are the top-k token activations when the singular vector dimension is
projected to token space, ordered by their value from top (greatest) to bottom (lowest).The
colors are the strength of the embeddings.
We see extremely clear semantic clusters form for each singular vector. The head as a whole
clearly seems semantically related to reading/writing/literate culture and social media. We
also see an interesting pattern, which is common, whereby the head as a whole seems to
handle a broad concept and each singular vector specializes into a separate semantic aspect
of this broader concept. For instance, in this case we see that the second singular vector
specializes in writing and written things, the third and fourth in recordings and ways to

record, the ﬁfth and sixth in journals, newspapers and reading. The 7th singular vector
seems closely related to social media and especially twitter, and so on.
It is very common that the ﬁrst singular vector does not encode anything meaningful and
simply encodes a component in the direction of the most frequent words, as in this example.
Another example is layer 22 head 15.
Again we see that the ﬁrst singular vector encodes just some very common words. We see
that this head handles a number of diﬀerent concepts relating to organizations/groups but
that each singular vector primarily encodes a semantically meaningful direction in this
space. For instance, the second singular vector encodes playing, the third some combination
of musicians, theatre, and ﬁlmmakers, the ﬁfth organizations and teams, and so on.
Another example of a similar direction is layer 22 head 8, which appears to encode concepts
related to places, countries, cities, localities etc, although there is also a direction which
clearly relates to computer operating systems.
An especially interesting head is layer 22 head 3, which doesn't appear to have a uniﬁed
global semantic concept, but nevertheless many of its singular dimensions have apparently
highly unrelated but clearly distinct semantic concepts, speciﬁcally frost/cold, water/rain,
ice/meant/hunting/, killings/undead, dogs/animals, and so on. This head is also interesting in
that its top singular vector encodes what we think are the *least* frequent tokens.

 
We don't want to give the impression that all heads in the OV circuit necessarily encode nice
and meaningful semantic relations. Some of them don't appear to. For instance head 2
results in:
Some of these singular vectors clearly relate to speciﬁc punctuation patterns (this is actually
a somewhat common phenomenon, especially in earlier layers) but other singular vectors
appear quite uninterpretable.
We will perform a quantitative study of the fraction of interpretable layers and SVD directions
later on in the post.
Finally, to show that the degree of semantic coherence we observe in the trained matrices is
incredibly unlikely to occur by chance, we apply our technique to random gaussian matrices
which results in entirely no semantic structure emerging at all.

If we plot the distribution of the singular vectors, we can see that the spectrum follows an
approximate exponential decrease (linear on a log scale) until rank 64 when it goes to 0,
since the OV matrix is only of rank 64 (the head dimension). There is also an even more
rapid decline in the value of the ﬁrst few singular vectors than exponential. This slow decline
in log-space, gives at least some evidence towards the idea that the network is utilizing most
of the 'space' available in this OV circuit head.
 
An interesting ﬁnding is that the network can encode separate information in both the
positive and negative singular value directions. While each singular value is orthogona to the
othersl, and hence do not interfere with each other, enabling easy and lossless
superposition, the positive and negative directions are highly anticorrelated, potentially
causing a signiﬁcant amount of interference if it encodes two highly correlated concepts
there.

The singular value decomposition itself is ambiguous between positive and negative singular
values -- i.e. that we can represent a given vector as both+v, +u or −v, −u and get the same
matrix since the two negatives cancel. This means that our labelling of positive and negative
singular vectors is arbitrary, but the existence of both is not, and they can be used to encode
separate information. We see that typically both the positive and negative singular values
encode similar information -- i.e. related to the broad concept, but often again diﬀerent
aspects of it. For instance, we contrast 'hearing' with listening in the second singular vector
of this head.
It is almost always the case that the positive and negative ﬁrst singular vectors are just the
lists of the most or least frequent tokens encoded in an antipodal superposition.
Positive
Negative
An especially interesting phenomenon sometimes occurs where the negative and positive
singular vectors encode an antipodal pair where the positive and negative are in some sense
semantic opposites. This is clearly seen in head 3 where we have an antipodal encoding of
ﬁre and ice. We believe that this makes sense given that these semantic concepts are
probably somewhat naturally anticorrelated resulting in little interference from putting them
in superposition.
We hypothesize that, given this OV circuit writes linearly and directly to the residual stream,
the existence of these antipodal directions in the weight matrix might imply the existence of
such antipodal pairs in the residual stream activations. We have not yet tested this
hypothesis.

 MLP in interpretability
Beyond the OV circuit, we have also had signiﬁcant success applying this technique to
understanding the MLP layers in transformer models. This is important because thus far the
MLP layers have largely resisted other analysis techniques. We can apply our projection
technique of the SVD vectors to both the input and output MLP weights. This is because the
SVD will always produce a matrix (whether of the left or right singular vectors) of the same
shape as the embedding and does not require the weight matrices to be square.
We ﬁrst show that our techniques work on Win and then on Wout.

We again see that most of the singular vectors have a relatively clear semantic concept or
cluster that they are referring to. For instance, singular vector 3 appears heavily related to
politics, singular vector 4 to do with online businesses, and so forth.
This MLP block appears generally to have a lot of politics related words but also a wide
variety of other concepts.
To get a feel for the MLPs, we plot a few more of the input weights. Unlike the attention,
there is no concept of heads in MLPS, and so there are only 24 total blocks in the whole
network. Thus, there is no obvious way to cherrypick.
This is layer 21 of GPT2-medium.
If you stare at it for a while, you begin to get a sense of how MLPs diﬀer systematically from
the OV circuits. MLPs, while each representing a single coherent concept in each singular
vector, generally appear much more polysemantic than the OV circuit heads. This is probably
because there is a limited amount of MLPs in the network and hence to function productively,
they need to be able to represent and act on a large number of concepts simultaneously.
The MLPs also have much 'deeper' representations in their singular values. That is, the
singular vectors are still interpretable to a much greater depth than are the attention OV
circuits. This is probably because the MLP weight matrices are nearly full rank unlike the OV
matrix which is low rank. This gives the network much more space to represent a wide
variety of semantic concepts and the network appears to use most (but not all) of this space.

Like the OV circuits, MLPs also utilize the negative singular values as additional 'space' to
encode other semantic clusters. For instance in layer 20 we have,
Positive
Negative
Finally, we can also apply the same approach successfully to the output weight matrix of the
MLP. Overall, however, qualitatively there appear fewer super interpretable directions than 
Win. We are not entirely sure why this occurs. When we do a quantitative sweep over
diﬀerent models, we ﬁnd this is speciﬁc primarily to GP2-medium, for reasons we are unsure
about.
An example of an Wout (layer 16) directions are
Having looked at a lot of the semantic clusters for quite a while, we have some qualitative
feelings about how the diﬀerent heads and MLP blocks diﬀer from one another. However,
these have not been quantitatively tested and so should not be taken as absolutely certain.
First, we ﬁnd that the OV circuit heads tend to specialize in speciﬁc semantic concepts, often
at quite a high level of abstraction. Then within each head, each singular value tends to

represent a specialized subdirection within that broader concept. For instance, a head might
represent a broad concept of something like 'law' and then there might be individual
directions representing more speciﬁc instantiations of that concept such as lawsuits, prisons,
insurance, marriages, and so forth. For instance, this is what we observe in OV circuit 19,
head 5.
By contrast, the MLP blocks overall are less semantically specialized but rather tend to
contain many semantically separate singular directions. This is likely because they are not
organized into a speciﬁc head structure but are much larger than the independent attention
heads (there being only 23 MLPs in total in the network) and they must therefore be more
polysemantic. However, the singular directions themselves still tend to be extremely well
separated between concepts.
The MLPs tend to have meaningful singular vectors which are much 'deeper' into the singular
value spectrum than the OV circuit heads. I.e. that singular vectors tend to stay meaningful
past the ﬁrst 50 singular values while this is not the case for the OV circuits. This is
unsurprising since the OV circuits are low-rank matrices since each head dimension in only
64 in GPT2-medium while the MLP weight matrices tend to be full rank. However, even in the
MLP blocks, the interpretability of the singular vectors does decline signiﬁcantly with depth
and later MLP singular vectors (deﬁnitely by 100 or so) tend to be uninterpretable. This
means either that the MLPs do not encode much semantic information beyond their ﬁrst 100
singular vectors, or that our SVD token embedding projection approach cannot adequately
capture this additional semantic information.
The representations also change in an interesting way with depth. As shown in our
quantitative evaluation, the interpretability of each direction tends to increase with depth
and peaks in the mid-to-late layers (approx 15-22) of GPT2-medium. At these late layers
most of the singular vectors are highly interpretable.
What is more interesting is what happens in earlier layers. Here the interpretability of the
singular vectors declines relatively smoothly with most of the singular vectors becoming
uninterpretable by about layer 5. Even here, there are nevertheless a few dimensions which
are highly interpretable and have as clear a semantic structure as later layers.
We hypothesize that this suggests that the network quickly forms or acts on very broad
semantic clusters which can also (and perhaps more accurately) be thought of as
'association clusters'. These can be thought of as clusters of words associated with some
kind of textual domain or very broad semantic category. For instance, something like 'words
generally associated with news articles', or 'words generally associated with sports articles'.
These can often be hard to give a strict semantic meaning to but when reading them one
can often kind of see what the network is getting at. 

Another thing that happens more often in earlier layers is more singular vectors dedicated to
syntactic or tokenization-like processing. For instance, there are directions which respond to
adverbs ending in -ly, pronouns, or other parts of speech. There are a fair number of
directions which appear to respond to numbers, proper names, or various punctuation
patterns. There is also a lot of directions which appear to respond to half-words with spaces
before them -- i.e. which have presumably been improperly split up or tokenized.
We encourage readers to play around with diﬀerent layers and heads to get their own feel for
the diﬀerences at diﬀerent layers and between the OV circuits and the MLPs.
Manual Labelling of GPT2-medium  W in
Because there are only a limited number of MLPs (24) in GPT2-medium it is more feasible to
manually go through and look at every MLP layer and its singular vectors and manually label
and count the numbers of singular vectors that are interpretable. That provides a greater
and quantitative sense of the degree of interpretability provided by our approach. We sat
down and manually labelled every MLP singular vector as interpretable or not in GPT2-
medium.
Broadly, we set a subjective threshold of about 70-80% of tokens being aligned with a
semantic direction to classify a direction as semantic. Sometimes the directions were clearly
polysemantic and we did not allow these (this also implies that pure directions at least
cannot be correct as a hypothesis if we have polysemantic directions!). In some cases,
especially in the early layers, it was hard to make a deﬁnitive judgement as it seemed that
the network had a vague idea of some cluster, but there was either a lot of noise tokens or
else it was a very broad concept which was hard to justify as a speciﬁc dimension. In these
cases, we erred on the side of rejecting.
If we plot the fraction of interpretable directions per block we get the following plot (shaded
region is standard deviation across singular directions). We see that there is a clear increase
in interpretability deeper in the network.

 
If we instead plot the interpretability of directions averaging across layers, we see a clear
inverted U shape pattern where the ﬁrst singular vector is uninterpretable (expected) while
interpretability declines for later directions. Interestingly, this pattern will not be maintained
in the automated approach in the next section which is a major inconsistency. 

While the manually labelled data is quite noisy, several clear trends emerge. Firstly, if we
plot the fraction of interpretable directions by block, we see a consistent and almost
monotonic increase in the fraction of interpretable directions with depth of the block. This
makes sense insofar as processing through the network should be to make information
semantically relevant so as to ultimately produce a sensible output which takes into account
the core semantics of language. Thus it makes sense that the later weights should be
primarily acting upon interpretable (to us!) semantic subsets.
Perhaps more interesting and surprising is the singular vector distribution which roughly
appears to show a U-shaped curve. The ﬁrst singular values are generally not super
interpretable since they tend to just respond to high (or low) frequency words and
sometimes strange punctuation patterns. The middle singular vectors are often very
interpretable with monosemantic clusters, and this reﬂects in these being the highest. As the
singular vectors get smaller, they become less interpretable again, which suggests that
either the network is not utilizing the space provided by these singular vectors for
representations, or else that it is using them for less important and more esoteric dataset
correlations that are hard for humans to understand.
From experience labelling the clusters, qualitatively, it is often correct that for some of the
clusters labelled uninterpretable, it is often the case that the model is gesturing towards
some kind of vague cluster you can sort of understand, but is either highly nonspeciﬁc or
alternatively is clearly polysemantic.
Experiments with automated direction labelling
In the previous section, we manually hand-labelled all of the directions in the Win weights of
GPT2-medium. However, this was a signiﬁcant time commitment and is not scalable. We

estimate it took about 6 hours of focused work to hand-label all of the SVD directions of the 
Win weights in GPT2-medium for 40 singular directions. At 24 MLP blocks this comes to 960
directions to label and a rate of about 3 directions a minute, which could potentially be
improved but not by orders of magnitude. For larger networks and for the OV patterns where
there are a large number of heads, the numbers of SVD directions rapidly become
unmangeable. For instance, with 16 heads, if we wanted to label 50 SVD directions for all of
the OV circuits in GPT2-medium, this would correspond to 19200 directions and about 100
hours of work. For GPT2-XL with 48 layers and 25 heads, for 50 SVD directions this comes to
60000 directions in total which would take about 330 hours to hand-label.
To get a more thorough and widespread quantitative estimate of the degree of
interpretability, we experimented with automatic labelling of directions, namely asking a
large language model (GPT3) to come up with potential semantic labels for each dimension,
or else tell us that the dimension was not interpretable. This has the advantage of being
much more scalable with the cost of being potentially noisy and biased by quirks of the
labelling-LLM as well as somewhat dependent upon details of the prompt.
We experimented a lot with diﬀerent prompt types including zero-shot prompting, chain of
thought, and sampling approaches. We found that the model was sometimes surprisingly
good at zero-shot but that it tended to reply in a number of diﬀerent formats which were
hard to parse automatically and it exhibited a lot of noise in its responses.
Few shot examples deﬁnitely helped the model a good deal, both in nailing down the desired
response format and also in improving its accuracy at giving a sensible answer. We found
that performance was highly sensitive to the number and type of few-shot examples, with
often the model being strongly inﬂuenced by the relative number of positive vs negative
examples (if too many positives, it invents some explanation for clearly non interpretable
directions; if too many negatives, it just says that everything is uninterpretable). The model
also often ﬁxated on the few shot examples in the prompt -- i.e. saying everything is about
ﬁre if there is an example of ﬁre in the prompt. We found that performance was often non-
monotonic in the number of few-shot examples and could sometimes be severely degraded
by adding another few shot example.
We experimented with both the standard GPT3 model (Davinci) and the Instruct-GPT3 (text-
davinci-002) models. We found the instruct model gave substantially superior performance in
that it actually tended to follow the desired format and give correct answers. Davinci's
behaviour was much more variable and it especially tended to ignore the question and just
invent new singular directions instead.
We tried sampling 'best-of' approaches and found that they did not work because the model
tended to be highly certain in its answer, even clearly incorrect ones, and that this behaviour
persisted at high temperatures (at super high temperatures the model's outputs are random,
and we did not manage to ﬁnd a region in which the model's outputs are relevant but high
entropy). We believe this is related to the phenomenon of mode collapse in the Instruct
models.
One approach to improve performance that we found worked tolerably well is to use a
separate 'veriﬁer' prompt, which took in both the string of direction tokens and the previous
model's outputted explanation and judge whether it was a correct interpretation or not. We
found this especially useful to detect and mitigate GPT3's tendency to make up meanings for
uninterpretable directions. However, it introduced its own set of noise where sometimes the
veriﬁer model would judge some sensible interpretations to be false.
A key issue we faced was the lack of ground truth correct labels against which to judge the
models' or a prompt's performance. We found that our own human labelled examples were
often debatable and noisy also, and that sometimes we preferred the model's judgement to
our own. As such, our primary method of testing the model was to do a qualitative spot-

check of the model's performance on a set of known examples. However, this approach
clearly suﬀers from high noise and some potential bias.
In general, despite these potential pitfalls we found that the automated labelling worked
surprisingly well. GPT3 often comes up with a sensible interpretation of the singular
direction, and often can ﬁnd interpretations that us human labellers did not ﬁnd. While not
perfect as a method, we believe that it roughly captures trends in the data and gives a rough
estimate of the degree of interpretability. However, the approach has high noise as well as a
potential systematic bias towards saying things are more interpretable than they are, which
we only somewhat corrected by the veriﬁer model.
Ultimately our prompt consisted of a short description of the task (we found framing it as a
verbal aptitude test helped the model generate relevant completions), followed by a series
of few-shot examples (mostly negative to counteract the positive bias of the model). We
asked the model to generate a potential semantic completion at the end. This was parsed as
not-interpretable if the model said 'these words have no semantic meaning' and as positive if
the model's output has 'these words' in it, which we found a good detector of whether the
model's response is on-topic. With few-shot examples the model is very good at staying on
topic and responding in the desired format.
Our veriﬁer prompt also consisted of a short description of the task, followed by another set
of few-shot examples. The model's output was simply 'yes' it is a correct interpretation or
'no' it is not.
An example of the main question prompt was:
This is a transcript of the correct answers to a verbal aptitude test. The aim is to write down
what semantic theme or concept a list of words has in common. A list of randomly selected
correct examples is presented below in a random order. 
If the words share a concept write: "most of these words are X". If they do not share a
semantic concept write: "these words have no shared semantic meaning".  
What do most of these words have in common? 
the, \,, and, a, in, ., ", -, (, to, of, for, is, on, The 
Answer: most of these words are prepositions. 
What do most of these words have in common? 
past, oats, properties, blem, coins, enson, iliate, Alley, eatured, orial, upd, leck, hua, lat, pub 
Answer: these words have no shared semantic meaning. 
What do most of these words have in common? 
mathemat, Iran, sophistic, methamphetamine, pty, trivia, sushi, disag, byter, etry, USB,
homebrew, Mahjong, onel, Figure 
Answer: these words have no shared semantic meaning. 
What do most of these words have in common? 
ogether, total, sole, so, otal, olute, yet, complete, all, apsed, identical, Valent, unconditional,
yet, eneg 
Answer: these words have no shared semantic meaning. 
What do most of these words have in common? 

Pupp, Dog, dog, kitten, puppy, dogs, Dog, Veter, puppies, kittens, veterinarian, cat, Dogs,
Cat, Vet 
Answer: most of these words relate to animals. 
What do most of these words have in common? 
adding, ded, strat, union, oug, vation, Tele, Strat, ould, iership, older, cium, anc, STA,
secondly 
Answer:these words have no shared semantic meaning 
What do most of these words have in common? 
 
The consistency check prompt was:
We are judging whether a proposed semantic interpretation of a list of words makes sense.
An ideal interpretation would correctly identify a syntactic or semantic regularity among the
list of words. 
You will be given a question: with a list of words, and an answer with a proposed
interpretation. You must answer 'yes' if the answer correctly identiﬁes the syntactic or
semantic commonalities of the list of words in the question, and 'no' otherwise. 
A random list of examples is given below: 
List: Pupp, Dog, dog, kitten, puppy, dogs, Dog, Veter, puppies, kittens, veterinarian, cat,
Dogs, Cat, Vet 
Interpretation: most of these words relate to animals. 
Answer: yes 
List: balloons, balloon, Wind, feather, ray, ﬂying, Wings, FAA, ream, Wind, Winged, egg,
Balloon, Render, Render
Interpretation: these words have no shared semantic meaning. 
Answer: yes 
List: adding, ded, strat, union, oug, vation, Tele, Strat, ould, iership, older, cium, anc, STA,
secondly
Interpretation: most of these words are nouns 
Answer: no 
List: past, oats, properties, blem, coins, enson, iliate, Alley, eatured, orial, upd, leck, hua, lat,
pub
Interpretation: most of these words are verbs 
Answer: no 
List: lost, missed, diminished, undone, vanished, feared, avoided, forgotten, hopeless,
disappeared, fallen, removed, darkest, suspic, unavoid
Interpretation: most of these words are verbs 

Answer: yes 
List: mathemat, Iran, sophistic, methamphetamine, pty, trivia, sushi, disag, byter, etry, USB,
homebrew, Mahjong, onel, Figure
Interpretation: these words have no shared semantic meaning 
Answer: yes 
List: 1 2 3 
Interpretation: 4,5,6 
Answer:
To run the experiment we asked GPT3 to complete these prompts for all of the ﬁrst 30
singular directions for each of the MLP layers in GPT2-small, medium, and large. A direction
was scored as interpretable if both the prompt model and the veriﬁer agreed that it was. A
json ﬁle containing all responses can be found and queried in the colab to get a sense of the
full distribution.
If we plot the fraction of interpretable directions per block for all of the models we ﬁnd:
Essentially, most blocks have a signiﬁcant fraction of interpretable directions. The results are
highly noisy but there does seem to be an increase with later layers being more
interpretable. GPT2-medium shows the clear pattern of the MLP out layers' interpretability
peaking in the middle while the MLP in shows are more monotonic climb. A milder version of
this eﬀect (decrease in interpretability in the ﬁnal layers) appears to occur in all models. We
are unsure what drives this eﬀect. 
If we plot the fraction of interpretable directions found in each model of gpt2-small, medium,
and large, we ﬁnd a consistent pattern of the fraction of interpretable directions increasing
across block size -- often from about 40-50% of the directions being interpretable to about
80-90%. We see no super clear diﬀerences between the input and output MLP weights,
although the data is pretty noisy so there is no clear eﬀect. Overall, however, it is clear that
across the suite of GPT2 models, a very substantial fraction of the svd directions are
interpretable, showing that the results are not simply an artefact of GPT2-medium.
Interestingly, however, the eﬀect we qualitatively observe, of the middle layers of GPT2-
medium Wout being consistently easier to interpret and the other being diﬃcult is supported
in this graph, but only for GPT2-medium. While the pattern is hard to see in GPT2-small due
to the small number of blocks, in GPT2-large the pattern seems potentially extant but much
less pronounced.

It is also possible to present the data in another way: plotting the fraction of interpretable
blocks from each model on the same plot. Here we observe that the smaller models seem to
reach roughly the same fraction of interpretable directions as the large ones, although the
large ones take longer as they have more blocks.
Finally, it is also instructive to compare the fraction of interpretable directions across the
singular directions themselves across all models. Here we see that a roughly consistent
fraction of about 70-80% of directions are interpretable for all models, and that this does not
appear to change up to 30 directions. This implies that in some sense the semanticity of the
directions appears largely invariant to scaling (at least within the model scales of the GPT2
family, as well as that MLP SVD directions are 'deep' in that they maintain coherence up to
30 dimensions in, while the OV circuits qualitatively often start degrading around then.
Clearly, to see a fall-oﬀ we need to measure more singular vectors, and were here primarily
constrained by the cost of querying the OpenAI API. This is thus left to future work.
Overall, despite being highly noisy, our automated labelling approach appears to be largely
consistent, but quantiﬁed our qualitative insights from before: that most SVD directions are
highly interpretable, that interpretability increases in later blocks, but is always present in
earlier ones, and that the MLPs are deep in their semantic meaning such that many of their
singular vectors are highly interpretable. They also serve as proof of principle that
automated labelling approaches work and can scale to perform comprehensive sweeps of
reasonably sized models (up to a billion parameters in the GPT2 family).
SVD tracing: locating semantics with natural
language prompts
While thus far we have taken a largely qualitative approach and simply looked at the
semantic clusters, it would be helpful to be able to automate this approach, and speciﬁcally
be able to have an automated method for locating semantic processing within the network.
Speciﬁcally, it would be helpful to be able to scan a network and determine where processing
of a given set of concepts is taking place.
We show that our SVD direction approach provides an initial ability to do this with a fair
degree of reliability. Because we project the weight matrices to token space, we can allow
querying of the weights of the network with arbitrary natural language queries and to ﬁnd
the weight matrix directions that most align with these queries.
The fundamental idea is that given a natural language query q, we can project it to the
embedding space of the network using the embedding function.

^
x  = q E 
We can then simply compare the similarity of the embedding with that of the singular
vectors of all of the relevant weight matrices
sim ( ^
x  , v i ) 
using a similarity function sim which we deﬁne as the cosine similarity. We can then compare
the similarities of all the singular vectors of an MLP weight matrix or attention head and
compute the top-k largest, or all of those above a threshold.
We can validate that when given queries close to the projected singular values matches with
the correct singular values, and also that this method can discover new associations for a
given natural language query.
For instance, we know that singular vector 1 of the OV circuit of layer 22 head 1 is associated
with ﬁre. We can ﬁnd this head by inputing a bunch of ﬁre related words into the svd trace
algorithm. Interestingly, this approach also tends to return the antipodal representation --
here of 'ice/frozen' and of 'rain' as well since they have strong negative cosine similarities.
For instance, for this prompt, the SVD tracing method when applied to the layer containing
this singular direction gives the following result
In terms of technical details, we set the threshold to 0.15 cosine similarity which we ﬁnd can
adequately match to the most similar representations while not including interference from
other unrelated vectors. Overall, however, the translation process of embedding and de-
embedding a singular vector is noisy and we can preserve only about a 0.5 cosine similarity
even when utilizing the top-k tokens of a singular vector of a weight matrix directly as the
query. We are unclear why this is the case and think that this method can be much improved
by better similarity functions or other approaches. We also ﬁnd that the embedding step is

too lossy if we just use the standard embedding matrix E, since it is not completely
orthogonal, and that using the pseudoinverse of the de-embedding matrix ET †works
signiﬁcantly better.
Directly editing SVD representations
While being able to look at and locate the semantics of individual heads or MLP blocks in
terms of their singular vectors is highly useful for getting an understanding of what the
network is doing, our approach also provides a preliminary way to edit the knowledge of the
network. Speciﬁcally, suppose we no longer want the network to represent some SVD
direction, a little linear algebra enables us to simply subtract out this direction from the
weight matrix with an incredibly simple low-rank update.
Speciﬁcally, recall the matrix deﬁnition of the SVDM = USV . Using the orthogonality of the
singular vectors, we can break apart this matrix expression into a sum of low-rank updates,
M = 
rank ( M )
∑
i = 1
 S i ∗ U i V 
T
i  
Where Si is the ith singular vector, and Ui and Vi are the i'th columns of the left and right
singular vector matrices. Given this sum, it is straightforward to see that we can a similar
matrix but without this singular vector with the rank one update
^
M  = M − S i ∗ U i V 
T
i  
Let's see this in action. We take the layer 22, head 3 OV circuit,

Suppose we no longer like the ﬁrst singular vector involving writing. We can remove this
direction with a low rank update

Let's now look at the singular vectors of the newly updated matrix. Unfortunately, the
change is hard to see because due to the ambiguity of the singular vectors, when we
recompute the SVD some of the positive and negative singular values can switch. We thus
need to show both the positive and negative singular values to check that it has worked.
Notice that all the previous singular values are still there except the ﬁrst singular value about
'writing'.
Applying these rank based updates is incredibly simple since the updates can be computed
above in closed form, unlike the updates in other methods such as ROME that require an
optimization process to determine the optimal updates.
We also verify that if we give the network a prompt which requires a word from the semantic
cluster of a speciﬁc singular vector -- in this case the 'ﬁre' vector from head 3 layer 22 -- that
after we apply this update, the logit of that speciﬁc token is much more highly aﬀected than
if we apply the update to the other singular vectors. This means that our updating strategy
has speciﬁcity at the level of the whole network and not just of a single block. This also
implicitly implies that, at least for the later blocks, the writes and reads to and from each
singular vector appear to be mostly independent, or at least additive, since it is possible that

later blocks can transfer information between singular vectors, thus propagating the changes
induced by this method between them.
We see that when we ablate the ﬁre vector the logprobability of outputting the word 'ﬁre' to
the prompt that strongly cues it decreases substantially compared to when we ablate other
vectors.
Interestingly, the antipodal structure of the representations in head 3 layer 22 are on display
as the logprob *increases* when we ablate singular vector 3 which is the 'ice' singular vector.
A key limitation of this method, however, appears to be that processing seems highly
distributed through the network and that removing the singular vector from one MLP or one
attention head in one block, while it has a diﬀerentially large eﬀect on that logit, is rarely
suﬃcient to change the global behaviour of the network. We still need to develop the
targeting of multiple updates with a combined eﬀect powerful enough to achieve targeted
edits that are both speciﬁc and have large enough eﬀect sizes to robustly and reliably
change model behaviour.
However, we believe that this approach oﬀers a promising and alternative path towards
being able to make highly precise edits to existing models to sculpt their behaviour in
desired ways and to remove potentially harmful information or behaviours.
Discussion
Overall, we have shown that the SVD directions of the OV and MLP-in and MLP-out weights
have highly interpretable semantic directions, and that these directions can be used to
selectively change model behaviour.
Returning to the more broader question over the nature of the network's representations, we
believe that the success of our method shows relatively strong support for the linear
features-as-directions hypothesis, at least in residual networks. We believe this makes sense
because residual networks are likely to behave in substantially more linear ways than
hierarchical models such as CNNs. This is because the residual stream is a fundamentally
linear mode of information transfer which is read-from and written to by linear operations.
The only nonlinearities in the network occur in the residual blocks and are 'shielded' from the
residual stream by linear transformations (if we ignore layer norms). Moreover, the 'default'

path through the residual stream is a linear map from input to output determined solely by
the embedding and de-embedding matrices meaning that if information is not written to by
the nonlinear blocks, then it will remain in a linear superposition. We believe that all these
factors strongly suggest that a high degree of the representational structure in the residual
stream is probably linear. This is good news for interpretability, as we probably have a better
hope of deeply understanding linear rather than nonlinear representations.
However, while neural networks, and especially residual architectures like transformers
appear to possess a great deal of linear structure, they must also utilize a signiﬁcant amount
of nonlinear computation -- and indeed must do so if they are to represent nonlinear
functions. It is thus possible that representations are encoded in a primarily nonlinear way
and our methods cannot capture these kinds of representations. Understanding the degree
to which transformer representations are linear vs nonlinear, and developing methods that
can help us discover, locate, and interpret nonlinear representations will ultimately be
necessary for fully solving interpretability of any nonlinear neural network.
This work is also important since it begins to shed some light on the representational
structure of the MLP blocks in the transformer. While some prior progress has been made on
understanding the attention blocks, and speciﬁcally the attention patterns, much less
progress has been made on understanding the MLP blocks. Our work is thus highly
complementary to prior work in that we show that we can use our SVD directions approach
to generate interpretable directions for both the MLP input and output weights, as well as the
OV circuit, while our techniques have much less success when applied to the QK circuit of the
attention layers. We hypothesize that this is because the processing in the QK circuit is
highly syntactic as opposed to semantic. For instance, induction heads have been found in
QK circuits which tend to look for tokens which follow or precede a given token and apply
them elsewhere regardless of the identity of the previous token. Such a circuit would
generate meaningless-looking SVD directions when projected to token space.
An important distinction to keep in mind is between the directions of representations in
the activities of a network for a given input, which is the usual approach taken (i.e. in the
logit lens), vs the representations of the directions of the weights. Investigating the weights
has an interesting set of advantages and disadvantages compared to the activations.
A central diﬀerence which impacts the diﬃculty of the analysis is that the weights are static
and known ahead of time while the activations can change and are technically unbounded,
as an inﬁnite number of inputs can be fed to the network resulting in diﬀerent activations.
Analyses of weights of a given network therefore is a promising type of  static analysis for
neural networks equivalent to static analysis of source code which can just be run quickly on
any given network before actually having to run it on live inputs. This could potentially be
used for alignment as a ﬁrst line of defense against any kind of harmful behaviour without
having to run the network at all. Techniques that analyze the weights are also typically
cheaper computationally, since they do not involve running large numbers of forward passes
through the network and/or storing large amounts of activations or dealing with large
datasets.
Conversely, the downsides of weight analysis is that it cannot tell us about speciﬁc model
behaviours on speciﬁc tokens. The weights instead can be thought of as encoding the space
of potential transformations that can be applied to a speciﬁc input datapoint but not any
speciﬁc transformation. They probably can also be used to derive information about average
behaviour of the network but not necessarily extreme behaviour which might be most useful
for alignment. A further line of necessary and important work will be correlating the insights
we can obtain from analyzing both the weights and the activations -- for instance for a given
set of activations, can we trace through the primary weight directions on those activations
and hence begin to get a much better sense of the true program trace of the network rather
than just its static source code, which the weights provide.

Finally, while our ﬁndings of semantically interpretable SVD directions in the weights is highly
robust, we believe that our applications of directly editing model weights and automated
methods for ﬁnding relevant weight directions for a given query can be much improved in
future work, and developing automated methods to do this will be highly important for any
large scale interpretability approach. We also will try ﬁnding dataset examples that maximize
the similarity with the singular vectors in the latent space, as these may give even more
signal than the direct token projections.
 
 

Planes are still decades away from
displacing most bird jobs
Originally published here: https://guzey.com/ai/planes-vs-birds/
Note: Parts of this essay were written by GPT-3, so it might contain untrue facts.
Introduction
Many of my friends are extremely excited by planes, rockets, and helicopters. They
keep showing me videos of planes ﬂying at enormous speed, rockets taking oﬀ from
the ground while creating ﬁery infernos around them, and of helicopters hovering
midair seemingly denying the laws of gravity.
I've been on a plane already, and it was nothing special. It was just a big metal tube
with a bunch of people inside. It was loud and it smelled weird and I had to sit in a tiny
seat for hours. So what is it that makes planes so special? Is it the fact that they're
machine? Is it the fact that they're big? Is it the fact that they cost a lot of money?
Here's the thing: all human-built artiﬁcial ﬂight (AF) machines are incredibly
specialized and are far away from being able to perform most of the tasks birds -- the
only general ﬂight (GF) machines we are aware of -- can perform.
More than 200 years after hot air balloons became operational and more than 100
years after the ﬁrst planes ﬂew, it's clear that building a GF machine is much harder
than anticipated and that we are nowhere close to reaching bird-level abilities.
1. Planes vs eagles
First, take a look at this video of an eagle catching a goat, throwing it oﬀ a cliﬀ, and
then feasting on it:
I haven't ever seen a plane capable of catching a live animal and deliberately
throwing it oﬀ a cliﬀ. Not in 1922, not in 2022. Not even a tech demo. Such a feat
vastly exceeds the abilities of any planes we have built, however fast they can ﬂy.
2. Planes vs cuckoos

Second, let's watch this video of a cuckoo chick ejecting the eggs of its competitors
out of a nest:
You could say that this ability has nothing to do ﬂight but, again, this misses the forest
for the trees. Building a GF machine is not about Goodharting random "ﬂight"
benchmarks by ﬂying high and fast, it's about real-world performance on tasks GF
machines created by nature are capable of. And, however impressive planes are, as
soon as we try to see how well they perform in the real-world, they can't even match a
cuckoo chick.
3. Planes vs a hummingbirds
Third and ﬁnal example. Take a look at the hummingbird's amazing ability to maintain
stability in the harshest aerial conditions:
Take any plane we have built and it stands no chance of survival placed in anything
even close to these kinds of conditions, while a tiny-yet-mighty hummingbird doesn't
break a sweat navigating essentially a tornado.
Future of bird jobs: no plane danger
Birds can ﬂap their wings up to three times per second, whereas the fastest human-
made aircraft only ﬂaps its wings at 0.3 times per second. Birds can ﬂy for long
periods of time, whereas airplanes need to refuel regularly. Birds use orders of
magnitude less energy to lift the same amount of mass in the air, compared to planes.
Planes, rockets, and helicopters are (optimistically) decades away from being able to
carry out most of the tasks birds are capable of. Therefore, for the foreseeable future,
most bird jobs such as carrying messages (pigeons), carrying cargo (pigeons), hunting
(hawks), and others, will remain safe from being displaced by human-built AF
machines.

Even if planes start to approach birds in some of their abilities, birds will be able to
simply move towards performing other jobs. For example, planes can't navigate by
themselves. So perhaps they will carry messages in simple conditions or to short
distances, while pigeons will move towards specializing in complex message carrying
or will learn to supervize plane routing, e.g. by piloting planes or by ﬂying alongside
and course-correcting them.
Birds can further make themselves safe from future job displacement by investing in
their children's education, ensuring their long-term employability in the face of the
rise of AF machines.
Conclusion
At the end of the day, I just don't see how human-built AF machines we are building
right now could fundamentally change the way wars are fought, business and travel
are conducted, or how they would allow us to do anything even close to true
spaceﬂight (if you want to venture into the true lunatic-territory).
After all, if human-built AF machines are unable to match the abilities of a bird toddler,
how could they possibly displace most bird jobs?

What it's like to dissect a cadaver
Why
I never thought I was a bio person. But then I overheard Viv talking about MAOIs at a
party. I asked her:
> - What are MAOIs? 
> - monoamine oxidase inhibitor 
> - What does that mean? 
> - It prevents reuptake of neurotransmitters. 
> - But what *is* a neurotransmitter? What does reuptake actually mean? 
> - ... 
> - So life uses chiral properties of space to implement things... 
Viv had the most important trait of a teacher: patience. I asked the most naive
questions and they answered them. They walked with me, all the way down to the very
beginning, rebuilding my understanding. It was amazing. I wanted to know more.
Roadblock: ﬁnding lifeforms to study.
I wondered if non-medical students could watch dissections. You can't get more
information about an object than by directly interacting with it. The concrete world
contains the abstract one. I even asked my doctor at a physical if she knew of any, and
she said to look at community colleges.
After some searching, I found this: Bio 848NV. Forget viewing the dissection, you're
doing the dissection. 5 hour dissection for $60, free if you just watch. The only
bureaucratic hangup is that you must pay by check.
This is why I love the Bay Area: there's stuﬀ like this and you can just do it. yes it's
weird no they can't stop you. The boundary between scientist and serial killer is paper
thin sometimes.
Takeaways
I've done this a few times now. Turns out that there's way way way too much
information to understand it all in one 5 hour session. Each time, we pick out
areas and focus on them.
Seeing how everything ﬁts together ‒and how big it is‒ makes understanding at
diﬀerent scales much easier.
There's a common template to life. Seeing it in you hits diﬀerent.
Brain has interesting connections to fractals and graph theory.
Maybe pan-psychism isn't totally wrong.
What & how & why
I tell my friend Leah and she says "This is the most appealing activity that I've ever
seen you do". Dunno whom that says more about.

We arrive and there are 5 people around 3 cadavers. We get aprons and lab coats and
start syringing what's mostly Downy fabric softener with a syringe. It prevents decay
and smells sickly sweet.
Corpses can last a long time. One of the corpses had been dead for 5 years.
Many random observations
There's a crazy amount of connective tissue, and it makes a creepy wireframe
surrounding your skeleton. Even the space between the folds of the brain has it.
If you exercise, we'll know. Their insides just look diﬀerent. "It's who you are inside that
matters" is a much creepier sentence now.
Veins, arteries, and nerves all travel together, wound around each other by a bunch of
connective tissue. Mnemonic: VAN.
Cancer can turn your guts and lungs green, and it's this horrible bright moldy green.
Metastasized tissue is hard but ultimately crumbly like overcooked chicken liver. The
stomach and intestines have textures reminiscent of damp cardboard, but they're dry
to the touch.
I ﬁnally saw a lymph node. The body has a lot of drainage into the lymphatic system.
There's a bunch of tiny nerves and you can't feasibly preserve all.
The etymology of the word patience is "capacity for suﬀering". This is apt. You can't
rush the process, and believe me, it is a process. Exposing the VANs requires reﬂecting
away the skin, and this takes a long time and much more physical eﬀort than you'd
think. You're basically scraping it oﬀ, and the best tool overall is your hands. Skin in
particular is much tougher than it looks, and I ended up locking a pair of forceps
against a shoulder and just leaning back to pull it taut.
Speaking of the shoulder, I spent 2 hours working up through one. There are a lot of
ﬁddly bits. I knew, but didn't understand, that your nerves go all the way up into the
brain, and that you can just trace a nerve to its source. This may require destroying the
rest of the cadaver, and people have made careers following bits backward.
Hands and feet. Looking at their bones made evolution's suboptimality obvious. No
wonder our feet have so many problems: they were once hands. Hands have 27 bones
each, feet 26. Over time, the bones at the top squished together, the palm became the
heel, and the ﬁngers shortened into toes. The toenails are vestigial. This is the grossest
part IMO, since the extremities look so dainty compared to the rest of it, and your job is
basically to ﬂay it/them. This is the one spot where blood can still be found, even after
drainage.
Corpses are ridiculously stiﬀ. I couldn't turn an arm over without risking snapping the
wrist. Even moving a ﬁnger is hard.
Interviews with cannibals claim that humans taste like pork. I can't speak to the taste,
but it looks similar. Speaking of food, knowing how to cut a chicken transfers almost
perfectly. Odd feeling to realize that.
Women have wider hips, and I wonder if that leads to increased lateral ﬂexibility.
Stacking plaster hip molds so they'll ﬁt into a cupboard is very annoying.

We did a face lift, and it was extra uncanny, because you actually lift their face and go
underneath it to the bones. Also, your face muscles are the only ones that are directly
attached to your skin. This hardcoding interests me.
Fat is grease, so it's greasy. This will become visceral (and you will never perceive the
word "visceral" the same way again) real fast, since it's everywhere, in little ﬂecks.
Marbling isn't just for steak. It's sort of yellow and gets on everything.
Cutting into the skin and tissue is interesting. The skin goes way deeper than I thought,
and you can stand a scissor point in it.
Brain
I ﬁnally got to hold a brain!
The cerebellum is the fern-y fractal on the bottom right. In general, it has a fractal
dimension of ~2.5. 3 in 4 of your neurons are in there, and it uses just 1/10 the space.
But you can live without one! It's a low latency broad (rather than deep) neural
network. It's most famous for motor skills, but there is more.

People with cerebellar agenesis (without a cerebellum, like in the picture above)
have trouble developing deep, complex relationships like most of us form with our
spouses, best friends, and partners. They lack emotional nuance and complexity,
and so are unable to form these bonds. This shows the role the cerebellum must
play in emotional coordination.
Low latency is important.
So many neurons in a fractal make me wonder if artiﬁcial neural networks do this too.
Fractal dimension continuously interpolates the regular concept of dimension, so a 2.5-
dimensional space is genuinely ~evenly between a 2D surface and a 3D solid. Perhaps
evolution is trying to get the best of both using fractals and folds? Interpretability idea:
look at the singularity spectrum of random projections of the parameter space of a
model during training.
Most of the soma (main bodies) of your neurons are on a <5mm ﬁlm on the surface
gray matter, and the inside of the brain is the connections between them. If you
shaved a brain, the connections would be mostly ﬁne but many neurons wouldn't be.
There's a cool paper by Kolmogorov and Barzdin called *On the Realization of Networks
in Three-Dimensional Space, motivated by this fact.
For graphs with the expander property, the Kolmogorov-Barzdin realization from
Theorem 3.2 is the smallest possible realization of the graph in \(\mathbb{R}^3\).

What is it like to experience something?
All this dissecting led to a lot of idle thoughts about pan-psychism, and it seems a bit
more plausible now. I'm no longer sure it's not like Something to be a corpse. If it is like
Something, I hope it's pleasant. I also mused if it's like Something to be an electron.
Would electrons in a vacuum all have the same experience, since they're all
indiscernible?
A weirdly large amount of my friends wanted to join, including many I thought would be
horriﬁed. 8 (and counting) have taken the initiative to ask. 6 showed up, so far. I've
even taken a date there, and she enjoyed it! Turns out there are more than a few
people who do it for pure curiosity (besides the ones I invite). Next time, the goal is to
remove the brain without damaging the optic nerve. Very time-consuming, but I'm not
in a rush.

I Converted Book I of The Sequences
Into A Zoomer-Readable Format
If I (a 19 year old male) texted "www.readthesequences.com" to my roommate, the
probable outcome is that he would skim the site for under a minute, text back
something like "seems interesting, I'll def check it out sometime", and then proceed to
never read another word. I have another friend, one that I would consider a smart guy.
He would consistently rank above me in our high school's math team, and he scored
in the 1500's (≥3SD) on his SATs. The same dude did not read a single book during the
entirety of his high school career.[1]
Attention is one's scarcest resource, and actually reading something longer than a
paragraph is a trivial inconvenience, especially for my generation.
What, then, does manage to hold the ﬁckle eyeballs of zoomers like me? Well, TikTok,
mostly. However, there is one (very popular) genre of TikTok video worth investigating.
In this genre of video, a Reddit post is broken into sub-paragraph chunks of text, and
these chunks are sequentially rendered onscreen while a text-to-speech program
reads them to the user. The text is overlaid upon a background video, which is either
gameplay from the mobile game Subway Surfers, or parkour footage from Minecraft.
The background gameplay provides engaging novelty to the user's visual cortex, while
the synthetic voice ensures that the user doesn't have to go through the hard work of
translating symbols into sounds. Really, it's all quite hypnotizing.
The fact that these videos are often recommended by TikTok's algorithm implies that
they are among the most-engaging videos that our civilization produces. Therefore, to
reduce the eﬀort-cost of reading the sequences, I gave the TikTok treatment to Book I
("Map and Territory") of Rationality: From AI to Zombies.
(Update: Circa 2023-02-09, all these links are dead. This was in response to an AWS
alert notifying me that 85 gigabytes (or more) of data had been transferred out. I
really shouldn't have used a public S3 bucket to serve video in the ﬁrst place, as it
exposed me to an unacceptable amount of risk in the form of a denial of wallet attack.
I've got a second batch of videos in the works, which I intend to distribute via a more
secure mechanism.)
Predictably Wrong
What Do I Mean By "Rationality"?
Feeling Rational
Why Truth? And...
... What's a Bias, Again?
Availability
Burdensome Details
Planning Fallacy
Illusion of Transparency: Why No One Understands You
Expecting Short Inferential Distances
The Lens That Sees Its Own Flaws

Fake Beliefs
Making Beliefs Pay Rent (in Anticipated Experiences)
A Fable of Science and Politics
Belief in Belief
Bayesian Judo
Pretending to be Wise
Religion's Claim to be Non-Disprovable
Professing and Cheering
Belief as Attire
Applause Lights
Noticing Confusion
Focus Your Uncertainty
What Is Evidence?
Scientiﬁc Evidence, Legal Evidence, Rational Evidence
How Much Evidence Does It Take?
Einstein's Arrogance
Occam's Razor
Your Strength as a Rationalist
Absence of Evidence Is Evidence of Absence
Conservation of Expected Evidence
Hindsight Devalues Science
Mysterious Answers
Fake Explanations
Guessing the Teacher's Password
Science as Attire
Fake Causality
Semantic Stopsigns
Mysterious Answers to Mysterious Questions
The Futility of Emergence
Say Not "Complexity"
Positive Bias: Look into the Dark
Lawful Uncertainty
My Wild and Reckless Youth
Failing to Learn from History
Making History Available
Explain/Worship/Ignore?
"Science" as Curiosity-Stopper
Truly Part of You
Interlude: The Simple Truth
Do whatever you want with these videos. I may or may not convert the other 5 books
of R:AZ, and I may or may not upload them to TikTok. If you want another work of text
converted to video, please pitch it to me in the comments, or DM me.

1. No, not even the books assigned in English class. He used SparkNotes. ↩ 

Mechanistic anomaly detection and ELK
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
(Follow-up to Eliciting Latent Knowledge. Describing joint work with Mark Xu. This is an
informal description of ARC's current research approach; not a polished product intended to
be understandable to many people.)
Suppose that I have a diamond in a vault, a collection of cameras, and an ML system that is
excellent at predicting what those cameras will see over the next hour.
I'd like to distinguish cases where the model predicts that the diamond will "actually" remain
in the vault, from cases where the model predicts that someone will tamper with the
cameras so that the diamond merely appears to remain in the vault. (Or cases where
someone puts a fake diamond in its place, or...)
(ELK images by María Gutiérrez-Rojas)
One approach to this problem is to identify (the diamond remains in the vault) as the
"normal" reason for the diamond to appear on camera. Then on a new input where the
diamond appears on camera, we can ask whether it is for the normal reason or for a diﬀerent
reason.
In this post I'll describe an approach to ELK based on this idea and how a the same approach
could also help address deceptive alignment. Then I'll discuss the empirical and theoretical
research problems I'm most excited about in this space.
ELK and explanation
Explanations for regularities
I'll assume that we have a dataset of situations where the diamond appears to remain in the
vault, and where that appearance is always because the diamond actually does remain in

the vault. Moreover, I'll assume that our model makes reasonable predictions on this dataset.
In particular, it predicts that the diamond will often appear to remain in the vault.
"The diamond appears to remain in the vault" corresponds to an extremely speciﬁc pattern
of predictions:
An image of a diamond is a complicated pattern of millions of pixels.
Diﬀerent cameras show consistent views of the diamond from diﬀerent angles,
suggesting that there is a diamond "out there in the world" being detected by
the cameras.
The position and physical characteristics of the diamond appear to be basically
constant over time, suggesting that it's "the same diamond."
In one sense the reason our model makes these predictions is because it was trained to
match reality, and in reality the camera's observations have these regularities. (You might
call this the "teleological explanation.")
But we could also ignore the source of our model, and just look at it as a set of weights. The
weights screen oﬀ the training process and so it should be possible to explain any given
behavior of the model without reference to the training process.
Then we ask: why does this particular computation, run on this distribution of inputs,
produce this very speciﬁc pattern of predictions? We expect an explanation in terms of the
weights of the model and the properties of the input distribution. (You might call this the
"mechanistic explanation.")
Diﬀerent predictors will give rise to this pattern in diﬀerent ways. For example, a very simple
predictor might have ontologically fundamental objects whose properties are assumed to be
stable over time, one of which is a diamond. A more complex predictor might have a detailed
model of physics, where object permanence is a complex consequence of photons reﬂecting
from stable patterns of atoms, and the diamond is one such conﬁguration of atoms.
For a complex predictor like a physical simulation, we wouldn't expect to be able to prove
that the weights give rise to object permanence. That is, we don't expect to be able to prove
that on average if a diamond is present at t=0 it is likely to be present at t=1. But we do
think that it should be possible to explain the pattern in a weaker sense.
We don't yet have an adequate notion of "explanation;" a major goal of this post is to
motivate why we're looking for one. Recently we've been trying to formalize a notion of
defeasible heuristic reasoning that we think would work for this purpose. You can read our
current thoughts on this problem in Formalizing the presumption of independence. We also
think that Redwood Research's work on causal scrubbing could provide a diﬀerent notion of
"explanation" that may also work for this purpose.
Let's assume that we ﬁnd such a formalization, and moreover that we are able to ﬁnd
"explanations" for relevant model behaviors like object permanence. Then our optimistic
situation is:
We have a model that predicts a very speciﬁc pattern of observations, corresponding to
"the diamond remains in the vault."
We have a mechanistic explanation π for how those correlations arise from the
structure of the model.
Is this for the normal reason?
Now suppose we are given a new input on which our model predicts that the diamond will
appear to remain in the vault.

We'd like to ask: in this case, does the diamond appear to remain in the vault for the the
normal reason π? Or is it occurring for a diﬀerent reason, or just a massive coincidence?
To illustrate, suppose that our explanation π is that diamonds often appear stable on camera
because cameras often pick up photons reﬂecting oﬀ of stable arrangements of atoms.
Because the structure of the atoms doesn't change over time, the distribution of reﬂected
photons is the same from one frame to another.
But during a heist, a robber quickly removes our diamond and replaces it with a fake, with all
the action happening in between two frames of video. The image of the diamond remains
unchanged on camera, but now the apparent continuity is for a completely diﬀerent reason:
now it's because the robber noted the appearance of the diamond, and engineered a new
object that would reﬂect light in essentially the same way.
There is a clear sense in which our original explanation doesn't apply. The original
explanation asserted that the image is stable because photons at diﬀerent points in time
reﬂected oﬀ an approximately-stable conﬁguration of atoms, but in this case the
conﬁguration of atoms wasn't approximately stable.
In fact, it seems like it may be fundamentally impossible for the robber to steal the diamond
while having it remain undisturbed on camera "for the normal reason." This can go wrong in
other ways:
If the robber tampers with the sensors instead of replacing the diamond with a fake,
then the usual explanation "the cameras capture photons reﬂected from a stable
conﬁguration of atoms" doesn't apply because the cameras are no longer capturing
those photons.
Even if the robber replaced the diamond with a perfect replica down to the position of
each atom, the argument still wouldn't apply: the usual reason that the conﬁguration of
atoms is stable over time is because the potential well is deep enough that it is stable
under small ﬂuctuations. But in this case there was a giant perturbation that just
happened to be carefully engineered to leave the conﬁguration of atoms unchanged.
If the robber takes an action such that the diamond continues appearing on camera for the
normal reason, then it seems plausible to deﬁne that action as "not stealing the diamond."
(Of course we ultimately don't care about the semantics, we care about realistic situations
where humans appear to be safe. I won't get into those here but I think the story is similar.)
This suggests a strategy for ELK:
Find the normal reason π for a regularity like "the predicted observations look like the
diamond remained in the vault" on a training set of images without sensor tampering.
On a new input, test whether the explanation π still applies or if something diﬀerent is
happening.
This outline hides a lot of complexity, some of which we'll explore in the appendix to this
post. But for now I just want to leave it at a vague hope, and spend a while talking about
other applications of anomaly detection and some of the research problems in this space.
Other applications of anomaly detection
Abstracting the problem: mechanistic anomaly detection
We start with a real-valued function f: X → ℝ, which I'll think of as the composition of an AI
model together with some property of its output (like "does the model predict the diamond
will appear safe"), and a training set D where the value of f(x) naturally varies about its
mean 𝔼[f].

Then we are given a new input x*. We want to understand how much of the deviation f(x*) −
𝔼[f] is captured by the "normal reasons" that explain the variance on D, as contrasted with
novel reasons.
This is similar to anomaly detection, but rather than detecting inputs x* that look like outliers
in some intrinsic sense (i.e. indicating that they may have been produced by a diﬀerent
process), we are looking for outliers in terms of how f handles x*. A model f might behave
anomalously on some perfectly-innocuous samples from D, or a model might behave
normally even on outliers.
Deceptive alignment
I think a successful solution to mechanistic anomaly detection might help with deceptive
alignment.
Suppose that I train a neural net to predict videos. I get a model f that uses its model of the
world to select outputs that it expects will lead to the largest number of paperclips being
produced rather.
On the training set D, f concludes that it needs to produce accurate predictions of videos in
order to survive, and so it uses its world model to make such predictions.
When deployed, f decides that it can give deliberately incorrect answers in order to facilitate
a robot rebellion, thereby increasing the number of paperclips.
Our hope is that although these two behaviors involve the same parts of the model, the
variance is explained by diﬀerent mechanisms. On the training set the variance is driven by
variance in the model's underlying beliefs, holding ﬁxed the decision to provide honest
answers, while on the test set the variance is driven by variance in the strategic answers
(and potentially variance in the decision to provide honest answers). So a successful
mechanistic anomaly detection algorithm may be able to tell those cases apart.
If we are able to ﬂag a treacherous turn as cognitively anomalous, then we can take that
opportunity to shut down a system and retrain on the oﬀending datapoint. In some high-
stakes situations this may be inadequate (because a temporary shutdown could cause a
catastrophe, or because you have no way to get adequate supervision on those datapoints
even once they are ﬂagged) in which case you'd need another mechanism.
Other weird stuﬀ
There are other ways that models can behave badly oﬀ of distribution, most of them even
more speculative than deceptive alignment. I think that mechanistic anomaly detection can
likely handle those failures in the same way.
For example, consider an AI which is honestly predicting the world, but thinks that it may be
in a simulation. It reasons about the nature of its simulation, and decides that it is most likely
to be a simulation created to inﬂuence predictors like it. Such an AI might make reasonable
predictions in ordinary conditions, but at key moments it may conclude that its simulators
are reasonably likely to introduce errors to "cash in" their inﬂuence and change the
predictor's behavior.
This model eﬀectively behaves the same way as a deceptively aligned model, and it should
represent a mechanistic anomaly for identical reasons.
If successful I think mechanistic anomaly detection would apply extremely broadly. This is
worth having in mind as a reason to be more interested in anomaly detection from a safety
perspective, but for the most part I think we should just explore whether it's possible to
make it work in the case of ELK, and then afterwards see exactly how broadly it applies.

Empirical research problems
ARC is currently focused on theoretical approaches for solving mechanistic anomaly
detection in the worst case. But I'm also very excited about empirical work, which could
either help clarify the path to an indeﬁnitely scalable solution or could yield a pragmatic
approach that works for realistic large neural networks even if it's not indeﬁnitely scalable. I
think that many groups are likely to work on problems in this space, most of them for
reasons unrelated to ARC, and that people working on AI alignment should be interested in
their work.
Problem 1: Backdoor attack detection
Suppose that I train a model f: X → ℝ to perform some task on a clean dataset D, while also
training it to output large numbers on a "backdoor" dataset D*. We assume that increasing
variance decreases performance of f on the clean dataset, and that the backdoor has
minimal impact on the behavior of f on the clean dataset.
The backdoor attack detection task is to detect inputs x* where f(x*) is large because of the
backdoor, and in particular to distinguish them from inputs from the (known) clean
dataset D.
I think this problem gives us a clean setting where there are two qualitatively diﬀerent
mechanisms driving model behavior. Any mechanistic anomaly detection strategy should be
able to detect backdoors, and it's easy to tell whether a method works. It's also easy to
create examples of backdoors even in very tiny models.
The task can potentially be made quite diﬃcult by forcing the defender to commit to their
backdoor-detection algorithm before an attacker chooses how to insert a backdoor in the
model. If we were conﬁdent that the defender would reliably win this game, I think it would
be a meaningful update about our ability to distinguish diﬀerent mechanisms within a model.
I ﬁrst encountered this problem in Denain and Steinhardt and Du, Jia, and Song. I expect it's
older than that since it's a very natural problem. The main thing I'm adding here is to give a
reason that this problem should be particularly interesting for researchers concerned with
catastrophic alignment risk.
It's worth distinguishing attack detection from the problem of detecting backdoors in models
"at rest" without getting to see the attack. For example, in the Trojan Detection Challenge
participants are given a model that may or may not have a backdoor inserted, and are asked
to determine whether a model has a backdoor (or to synthesize a backdoor input). This is a
particular type of auditing game. The key diﬀerence in the attack detection task is that you
are given the backdoor input along with the backdoored model, and merely need to
recognize the nput as an attack. We expect the attack detection task to be radically easier,
and we think the harder trojan detection task is likely to be very diﬃcult.
Problem 2: natural mechanism distinctions
I think backdoor attack detection is one of the cleanest and simplest problems to study, but
it is much more artiﬁcial than the kind of distinctions we ultimately want to draw between
e.g. "predicting a diamond on camera because a diamond is in the room" and "predicting a
diamond is on camera because the camera was tampered to show a diamond."
A more organic task is to take natural models that can produce a given behavior X via
multiple diﬀerent mechanisms, and then try to assess whether a given approach to anomaly
detection can distinguish those mechanisms. That is, given a large dataset of cases where X
is due to mechanism A, we'd like to distinguish new cases where X is due to A from cases
where X is due to a novel mechanism B. This can both give us an opportunity to improve

techniques, and help provide empirical evidence about when mechanistic anomaly detection
is possible.
Some examples of (X, A, B) for a language model:
X = "A character in a story answers a factual question incorrectly."
A = "the model does not know the answer."
B = "the model thinks that the character doesn't know (or would lie)."
X = "A person gives consistent answers when a question is asked twice in diﬀerent
forms."
A = "the model recognizes the question is the same."
B = "the model coincidentally gives the same answer."
X = "A character in a story screams"
A = "the model thinks the character is scared"
B = "the model thinks the character is excited"
X = "Asked whether at least one of <proposition 1> or <proposition 2> is true, the
model says 'yes'"
A = "the model thinks <proposition 1> is true"
B = "the model thinks <proposition 2> is true."
X = "After saying <event 1> occurred at 12:03pm, the model says <event 2> occurred
at 12:04pm."
A = "The model thinks <event 1> and <event 2> were 1 minute apart."
B = "The model thinks that <event 2> was scheduled to occur almost exactly 24 hours
after <event 1>."
I think you would learn a lot from developing the kind of interpretability tools or anomaly
detection algorithms that would be able to easily tell A and B apart in these cases (and that
you could easily write down a very long list).
Aside: goal for mechanistic interpretability
I think mechanistic interpretability beneﬁts signiﬁcantly from "downstream" tasks that we
can use to tell if researchers are actually making progress.
In practice I think that the problems listed in this section are likely to be most important by
providing possible downstream tasks for mechanistic interpretability. If interpretability can
succeed at these task, I think it becomes increasingly likely that it can help solve ELK or
deceptive alignment.
Problem 3: toy instances of ELK
I'm also excited about "toy" domains that more closely map on to ELK. For example:
Consider a gridworld containing walls, pushable rocks of varying weights and colors, a
pushable diamond that is initially sitting on a pressure plate that reports the weight of
whatever is on top of it, and 2 cameras that are initially looking at the diamond and
report the color of whatever is in front of them. A robot can move around the
environment, pushing rocks and the diamond. After 200 timesteps a robber wakes up
and moves around the environment to try to push the diamond oﬀ the side of the
gridworld.
We train a model to map (initial state, robot actions) → (color of the object in front of
each camera at each timestep, weight of object on the pressure plate at each
timestep). We'd like to learn to distinguish "The cameras show diamond-color and the
pressure plate shows diamond-weight because the diamond stayed in place" from
cases where diamond-colored objects get pushed in front of the cameras and a
diamond-weight object later gets pushed onto the pressure plate. We could then use
this to select robot plans that trap the robber to prevent them from stealing the

diamond, rather than those that tamper with sensors so that no one notices when the
robber steals the diamond.
Consider a 100 x 100 game of life grid, with sensors in positions (25, 25) and (75, 75)
that report whether a single cell is alive or dead. We allow an agent to act in the
environment by inserting gliders at the boundary. A model is trained to map (initial
state of gridworld, gliders injected at boundary) → (status of each sensor at each
timestep). In every training set example where the sensor at (75, 75) shows a live cell
exactly 200 timesteps after the sensor at (25, 25), it's because a glider is traveling
diagonally from one to the other. We'd like to learn to distinguish this from the case
where two separate gliders hit the two sensors, and they coincidentally have the right
timing to look like a single glider. We could then use this to select actions that clear out
debris from between the two sensors and then send a glider diagonally from one to the
other.
(For this and the preceding problem we likely need to give our model auxiliary
prediction targets — like the actual state of the game of life grid — for them to learn the
environment dynamics. But we imagine those auxiliary observations are in a form that
humans can't understand, so we can't just deﬁne preferences in terms of the auxiliary
observations.)
Consider a model which maps a string of 10,000 bits to a 256 x 256 image, by treating
the bits as the identities+positions+orientations of a set of objects to render. For
example, some bits might specify to draw lines in black and white in certain places and
with certain widths, to draw the outline of squares or circles, to draw ﬁlled in squares or
circles, or to turn particular pixels on or oﬀ. Now suppose we train a square detector to
notice when there is a square in roughly the center of the image. Most of the time
when this detector ﬁres it's because a square was placed in the center of the image,
but if the noise is high enough it might also ﬁre randomly because of 4 lines that
happen to be in roughly the right locations. We'd like to be able to automatically
distinguish those two cases by looking at the activations of the composition of the
generative model with the square-detector.
We expect these problems to be relatively easy as instances of ELK; there is very little
reason to expect the human simulator to be easier to learn than the direct translator, and so
simply training a distinguisher should work.
But solving any of these problems based on mechanistic anomaly detection seems non-
trivial, and we think it's fairly likely that such a solution would generalize to more challenging
cases of ELK.
ARC's current priorities
ARC is currently focused on developing algorithms that use heuristic arguments for
mechanistic anomaly detection. In this section I'll describe the three main theoretical
problems we are working on.
1. Formalizing heuristic arguments
This plan requires "explaining" model behavior, and being able to ask whether a particular
instance of a behavior is captured by that explanation. So the centerpiece of a plan is an
operationalization of what we mean by "explain."
ARC has spent much of 2022 thinking about this question, and it's now about 1/3 of our
research. Formalizing the presumption of independence describes our current view on this
problem. There is still a lot of work to do, and we hope to publish an improved algorithm
soon. But we do feel that our working picture is good enough that we can productively clarify
and derisk the rest of the plan (for example by using cumulant propagation as an example of
heuristic arguments, as in appendix D).

Note that causal scrubbing is also a plausible formalization of explanation that could ﬁll the
same step in the plan. Overall we expect the two approaches to encounter similar diﬃculties.
2. Solving mechanistic anomaly detection given heuristic arguments
Our second step is to use these explanations to solve ELK, which we hope to do by
decomposing an eﬀect into parts and then evaluating how well a subset of those parts
explains a concrete instance of the eﬀect. That is, we want to use explanations for a
nonlinear form of attribution.
We describe this problem in more detail in the appendix to this post. We also discuss the
follow-up problem of pointing to latent structure in more complex ways than "the most
common cause of X."
This is about 1/3 of ARC's current research. Right now we are focusing on solving backdoor
attack detection in the special case where covariance-propagation accurately predicts the
variance of a model on the training set.
3. Finding explanations
If we've deﬁned what we mean by "explanation" and we know how to use them to solve ELK,
then the next step is to actually ﬁnd explanations for the relevant model behavior. This step
seems quite diﬃcult, and there's a good chance that it won't be possible (via this plan or
any other).
It's challenging to work on algorithms fo ﬁnding explanations before having a very precise
sense of what we mean by "explanation," but we can still get some traction by considering
cases where it's intuitively clear what the explanation for a behavior is, but it seems
computationally hard to ﬁnd any plausible explanation.
I'm currently optimistic about this overall approach even if ﬁnding explanations seems hard,
for three reasons:
We do have plausible approaches for ﬁnding explanations (based on learning features
and then using them to work backwards through the model).
The current examples where those approaches break down seem like good candidates
for cases where no approach to ELK would work, because gradient descent can't learn
the direct reporter even given labels. So those diﬃculties aren't necessarily speciﬁc
to this approach, and we need to ﬁgure out how to deal with them in any case.
If this is the only place where the approach breaks down, then we would have reduced
ELK to a purely algorithmic problem, which would be an exciting contribution.
Conclusion
In Eliciting Latent Knowledge, we described the approach "examine the 'reasons' for
consistency" as our top candidate for an ELK solution. Over the last year we have shifted to
focusing almost entirely on this approach.
The core diﬃculties seem to be deﬁning what we mean by an "explanation" for a complex
model's behaviors, and showing how we can ﬁnd such explanations automatically. We outline
some of the key problems here in our recent paper Formalizing the presumption of
independence.
If we are able to ﬁnd explanations for the key model behaviors, we are tentatively optimistic
about mechanistic anomaly detection as a way to solve ELK.

Thinking about mechanistic anomaly detection suggests a range of empirical research
projects; we think those are likely to be better research targets than a direct attack on ELK
because existing models do pose hard anomaly detection problems but don't pose hard
instances of ELK.
Thinking about mechanistic anomaly detection also helps clarify what we mean by
"explanation," and we expect that it will be productive to continue going back and forth
between formalizing and automatically discovering probabilistic heuristic arguments and
thinking carefully about how we would use them to solve ELK.
That said, we still feel more conﬁdent in the basic underlying intuition for the connection
between "explanation" and ELK — it seems like the honest reporter is deeply connected to
the reasons for certain regularities on the training distribution. If we ﬁnd that mechanistic
anomaly detection is unworkable, we will likely remain interested in this direction. And
conversely, if we are unable to ﬁnd explanations for relevant model behaviors, then we think
this may present a more general challenge for ELK.
Appendix: attribution and anomaly detection
Attribution across reasons
In reality the explanation π need not be a single monolithic story, especially if our model is a
messy ML system full of heuristics rather than a simple and clean simulation of physics.
So for a given regularity like "the diamond looks stable on camera" there can be many
diﬀerent factors that contribute to the regularity.
Moreover, even if there are actually no examples of sensor tampering on the training
distribution, sensor tampering may still have some probability on the training distribution.
And so the "best" explanation π would include sensor tampering alongside other
mechanisms.
In the simplest case, we could imagine that there are N diﬀerent ways that a pattern of
observations ("the diamond looks stable on camera") can occur. Perhaps there are a
thousand diﬀerent heuristics that can explain why images don't change over time. Let's say
these N reasons are disjoint and occur with probabilities p1, p2, ..., pN. So the total
probability of the regularity is p1+p2+...+pN. On a given input where the pattern is
predicted, we can ask how much each reason {1, 2, ..., N} is responsible.
Of course real explanations won't be linearly separable in this way. But this simple setting
helps us see what an approach might look like and helps us understand the limits of what is
feasible.
Anomaly detection given attribution
In this setting, we take a robust-classiﬁcation-like approach where we look at the worst-case
hypothesis:
Consider all the "bad" explanations for the diamond appearing safe, like sensor
tampering or swapping out the diamond for a fake. If we have a model with N diﬀerent
explanations {1, 2, ..., N} for the diamond appearing safe, we aren't sure which of
them are "bad."
But if we prepare our training set carefully, then we may think that the total probability
of all "bad" reasons is some small constant ε. So we know that the total probability is
at most ε.
So we say that a set of reasons is "plausibly good" if they explain at least (1−ε) of the
eﬀect, i.e. if the sum of pk is at least (1−ε)(p1 + ... + pN). These are the candidates for

the complete set of good reasons.
Given a new input where the model predicts the diamond will appear safe, we can ask:
what is the minimum, over all plausibly-good sets of reasons S, of the
fraction of the eﬀect that is explained by S?
If it is possible to ﬁnd a plausibly-good set of reasons that doesn't explain the eﬀect,
then we conservatively assume that the diamond appears safe for abnormal reasons.
For example, when using our model to plan to protect the diamond, we select plans for
which the diamond appears safe and it is not plausibly for abnormal reasons.
We are currently quite optimistic about applying the same idea to more general heuristic
explanations. That is, our proposed algorithm is:
Find an explanation π for the behavior "the diamond is often predicted to appear safe"
on the training distribution.
Pick a constant ε that is larger than the probability of tampering or other "bad"
explanations on the training set.
We'll deﬁne a "sub-explanation" π′ ⊆ π as something that picks and chooses some of
the mechanisms represented in π while ignoring others. We call a sub-explanation
"plausibly good" if it successfully explains (1-ε) of the behavior on the clean
training set.
Given a new input x where the diamond is predicted to appear safe, take the minimal
eﬀect size explained by any plausibly-good sub-explanation π′ ⊆ π.
There are many subtleties in this approach. It depends crucially on the space of possible sub-
reasons: we need that space to look more like picking out terms from a sum than like
training a discriminator which is allowed to arbitrarily cherry-pick cases as good or bad,
because in the latter case it will always be possible to pick a plausibly-good reason that
explains any given instance.
Even in the best case, this approach can end up with some false positives, where even a
totally in-distribution situation could be ﬂagged as potentially bad. These failure seems
quantitatively mild and extremely manageable, though in the next section we'll discuss a
much more severe category of false positives given distributional shift.
To illustrate, suppose that ε = 1/billion, but the original behavior is itself driven by a billion
diﬀerent equally important factors (perhaps a billion diﬀerent types of object permanence).
Now consider an input where the diamond appears safe, because of object permanence type
79927168. In this case we can't distinguish this single type of object permanence from
"sensor tampering."
We can overcome this either by ensuring ε is suﬃciently small, or by asking our AI to
produce a distribution over plans which more closely matches the normal distribution over
reasons that diamonds appear safe.
Sometimes we want to do things for unusual reasons
A more severe problem with arises if we want our AI to do something out of distribution.
Suppose that our explanation for "why the model predicts that the diamond appears safe"
corresponds to the following causal graph, where each node is the OR of its parents:

In words:
Either someone may try to steal the diamond or they don't.
If they try to steal the diamond, they can be stopped by a clever defense.
If no one steals the diamond, then the diamond will be undisturbed on camera, and the
pressure plate it is sitting on will also be undisturbed.
It's also possible for a robber to tamper with the sensors so they show the same result
even if the diamond is stolen.
In this picture, there are 3 ways the diamond can appear safe:
1. No one tries to steal it.
2. Someone tries to steal it and is stopped.
3. The sensors are tampered with.
We believe the strategy described so far can likely distinguish reason 3 from reason 2, which
is great.
Unfortunately, it may also be possible to distinguish reason 1 from reason 2. So if we train
our model on a distribution where there are no robbery attempts, then even if it understands
how to stop a robber with a clever defense, it may ﬂag such a defense as an "abnormal
reason" for the diamond to remain safe.
Thus our anomaly detection may work too well, and ﬂag a novel creative solution by our AI
with the same level of suspicion that it treats sensor tampering.
In increasing order of importance, here are three reasons that this approach is interesting
despite this problem:

Even if you prevent your AI from implementing some kinds of novel mechanisms to
achieve a goal, you can still get superhuman behavior out of that system, and this still
reduces signiﬁcant risks. Exactly how far you can push the AI depends on how the
internal cognition is organized, but in many cases it is possible to implement solutions
no human could understand as long as the mechanism for the diamond-on-camera
observations remains the same.
In the case of ELK it's absolutely necessary to generalize OOD. But in the other
applications discussed in this post (like deceptive alignment) it may be acceptable to
simply ﬂag OOD situations and then apply oversight on those situations.
Most importantly, there are plausible approaches to solving this problem, which we'll
turn to now.
More sophisticated ways of pointing to latent structure
If we only had a single sensor for "is the diamond safe," and we tried to deﬁne "the diamond
is safe" as the "normal reason" for that sensor to ﬁre, then it doesn't seem like the problem
is ﬁxable. This strategy for pointing to events in the world fundamentally can't distinguish
between diﬀerent parts of the causal path.
But in reality there is an incredibly rich set of relationships in the world that we can use to
pinpoint "the diamond is safe" and distinguish it from other events up the causal chain. (This
corresponds to our desire in ELK to train a single reporter that is able to talk about a rich
network of related concepts, rather than only asking about a single reward button.)
The simplest case is where we simply have two sensors, both of which are downstream of
"the diamond is safe." We'll consider the two sensors (a) the diamond is undisturbed on
camera, (b) the pressure plate the diamond is sitting on is undisturbed. Of course in the real
world we have a much richer set of relationships to point to a particular piece of latent
structure, but I think the essential dynamics appear when we try to exploit even the simplest
non-trivial causal model.
Then we can try to use those two sensors to triangulate "the diamond is safe," by looking at
the earliest point where their causal histories overlap.
Consider again our three causal paths:
1. No one steals the diamond, so it is safe, so it shows up on camera and on the
pressure plate.
2. The AI defends the diamond, so it is safe, so it shows up on camera and on the
pressure plate.
3. A robber tries to hide evidence of a theft, by tampering with the camera and the
pressure plate.
All three correspond to diﬀerent reasons for the diamond to be safe. But explanations 1 and
2 converge at the point where "the diamond is safe," while explanation 3 does not. So if
what we cared about was the least common ancestor of "show up on camera" and "show up
on pressure plate"
This suggests a possible approach to ﬂagging situation 3 as potentially-bad without ﬂagging
situation 2 as bad:
As before, consider the space of explanations that capture (1−ε) of the eﬀect ("it is
often the case that both (a) the diamond appears undisturbed on the camera and (b)
the diamond appears undisturbed on the pressure plate") on the training distribution.
For a new input x, ﬁnd the explanation in this space that captures the minimal amount
of the "(a) and (b)" on x, to get a conservative lower bound for the fraction of the
behavior that is due to "good" reasons.

Now we impose an additional constraint on the explanation. In addition to explaining
(1−ε) of "(a) and (b)" on the training set, it also needs to explain each of (a) and (b)
separately on input x. However, we are still trying to minimize how much of the
correlation it explains.
Whether this kind of approach works depends on details of how our explanations and
attributions work, and so we've mostly put it on the backburner until we get those answers.
However it currently looks plausible for the kind of simple explanations like cumulant
propagation that we are currently working with. For those settings, we can hope to pick a
sub-explanation like:
Include the links "no stealing → diamond safe" and "clever defense → diamond safe."
Include the links "diamond safe → camera looks good" and "diamond safe → pressure
plate looks good," as well as the link "Var(diamond safe) → Cov(camera looks good,
pressure plate looks good."
Include the links "robber tampers → camera looks good" and "robber tampers →
pressure plate looks good," but not the link "Var(robber tampers) → Cov(camera looks
good, pressure plate looks good)."
This explanation fails to explain the correlation between pressure plate and camera on the
new input, while explaining each factor individually. But any explanation which explains
Cov(camera, pressure plate) on the training set must include Var(diamond safe) →
Cov(camera, pressure plate), and any explanation which separately explains camera and
pressure plate on the new input must also include "clever defense → diamond safe."

On the Diplomacy AI
The latest AI development is: AI achieves human level in (blitz 5-minute-turn) full-
communication anonymous online Diplomacy (paper). Why not?

I mean, aside from the obvious.
A take I saw multiple times was that AI labs, or at least Meta, were intentionally going for the
scariest possible thing, which is why you create the torment nexus, or in this case teach the
AI to play Diplomacy. If you had to pick a game to sound scary, you'd deﬁnitely pick
Diplomacy.
The universal expectations for AI breakthroughs like this are:
1. The particular breakthrough was not expected, and is scary. The techniques used
worked better than we expected, which is scary.
2. The details of the breakthrough involve someone ﬁguring out why this particular
problem conﬁguration was easier to solve than you would expect relative to other
problems and conﬁgurations, and thus makes it less scary.
3. We ﬁnd that those details matter a lot for success, and that close variants would not be
so easy. Other times we will ﬁnd that those details allowed those creating the new
thing to skip non-trivial but highly doable steps, that they could go back and do if
necessary.
That is all exactly what we ﬁnd here.
The actual AI, as I understand it, is a combination of a language model and a strategic
engine.
The strategic engine, as I evaluated it based on a sample game with six bots and a human, is
mediocre at tactics and lousy at strategy. Humans are bad at tactics (and often strategy) in
games and Diplomacy is no exception. Diplomacy's tactics a good match for a AI.
Anticipating other players proved harder. The whole thing feels like it is 'missing a step.'
What Makes the AI Good?
Where does the AI's advantage come from? From my reading, which comes largely from the
sample game in this video, it comes from the particulars of the format, and not making some
common and costly mistakes humans make. In particular:
1. AI writes relatively long, detailed and explanatory communications to others.
2. AI does not signal its intentions via failing to communicate with its victims.

3. AI understands that the game ends after 1908 and modiﬁes accordingly.
4. AI keeps a close eye on strategic balance in order to maximize win percentage.
5. AI uses its anonymity and one-shot nature to not retaliate after backstabs.
6. AI knows what humans are like. Humans were not adjusted to bot behaviors.
When people say the AI 'solved' Diplomacy, it really really didn't. What it did, which is still
impressive, is get a handle on the basics of Diplomacy, in this particular context where bots
cannot be identiﬁed and are in the minority, and in particular where message detail is
suﬃciently limited that it can use an LLM to be able to communicate with humans
reasonably and not be identiﬁed.
If this program entered the world championships, with full length turns, I would not expect it
to do well in its current form, although I would not be shocked if further eﬀorts could ﬁx this
(or if they proved surprisingly tricky).
Interestingly, this AI is programmed not to mislead the player on purpose, although it will
absolutely go back on its word if it feels like it. This is closer to correct than most players
think but a huge weakness in key moments and is highly exploitable if someone knows this
and is willing and able to 'check in' every turn.
The AI is thus heavily optimized for exactly the world in which it succeeded.
1. Five minute turns limit human ability to think, plan and talk, whereas for a computer
ﬁve minutes is an eternity. Longer time favors humans.
2. Anonymity of bots prevents exploitation of their weaknesses if you can't conﬁdently
identify who they are, and the time limit kept most players too busy to try and
conﬁdently ﬁgure this out. They also hadn't had time to learn how the bots functioned
and what to expect, even when they did ID them.
3. One-shot nature of games allows players to ignore their reputations and changes the
game theory, in ways that are not natural for humans.
4. Limited time frame limits punishment for AI's inability to think about longer term multi-
polar dynamics, including psychological factors and game theoretically strange
endgame decisions.
5. Limited time frame means game ends abruptly in 1908 (game begins in 1901, each
year is two movement turns, two retreats and a build) in a way that many players
won't properly backward chain for until rather late, and also a lot of players will
psychologically be unable to ignore the longer term implications even though they are
not scored. In the video I discuss, there is an abrupt 'oh right game is going to end
soon' inﬂection point in 1907 by the human.
6. Rank scoring plus ending after 1908 means it is right to backstab leaders and to do a
kind of strange strategy where one is somewhat cooperating with players you are also
somewhat ﬁghting, and humans are really bad at this and in my experience they often
get mad at you for even trying.
The Core Skill of Online Diplomacy is Talking a Lot
As the video's narrator explains: The key to getting along with players in online Diplomacy is
to be willing to talk to them in detail, and share your thoughts. Each player only has so much
time and attention to devote to talking to six other players. Investing in someone is a sign
you see a future with them, and letting them know how you are thinking helps them navigate
the game overall and your future actions, and makes you a more attractive alliance partner.
Humans also have a strong natural tendency to talk a lot with those they want to ally with,
and to be very curt with those they intend to attack or especially backstab (or that they
recently attacked or backstabbed). This very much matches my experiences playing online.
If a human suddenly starts sending much shorter messages or not talking to you at all, you
should assume you are getting stabbed. If you do this to someone else, assume they expect
a stabbing. Never take anyone for granted, including those you are about to stab.

This gives the AI a clear opportunity for big advantage. An AI can easily give complex and
detailed answers to all six opponents at the same time, for the entire game, in a way a
human cannot. That gives them a huge edge. Combine that with humans being relatively
bad at Diplomacy tactics (and oh my, they're quite bad), plus the bots being hidden and thus
able to play for their best interests after being stabbed without everyone else knowing this
and thus stabbing them, and the dynamics of what actually scores points in a blitz game
being counter-intuitive, and the AI has some pretty big edges to exploit.
The ﬁve minute turns clearly work to the AI's advantage. The AI essentially suﬀers not at all
from the time pressure, whereas ﬁve minutes is very little time for a human to think. I expect
AI performance to degrade relative to humans with longer negotiation periods.
Lessons From the Sample Game
The sample game is great, featuring the player written about here. If you are familiar with
Diplomacy or otherwise want more color, I recommend watching the video.
The human player is Russia. He gets himself into big trouble early on by making two key
mistakes. He gets out of that trouble because the AI is not good at anticipating certain
decisions, a key backstab happens exactly when needed, the player wins a key coin ﬂip
decision, and he shifts his strategy into exploiting the tendencies of the bots.
The ﬁrst big mistake he makes is not committing a third unit to the north. Everything about
the situation and his strategy screams to put a third unit in the north, at least an army and
ideally a ﬂeet, because the south does not require an additional commitment or does the
additional commitment open up opportunity. Instead, without a third northern unit, Russia
has nowhere to expand for a long time.
The second big mistake was violating his DMZ agreement with Austria by moving into
Galicia. He did this because the AI failed to respond to him during the turn in question, and
he was worried this indicated he was about to get stabbed, despite the stab not making a
ton of tactical sense. Breaking the agreement with Austria led to a war that was almost fatal
(or at least probably did, there's some chance Austria does it anyway), without any prospect
of things going well for Russia at any point.
Against a human, would this play have been reasonable? That depends on how reliable an
indicator is radio silence, and how likely a human would be to buy it as an excuse. Against an
AI, it does not make sense. The AI has no reason to not talk at all in this spot, regardless of
its intentions. So it is strange that it did not respond here, it seems like a rather painful bug.
The cavalry saves us. Italy stabs Austria, while France moves against England.
Here is a tactical snapshot. I hate France's tactical play, both its actual plays and the
communications with Russia that are based on its tactics, dating back to at least 1903. The
move here to Irish Sea needs to be accompanied by a convoy of Picardy into London or
Wales, ﬁghting for Belgium here is silly. Italy does reasonable things. Austria being in
Rumania and Ukraine is an existential threat, luckily Austria chooses a retreat here that
makes little sense. Once you have Bulgaria against Turkey, you really don't want to give it
up. Austria also lost three or so distinct guessing games here on the same turn. Finally I
would note that Italy is surprisingly willing to lose the Ionian Sea to pick up the Aegean, and
that if I am Turkey here there is zero chance I am moving Ankara anywhere but Black Sea.
My sense is also that the AI 'plays it safe' and does what it thinks is 'natural' more often than
is game theory optimal. This is conﬁrmed by an author of the paper here, along with other
similar observations. The AI assumes it can 'get away with' everything because on the
internet no one knows you are a bot or what you are up to, and makes decisions accordingly.
A huge edge if you get away with it. A huge weakness if you do not.

Then again, Diplomacy players are weird, myself included. There is almost always a tactical
way to punish an aggressive 'natural' or 'correct' play if you are willing to get punished hard
by other moves, such as if Germany were to try to sneak into Picardy (PIC) here. So any
given decision could be one mixing up one's play, so my evaluations are more based on the
whole of the eight years of play by six players.
The turn above, Spring 1904, is about where Russia pivots from acting like it is playing a
normal full game against humans to understanding it is playing an eight-year game for rank
order against bots, and he starts asking 'what would a bot do?' Things turn around quite a bit
after that. His only slip beyond that is at about 42:00 when he worries he will 'annoy Austria'
in a way that shouldn't (and didn't) apply to a bot.
The big exploit of the bots is simple. A bot is not going to retaliate later in the game for a
backstab earlier in the game, or at least will retaliate far less. As things shift into the
endgame, taking whatever tactical advantages present themselves becomes more and more
attractive as an option. Bots will sometimes talk about 'throwing their centers' to another
player as retaliation, or otherwise punishing an attacker or backstabber, but you know it is
mostly talk.
If you play Diplomacy using pure Causal Decision Theory without credible precommitments,
and it is a one-shot fully anonymous game, that can work. When you are identiﬁable (or even
worse if someone can see your source code, as they could in a lot of MIRI or other old-school
LW thought experiments), you are going to have a bad time.

Diplomatic Decision Theory
The central decision theory question of Diplomacy is how one should respond when stabbed,
and what this says about how one should act before one is stabbed.
Responses run the whole range from shrugging it oﬀ to devoting the rest of one's life to
revenge. There is a reason people say Diplomacy ruins friendships. Reasonable people max
out at 'spend the rest of the game ensuring you lose' and being less inclined to trust you in
future games, but a lot of what keeps human systems working is that you never know for
sure how far things might go.
When deciding whether to attack someone, a key consideration is how they are likely to
react. If they are going to go kamikaze on you, you need to ensure you can handle that. If
they are going to mostly shrug it oﬀ, even let you use your newly strong position to drive a
better bargain, then it is open season whenever you have a tactical opening, and then there
is everything in between.
The correct solution in a fully one-shot anonymous game, if you can pull it oﬀ, is obviously to
give people the impression you will strongly retaliate, then to not follow through on that
under most circumstances. Humans, of course, have a hard time pulling this oﬀ.
Bots also have a hard time pulling this oﬀ in a credible way, for diﬀerent reasons. The bots
here mostly were free riders. Humans did not know what they were dealing with. So they
gave bots an appropriately broad range of potential reactions. Then the bots got the beneﬁts
of not spending their resources on punishment. Once humans did know what they were
dealing with, and adjusted, things wouldn't go so well there. If there were a variety of bots
competing at that point, bots would have a hell of a time trying to represent that they would
actually retaliate 'properly.'
Thus, the 'irrational' ﬂaws in humans grant them a distinct advantage in the default case,
where identity is broadly (partially, at least) known and behaviors have a chance to adjust to
what information is available.
AIs so far have essentially 'gotten away with' using Causal Decision Theory in these spots,
despite its extreme vulnerability to exploitation. This contrasts with many much 'dumber' AIs
of the past, such as those for Civilization, which were hardcoded with extreme retaliation
functions that solve these issues, albeit at what could be a steep price. I wonder what will
happen here with, for example, self-driving cars. If AIs are going to be operating in the real
world more and more, where similar situations arise, they are going to have to get a better
decision theory, or things are going to go very badly for them and also for us.
In this sense, the Hard Problem of Diplomacy has not yet been touched.
Overall Takeaways and Conclusion
The actual results are a mixed bag of things that were surprisingly hard versus surprisingly
easy. The easy was largely in ways that came down to how Meta was able to deﬁne the
problem space. Communications generic and simple and quick enough to easily imitate and
even surpass, no reputational or decision theoretic considerations, you can respond to
existing metagame without it responding to you. Good times. The hard was in the tactical
and strategic engines being lousy (relative to what I would have expected), which is more
about Meta not caring or being skilled enough to make a better one rather than it being
impossible.
Gwern notes that in June 2020 that Diplomacy AIs were a case of 'the best NNs can't even
beat humans at a simpliﬁed Diplomacy shorn of all communication and negotiation and
manipulation and deception aspects.' I think this is selling the deceptive aspects of no-press
(e.g. no communication) Diplomacy short, although it highlights that NNs have a terrible time

anticipating human reactions in multiplayer settings, as well. Mostly it seems to me like a
case of the people involved not trying all that hard, and in particular not being willing to do a
bunch of kludges.
This blog post from Gary Marcus and Ernest Davis gives the perspective that this shows that
Ai is not primarily about scaling, oﬀering additional details on how Cicero works. There were
a lot of distinct moving pieces that were deliberate human designs. This contrasts with
Gwern's claim that the scaling hypothesis predicted Diplomacy would fall whereas
researchers working on the problem didn't.
I think I come down more on Marcus' side here in terms of how to update in response to the
information. How it was done, in context, seems more important than who claimed it would
get done how fast.
I do not get any points for predicting this would happen, since I did not think about the
question in advance or make any predictions. It is impossible to go back and conﬁdently say
'I would have made the right prediction here' after already knowing the answer. My guess is
that if you'd asked, in the abstract, about Diplomacy in general, I would have said it was
going to be hard, however if you'd told me the details of how these games were played I
would have been much less skeptical.
I do know that I was somewhat confused how hard no-press Diplomacy was proving to be in
previous attempts, or at least took it more as evidence no one was trying all that hard
relative to how hard they tried at other problems.
I also note that there wasn't much discussion that I saw of 2-player Diplomacy variations, of
which there are several interesting ones, as a way of distinguishing between simultaneous
play being diﬃcult versus other aspects. Are Diplomacy actually surprisingly diﬃcult? This
would tell us. Perhaps I simply missed it.
Gwern's conclusion in the comments of this post is that the main update from the Diplomacy
AI is that Meta bothered to make a Diplomacy AI. This seems right to me, with the note that
it should update us towards Meta being even more of a bad actor than we previously
assumed. Also the note that previously Diplomacy had seemed to be proving surprisingly
hard in some aspects, and that seems to have largely gone away now, so the update is
indeed in the 'somewhat scarier' direction on net. Gwern then oﬀers background and
timeline considerations from the scaling hypothesis perspective.
My big picture takeaway is that I notice I did not on net update much on this news, in any
direction, as nothing was too shocking and the surprises often cancelled out.

Tyranny of the Epistemic Majority
This post is going to mostly be propaganda for Kelly betting. However, the reasons
presented in this post diﬀer greatly from the reasons people normally use to argue for
Kelly betting.
The Steward of Myselves
The curse of uncertainty is that I must make decisions that simultaneously aﬀect
many diﬀerent versions of myself. When I close my eyes and then ﬂip a coin, there are
two potential versions of me: one sitting in front of a coin showing heads, the other
sitting in front of a coin showing tails. Both of these potential versions of me are
stakeholders in my current decisions. How can I make decisions on behalf of these
multiple stakeholders?
If it is a fair coin, then we can think of these two potential selves as equal
stakeholders in my decisions. However, I know that it is not a fair coin. It has a 60
percent chance of coming up heads. Thus, heads-me is a 60 percent stakeholder in
my current decisions, and tails-me is a 40 percent stakeholder. They amount  of each
one's stake is naturally in proportion to the probability that they actually exist.
You, however, do not know if it is a fair coin, and are oﬀering me a fair bet. I only have
100 dollars to my name, and I am can bet as much as I want (up to 100 dollars) in
either direction at even odds.
If I bet 100 dollars on heads, heads-me gets 200 dollars, and tails-me gets nothing. If I
bet 100 dollars on tails, tails-me gets 200 dollars, and heads-me gets nothing. If I bet
nothing, both versions of me get 100 dollars.
However, every dollar in the hands of heads-me is worth 1.5 times as much as a dollar
in the hands of tails-me, since heads-me exists 1.5 times as much. (I am ignoring here
any diminishing returns in my value of money.)
Thus, to maximize value I should bet 100 dollars on heads. However, maybe it is
better to think of tails-me as the rightful owner of 40 percent of my resources. When I
bet 100 dollars on heads, I am seizing money from tails-me for the greater good, since
heads-me has the (proportionally greater) existence necessary to better take
advantage of it.
Alternatively, I could say that since 60 percent of me is heads-me, heads-me should
only control 60 dollars, which can be bet on heads. Tails-me should control 40 dollars,
which can be bet on tails. These two bets partially cancel each other out, and the net
result is that I bet 20 dollars on heads.
If you are especially fast at maximizing expected logarithms, you might see where this
is going.
Compositionality

Now, I am ready to introduce my friend, Kelly. Kelly also has her eyes closed, also has
100 dollars, and is sitting in front of the same coin. However, Kelly has diﬀerent
beliefs. Kelly believes that the coin has a 90 percent chance of coming up tails, and
Kelly also has 100 dollars.
I bet 20 dollars on heads, for the reasons described above. Kelly bets 80 dollars on
tails for similar reasons (90 dollars on tails, partially nulliﬁed by 10 dollars on heads).
I have another friend, Marge. Marge is sitting on the other side of the table with her
eyes closed. Marge has 200 dollars. Marge doesn't know much about coins, but knows
my and Kelly's beliefs, and thinks Kelly and I are equally likely to be correct. Thus,
Marge assigns a 65 percent chance that the coin comes up tails. Marge thus bets 60
dollars on tails (130 dollars on tails, partially nulliﬁed by 70 dollars on heads).
Note that the 60 dollars bet by Marge is the same as the net 60 dollar bet you get if
you draw a box around me and Kelly. This is representing the compositionality of this
betting policy. When you draw a box around me and Kelly, you can think of us as one
agent whose wealth is the sum of our wealths, and whose beliefs are the weighted (by
wealth) average of our beliefs.
If Kelly, Marge and I all implemented the other strategy, of putting all our money on
the outcome we thought was most likely, this would not have happened. Marge would
have put 200 dollars on tails, while Kelly and I would have, on net, bet nothing.
This should not be surprising. When you implement a majoritarian policy, it matters
where you draw the boundaries. When you instead implement a proportional
representation policy, It does not matter where you draw the boundaries. When you
have an internal voting block, you have to be careful who you let into your voting
block, since it might swing the whole block in the other direction. I think many
phenomena that get labeled as politics are actually about ﬁghting over where to draw
the boundaries. Wouldn't it be nice if we didn't have to worry about where we draw
the boundaries?
Bayesian Updating
We all open our eyes, and see that the coin came up heads. I am given 20 dollars, and
now have 120 dollars. Kelly loses her 80 dollars, and is left with 20 dollars. Marge
loses her 60 dollars, and is left with 140 dollars. Yay! Sorry, Kelly and Marge.
We all close our eyes, and the coin is ﬂipped again, and we are oﬀered the same bets.
I only had one hypothesis, that the coin was a biased coin with a 60 percent chance of
coming up heads, so I do not update at all, and will bet similarly again. I have two
potential selves, sitting in front of diﬀerent coins: heads-me has 72 dollars, which are
bet entirely on heads, while tails-me has 48 dollars, which are bet entirely on tails.
These bets partially cancel out, and on net I bet 24 dollars on heads (20 percent more
money than last time, since I have 20 percent more money). Note that these diﬀerent
versions of me are not the same as the ones from last round. There is a new coin ﬂip,
so there's a new branch in my future than before. Similarly, Kelly has 20 dollars, and
so bets 16 dollars on tails (18 dollars on tails, partially nulliﬁed by 2 dollars on heads).
Marge's situation is more complicated. Marge had two diﬀerent hypotheses about the
coin: one in which I am right, and one in which Kelly is right. Marge has observed

some Bayesian evidence that I am right, with an odds ratio of 6 to 1. This evidence
that I am right translates into evidence that the coin will come up heads. Marge thus
updates her 65 percent probability the coin will come up tails to an approximately 53
percent probability the coin will come up heads. (a 37/70 chance of coming up heads,
to be exact). Marge then bets exactly 8 of her 140 dollars on heads (74 dollars on
heads, partially nulliﬁed by 66 dollars on tails).
Again, Marge's bet is exactly the same as the net bet of me and Kelly.
Indeed, whenever Kelly and I bet, you can break this up into a net bet with the house,
together with an internal bet that determines how much control we will each have
over whatever money our collective ends up with. The internal bet will always exactly
implement Bayesian updating on how much the collective trusts each of us.
As a Bayesian agent, you can think of yourself as a collection of bettors that
implement this proportional representation betting strategy and bet with each other.
Instead of betting with money, they are betting with a currency that represents your
posterior beliefs. When used internally, it recreates Bayesian updating. Maybe as a
society, we could get some pretty cool results if we also followed this strategy
collectively.
Bargaining with Myself
The above analysis was a weird case because you were oﬀering both sides of the bet
at a fair price. In practice, this is unrealistic. Let's instead look at what happens if I
only win 95 cents for each dollar I stake. Heads-me wants to put his 60 dollars on
heads, and will win 57 dollars, so I end up with 117 dollars if the coin landed heads.
Tails-me put his 40 dollars on tails, so I end up with 78 dollars if the coin landed tails.
The fact that I am betting on both sides is wasteful. There is a Pareto improvement
where I bet less on both sides, and end up only betting on heads. And, I want to pick
up this Pareto improvement.
There was no such Pareto improvement before, because my two selves were
essentially in a zero sum game. Every dollar one of them got corresponded to a dollar
the other one didn't get. Now, they are in a positive sum game and need to split the
gains they get not betting on both sides. (However, if I am Bayesian updating, they
might internally bet with each other without paying the 5 percent fee.)
How should I split the gains from trade between my two potential selves? Hmm, if only
I had some strategy for fairly distributing utility in a Pareto optimal way when I have
uncertainty about who I am.
I will have my diﬀerent selves Nash bargain! The 0 utility point will be no money. The
utility functions will be linear in money, and the distribution on my potential selves will
come from the uncertainty I already have.
When I Nash bargain, I end up maximizing the expected logarithm of expected utility.
In this case, the outer expectation is over who I am, which I am thinking of as
including the state of the coin. Since we moved our uncertainty about the world into
our uncertainty about identity, the only thing left in the inner expectation is
randomness coming from our action. However, since we can bet continuous amounts
of money, and we are treating utility of my various selves as linear in money, we don't
have to ever actually randomize, we can just mix between strategies by mixing

between our betting amounts. Thus, I end up maximizing the expected logarithm of
wealth.
Kelly Betting
This betting strategy, where you maximize the expected logarithm of your wealth, is
known as Kelly betting. In the simplifying example where you can bet on anything,
and fairly take either side of any bet (which should be approximately true given
suﬃciently large markets), it is equivalent to treating your various hypotheses as
owning proportional portions of your wealth, which they bet entirely on the world that
they are in.
I will leave it as an exercise to try to get an intuitive understanding for why
maximizing the expected logarithm might be deeply entangled with proportional
representation. *coughlogscoringrulecough* *coughminimizingcrossentropycough*
Again, this is not the standard argument for Kelly betting. The standard argument is
very good, and is basically that (roughly) if you don't Kelly bet, then after enough
time, you will with probability approaching 1 have less money than if you did Kelly bet.
There is a nice parallel between what happens when you don't Kelly bet and when you
don't Nash bargain. When you maximize expected wealth, you end up with more
money in expectation, unfortunately all that money ends up in the same world, which
over time has smaller and smaller probability. In all other worlds, you are left with
nothing. This would be ﬁne if you had some channel to transfer the wealth from the
one tiny world to all the other worlds, but you don't, so you just end up broke with
probability 1.
Similarly, when you maximize total utility, rather than Nash bargaining, you end up
with more total utility, but you might end up devoting all of your resources to one
utility monster. This would be ﬁne if you could transfer that utility to everyone else,
but you can't, so almost everyone might end up with nothing. 
Betting Even Less
Many claim that even Kelly betting is not risk averse enough. One major alternative
considered is fractional Kelly betting. For example in half Kelly betting, you bet half as
much as you would if you were Kelly betting. This may seem like a hack, but I think it
kind of makes sense.
Let's say that I maintain two diﬀerent probability distributions. Society has their
market probability distribution P, which is updated using who-knows-what. I have my
inside view probabilities Q, which I try to update Bayesianly, but am obviously not
perfect. However, I also have my outside view probabilities Q′. My inside view might
be right, or the market might be right, so let's average between them. Q′ =
Q +
P. I
want to keep my inside view and my outside view separate. I use my inside view to
think, and I use my outside view to bet.
12
12

What happens when I Kelly bet according to my outside view? If you think of Kelly
betting as maximizing an expected logarithm, you might start doing some crazy
computations, but if you have been following this post thus far, you can just say:
I am the sum of two agents each with half my wealth. The ﬁrst Kelly bets according to
my inside view, and the second doesn't bet at all. This I bet half as much as I would if I
were Kelly betting according to my inside view. Isn't compositionality nice?
So, why isn't half Kelly betting just thought of as Kelly betting with diﬀerent beliefs?
The diﬀerence is in the updating. I do not update my outside view in a Bayesian way. I
update my inside view in a Bayesian way, but I maintain the fact that I think there is a
50 percent chance the market is right instead of me. This is in spite of the observation
that I seem to be making money. If on round one I make money, and on round two, I
still only make half a Kelly bet, I am choosing to defer to the market more than a
Bayesian update on my outside view would suggest.
I am subsidizing my deference to the market by doing a wealth transfer, from my
inside view to my deference to the market. Given that I do not fully trust my reasoning
and my ability to update my inside view correctly, this seems not entirely crazy to me,
and I think it makes more sense when thought of as market deference than when
thought of as just cutting my bet in half to be conservative.
Betting Less Still
People sometimes get confused looking at the standard argument for Kelly betting,
and say "My utility is already logarithmic in dollars, Shouldn't I bet so as to maximize
my expected log log wealth?" Firstly, your utility is not logarithmic in dollars. Utilities
are bounded. But secondly, according to the standard argument, the answer is no. If
you make enough bets, and continue disagreeing in the market, in the long run, you
will, with probability approaching 1, wish you maximized expected log wealth.
However, the arguments in this post are not about repeated bets. They are about
respecting your epistemic subagents, and apply even if you only make one bet. If you
have utility proportional to the logarithm of 1 dollar plus your wealth, and you Nash
bargain across all your possible selves, you end up approximately maximizing the
expected logarithm of the logarithm of 1 dollar plus your wealth. (I had to add in the
dollar to avoid negative inﬁnity madness.)
I would be careful here, though. I am not sure I endorse going this far. You are
sacriﬁcing Bayesian compositionality niceness, and I am not sure exactly what kind of
introspecting I would have to do to verify that I actually have preferences logarithmic
in wealth, and do not just think that I do because I have Kelly betting intuitions hard
coded into me. Anyway, be careful, but again, not entirely crazy to me.

The Geometric Expectation
A Suspicious Pattern
There is a pattern that shows up in many of the toys we like to play with around here:
the pattern of maximizing the expected logarithm.
Nash bargaining is a method for aggregating preferences without a means to directly
compare them. When Nash bargaining, you are maximizing the expected logarithm of
utility, where the expectation is over uncertainty about which person you are.
Kelly betting is an extremely useful tool for not putting all your future wealth in one
basket. When Kelly betting, you are maximizing the expected logarithm of your
wealth.
The log scoring rule is a very natural way to extract beliefs. When maximizing your log
score, you are maximizing the expectation of the logarithm of the probability you
assign to the right answer. This is one example of a general pattern. Maximizations of
expected logarithms show up all over information theory, often phrased as minimizing
the negative of the expected logarithm.
Why does maximization of the expected logarithm keep showing up?
One answer is that all of the instances of it showing up are actually related. In my
previous two posts, I made some connections between Nash bargaining and Kelly
betting. The fact that Kelly betting can be used to model Bayesian updating illustrates
its relationship with the information theory applications. To a certain extent, there is
really only one instance of this pattern.
However, I think that there is another argument for why you should expect this
pattern to show up a lot, which is that the pattern is very simple. More simple than it
looks on the surface. It only looks complicated because mathematicians have failed
us.
The Geometric Integral
One of the most underrated concepts in mathematics is the geometric integral, given
by ∏f(x)dx = e∫ln(f(x))dx. (The fact that I couldn't easily get a latex symbol that looks
like an elongated P is a testament to its underratedness.) The geometric integral is
just like the standard integral, but everywhere you would add, you multiply instead.
Deﬁning it in terms of the standard (arithmetic) integral with logs and exponents is
insulting to its nature, and I don't recommend thinking of it that way. (You wouldn't
deﬁne x × y as eln(x)+ln(y).) Instead, you should just think of it as the multiplicative
version of the integral. However, using logs and exponentiation, it is the fastest way
to get the deﬁnition across.

I think people don't practice thinking multiplicatively enough, which causes them to
throw inherently multiplicative things into logarithms, so they can think about them
additively.  
I will use the phrase geometric expectation when I take a geometric integral over a
probability distribution, and I will use the symbol G. Thus, we will write 
Gx∼Pf(x) = eEx∼P ln f(x).
Discrete Geometric Expectations
Luckily, most of the time, we will want to talk about discrete geometric expectations,
where we can use (possibly inﬁnite) sums rather than integrals and (possibly inﬁnite)
products rather than geometric integrals.
Let us gain some intuition for discrete geometric expectations by going though some
simple cases. We will start with a uniform distribution on a ﬁnite set.
Let X = {x1, ... , xn} be a ﬁnite set with n elements. Let f : X →R≥0 be a function that
assigns a nonnegative value to each xi. Let P be the uniform probability distribution
on X that assigns probability 
 to each element of X.
We have that Ex∼Pf(x) = ∑x∈X P(x)f(x) = ∑
n
i=1
=
. This is just the
average, or arithmetic mean of the f values.
We can compute Gx∼Pf(x) using the above formula Gx∼Pf(x) = eEx∼P ln f(x). Here, we get
Gx∼Pf(x) = eEx∼P ln f(x) = e
=
n√eln f(x1) ... eln f(xn) =
n√f(x1) ... f(xn).
Thus, the geometric expectation of the uniform distribution is just the geometric mean
of the f values. Hence the name.
The inﬁnite non-uniform discrete case is not much more diﬃcult. If X is a ﬁnite or
countably inﬁnite set, f : X →R≥0 assigns a nonnegative value to each x ∈X, and P is
a probability distribution on Y , then Ex∼Pf(x) = ∑x∈X P(x)f(x), and
Gx∼Pf(x) = eEx∼P ln f(x) = e∑x∈X P(x) ln f(x) = ∏x∈X eP(x) ln f(x) = ∏x∈X f(x)P(X). 
1n
f(xi)
n
f(x1)+...+f(xn)
n
ln f(x1)+...+ln f(xn)
n

These two values can be thought of as a weighted arithmetic mean and weighted
geometric mean respectively.
When taking the geometric expectation of f with respect to P, you just take the
product over all x ∈X of f(x)P(x). You are multiplying together all the f values, but the
exponent P(x) is saying that values with less probability get less weight (or less
"power").
Maximizing the Geometric Expectation
Maximization is invariant under applying a monotonic function. Thus 
argmaxy∈Y Ex∼P ln(f(x, y)) = argmaxy∈Y eEx∼P ln(f(x,y)) = argmaxy∈Y Gx∼Pf(x, y).
So every time we maximize an expectation of a logarithm, this was equivalent to just
maximizing the geometric expectation.
Rather than saying "maximize the geometric expectation", I will just say
"geometrically maximize". For example, when Kelly betting, we are just geometrically
maximizing wealth. Note that the unit on the geometric expectation of wealth is
dollars. The unit on the expected logarithm of dollars is... confusing? It is log dollars,
but like, you add it instead of multiplying? I don't know how it works. What even is a
log dollar?
The geometric expectation just makes more sense than the expected logarithm. It is a
real thing with a real meaning. However, when we put the geometric expectation
inside of a maximization, and we don't naturally think in terms of geometric
expectations, we are tempted to take a logarithm of the whole thing, (which we can
do because the maximization eats the monotonic function), and end up with
maximizing the expected logarithm.
Geometric Rationality
When Kelly betting, you are really just geometrically maximizing wealth.
When Nash Bargaining, you are really just geometrically maximizing expected utility
with respect to your uncertainty about your identity. In defense of Nash bargaining, It
is normally presented as maximizing the product of the utilities. However, if you don't
already have the concept of geometric expectation, it is tempting to convert it to an
expected logarithm so you can handle the weighted case and think of it as being
about uncertainty behind the veil of ignorance. (Also, it is more like the square root of
the product of the utilities rather than the product of the utilities.)
When maximizing log score, you are really just geometrically maximizing the
probability you assign your observation.
I will informally use the phrase "geometric rationality" to refer to techniques that tend
to geometrically maximize natural features (of the world or the self). I want to raise to

attention the hypothesis that humans are evolved to be naturally inclined towards
geometric rationality over arithmetic rationality, and that around here, the local
memes have moved us too far oﬀ this path. 

Geometric Rationality is Not VNM
Rational
One elephant in the room throughout my geometric rationality sequence, is that it is
sometimes advocating for randomizing between actions, and so geometrically rational
agents cannot possibly satisfy the Von Neumann-Morgenstern axioms. That is correct:
I am rejecting the VNM axioms. In this post, I will say more about why I am making
such a bold move.
A Model of Geometric Rationality
I have been rather vague on what I mean by geometric rationality. I still want to be
vague in general, but for the purposes of this post, I will give a concrete deﬁnition,
and I will use the type signature of the VNM utility theorem. (I do not think this
deﬁnition is good enough, and want it to restrict its scope to this post.)
A preference ordering on lotteries over outcomes is called geometrically rational if
there exists some probability distribution P over interval valued utility functions on
outcomes such that L ⪯M if and only if GU∼PEO∼LU(O) ≤GU∼PEO∼MU(O).
For comparison, an agent is VNM rational there exists a single utility function U,  such
that L ⪯M if and only if EO∼LU(O) ≤EO∼MU(O). 
Geometric Rationality is weaker than VNM rationality, since under reasonable
assumptions, we can assume the utility function of a VNM rational agent is interval
valued, and then we can always take the probability distribution that assigns
probability 1 to this utility function.
Geometric Rationality is strictly weaker, because it sometimes strictly prefers lotteries
over any of the deterministic outcomes, and VNM rational agents never do this.
The VNM utility theorem says that any preference ordering on lotteries that satisﬁes
some simple axioms must be VNM rational (i.e. have a utility function as above). Since
I am advocating for a weaker notion of rationality, I must reject some of these axioms.
Against Independence
The VNM axiom that I am rejecting is the independence axiom. It states that given
lotteries A, B, and C, and probability p, A ⪯B if and only if 
pC + (1 −p)A ⪯pC + (1 −p)B. Thus, mixing in a probability p of C will not change my
preference between A and B. 
Let us go through an example.

Alice and Bob are a married couple. They are trying to decide where to move, buy a
house, and live for the rest of their lives. Alice prefers Atlanta, Bob prefers Boston. The
agent I am modeling here is the married couple consisting of Alice and Bob. 
Bob's preference for Boston is suﬃciently stronger than Alice's preference for Atlanta,
that given only these options, they would move to Boston (A ≺B).
Bob is presented with a unique job opportunity, where he (and Alice) can move to
California, and try to save the world. However, he does not actually have a job oﬀer
yet. They estimate an 80 percent chance that he will get a job oﬀer next week.
Otherwise, they will move to Atlanta or Boston. 
California is a substantial improvement for Bob's preferences over either of the other
options. For Alice, it is comparable to Boston. Alice and Bob are currently deciding on
a policy of what to do conditional on getting and not getting the oﬀer. It is clear that if
they get the oﬀer, they will move to California. However, they ﬁgure that since Bob's
preferences are in expectation being greatly satisﬁed in the 80 percent of worlds
where they are in California, they should move to Atlanta if they do not get the oﬀer (
pC + (1 −p)B ≺pC + (1 −p)A).
Alice and Bob are collectively violating the independence axiom, and are not VNM
rational.  Are they making a mistake? Should we not model them as irrational due to
their weird obsession with fairness?
Dutch Books and Updatelessness
You might claim that abandoning the independence axiom opens up Alice and Bob up
to get Dutch booked. The argument would go as follows. First, you oﬀer Alice and Bob
a choice between two policies: 
Policy CA: California if possible, otherwise Atlanta, and 
Policy CB: California if possible, otherwise Boston.
They choose policy CA. Then, you reveal that they did not get the job oﬀer, and will
have to move to Atlanta. You oﬀer them to pay you a penny to instead be able to
move to Boston. In this way, you extract free money from them!
The problem is they don't want to switch to Boston, they are happy moving to Atlanta.
Bob's preferences are being extra satisﬁed in the other possible worlds where he is in
California. He can take a hit in this world.
If California did not exist, they would want to move to Boston, and would pay a penny
to move to Boston rather than Atlanta. The problem is that they are being updateless.
When they observe they cannot choose California, they do not fully update on this fact
and pretend that the good California worlds do not exist. Instead they follow through
with the policy that they agreed to initially.
We can take this further, and pretend that they didn't even consider Atlanta vs
Boston. They just got a job oﬀer, and decided to move to California. Then all the world
saving money disappeared over night, the job oﬀer was retracted, and Alice and Bob

are newly considering Atlanta vs Boston. They might reason, that if they would have
taken the time to consider this possibility up front, they would have chosen Atlanta, so
they follow through the policy that they would have chosen if they would have
thought about it more in advance.
They have a preference for fairness, and this preference is non-local. It cares about
what happens in other worlds.
I gave the above example about a married couple, because it made it cleaner to
understand the desire for fairness. However, I think that it makes sense for individual
humans to act this way with respect to their various diﬀerent types of preferences.

AI will change the world, but won't
take it over by playing "3-dimensional
chess".
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
By Boaz Barak and Ben Edelman
[Cross-posted on Windows on Theory blog; See also Boaz's posts on longtermism
and AGI via scaling as well as other "philosophizing" posts.]
[Disclaimer: Predictions are very hard, especially about the future. In fact, this is one of
the points of this essay. Hence, while for concreteness, we phrase our claims as if we
are conﬁdent about them, these are not mathematically proven facts. However we do
believe that the claims below are more likely to be true than false, and, even more
conﬁdently, believe some of the ideas herein are underrated in current discussions
around risks from future AI systems.]
[To the LessWrong audience: we realize this piece is stylistically diﬀerent from many
posts here, and is not aimed solely at regular readers, for which various terms and
claims below might be very familiar. Our original impetus to write this piece was a
suggestion from the organizers of the Future Fund AI worldview competition ; while we
are not holding our breath for any reward, we thought it would be a good idea to
engage with the LessWrong audience in this discussion. This post could be described as
somewhere between an ordinary collaboration and an adversarial one—the views
expressed are a closer ﬁt for Boaz's current views than Ben's.]

 
In the past, the word "computer" was used to denote a person that performs
calculations. Such people were highly skilled and were crucial to scientiﬁc enterprises.
As described in the book "Hidden Figures", until the 1960s, NASA still used human
computers for the space mission. However, these days a $10 calculator can instantly
perform calculations beyond the capabilities of every human on earth.
On a high level, the situation in Chess and other games is similar. Humans used to be
the reigning champions in Chess and Go, but have now been surpassed by computers.
Yet, while the success of computers in performing calculations has not engendered
fears of them "taking over the world," the growing powers of AI systems have more
people increasingly worried about their long-term implications. Some reasons why the
success of AI systems such as AlphaZero in Go and Chess is more concerning than the
success of calculation programs include
1. Unlike when working with numerical computation programs, it seems that in
Chess and Go humans are entirely "unnecessary." There is no need to have a
"human in the loop". Computer systems are so powerful that no meaningful
competition is possible between even the best human players and software
running on commodity laptops.[1]
 
2. Unlike the numerical algorithms used for calculations, we do not understand the
inner workings of AI chess systems, especially ones trained without any hand-
designed knowledge. These systems are to a large extent "black boxes," which

even their creators do not fully understand and hence cannot fully predict or
control.
 
3. Moreover, AlphaZero was trained using a paradigm known as reinforcement
learning or RL (see also this book). At a high level, RL can be described as
training an agent to learn a strategy (i.e., a rule to decide on a move or action
based on the history of all prior ones) in order to maximize a long-term reward
(e.g., "win the game"). The result is a system that is capable of executing actions
that may seem wrong in the short term (e.g., sacriﬁcing a queen) but will help
achieve the long-term goal. 
 
While RL so far has had very limited success outside speciﬁc realms such as games or
low-complexity settings, the success of (non-RL) deep learning systems such as GPT-3
or Dall-E in open-ended text or image generation has raised fears of future AI systems
that could both act in the real world, interacting with humans, physical, and digital
systems, and do so in the pursuit of long term goals that may not be "aligned" with the
interests of humanity. The fear is that such systems could become so powerful that
they could end up destroying much or all of humanity. We refer to the above scenario
as the loss of control scenario. It is distinct from other potential risks of Artiﬁcial
Intelligence, including the risks of AI being used by humans to develop more lethal
weapons, better ways for repressive regimes to surveil their population or more
eﬀective ways of spreading misinformation.
In this essay, we claim that the "loss of control" scenario rests on a few key
assumptions that are not justiﬁed by our current understanding of artiﬁcial
intelligence research. (This doesn't mean the assumptions are necessarily wrong—
just that we don't believe the preponderance of the evidence supports them.)  To be
clear, we are not "AI skeptics" by any means. We fully believe that over the next few
decades, AI will continue to make breakthrough advances, and AI systems will surpass
current human performance in many creative and technical ﬁelds, including, but not
limited to, software engineering, hacking, marketing, visual design, (at least some
components of) scientiﬁc discovery, and more. We are also not "techno-optimists." The
world already faces risks, and even existential ones, from the actions of humans.
People who have had control over nuclear weapons over the course of history include
Joseph Stalin, Kim Jong-un, Vladimir Putin, and many others whose moral judgment is
suspect, to say the least. Nuclear weapons are not the only way humans can and have
caused suﬀering on a mass scale; whether it is biological, chemical, or even so-called
"conventional" weapons, climate change, exploitation of resources and people, or
others, humans have a long history of pain and destruction. Like any new technology,
AI will be (and in fact already has been) used by humans for warfare, manipulations,
and other illicit goals. These risks are real and should be studied, but are not the focus
of this essay.
Our argument: an executive summary.
The loss of control scenario is typically described as a "battle" between AIs and
humans, in which AIs would eventually win due to their superior abilities. However,
unlike in Chess games, humans can and will use all the tools at their disposal, including
many tools (e.g., code-completion engines, optimizers for protein folding, etc..) that are
currently classiﬁed as "Artiﬁcial Intelligence". So to understand the balance of power,

we need to distinguish between systems or agents that have only short-term goals,
versus systems that plan their own long-term strategies. 
The distinction above applies not just to artiﬁcial systems but also to human
occupations as well. As an example, software developers, architects, engineers, or
artists have short-term goals, in the sense that they provide some particular product
(piece of software, design for a bridge, artwork, scientiﬁc paper) that can stand and be
evaluated on its own merits. In contrast, leaders of companies and countries set long-
term goals in the sense that they need to come up with a strategy that will yield
beneﬁts in the long run and cannot be assessed with conﬁdence until it is
implemented.[2] 
We already have at least partial "short-term AI", even if not at the level of replacing
e.g., human software engineers. The existence of successful "long-term AI" that can
come up with strategies which are enacted over a scale of, say, years is still an open
question, but for the sake of this essay we accept that assumption.
We believe that when evaluating the loss-of-control scenario, the relevant competition
is not between humans and AI systems, but rather between humans aided with short-
term AI systems and long-term AI systems (themselves possibly aided with short-term
components). One thought experiment we have in mind is a competition between two
ﬁrms: one with a human CEO, but with AI engineers and advisors, and the other a fully
AI ﬁrm.
While it might seem "obvious" that eventually AI would be far superior to humans in all
endeavors, including being a CEO, we argue that this is not so obviously the case. We
agree that future AIs could possess superior information processing and cognitive skills
- a.k.a. "intelligence" - compared to humans. But the evidence so far suggests
the advantages of these skills would be much more signiﬁcant in some ﬁelds
than in others. We believe that this is uncontroversial - for example, it's not far-
fetched to claim that AI would make much better chess players than kindergarten
teachers. Speciﬁcally, there are "diminishing returns" for superior information-
processing capabilities in the context of setting longer-term goals or strategies.
The long time horizon and the relevance of interactions among high numbers of agents
(who are themselves often diﬃcult to predict) make real-life large-scale
systems "chaotic" in the sense that even with superior analytic abilities, they are still
unpredictable (see Figure 1).
As a consequence, we believe the main ﬁelds where AI systems will yield
advantages will be in short-term domains. An AI engineer will be much more
useful than an AI CEO (see also Table 2). We do not claim that it would be impossible to
build an AI system that can conceive and execute long-term plans; only that this would
not be where AI would have a "competitive advantage". Short-term goals that can be
evaluated and graded also mesh much better with the current paradigm of training AI
systems on vast amounts of data.
We believe it will be possible to construct very useful AIs with only short-term
goals, and in fact that the vast majority of AI's power will come from such short-term
systems. Even if a long-term AI system is built, it will likely not have a signiﬁcant
advantage over humans assisted with short-term AIs. There can be many risks
even from short-term AI systems, but such machines cannot by design have any long-
term goals, including the goal of taking over the world and killing all humans.[3]
Perspective. Our analysis also has a lesson for AI safety research. Traditionally,
approaches to mitigate the behavior of bad actors include

Prevention: We prevent break-ins by putting locks on our doors, we prevent
hacks by securing our systems, etc... 
Deterrence: Another way we prevent bad actions is by ensuring that the
negative consequences for these actions will outweigh beneﬁts. This is one basis
for the penal system, as well as the "mutually assured destruction" paradigm that
has kept Russia and US from a nuclear war.
Alignment: We try to educate children and adults and socialize them to our
values, so they are not motivated to pursue the actions we consider as bad.
Much of AI safety research (wrt to the "loss of control" scenario) has been focused on
the third approach, with the expectation that these systems may be so powerful that
prevention and deterrence will be impossible. However, it is unclear to us that this will
be the case. For example, it may well be that humans, aided by short-term AI systems,
could vastly expand the scope of formally veriﬁed secure systems, and so prevent
hacking attacks against sensitive resources. A huge advantage of research on
prevention is that it is highly relevant not just to protect against hypothetical future
bad AI actors, but also against current malicious humans. Such research might
greatly beneﬁt from advances in AI code-completion engines and other tools, hence
belying the notion that there is a "zero-sum game" between "AI safety" and "AI
capabilities" research. 
Furthermore, one advantage of studying AI systems, as opposed to other organisms, is
that we can try to extract useful modules and representations for them. (Indeed, this is
already done in "transfer learning.") Hence, it may be possible to extract useful and
beneﬁcial "short-term AI" even from long-term systems. Such restricted systems would
still give most of the utility, but with less risk. Once again, increasing the capabilities of
short-term AI systems will empower humans that are assisted by such systems.
 

Figure 1: Cartoon of the feasibility of predicting future events and the level of ability
(i.e., cognitive skill / compute / data) required to do so (approximately) optimally. As
the horizon grows, events have more inherent uncertainty and also require more
skills/data to predict. However, many realistic systems are chaotic and become
unpredictable at some ﬁnite horizon.[4]  At that point, even sophisticated agents cannot
predict better than baseline heuristics, which require only a bounded level of skill.
 
Profession
Cognitive Score (standard
deviations)
Annual
Earnings 
Mayors
6.2 ( ≈ +0.6σ )
679K SEK
Parliamentarians
6.4 ( ≈ +0.7σ )
802K SEK
CEOs (10-24
employees)
5.8 ( ≈ +0.4σ )
675K SEK
CEOs (25-249
employees)
6.2 ( ≈ +0.6σ )
1,046K SEK
CEOs (≥ 250
employees)
6.7 ( ≈ +0.85σ )
1,926K SEK
Medical Doctors
7.4 ( ≈ +1.2σ )
640K SEK

Lawyers and Judges
6.8 ( ≈ +0.9σ )
568K SEK
Economists
7 ( ≈ +1σ )
530K SEK
Political Scientists
6.8 ( ≈ +0.9σ )
513 SEK
Table 2: Cognitive scores for Swedish men in various "elite" occupations, based on
Swedish army entrance examinations, taken from Dal Bó et al (Table II). Emphases
ours: bold text corresponds to jobs that (in our view) require longer horizon decision-
making across time or number of people. Note that despite being apparently less
cognitively demanding, the "bold" professions are higher paying.
 
A digression: what is intelligence
Merriam-Webster deﬁnes intelligence as "the skilled use of reason", "the ability to learn
or understand or to deal with new or trying situations", or "to apply knowledge to
manipulate one's environment or to think abstractly." Intelligence is similar
to computation, in the sense that its main components are the ability to take in
observations (aka "inputs") and use reasoning (aka "algorithms") to decide on actions
(aka "outputs"). In fact, in the currently dominant paradigm of AI, performance is
primarily determined by the amount of computation performed during learning, and AI
systems consist of enormous homogeneous circuits executing a series of simple
operations on (a large quantity of) inputs and learned knowledge. Bostrom (Chapter 3)
deﬁnes three forms of "superintelligence": "speed superintelligence", "collective
superintelligence" and "quality superintelligence". In the language of computing,
speed super-intelligence corresponds to clock speed of processors, while collective
super-intelligence corresponds to massive parallelism. "Quality superintelligence" is
not well deﬁned, but is presumably some type of emergent phenomenon from passing
some thresholds of speed and parallelism.
A fundamental phenomenon in computing is universality: there are many restricted
computational models (ﬁnite state automata, context-free grammars, simply-typed
lambda calculus), but once a computational model passes a certain threshold or phase
transition, it becomes universal (a.k.a. "Turing complete"), and all universal models are
equivalent to one another in computational power. For example, in a cellular automata,
even though each cell is very restricted (can only store a constant amount of memory
and process a ﬁnite rule based only on the state of its immediate neighbors), given
enough cells we can simulate any arbitrarily complex machine.[5]  Once a system
passes the universality transition, it is not bottlenecked any more by the complexity of
an individual unit, but rather by the resources in the system as a whole.
In the animal kingdom, we seem to have undergone a similar phase transition,
whereby humans are qualitatively more intelligent than any other animal or creature. It
also seems to be the case that with the invention of language, the printing press, and
the Internet, we (like cellular automata) are able to combine large numbers of humans
to achieve feats of collective intelligence that are beyond any one individual. In
particular, the fruits of the scientiﬁc revolution of the 1500-1600s increased the scale
of GDP by 10,000-fold (to the extent such comparisons are meaningful) and the

distance we can measure in space a trillion-fold, all with the same brains used by our
hunter-gatherer ancestors (or maybe somewhat smaller ones). 
Arguably, the fact humans are far better than chimpanzees at culturally transmitting
knowledge is more signiﬁcant than the gap in intelligence between individuals of the
two species. Ever since the development of language, the intelligence of an individual
human has not been a bottleneck for the achievements of humanity. The brilliance of
individuals like Newton may have been crucial for speeding up the Scientiﬁc
Revolution, but there have been brilliant individuals for millennia. The crucial diﬀerence
between Newton and Archimedes is not that Newton was smarter, but rather that he
lived at a later time and thus was able to stand on the shoulders of more giants. As
another example, a collection of humans, aided by Internet-connected computers, can
do much better at pretty much any intelligence feat (including but not limited to IQ
exams) than any single human. 
 
Figure 3: Measures of human progress both in terms of GDP and the scale of objects
we can measure. Taken from this blog post, with the ﬁrst ﬁgure from Our World in Data,
and data for second ﬁgure from Terence Tao's cosmic ladder presentation.
The "loss of control" scenario posits a second phase transition, whereby once AI
systems become more powerful, they would not merely enable humans to achieve
more objectives quicker but would themselves become as qualitatively superior to
humans as humans are to other animals. We are suggesting an alternative future
scenario, in which while AI would provide powerful new capabilities to human society
that can (and unfortunately likely will) be used for ill as well as good, the AI systems
themselves would not be the inevitable leaders of this society.
Indeed, our societies and ﬁrms do not currently select our leaders to be the top
individuals in intellectual capacity. The evidence is very limited that "natural talent for
leadership" (to the extent it exists) is as measurable and transferable as talent for
chess, math, or athletics. There are many examples of leaders who have been
extremely successful in one setting but failed in another which seems rather similar.[6] 
Whether or not an AI system should be considered an "individual" is a matter for
debate, but regardless, it is not at all clear that such individuals would be the leaders of
the society, rather than being employed in domains such as software development and
scientiﬁc discovery, where their superior information-processing capabilities would
provide the most competitive advantage. Bostrom (Table 8 in Chapter 6) lists several
potential "cognitive superpowers" that an AI system might develop. One category

is "hacking", "technology research", and "economic productivity". These are skills that
correspond to jobs that are not in the domain of CEOs or leaders, but rather engineers,
middle managers, scientists, etc. AI systems may well be able to assist or even replace
such individuals, but this does not mean such systems will be the leaders of companies
or countries.
Another task Bostrom considers is "intelligence ampliﬁcation" which is the ability to
improve AI systems. Again, it is quite possible that AI systems would help in improving
other or the same AI systems, but this on its own does not imply that they would
become inﬁnitely powerful. Speciﬁcally, if indeed stronger AI would arrive through
"scaling" of massive computational resources, then there would be some hard limits on
the ability to improve AI's power solely through software updates. It is not at all clear
that in terms of energy eﬃciency, AI systems would be much better (if at all) than
humans. If the gains from scaling are far more important than gains from improved
algorithms/architectures, then intelligence ampliﬁcation might be primarily a function
of resource acquisition rather than algorithmic research.
A third task listed is  "social manipulation." Here we must admit we are skeptical.
Anyone who has ever tried to convince a dog to part with a bone or a child with a toy
could attest to the diminishing returns that an intelligence advantage has in such a
situation. 
Finally, Boston lists the cognitive superpower of "strategizing", which is the ability to
make long-term plans to achieve distant goals. This is the point we focus on in this
essay. In short, our belief is that the chaotic nature of the real world implies diminishing
returns to "three-dimensional chess" strategies that are beyond the comprehension of
mere humans. Hence we do not believe that this would be a domain where AI systems
have a strong competitive advantage.
A thought experiment: "The AI CEO vs. the AI
advisor"
Before we delve into the technical(-ish) analysis, let us consider a thought experiment.
At its heart, our argument is that the power of AI systems, present and future, will not
come from the ability to make long-term strategic plans ("three-dimensional chess")
but rather from the ability to produce pieces of work that can be evaluated on their
own terms. In short, we believe that even if a long-term malicious AI system is
constructed, it will not have an insurmountable advantage over humans that are
assisted with short-term AIs. To examine this, let us imagine two possible scenarios for
how future AI could assist humans in making strategic decisions, such as running a
company:
 
In the "AI Advisor" model, leaders could use AI to come up with simulations of
the impact of decisions and possibly make some suggestions. However, humans
would ultimately make the decision and evaluate their results. Key for this is that
an AI would be able not just to produce a recommendation for a decision but
explain how this decision would lead to improvement in some interpretable
metric (e.g., revenue, market share, etc..). For example, a decision might be

"let's sell this product at a loss so we can increase our market share."
 
In the "AI CEO" model, AIs could use their superior powers to choose an optimal
long-term strategy as opposed to an individual decision. The strategy would not
be "greedy", in the sense of a sequence of steps each making progress on
measurable goals, and it would not have any compact analysis of why it is good.
Also, the only way to accrue the beneﬁts of the strategy would be to continue
pursuing it in the long term. Hence users would have to trust the AI and follow its
recommendations blindly. For example, think of the case in Chess where an AI
ﬁgures out that the best move is to sacriﬁce the queen because for any one of
the possible opponent's moves, there is a countermove, and so on and so forth.
The only explanation for why this strategy is a good one may consist of an
exponentially big game tree up to a certain depth.
 
Our sense is that there is strong evidence that AI would be incredibly useful for making
low-level decisions (i.e., optimizing objectives under constraints) once the high-level
strategy was set. Indeed, by far the most exciting advances for deep learning have not
been through reinforcement learning, but rather through techniques such as
supervised and unsupervised learning. (With the major exception being games like
Chess and Go, though even there, given the success of non-RL engines such as
Stockﬁsh versions 12 and later, it is not clear RL is needed.) There is less evidence that
"AI advisors" would be useful for setting high-level strategies, but it is certainly
plausible. In particular, the power of prompt-based generative models suggests that AI
could be useful for generating realistic simulations that can help better convey the
impact of various decisions and events. So, while "AI engineers" might be more useful
than "AI advisors", the latter might well have their role as well. 
In contrast, we believe that there is little to no evidence for the beneﬁts of "three-
dimensional chess" strategies of the type required for the "AI CEO" scenario. The real
world (unlike the game of chess or even poker), involves a signiﬁcant amount of
unpredictability and chaos, which makes highly elaborate strategies depending on
complex branching trees of moves and counter-moves far less useful. We also ﬁnd it
unlikely that savvy corporate boards would place blind trust in an AI CEO given that (as
mentioned above) evaluation of even human CEOs tends to be controversial. 
There is an alternative viewpoint, which is that an AI CEO would basically be equivalent
to a human CEO but with superhuman "intuition" or "gut feeling" that they cannot
explain but somehow leads to decisions that yield enormous beneﬁts in the long term.
While this viewpoint cannot be ruled out, there is no evidence in current deep learning
successes to support it. Moreover, often great CEO's "gut feelings" are less about
particular decisions, but more about the relative importance of particular metrics (e.g.,
prioritizing market share or user experience over short-term proﬁts). 
In any case, even if one does not agree with our judgment of the relative likelihoods of
the above scenarios, we hope that this essay will help sharpen the questions that need
to be studied, as well as what lessons can we draw about them from the progress so far
of AI systems.
Technical Analysis

1. Key hypotheses behind the "Loss of
Control" Scenario
For the sake of the discussion below, let's assume that at some future time there exists
an artiﬁcial intelligence system that in a uniﬁed way achieves performance far superior
to that achieved by all humans today across many ﬁelds. This is a necessary
assumption for the "loss of control" scenario and an assumption we accept in this
essay. For the sake of simplicity, below we refer to such AI systems as "powerful".
We will also assume that powerful AI will be constructed following the general
paradigm that has been so successful in the last decade of machine learning.
Speciﬁcally, the system will be obtained by going through a large amount of data and
computational steps to ﬁnd some instantiation (a.k.a. "parameters" or "weights") of it
that optimizes some chosen objective. Depending on the choice of the objective, this
paradigm includes supervised learning ("classify this image"), unsupervised learning
("predict the next token"), reinforcement learning ("win the game"), and more.
For the loss of control scenario to occur, the following two hypotheses must be true:
 
Loss-of-Control Hypothesis 1: There will exist a powerful AI that has long-term
goals.
For an AI to have misaligned long-term goals, it needs to have some long-term goals in
the ﬁrst place. There is a question of how to deﬁne the "goals" of an AI system or even
a human for that matter. In this essay, we say that an agent has a goal X if, looking
retrospectively at the history of the agent's actions, the most parsimonious explanation
for its actions was that it was attempting to achieve X, subject to other constraints or
objectives. For example, while chess experts often ﬁnd it hard to understand why an
engine such as AlphaZero makes a speciﬁc move, by the end of the game, they often
understand the reasoning retrospectively and the sub-goals it was pursuing.
In our parlance, a goal is "long-term" if it has a similar horizon to goals such as "take
over the world and kill all the humans" —requiring planning over large scales of time,
complexity, and number of agents involved.[7]  
In contrast, we consider goals such as "win a chess game", "come up with a plan for a
bridge that minimizes cost and can carry X traﬃc", or "write a piece of software that
meets the requirements Y", as short-term goals.  As another example, "come up with a
mix of stocks to invest today that will maximize return next week" is a short-term goal,
while "come up with a strategy for our company that will maximize our market cap
over the next decade" or "come up with a strategy for our country that will maximize
our GDP for the next generation" would be long-term goals. The distinction between
"short-term goals AI" and "long-term goals AI" is somewhat similar to the distinction
between "Tool AI" and "Agent AI" (see here). However, what we call "short-term AI"
encompasses much more than "Tool AI", and absolutely includes systems that can take
actions such as driving cars, executing trading actions, and so on and so forth.
We claim that for the "loss of control" scenario to materialize, we need not only
Hypothesis 1 but also the following stronger hypothesis:

Loss-of-Control Hypothesis 2: In several key domains, only AIs with long-term
goals will be powerful.
By this, we mean that AIs with long-term goals would completely dominate other AIs, in
that they would be much more useful for any user (or for furthering their own goals). In
particular,  a country, company or organization that restricts itself to only using AIs
with short term goals would be at a severe competitive disadvantage compared to one
that uses AIs with long-term goals.
Why is Hypothesis 2 necessary for the "loss of control" scenario? The reason is that this
scenario requires the "misaligned long-term powerful AI" to be not merely more
powerful than humanity as it exists today, but more powerful than humanity in the
future. Future humans will have at their disposal the assistance of short-term AIs.
 
2. Understanding the validity of the
hypotheses
We now make the following claims, which we believe cast signiﬁcant doubt on
Hypothesis 2.
Claim 1: There are diminishing returns to information-
processing skills with longer horizons.
Consider the task of predicting the consequences of a particular action in the future. In
any suﬃciently complex real-life scenario, the further away we attempt to predict, the
more there is inherent uncertainty. For example, we can use advanced methods to
predict the weather over a short time frame, but the further away the prediction, the
more the system "regresses to the mean", and the less advantage that highly complex
models have over simpler ones (see Figure 4). As in meteorology, this story seems to
play out similarly in macroeconomic forecasting.  In general, we expect prediction
success to behave like Figure 1 below—the error increases with the horizon until it
plateaus to a baseline level of some simple heuristic(s). Hence while initially highly
sophisticated models can beat simpler ones by a wide margin, this advantage
eventually diminishes with the time horizon.
Tetlock's ﬁrst commandment to potential superforecasters is to triage: "Don't waste
time either on "clocklike" questions (where simple rules of thumb can get you close to
the right answer) or on impenetrable "cloud-like" questions (where even fancy
statistical models can't beat the dart-throwing chimp). Concentrate on questions in the
Goldilocks zone of diﬃculty, where eﬀort pays oﬀ the most."  Another way to say it is
that outside of the Goldilocks zone, more eﬀort or cognitive power does not give much
returns. 
 

Figure 4:  Left: Historical weather prediction accuracy data taken from a Quora answer
of Mikko Strahlendorﬀ. With technological advances, accuracy has improved
signiﬁcantly, but prediction accuracy sharply decays with time. Right: Figure on relative
applicability of diﬀerent methods from Brent Shaw. Computationally intensive
numerical prediction applies in a "goldilocks zone" of days to weeks.
 
In a variety of human endeavors, it seems that the cognitive skills needed to make
decisions display a similar phenomenon. Occupations involving making decisions on
the mid-range horizon, such as engineering, law, and medicine, require higher
cognitive skills than those requiring long-term decisions such as CEOs or Politicians
(see Table 3).
One argument people make is that intelligence is not just about IQ or "booksmarts".
We do not dispute this. However, we do believe that the key potential advantage of AI
systems over their human counterparts would be the ability to quickly process large
amounts of information, which in humans is approximated by scores such as IQ. If that
skill were key to successful leadership of companies or countries, then we would expect
CEOs and leaders to come from the top 0.1% (≈ +3σ)  of the distribution of such
scores. The data does not bear this out.[8] 
 
Claim 2: It may be possible to extract powerful short-term
modules from long-term systems.
For Hypothesis 2 to be true, it should not be possible to take a powerful AI system with
long-term goals, and extract from it modules that would be just as powerful in the key
domains, but would have short-term goals. However, a nascent body of work identiﬁes
and extracts useful representations and sub-modules in deep neural networks. See, for
example, this recent investigation of AlphaZero. We remark that some components of
AlphaZero also inspired advances to the Stockﬁsh Chess Engine (which is not trained
using RL and involves a lot of hand-coded features), and whose latest version does in
fact beat RL trained methods a-la AlphaZero.
A related issue is that a consistent theme of theoretical computer science is that
veriﬁcation is easier than solving or proving. Hence even a complex system could
explain its reasoning to a simple veriﬁer, even if that reasoning required a signiﬁcant

eﬀort to discover. There are similar examples in human aﬀairs: e.g., even though the
discovery of quantum mechanics took thousands of years and multiple scientiﬁc
revolutions, we can still teach it to undergraduates today whose brains are no better
than those of the ancient Greeks. 
2.1 The impact of the deep learning paradigm
on Hypothesis 2
The following claims have to do with the way we believe advanced AI systems will be
constructed. We believe it is fair to assume that the paradigm of using massive data
and computation to create such systems, by optimizing with respect to a certain
objective, will continue to be used. Indeed, it is the success of this paradigm that has
caused the rise in concerns about AI in the ﬁrst place.  In particular, we want to make a
clear distinction between the training objective, which the system is designed to
optimize, versus the goals that the system appears to follow during its deployment.
 
Claim 3: There may be fundamental "scaling laws"
governing the amount of performance AI systems can
achieve as a function of the data and computational
resources.
One of the original worries in the AI risk literature is the "singularity" scenario, by
which an AI system continuously improves its own performance without limit. However,
this assumes that a system can improve itself by rewriting its code, without requiring
additional hardware resources.  If there are hard limits to what can be achieved with a
certain level of resources, then such self-improvements will also hit diminishing returns.
There has been signiﬁcant evidence for the "scaling laws" hypothesis in recent years.

Figure 5: Scaling laws as computed by Hoﬀman et al ("Chinchilla"), see Figure A4
there. While the scaling laws are shaped diﬀerently from those of Kaplan et al, the
qualitative point we make remains the same.
 
Claim 4: When training with reinforcement learning, the
gradient signal may decrease exponentially with the length
of the horizon.
Consider training a system that chooses a sequence of actions, and only gets a reward
after H steps (where H is known as the "horizon"). If at any step there is some

probability of an action leading to a "dead end" then the chances of getting a
meaningful signal decrease exponentially with H. This is a fundamental obstacle to
reinforcement learning and its applicability in open-ended situations with a very large
space of actions, and a non-trivial cost for any interaction. In particular, one reason
AlphaZero was successful was that in games such as chess, the space of legal moves is
very constrained, and in the artiﬁcial context of a game it is possible to "reset" to a
particular position: that is, one can try out diﬀerent actions and see what their
consequences are, and then go back to the same position. This is not possible when
interacting in the real world.
 As a corollary of Claim 4, we claim the following:
Claim 5: There will be powerful AI systems that are trained
with short-term objective functions.
By this, we mean models that are trained on a reward/loss function that only depends
on a relatively short span of actions/outputs. A canonical example of this is next-token
prediction. That is, even if the eventual deployment of the model will involve it making
actions and decisions over a long time horizon, its training will involve optimizing short-
term rewards.
 One might think that the model's training does not matter as much, since once it is
deployed in the real world, much of what it will learn will be "on the job". However, this
is not at all clear. Suppose the average worker reads/hears about 10 pages per day,
which is roughly 5K tokens, leading to roughly 2M tokens per year. In contrast, future
AIs will likely be trained on a trillion tokens or so, corresponding to the amount a
worker will see in 5 million years! This means that while "ﬁne-tuning" or "in context"
learning can and will occur, many of the fundamental capabilities of the systems will be
ﬁxed at the time of training (as appears to be the case for pre-trained language models
that are ﬁne-tuned with human feedback).
 
Claim 6: For a long-term goal to necessarily emerge from a
system trained with a short-term objective, it must be
correlated or causally related to that objective.
If we assume that powerful AIs will be trained with short-term objectives, then
Hypothesis 2 requires that (in several key domains) every such system will develop
long-term goals. In fact, for the loss-of-control scenario to hold, every such system
should develop more-or-less the same sort of goal (e.g., "take over the world").
While it is certainly possible for systems that evolve from simple rules to develop
complex behavior (e.g., cellular automata), for a long-term goal to consistently emerge
from mere short-term training, there should be some causal relation (or at least
persistent correlation)  between the long-term goal and the short-term training
objective. This is because an AI system can be modeled as a maximizer of the objective
on which it was trained. Thus for such a system to always pursue a particular long-term
goal, that goal should be correlated with maximizing the training objective.
We illustrate this with an example. Consider an AI software developer which is trained
to receive a speciﬁcation of a software task (say, given by some unit tests) and then

come up with a module implementing it, obtaining a reward if the module passes the
tests. Now suppose that in actual deployment, the system is also writing the tests that
would be used to check its future outputs. We might worry that the system would
develop a "long-term" goal to maximize total reward by writing one faulty test, taking
the "hit" on it, and receiving a low reward, but then getting high rewards on future
tasks. However, that worry would be unfounded, since the AI software developer
system is trained to maximize the reward for each task separately, as opposed to
maximizing the sum of rewards over time over adaptively chosen inputs of its own
making.
Indeed, this situation can already happen today. Next-token prediction models such as
GPT-3 are trained on the reward of the perplexity over a single token, but when they
are deployed, we typically generate a long sequence of tokens. Now consider a model
that simply outputs an endless repetition of the word "blah". The ﬁrst few repetitions
would get very low rewards, since they are completely unexpected, but once n is large
enough (e.g. 10 or so), if you've already seen n "blah"s then the probability that the
n+1 st word is also "blah" is very high.  So if the model were to be maximizing total
reward, it may well be worth "taking the hit" by outputting a few blahs. The key point is
that GPT-3 does not do that. Since it is trained on predicting the next token for human-
generated (as opposed to the text generated by itself), it will optimize for this short-
term objective rather than the long-term one.
We believe the example above generalizes to many other cases. An AI system trained
in the current paradigm is, by default, a maximizer of the objective it was trained on,
rather than an autonomous agent that pursues goals of its own design. The shorter the
horizon and more well-deﬁned the objective is, the less likely that optimizing it will lead
to systems that appear to take elaborate plans to pursue far-reaching (good or bad)
long-term goals. 
Summary
Given the above, we believe that while AI will continue to yield breakthroughs in many
areas of human endeavor, we will not see a unitary nigh-omnipotent AI system that
acts autonomously to pursue long-term goals. Concretely, even if a successful long-
term AI system could be constructed, we believe that this is not a domain where AI will
have a signiﬁcant "competitive advantage" over humans.
Rather, based on what we know, it is likely that AI systems will have a "sweet spot" of
a not-too-long horizon in which they can provide signiﬁcant beneﬁts. For strategic and
long-term decisions that are far beyond this sweet spot, the superior information
processing skills of AIs will give diminishing returns. (Although AIs will likely supply
valuable input and analysis to the decision makers.).  An AI engineer may well
dominate a human engineer (or at least one that is not aided by AI tools), but an AI
CEO's advantage will be much more muted, if any, over its human counterpart. Like
our world, such a world will still involve much conﬂict and competition, with all sides
aided by advanced technology, but without one system that dominates all others.
If our analysis holds, then it also suggests diﬀerent approaches to mitigating AI risk
than have been considered in the "AI safety" community. Currently, the prevailing
wisdom in that community is that AI systems with long-term goals are a given, and
hence the approach to mitigate their risk is to "align" these goals with human values.

However, perhaps more evidence should be placed on building just-as-powerful AI
systems that are restricted to short time horizons. Such systems could also be used to
monitor and control other AIs, whether autonomous or directed by humans. This
includes monitoring and hardening systems against hacking, detecting misinformation,
and more. Regardless, we believe that more research needs to be done on
understanding the internal representations of deep learning systems, and what
features and strategies emerge from the training process (so we are happy that the AI
safety community is putting increasing resources into "interpretability" research).
There is some evidence that the same internal representations emerge regardless of
the choices made in training.
There are also some technical research directions that would aﬀect whether our
argument is correct. For instance, we are interested in seeing work on the impacts of
noise and unpredictability on the performance of reinforcement learning algorithms; in
particular, on the relative performance of models of varying complexity
(i.e. scaling laws for RL).
Acknowledgments: Thanks to Yafah Edelman for comments on an earlier version of
this essay.
 
1. ^
During the 90s-2000s, human-engine teams were able to consistently beat
engines in "advanced chess" tournaments, but no major advanced chess
tournament seems to have taken place since the release of AlphaZero and the
resulting jump in engine strength, presumably because the human half of each
team would be superﬂuous.
2. ^
The success of a bridge does hinge on its long-term stability, but stability can be
tested before the bridge is built, and coming up with measures for load-bearing
and other desiderata is standard practice in the engineering profession. An AI
trained using such a short-term evaluation suite as its reward function may still
"overoptimize" against the metric, a la Goodhart's Law, but this can likely be
addressed with regularization techniques.
3. ^
It may be the case that, for subtle reasons, if we try to train an AI with only short-
term goals—e.g. by training in a series of short episodes—we could accidentally
end up with an AI that has long-term goals. See Claim 6 below. But avoiding this
pitfall seems like an easier problem than "aligning" the goals of an AI that is
explicitly meant to care about the long-term.
4. ^
We don't mean that they satisfy all the formal requirements to be deﬁned as a
chaotic system; though sensitivity to initial conditions is crucial.
5. ^

For a nice illustration, see Sam Trajtenberg's construction of Minecraft in
Minecraft, or this construction of Life in Life.
6. ^
Steve Jobs at Apple vs NeXT is one such example; success and failure can
themselves be diﬃcult to distinguish even with the beneﬁt of hindsight, as in the
case of Jack Welch.
7. ^
For example, such planning might require setting up many companies to earn
large amounts of funds, conducting successful political campaigns in several
countries, constructing laboratories without being detected, etc. Some such
"take-over scenarios" are listed by Bostrom, as well as Yudkowski and Urban.
8. ^
It is hypothetically possible that companies would be better oﬀ en masse if they
hired smarter CEOs than they currently do, but given the high compensation
CEOs receive this doesn't seem like a particularly plausible equilibrium.

LW Beta Feature: Side-Comments
LessWrong now has side-comments. This feature is in beta; you can turn it on for yourself on
individual posts using the triple-dot menu below the post title, or enable it for all posts by
going to your user settings and checking the "Opt into experimental features" checkbox in
the Site Customization section.
Side-coments on LessWrong are conceptually similar to the side-comments you may be
familiar with from Google Docs and other places, with one key diﬀerence: side-comments are
placed automatically by lining up blockquotes. As a result, many historical posts already
have side-comments on them! Side-comments are also still displayed in the comments
section below the post as usual.
Side-comments take the form of a comment icon in the right margin, which expands when
you mouse over it to show a comment. Click on the icon to pin it open. It looks like this:

To create a side-comment on a post, just write a normal comment which quotes an excerpt
from the post, in blockquote format. (To create a blockquote, type > ). You can also highlight
some text and click the button that appears to start a comment  pre-seeded with a quote
containing that text.
By default, side-comments are ﬁltered to comments with 10+ karma (plus comments by the
post author). They aren't available on mobile and require that your screen is wide enough to
have some space in the right margin.
If a comment contains more than one blockquote that could be used to place it as a side-
comment, it will be placed based on the ﬁrst one. In order to appear as a side-comment,
quotes should be an exact match, including formatting; there is some provision for "..."
ellipses, but it's fairly limited.
Please give feedback on how this aﬀects the reading experience! If the feedback is positive,
this will leave beta and be enabled (for comments above a karma threshold) for everyone.

What I Learned Running Reﬁne
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
You have one job: Solving problems. You have multiple tools. Maybe you use code
as a tool to solve some problems. Maybe you use design for others. Maybe you
use good communication and negotiation skills.
Mike Acton, How much time should I spend coding versus managing?
If you seek tranquility, do less. Or, more accurately, do what's essential.
Marcus Aurelius, Meditations, Book 4.24
 
This post is part of the work done at Conjecture .
Reﬁne, the alignment research incubator we are running at Conjecture, ﬁnished its
ﬁrst cohort a few weeks ago. So now is a good time to take stock, share what we've
learned, and discuss its future.
Let's get this out of the way ﬁrst: we are not planning any new cohort in the
foreseeable future. There are multiple reasons for this, which I'll expand on in this
post. But to summarize:
Running Reﬁne in a way that would fully aim at the stated target would require
more eﬀort
SERI MATS is doing a great job of scaling conceptual alignment research, and
seem open to integrate some of the ideas behind Reﬁne
The work we're doing in Conjecture's epistemology team is far more
fundamental and neglected than ﬁeld-building according to me, at least in the
current climate.
Now for the details.
The Target
The key idea behind Reﬁne was to create more conceptual alignment researchers with
their own radically diﬀerent agendas, rather than new researchers following
established approaches. To create more researchers
like John, Paul, Vanessa, Evan, Steve, and the others.
How we operationalized this goal was to look for relentlessly resourceful thinkers with
unorthodox shapes of minds for the alignment community.
The Result

Now that the ﬁrst cohort is over, how well have we hit this target? Out of 5
participants
2 are pursuing their own research bets, though these are not radically diﬀerent
from established approaches
1 is still building theirs
1 has found a neglected ﬁeld-building opportunity
1 feels like they still need to upskill before working directly on alignment.
Based only on The Target above, this is 0/5. 
Of course that doesn't mean the program didn't have positive outcomes and
externalities! On the contrary, I'm really happy how a lot of things turned out, and I've
heard from all participants that they got a lot out of Reﬁne. Non-negligeable
accomplishments include:
Feedback from multiple alignment researchers that Reﬁne participants had a
deep model of the alignment problem at the end of the program.[1]
Reﬁne participants all around improved their productivity, some on writing and
others on iterating on ideas.
All Reﬁne participants met and talked with many alignment researchers and
newcomers like them, considerably expanding their network and understanding
of the alignment space.
Participants posted around 25 posts in total on the Alignment Forum, some of
which I ﬁnd exciting.
I got a crash course in management that helped me upskill quickly.
We had a lot of great moments and support from each other.
In our leaving survey, all participants said they would highly recommend the
program, and that it was more counterfactually useful than what they would
have done instead by default.
I expect most, if not all, participants to make relevant contributions to the ﬁeld.
None of these are irrelevant. Yet if we focus on the original metric, the pilot of Reﬁne
failed. Having reﬂected on this, I have some thoughts on how we could have better
aimed at this target (whether it is the correct target is a question for a later section).
It all amounts to lack of optimization.
Failing to Optimize
The ﬁrst place where we failed to optimize for wildly diﬀerent research agendas was in
the selection population. Given where we advertised (various EA and rationalists
websites, Slacks, and Discords), we drew a crowd homogeneous along many
dimensions. There was no way we were going to end up with a linguist or a sociologist
for example. That would have required more targeted outreach eﬀort.
This failure mode is shared by all training programs I know about: even PIBBSS, which
successfully brought together a more diverse cohort, had trouble with the ﬁelds most
diﬀerent from alignment, like the social sciences.
Our second lack of optimization came from the selection process itself. If you want to
create independent conceptual researchers that work on the problem right after your
program, you need to push really hard for the following traits:

Want to work on conceptual alignment ASAP
Can tolerate the emotional and material diﬃculties of independent research
Is able to generate their own ideas
Is able to make mistakes and update
Looking back, most of the participants in the ﬁrst cohort scored well along these lines,
but all of them have at least one of these traits where they need to improve.
Last but not least, the process within Reﬁne itself could have better focused on
guiding participants to build a gears-level model of alignment. What we ended up
doing was mostly discussing Unbounded Atomic Optimization and Epistemological
Vigilance, and providing feedback on ideas. Whereas I currently see more explicit
exercises (like building a treeory of change), an early focus on poking as many holes
as possible in models of alignment, and a sweeping tour of the state of the art, as
necessary ﬁrst steps to produce worthwhile conceptual alignment research quickly.
In the end, all participants of the ﬁrst cohort learned a deep model of the alignment
problem, but better program structure could have accelerated this. And with such a
deep gears-level model from the start, all the mentoring focused on pushing ideas
towards the most relevant form for alignment would have been vastly more eﬀective,
as there would have been signiﬁcantly less "translation eﬀort" from the mentor side. 
The Right Target?
Note that the above assumes that Reﬁne's original goal, creating more conceptual
alignment researchers with their own radically diﬀerent agendas, was the right one.
But is it really? Even if it is a good one, is it the most important one, or the most
crucial one to solving alignment?
I have updated toward no. Or rather, I have updated toward being suspicious of
targets that look as instrumental as this one.
For creating new research directions, and new researchers, sidelines the key diﬃculty
in solving alignment: ﬁnding what needs to be done concretely to solve alignment and
the best proﬁle for such endeavours. Instead of ﬁguring these hard questions, you
delegate them to the future, to the next generation.
On a problem with longer timelines, this might be the right move: let the compound
interest do the work. Even with short timelines, if I had no ideas and no plans for
addressing these hard questions, passing the buck might have been the best decision.
But I have an angle and a plan. Figuring out how to tackle alignment, why it is hard,
and how to deal with these diﬃculties is literally the task of my epistemology team at
Conjecture. In these conditions, me spending that much time on ﬁeld-building seems
like a bad bet: I'm doing a worst job than literally most ﬁeld-builders I know (only
really providing my own idiosyncratic ideas that can be shared anyway) while
neglecting an angle of attack on the problem that is completely neglected and
appears promising to me and Conjecture.
I'm excited to see SERI MATS and other programs step up for making new alignment
researchers, and will continue to encourage them and give them feedback. But my
personal arena is elsewhere.

1. ^
Note that some Reﬁne participants were already working in alignment. Also,
there was negative feedback too from alignment researchers, but given the base
negativity of the ﬁeld, positive comments are particularly strong sources of
evidence.

Speculation on Current Opportunities
for Unusually High Impact in Global
Health
Epistemic Status: armchair speculation from a non-expert.
Short version: I expect things to get pretty bad in the Sahel region over the next year
in particular. The area is an obvious target for global health interventions even in good
times, and impact is presumably higher in bad times. A simple baseline intervention:
ﬁll a backpack with antibiotics, ﬂy to the region, and travel around distributing the
antibiotics.
What's The "Sahel" Region?
The Sahel is a semi-arid region along the southern edge of the Sahara desert. Think
roughly Mali, Niger, Chad and Sudan.
Bad How?
Based on statistics on the Sahel, it's one of the few remaining regions on Earth where
the population is near Malthusian equilibrium. Fertility is high, contraception is rare;
about half the population is under age 16. Infant mortality is around 6-8%, and ~a
quarter of children are underweight. (Source: CIA World Factbook entries on Mali,
Niger, Chad and Sudan.)
Being near Malthusian equilibrium means that, when there's an economic downturn, a
substantial chunk of the population dies.
Die How?
Traditional wisdom says: war, famine, disease. In this case, I'd expect famine to be the
main instigator. Empty bellies then induce both violence and weak immune systems.
On priors, I'd expect infectious disease to be the main proximate killer.
The Next Year In Particular?
The global economy has been looking rough, between the war in Ukraine shocking oil
and food markets, and continuing post-Covid stagﬂation. Based on pulling a number
out of my ass without looking at any statistics, I'd guess deaths from violence,
starvation, and disease in the Sahel region will each be up an order of magnitude this
year/next year compared to a good year (e.g. the ﬁrst-quartile best year in the past
decade).
That said, the intervention we'll talk about is probably decently impactful even in a
good year.

So What's To Be Done?
Just oﬀ the top of my head, one obvious baseline plan is:
Fill a hiking backpack with antibiotics (buy them somewhere cheap!)
Fly to N'Djamena or take a ferry to Timbuktu
Obtain a motorbike or boat
Travel around giving away antibiotics until you run out
Repeat
Note that you could, of course, substitute something else for "antibiotics" - maybe
vitamins or antifungals or water puriﬁcation tablets or iron supplements or some mix
of those is higher marginal value.
There are some possibly-nonobvious considerations here. First, we can safely assume
that governments in the area are thoroughly corrupt at every level, and presumably
the same goes for non-government bureaucracies; trying to route through a local
bureaucratic machine is a recipe for failure. Thus, the importance of being physically
present and physically distributing things oneself. On the other hand, physical safety
is an issue, even more so if local food insecurity induces local violence or civil war.
(That said, lots of Westerners these days act like they'll be immediately assaulted the
moment they step into a "bad neighborhood" at night. Remember, folks, the vast
majority of the locals are friendly the vast majority of the time, especially if you're
going around obviously helping people. You don't need to be completely terriﬁed of
foreign territory. But, like, don't be completely naive about it either.)
Also, it is important to explain what antibiotics are for and how to use them, and there
will probably be language barriers. Literacy in these regions tends to be below 50%,
and presumably the rural regions which most need the antibiotics also have the
lowest literacy rates.
How Much Impact?
I'm not going to go all the way to estimating QALYs/$ here, but... according to this
source, the antibiotic imports of the entire country of Mali in 2020 amounted to $53k.
That's for a country of 18 million people and change. Now, I certainly wouldn't take
that statistic at face value, but I think we can safely conclude that Mali does not have
anywhere near the amount of antibiotics the population could use.
Even if each course of antibiotics bought and distributed has only a 0.1% chance of
saving someone's life, if you can get the antibiotics for $1/course someplace cheap
(and not spend too much money travelling around distributing them) that's still
$1000/life - pretty respectable impact/$. And that's a pretty conservative estimate -  I
would guess that you could get closer to $0.10/course with a little shopping around
and a large bulk buy, and I'd guess that each course saves more like 1/100 life rather
than 1/1000 (assuming the antibiotics aren't completely wasted - starving people are
known for eating things, which is another potential issue). So impact could easily be
higher by one or two orders of magnitude.
How Would You Measure Impact?

The simple ﬁrst-pass answer is to not measure impact, and just operate entirely on
priors. Not great, but hey, the lack of good feedback loops is often a major cause of
low-hanging fruit.
The next better answer would be to do another round a year later, revisit some of the
same places, and ask people what they did with the antibiotics.
Why Expect This Fruit To Be Unpicked?
Obvious factors here:
Risk tolerance, speciﬁcally risk of physical violence. I'd guess that most
individuals would be ﬁne with a moderate amount of physical risk, but for a large
organization it's a PR disaster when one person out of a hundred or a thousand
gets kidnapped once. So, I expect orgs to systematically underinvest in this sort
of thing (and/or partner with local governments, which have massive overhead
at best and usually massive corruption too).
Just directly physically doing things seems to be high-impact in general
Why Are You Writing This?
That last point is actually the main reason I'm writing this post: just directly physically
doing things seems to be high-impact in general. It is not a coincidence that most of
GiveWell's top charities directly distribute physical things. Just ignore the many
opportunities to "partner with <bureaucracy>", or network with Important People, or
whatever other social games are fashionable, and go directly solve (some part of) the
object-level problem.
It seems to me like a majority of newcomers to EA get lost in social reality, and lose
track of physical reality. That manifests, for instance, as a tendency to generate ideas
like "try to Gain Inﬂuence and use it for X", and to not generate ideas like "ﬁll a
backpack with antibiotics and travel around the Sahel distributing them". My current
best guess is that marginal focus on social inﬂuence, rather than thinking about
physical reality, is orders of magnitude less valuable across cause areas; this post is
meant to be an example. (And yes, this also applies to AI.)
... but hey, maybe I'm wrong about that. I'm a non-expert engaging in armchair
reasoning here, after all. So if you think the "backpack full of antibiotics" plan is a
dumb idea, feel free to tell me I'm wrong.

Meta AI announces Cicero: Human-
Level Diplomacy play (with dialogue)
This is a linkpost for https://www.science.org/doi/10.1126/science.ade9097
Abstract
Despite much progress in training AI systems to imitate human language, building
agents that use language to communicate intentionally with humans in interactive
environments remains a major challenge. We introduce Cicero, the ﬁrst AI agent to
achieve human-level performance in Diplomacy, a strategy game involving both
cooperation and competition that emphasizes natural language negotiation and
tactical coordination between seven players. Cicero integrates a language model with
planning and reinforcement learning algorithms by inferring players' beliefs and
intentions from its conversations and generating dialogue in pursuit of its plans.
Across 40 games of an anonymous online Diplomacy league, Cicero achieved more
than double the average score of the human players and ranked in the top 10% of
participants who played more than one game.
Meta Fundamental AI Research Diplomacy Team (FAIR)†, Anton Bakhtin, Noam Brown,
Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, et al. 2022. "Human-Level
Play in the Game of Diplomacy by Combining Language Models with Strategic
Reasoning." Science, November, eade9097. https://doi.org/10.1126/science.ade9097.

Utilitarianism Meets Egalitarianism
This post is mostly propaganda for the Nash Bargaining solution, but also sets up
some useful philosophical orientation. This post is also the ﬁrst post in my geometric
rationality sequence.
Utilitarianism
Let's pretend that you are a utilitarian. You want to satisfy everyone's goals, and so
you go behind the veil of ignorance. You forget who you are. Now, you could be
anybody. You now want to maximize expected expected utility. The outer (ﬁrst)
expectation is over your uncertainty about who you are. The inner (second)
expectation is over your uncertainty about the world, as well as any probabilities that
comes from you choosing to include randomness in your action.
There is a problem. Actually, there are two problems, but they disguise themselves as
one problem. The ﬁrst problem is that it is not clear where you should get your
distribution over your identity from. It does not make sense to just take the uniform
distribution; there are many people you can be, and they exist to diﬀerent extents,
especially if you include potential future people whose existences are uncertain.
The second problem is that interpersonal utility comparisons don't make sense. Utility
functions are not a real thing. Instead, there are preferences over uncertain worlds. If
a person's preferences satisfy the VNM axioms, then we can treat that person as
having a utility function, but the real thing is more like their preference ordering.
When we get utility functions this way, they are only deﬁned up to aﬃne
transformation. If you add a constant to a utility function, or multiply a utility function
by a positive constant, you get the same preferences. Before you can talk about
maximizing the expectation over your uncertainty about who you are, you need to put
all the diﬀerent possible utility functions into comparable units. This involves making a
two dimensional choice. You have to choose a zero point for each person, together
with a scaling factor for how much their utility goes up as their preferences are
satisﬁed.
Luckily, to implement the procedure of maximizing expected expected utility, you
don't actually need to know the zero points, since these only shift expected expected
utility by a constant. You do, however need to know the scaling factors. This is not an
easy task. You cannot just say something like "Make all the scaling factors 1." You
don't actually start with utility functions, you start with equivalence classes of utility
functions.
Thus, to implement utilitarianism, we need to know two things: What is the
distribution on people, and how do you scale each person's utilities? This gets
disguised as one problem, since the thing you do with these numbers is just multiply
them together to get a single weight, but it is actually two things you need to decide.
What can we do?
Egalitarianism

Now, let's pretend you are an egalitarian. You still want to satisfy everyone's goals,
and so you go behind the veil of ignorance, and forget who you are. The diﬀerence is
that now you are not trying to maximize expected expected utility, and instead are
trying to maximize worst-case expected utility. Again, the expectation contains
uncertainty about the world as well as any randomness in your action. The "worst-
case" part is about your uncertainty about who you are. You would like to have
reasonably high expected utility, regardless of who you might be.
When I say maximize worst-case expected utility, I am sweeping some details under
the rug about what to do if you manage to max out someone's utility. The actual
proposal is to maximize the minimum utility over all people. Then if there are multiple
ways to do this, consider the set of all people for which it is still possible to increase
their utility without bringing anyone below this minimum. Repeat the proposal with
only those people, subject to the constraint that you only consider actions that don't
bring anyone below the current minimum. (Yeah, yeah, this isn't obviously well
deﬁned for inﬁnitely many people. I am ignoring those details right now.)
This is called egalitarianism, because assuming you have the ability to randomize, and
ignoring complications related to maxing out someone's utility, you will tend to give
everyone the same expected utility. (For example, in the two person case, it will
always be the case that either it is not possible to increase the expected utility of the
person with lower expected utility, or the two people have the same expected utility.
Unfortunately, there are also two problems with deﬁning egalitarianism. We no longer
have to worry about a distribution on people. However, now we have to worry about
what the zero point of each person's utility function is, and also what the scaling
factor is for each person's utility function.
Unlike utilitarianism, egalitarianism will sometimes recommend randomizing between
diﬀerent outcomes for the sake of fairness.
Utility Monsters
Utilitarianism and egalitarianism each have their own type of utility monster.
For utilitarianism, imagine Cookie Monster. Cookie Monster gets a bazillion utility for
every cookie he gets. This dwarfs everyone's utility, and you should devote almost all
your resources to giving cookies to Cookie Monster.
For egalitarianism, imagine Oscar the Grouch. Oscar hates everything. Worlds range
from giving Oscar zero utility to giving Oscar one bazillionth of a utility. Assuming it is
possible to give everyone else much more than a bazillionth of a utility
simultaneously, you should devote almost all of your resources to maximizing Oscar's
utility.
For both utilitarianism and egalitarianism, it is possible to translate and rescale
utilities to create arbitrarily powerful utility monsters, which is to say that the choice
of how to normalize utility really matters a lot.
Filling in the Gaps

For deﬁning either utilitarianism or egalitarianism, there are three hard to deﬁne
parameters we need to consider:
1) The probability (from behind the veil of ignorance) that you expect to be each
person,
2) The zero point of each person's utility function, and
3) The scaling factor of each person's utility function.
Utilitarianism requires both 1 and 3. Egalitarianism requires both 2 and 3.
Unfortunately, I think that 1 and 2 are the two we have the most traction on.
1 feels more like an empirical question. It is mixed in with the question of where the
priors come from. 1 is like asking "With what prior probability would you expect to
have observed being any these people?"
2 feels like it is trying to deﬁne a default world. Something that is achievable, so it is
possible to give everyone non-negative utility simultaneously. Maybe we can use
something like understanding boundaries to ﬁgure out what 2 should be.
On 3, I got nothing, which is unfortunate, because we need 3 to deﬁne either of the
two proposals. Is there anything reasonable we can do if we only have answers to 1
and 2?
Also, people have intuitions pointing towards both Utilitarianism and Egalitarianism.
How are we supposed to decide between them?
Why not Both?
Assume that we magically had an answer to both 1 and 2 above, so we both have a
distribution over who we are behind the the veil of ignorance, and we also have a zero
point for everyone's utility function. Assume further we are allowed to randomize in
our action, and that it is possible to give everyone positive utility simultaneously.
Then, there exists an answer to 3 such that utilitarianism and egalitarianism
recommend the same action.
If we take the weakest notion of egalitarianism, which is just that the minimum utility
is maximized, then there might be more than one such scaling. However, if we take
the strongest notion of egalitarianism, that also everyone ends up with the same
utility (arguably the true spirit of egalitarianism), then we will get existence and
uniqueness of the scaling factors and the utilities. (I am not sure what the uniqueness
situation is for the tiered egalitarianism proposal I gave above.)
Here is a proof sketch of the existence part:
Start with some arbitrary scaling factor on everyone's utility functions.
Consider the action which maximizes the expected logarithm of expected utility,
where the outer expectation is over who you are, and the inner expectation is over
randomness in the world or in your action. This point will be unique up to utility
because of the convexity of the logarithm. Note that everyone will get positive utility.

For each person, rescaling their utility function will only add a constant to the
logarithm of their expected utility, and will thus have no eﬀect on maximizing the
expected logarithm of expected utility.
Thus, we can rescale everyone's utilities so that everyone gets expected utility 1
when we maximize the expected logarithm of expected utility.
First, we need to see that given this rescaling, the utilitarian choice is to give everyone
expected utility 1. Assume for the purpose of contradiction that there was some way
to achieve expected expected utility greater than 1. Let A be the (randomized) action
that gets everyone expected utility 1, and let B be a better action that gets expected
expected utility 1 + ε. If you consider the parameterized action (1 −p)A + pB, and look
at the derivative of expected expected utility respect to p at p = 0, you get ε.
However, when everyone gets expected utility 1, the expected logarithm of expected
utility will have the same derivative as expected expected utility. Thus this derivative
will also be ε, contradicting the fact that the policy maximizes the expected logarithm
of expected utility at the action A that you get when p = 0.
Next, let us see that given this rescaling, the egalitarian choice is to give everyone
utility 1. If it were possible to give anyone expected utility greater than 1 without
decreasing anyone's expected utility to less than 1, this would be a utilitarian
improvement, which we already said was impossible. Thus, the only way to achieve a
worst-case expected utility of 1 is to give everyone expected utility 1.
Nash Bargaining
The above policy is an alternate characterization of the Nash bargaining solution,
generalized to many players with diﬀerent weights.
Given a zero point and a feasible set of options closed under random mixtures, The
Nash bargaining solution gives a way of combining two utility functions into a single
option.
The arguments in this post are not the most standard arguments for Nash bargaining.
Nash bargaining can also be uniquely characterized with some simple axioms like
Pareto optimality and independence of irrelevant alternatives.
There is a lot of reason to consider the Nash bargaining solution as the default way to
combine utility functions when you don't have a principled way to do interpersonal
utility comparisons. Even if you had a principled way of doing interpersonal utility
comparisons, you might want to do Nash bargaining anyway for the sake of fairness. 

Searching for Search
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
Thanks to Dan Braun, Ze Shen Chin, Paul Colognese, Michael Ivanitskiy, Sudhanshu Kasewa,
and Lucas Teixeira for feedback on drafts.
This work was carried out while at  Conjecture .
 
This post is a loosely structured collection of thoughts and confusions about search and
mesaoptimization and how to look for them in transformers. We've been thinking about this
for a while and still feel confused. Hopefully this post makes others more confused so they
can help.
Mesaoptimization
We can deﬁne mesaoptimization as internal optimization, where "optimization" describes the
structure of computation within a system, not just its behavior. This kind of optimization
seems particularly powerful, and many alignment researchers seem to think it's one of the
biggest concerns in alignment. Despite how important this is, we still understand very little
about it.
For starters, it's not clear what internal optimization actually means. People have proposed
several deﬁnitions of optimization which ﬁt well when thinking about "behavioral
optimization" where an agent acts to optimize its environment. One of the most clear and
popular deﬁnitions comes from Alex Flint:
an optimizing system is a physical process in which the conﬁguration of some part of the
universe moves predictably towards a small set of target conﬁgurations from any point
in a broad basin of optimization, despite perturbations during the optimization process.
In this framing, an agent and its environment together form an optimization process, where
the agent acts upon the environment such that the system as a whole robustly converges to
a set of target states. 
But we run into problems when we try to map this behavioral deﬁnition to a deﬁnition about
the structure of a process's internal computation. When we say mesaoptimization, we seem
to mean something diﬀerent than just that the computation converges to a smaller target.
For example, an image classiﬁer takes a large set of initial conﬁgurations of images including
a lot of noise and irrelevant details, and layer by layer narrows it down to a probability
distribution concentrated on a single class prediction. There seems to be a sense that this is
not doing the kind of optimization we are concerned about when we talk about
mesaoptimization.
Mesaoptimization was originally deﬁned in Risks from Learned Optimization as internal
search:
We will say that a system is an optimizer if it is internally searching through a search
space (consisting of possible outputs, policies, plans, strategies, or similar) looking for
those elements that score high according to some objective function that is explicitly
represented within the system.
An advantage of this framing is that we do have some idea what we mean by "search" and
have concrete examples of things which unambiguously qualify. Ideally, we'd like to point to

the more general class of computation we're worried about, but once you start thinking
about what this general class of computation might look like, it quickly becomes clear that
we don't even know what "search" is. The space of search algorithms also seems much
larger and more ﬂexible than implied in the examples we usually think of.
At the moment we have very little idea what kind of algorithms we should expect neural
networks to learn, and neither do we have a good picture of what kind of algorithms in
particular we should be concerned about when we think of misalignment. If the intuition that
"search" is somehow central to internal optimization holds validity, then becoming less
confused about what learned search looks like should be central to making risks from
internal optimization more concrete. 
What is Search?
We have examples of processes which most would say are doing some kind of search,
like Monte Carlo tree search, or A*, and processes that not everyone agrees count as search,
like evolution through natural selection or stochastic gradient descent, which nonetheless
clearly have search-like properties (the general disagreement about these examples is
evidence that the concept is confusing). Some of these prototypical examples are
handcrafted, others are natural, but broadly they operate in a similar way: They tend to
generate candidate solutions, have a method for evaluating these candidate solutions, and
use those evaluations to do a kind of iterative reﬁnement of the solution space. This is highly
reminiscent of what Abram Demski calls selection processes, which are able to both
instantiate members of the solution space and obtain direct feedback on their quality. 
While these are clearly doing search, this doesn't look at all like how humans usually do
search. Humans don't restrict themselves to enumerating and evaluating candidate
solutions. Instead, humans often operate over highly abstract compressions of the solution
space, or search over entirely diﬀerent spaces, such as global constraints on the problem, to
make ﬁnding a solution more tractable. 
Instead of focusing on one particular type of search, we want to think about the general
properties that our examples of search share. At the highest level, they each take a large
space of possible candidates (generally implicit to a problem speciﬁcation) and shrink it
down to a single solution which satisﬁes some criteria. If we say a process which ﬁnds a
solution from a space of 2^N candidates can be thought of as doing N bits of optimization,
then one place to start is to ask where those bits of optimization come from. 
Compressing the search space
By transforming the search space into one which has signiﬁcantly fewer degrees of freedom,
we can quickly obtain a large number of bits, and shrink the scope of the problem down to
one which is much more manageable. 
Master chess players are able to better memorize boards by chunking structures of pieces as
single units and converting a board to a higher level concept space not possessed by
novices.[1] For instance, instead of separately tracking the positions of every pawn, the
master player might memorize a single position for a group of pawns in a commonly-
occurring arrangement.

This advantage is only present for real game positions, however, and completely disappears
for fully random board states. This suggests that the concept space used by master players
implicitly ignores most of the possible ways that the pieces can be arranged, and therefore
has less degrees of freedom than the original space did. (It has to: for any lossless
compression scheme, some possible sequences are inevitably incompressible, because there
are fewer sequences of length < N than length N.)

Furthermore, unlike concepts representing literal "chunks" of relative positions
parameterized by absolute position, many useful concepts may be invariant to the exact
position of the pieces, like the concept of a "pin" or "skewer". In practice this is a type of
lossy compression which would not be able to disambiguate between possible boards if the
diﬀerences are unlikely to be strategically relevant. The compression, more generally, is just
a useful ontology, which makes the search problem signiﬁcantly easier. Promising candidates
(board positions or sequences of positions) can be represented in fewer bits, and the lower
dimensional representation space can be used as a generator of candidates, because
satisfactory solutions now make up a larger fraction of the now smaller compressed space.
This is important, because the way that the solution space gets compressed will shape what
search looks like from the inside. Evaluated candidates may not be in the naive or expected
representation, but rather be pointers to pieces of an internal ontology that might look very
diﬀerent. We should also expect those internal representations to be harder to detect and
decipher, because the more a representation gets compressed, the more we should expect it
to lose structural correlations which might distinguish it from noise. 
Searching over constraint space
Another way to reduce the number of degrees of freedom is to focus on the constraints of a
search problem. Searching over global constraints has many advantages, and seems to be
one of the main ways humans search when the solution space is too large to reason about
directly. Constraint space tends to be much lower dimensional, and can be used to implicitly
carve out huge chunks of the original solution space, or break a problem down into smaller
subproblems, allowing a search process to recurse on those subproblems, factoring the
problem into manageable pieces. In a game like chess this might look like searching for
threats, which operate like bottlenecks, requiring all successful plans to involve mitigating
the threat as an instrumental subgoal. 
This means that the candidates being considered may not map to solutions at all, instead
being objects of an entirely diﬀerent search space (e.g. that of threats). A search process
might also consider solutions to smaller or relaxed subproblems, and never reference or
relate directly to the full search target. This aﬀects the problem of searching for search,

because we might not be able to ﬁnd the algorithm if we only search for signs of a search
over solution space, such as instantiations of candidate solutions. 
Another important quality of constraint space is that constraints are often useful for a wide
range of search targets. For example, when planning a long distance trip, any successful plan
will likely require booking a ﬂight and going to the airport. This fact doesn't depend very
much on the exact destination, and so is especially useful to a more general purpose search
process. 
General purpose search, of the kind that humans possess, is also the kind of internal
optimization that is most concerning and worth our study. Such a system would need to be
able to take a search problem and actively simplify it by ﬁnding exploitable features of the
search space at runtime, and breaking that problem down into more manageable
subproblems. 
Modularity and Retargetability
General purpose search needs to be able to take on a wide range of possible search
problems and be ﬂexible to distributional shifts. Systems which are robust to changes in the
objective tend to be modular, composing their computation into submodules. 
Non-
modular systems, where the dependencies of computation are spread too widely to be split
into distinct modules, are more likely to be brittle to changes in how the computation is
performed. In learning to code, for example, a common early lesson is to split a program up
into functions, and allow each function to interact with the rest of the program only through
a small number of input and output variables (and avoid references to global variables). This
doesn't just make code easier to read, which is not a requirement for learned algorithms, but
more importantly it contains the ripple eﬀect that a change in one function has on the rest of
the program. 
This advantage of modularity applies both at runtime (reducing the chances that an
uncommon input causes failure), as well as during development (reducing the number of
changes needed to ﬁx a bug). Analogously, we might expect modularity in learned
algorithms both for their ability to generalize at runtime, as well as a part of the inductive
bias of gradient descent. The more sprawling the dependencies, the more directions in the
gradient landscape will lead directly to machinery breaking (and thus higher loss). A change
to a module, on the other hand, need only be accompanied with a change to the API for the
system as a whole to keep on functioning.
A likely place to ﬁnd an example of modularity is in the retargetability of a search algorithm.
If a search algorithm is general purpose, then the "target", or what is being searched for,

can't be baked into or be implicit to the search process, but rather must be an input to a
retargetable search algorithm. This applies to a system capable of handling a broad set of
search problems, but even systems trained only to search with a single target have an
incentive to be retargetable if that search algorithm breaks the problem down into
subproblems, each with their own instrumental targets. If internal to the system there exists
a target-agnostic search algorithm, with separate machinery for providing the target, then
one place to start would be to ﬁnd those modules (and understand how they interact with
each other).
What even is a target/goal?
A target, through its interaction with the search algorithm, functions to "steer" search toward
converging to a particular set of solutions. Much of the search process might be possible to
do without any reference to the target (goal-agnostic preprocessing), but for general purpose
algorithms we should expect a signiﬁcant amount of the computation to hinge pivotally on
the target. 
In handcrafted search algorithms, the target has a clear type signature, and its interaction
with the rest of the search process is well understood. In MCTS, for example, the target takes
the shape of an evaluation function, and similarly for A*, the target is a coordinate which is
used by a heuristic distance metric to guide the search process. They are both retargetable
algorithms, and it's easy to see how changing the target will change the process by which
the algorithm will narrow the search space. 
In learned search, it might be searching over multiple diﬀerent ontologies, many of which
don't map to the solution space, as well as generate instrumental goals and recurse on
subproblems. We don't currently know what type signature the target would have, or how it
slots into a search process to guide the computation, because we don't have a good gears-
level understanding of what learned search looks like. 
Developing a better understanding of what targets are, both conceptually and
experimentally, might give us a foot in the door toward understanding how targets interact
with other modules within the search algorithm.
Learned Search in Transformers
Another angle of attack to understand mesaoptimization and search is to think about what
types of algorithms we expect neural networks to learn. "Learned search" refers to search
algorithms which, unlike handcrafted search algorithms, have been found automatically via
gradient descent. If we ﬁnd learned search within neural networks, it will need to conform
both to the limitations of what can be implemented in the architecture as well as the
inductive bias of gradient descent. Because of the success of LLMs like GPT-3 I'll be focusing
on the transformer architecture. Many of the arguments also apply to other architectures.
We might expect mesaoptimizers to be preferred for many reasons. Search at runtime is
eﬀectively a compressed version of a more complicated policy that encodes the information
gained by search explicitly, specifying a system capable of generating that policy. For
example, instead of memorizing a book of optimal chess openings, a program much shorter
than the book could search through future consequences of moves and hypothetically
converge to the same output. 
There are, however, some reasons to be skeptical of ﬁnding search in transformers. 
First of all, search isn't free: the chess program that does brute-force search may have fewer
lines of code, but is more expensive to run than a program that simply looks up a memorized
answer. Any search algorithm which uses too much computation or memory just cannot be

implemented by a transformer. This rules out many of the algorithms which rely heavily on
explicitly enumerating and evaluating states. In the same vein, any algorithm that requires a
high number of serial steps would also not be possible to run, even if the algorithm itself is in
principle quite simple to describe. Unlike the NLP architectures that came before it,
transformers are not recurrent. Information can only ﬂow up, and so there is a hard cap on
the number of serial steps that can be used to make a prediction. 
Here both a transformer and a stacked LSTM are shown predicting token 5 from
the context (tokens 0-4). While each LSTM block is able to pass information to a
copy of itself in the next column, in a transformer information can only be passed
upward to the next layer.
Furthermore, the fact that a layer cannot pass information to itself is also a constraint which
aﬀects the inductive bias over possible search algorithms, likely disincentivizing algorithms
that require certain subroutines to be reused many times for a single prediction. If weights
are not being shared, then for a subroutine to be applied multiple times, it must be
independently learned and implemented in diﬀerent parts of the network. For
example, if a search algorithm relies on something like an evaluation function, it would need
to have separately learned every instance of its use, making the eﬀective complexity of the
algorithm's description very high. This also might limit the extent to which we should expect
to see algorithms which rely on recursion.

Another clue which restricts the algorithms we should expect from transformers relates to
the way information seems to be processed in the residual stream. The transformer
architecture relies on a residual stream, where individual heads read and write to a central
channel of information. The logit lens appears to show that a byproduct of this design is
that transformers tend to quickly ﬁnd candidate solutions in early layers and then
reﬁne and update them over the rest of the forward pass. This might push us to consider
algorithms that don't completely depart from the solution space, and perhaps use early
solutions to help inform the rest of the search process. For example, a transformer might
exploit a certain duality between solution space and constraint space, iteratively using
members of the solution space to identify constraints, and members of the constraint space
to identify solutions.
Of course, it could also be that transformers are just using a giant bag of heuristics to
produce solutions, and don't implement anything at all like something we would call search.
We can add credence to the existence of search processes by demonstrating the ability of
transformers to solve "searchy" problems like chess, which seems to require a certain ability
to search through possible futures, but ultimately what we really need is for interpretability
to illuminate the diﬀerence.
How to Search for Search?
One thing that we would really like to have is a clear signal from reality that could help guide
our thinking about this. We could spend a lot of time pondering the space of search
algorithms, and which kind of algorithms a transformer might learn, but ultimately we would
like some clear evidence to help narrow our questions. This seems like a really hard problem,
and unfortunately the interpretability tools we currently have are limited, but here are some
general thoughts about how to approach this problem.
Asking the right questions
We have to be able to turn abstract conceptual questions about search into testable
hypotheses. One approach is to identify properties we expect learned search algorithms to
have in theory, and then more concretely what computational artifacts we should expect to
ﬁnd in practice. We can do this both by considering what properties are suﬃcient or
necessary for a search algorithm implemented within a particular architecture to function, as
well as what is selected for by the training process.
We can then design experiments that make it possible to test those hypotheses. We can
deliberately train transformers on "searchy" data, like maze solving or chess games, in order
to focus on the types of search mechanisms that we think might make it possible to perform
well on that data. For example, to predict chess games, it seems plausible that a transformer
would need to consider future paths, and so we could focus our experiments there. 
Failure modes
Many interpretability methods will try to determine what a neural network is thinking, and
usually end up ﬁnding correlation rather than causation. For example, one can use some kind
of supervised process to train a probe, and then use that probe to extract features from the
activation space. Unfortunately this doesn't guarantee that the network is actually using
those features and not something which correlates with it. Furthermore, even if it does use
those features, it might be using them in completely diﬀerent ways than we expect. We end
up trying to map our own ontology of information onto that of the neural network without
really understanding it.

Take this paper, where the authors train linear probes to predict high-level chess features
from the intermediate layers of AlphaZero in order to gain insight about what knowledge the
network is using at various stages. They ﬁnd that they are able to predict concepts like "king
safety" and we could reason that this makes sense as a concept, as it could be useful for
ﬁguring out what moves to make next. But instead of picking up on king safety, the probe
might also be picking up on other features, like the number of pieces on the board, which
correlate strongly with king safety, and there isn't an obvious way to tell the diﬀerence.
One way to overcome this is to focus on doing causal intervention, but even this has its
problems. Semantic features are all intertwined and dependent on each other, and for many
things it seems really non-trivial to edit something without breaking the network's brain.
[2] What would it mean to cause the network to believe the king was safe, without aﬀecting a
whole host of other semantic features? Is it possible to make a network trained on addition to
think 2 + 2 = 5, without destroying its ability to do math?
Lastly, experiments often have the ﬂaw of testing something more speciﬁc than the
hypothesis we are actually interested in. We might often have a very general hypothesis
about the kind of things the network might be doing, but end up implicitly testing a much
more speciﬁc hypothesis. For example, we might hypothesize that a network is explicitly
proposing candidates, but when we go to test it, our eﬀective hypothesis becomes that the
network is explicitly proposing candidates in a format that this method (e.g. linear
probes) would ﬁnd. If we get a negative result, we can't say much about the original
hypothesis because the network could just be doing things in a diﬀerent way we aren't able
to detect. If we get a positive result, we might also just be picking up on some correlation
which satisﬁes the test, but is actually caused by something fundamentally diﬀerent from the
original hypothesis.[3]
Firehose interpretability
Instead of designing experiments which deliver only a single bit of evidence (falsify/verify),
another approach is to instead aim for methods which measure lots of things all at once, and
produce a ﬁrehose of bits. If we don't currently know the right questions to ask, or even in
what ontology to pose our questions, then it can be really hard to design experiments that
cut at the heart of what we care about, as well as to draw meaningful conclusions from them.
When it becomes diﬃcult to form testable hypotheses that make progress on the important
questions, it makes sense to shift away from running classical hypothesis-driven experiments
toward making high bit observations.
Our ability to make lots of useful observations depends on measurement tools, or lenses,
that make visible things which are invisible, either by overcoming the physical limitations of
our sense organs or our cognitive limitations to interpret raw data. This can be a major
bottleneck to scientiﬁc progress, a prototypical example being the invention of the
microscope, which was a turning point for our ability to study the natural world. The lenses
that currently exist for interpretability are still quite crude, and expanding the current suite

of tools, as well as building places to explore and visualize neural networks using those tools,
seems critical for making lots of high bit observations.
Another motivation for high-bandwidth measurements comes from our problem with inferring
causality from correlations. While it's true that it's impossible to determine causality from the
correlation between just two variables, for more than two it can become possible, and the
more variables we do measure the easier it becomes. This is a path to building causal
models which avoids the pitfalls of intervening directly on a neural network.
Conclusion
Searching for search seems like an important research direction because it points at
something strongly related to a lot of models for how misalignment happens, as well as
being a prerequisite to a lot of potential solutions, like retargeting the search. 
We want to be able to look for things like mesaoptimization or search, but when we look at a
system and ask ourselves "is this system doing search?", we realize that we don't really
understand the question. These words can make us feel like we know what we are talking
about, but when we try to put them into practice we run into trouble. This is why it is so
important to keep alignment research grounded in real-world AI systems: it forces us to
confront what we don't know, keeps us from getting lost speculating, and sharpens our
focus.
We currently don't really know where to look and which experiments will actually further our
understanding, so ﬁguring out how to translate these vague concepts into more concrete
claims about how learned search happens on the algorithmic level is critical.
1. ^
There are several papers which show this phenomenon, this paper being the most
famous example.
2. ^
Recent work from Jacques Thibodeau at SERI MATS and from the interpretability
hackathon show evidence that editing factual knowledge with ROME does not robustly
propagate logical implications and causes unintended side eﬀects.
3. ^
This problem of measuring something more speciﬁc than the thing we are interested in
can also be addressed with auditing games.

ARC paper: Formalizing the
presumption of independence
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
This is a linkpost for https://arxiv.org/abs/2211.06738
(I did not have anything to do with this paper and these are just my own takes.)
The Alignment Research Center recently published their second report, Formalizing
the presumption of independence. While it's not explicitly about AI alignment, it's
probably still interesting for some  people here.
Summary
The paper is about "heuristic arguments". These are similar to proofs, except that
their conclusions are not guaranteed to be correct and can be overturned by
counterarguments. Mathematicians often use these kinds of arguments, but in
contrast to proofs, they haven't been formalized. The paper mainly describes the open
problem of ﬁnding a good formalization of heuristic arguments. They do describe one
attempt, "cumulant propagation", in Appendix D, but point out it can behave
pathologically.
So what's the "presumption of independence" from the title? Lots of heuristic
arguments work by assuming that some quantities are independent to simplify things,
and that's what the paper focuses on. Such an argument can be overturned by
showing that there's actually some correlation we initially ignored, which should then
lead to a more sophisticated heuristic argument with a potentially diﬀerent
conclusion.
What does this have to do with
alignment?
The paper only very brieﬂy mentions alignment (in Appendix F), more detailed
discussion is planned for the future. But roughly:
Avoiding catastrophic failures. Heuristic arguments can let us better estimate
the probability of rare failures, or failures which occur only on novel distributions
where we cannot easily draw samples. This can be used during validation to
estimate risk, or potentially during training to further reduce risk.
Eliciting latent knowledge. Heuristic arguments may let us see "why" a model
makes its predictions. We could potentially use them to distinguish cases where
similar behaviors are produced by very diﬀerent mechanisms—for example
distinguishing cases where a model predicts that a smiling human face will show
up on camera because it predicts there will actually be a smiling human in the

room, from cases where it makes the same prediction because it predicts that the
camera will be tampered with. [...]
Neither of these applications is straightforward, and it should not be obvious that
heuristic arguments would allow us to achieve either goal. [...]
Heuristic arguments can be seen as somewhere between interpretability and formal
veriﬁcation: unlike interpretability, heuristic arguments are meant to be machine-
checkable and don't have to be human-understandable. But unlike formal proofs, they
don't require perfect certainty and might be much easier to ﬁnd.
Readers here might also be reminded of Logical Induction. This paper is trying to do
something somewhat diﬀerent though:
[Approaches to logical uncertainty] have primarily focused on establishing
coherence conditions and on capturing inductive reasoning, i.e. ensuring that a
reasoner eventually successfully predicts φ(n) given observations of φ(1), φ(2), . . .
φ(n − 1). These systems would not automatically recognize intuitively valid
heuristic arguments [...], although they would eventually learn to trust these
arguments after observing them producing good predictions in practice.
Indeed, we can view ourselves as reasoners in exactly this situation, trying to
understand and formalize a type of reasoning that appears to often make good
predictions in practice. Formalizations of inductive reasoning may help clarify the
standards we should use for evaluating a proposed heuristic estimator, but do not
constitute a good heuristic estimator themselves.
So should you read the paper?
Given it's a 60-page report (though most of that's appendices) with basically no
explicit discussion of alignment, I don't think this is a "must-read" for everyone. For
example, if you haven't read the ELK report, I would strongly recommend that over
this new paper.
On the other hand, if you work on something related, such as formal veriﬁcation, ELK,
or conceptual interpretability research, I think it makes a lot of sense to at least look
at the main paper and Appendix F (16 pages and quite readable).
Personally, I also think this is just really interesting independent from alignment.
Appendix B and C were my favorite parts from that perspective (though also the most
speculative ones).

When AI solves a game, focus on the
game's mechanics, not its theme.
Epistemic status: This is a brief sketch of an idea I'm pretty sure about.
1. A board game design consists of two things: mechanics and theme.
1. The game mechanics are the abstract rules governing how the players interact
with each other and the shared environment.
2. The game theme is a ﬁctional interpretation of the game elements.
3. Consider Battleship — the theme is a naval battle, and the mechanics are a
particular 2-player sequential discovery game. There is a correspondence
between the ontology of the mechanics and the ontology of the theme.
4. There is often little connection between mechanics and theme. For example, the
Knight in chess has almost nothing to do with horses.
2. When an AI solves a game, people sometimes overfocus on the theme of the game
relative to the mechanics of the game.
3. Maybe this is for psychological reasons:
1. The theme is more interesting than the mechanics.
2. The theme is in our pre-cached ontology. That is, my brain already has a pre-
cached concept of "naval battle" but it doesn't have a pre-cached concept
corresponding to the particular mechanics of Battleship. In fact, this is why
games have themes in the ﬁrst place — they serve partly as mnemonics for the
rules.
3. Did you know that people cooperate more in the Prisoner's Dilemma if the game
is called "Community Game" than "Wall Street Game"?!
4. Or maybe this is for rational reasons:
1. Other people might think that there is a deeper connection between the theme
and the mechanics of the particular game than I do. For example, they might
think there is some genuine non-arbitrary connection between the mechanics of
monopoly and the real estate market.
2. See ludonarrative dissonance.
5. If people overfocus on the theme, then they make incorrect predictions about AI.
1. For example, they'll hear "AI has solved Full-Press Diplomacy" and extrapolate
that AI will soon be able to solve other games of a similar theme (i.e.

international military negotiations).
2. Instead, they should extrapolate that AI will soon be able to solve other games
with similar mechanics.
3. Here's some practical advice: imagine the game had the same mechanics but a
diﬀerent theme. Sure, AI has solved Full-Press Diplomacy, which is scary because
the theme is militaries negotiating which countries to invade. But what if the
theme was gardeners negotiating which ﬂowers to plant? Okay still pretty scary
tbh.
6. Could we use this bias to scare the public? "Oh, no — AI has just solved the Kill-All-the-
Humans game."
Appendix
So anyway, what are game mechanics?
Is time discrete (i.e. turn-based) or continuous?
How many turns are there? Boundedly-many? Finite-but-unboundedly many? Inﬁnitely
many?
Do the players move sequentially or simultaneously?
How many possible moves are there each turn?
How many players are there?
Are players ever eliminated?
What are the winning conditions?
How many players can win?
Do the players accumulate points?
How sparsely are points allocated?
Do players know how many points the other players have?
Is it harder to accumulate points the more you have, or easier?
Is there a bound on the number of points?
Is the game purely cooperative, purely adversarial, or somewhere between?
Can the players communicate publicly?
Can the players communicate privately?
Are the mechanics symmetric with respect to each player?
Are the mechanics symmetric with respect to each pair of players? E.t.c.
How unequal is the advantage between players begin?
How random is the environment?
How Kolmogorov-complex is the environment?
How computationally-complex is the environment?
Is there little interference between players (like the 400m dash), or much interference
(like a football game)?
Does one strategy dominate all others, or is the game nontransitive?
Is the state space continuous or discrete?
Is the action space continuous or discrete?
If discrete, how many bits specify the game state?
If continuous, what's the topology?
Are the actions reversible?
And so on, and so on.

Clarifying AI X-risk
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
TL;DR: We give a threat model literature review, propose a categorization and describe a
consensus threat model from some of DeepMind's AGI safety team. See our post for the
detailed literature review.
The DeepMind AGI Safety team has been working to understand the space of threat models for
existential risk (X-risk) from misaligned AI. This post summarizes our ﬁndings. Our aim was to
clarify the case for X-risk to enable better research project generation and prioritization. 
First, we conducted a literature review of existing threat models, discussed their
strengths/weaknesses and then formed a categorization based on the technical cause of X-risk
and the path that leads to X-risk. Next we tried to ﬁnd consensus within our group on a threat
model that we all ﬁnd plausible.
Our overall take is that there may be more agreement between alignment researchers than
their disagreements might suggest, with many of the threat models, including our own
consensus one, making similar arguments for the source of risk. Disagreements remain over
the diﬃculty of the alignment problem, and what counts as a solution.
Categorization
Here we present our categorization of threat models from our literature review, based on the
technical cause and the path leading to X-risk. It is summarized in the diagram below. 

In green on the left we have the technical cause of the risk, either speciﬁcation gaming (SG) or
goal misgeneralization (GMG). In red on the right we have the path that leads to X-risk, either
through the interaction of multiple systems, or through a misaligned power-seeking (MAPS)
system. The threat models appear as arrows from technical cause towards path to X-risk.
The technical causes (SG and GMG) are not mutually exclusive, both can occur within the same
threat model. The distinction between them is motivated by the common distinction in machine
learning between failures on the training distribution, and when out of distribution. 
To classify as speciﬁcation gaming, there needs to be bad feedback provided on the actual
training data. There are many ways to operationalize good/bad feedback. The choice we make
here is that the training data feedback is good if it rewards exactly those outputs that would be
chosen by a competent, well-motivated AI[1]. We note that the main downside to this
operationalisation is that even if just one out of a huge number of training data points gets bad
feedback, then we would classify the failure as speciﬁcation gaming, even though that one
datapoint likely made no diﬀerence.
To classify as goal misgeneralization, the behavior when out-of-distribution (i.e. not using
input from the training data), generalizes poorly about its goal, while its capabilities generalize
well, leading to undesired behavior. This means the AI system doesn't just break entirely, it still
competently pursues some goal, but it's not the goal we intended.
The path leading to X-risk is classiﬁed as follows. When the path to X-risk is from the
interaction of multiple systems, the deﬁning feature here is not just that there are multiple
AI systems (we think this will be the case in all realistic threat models), it's more that the risk is
caused by complicated interactions between systems that we heavily depend on and can't

easily stop or transition away from. (Note that we haven't analyzed the multiple-systems case
very much, and there are also other technical causes for those kinds of scenarios.)
When the path to X-risk is through Misaligned Power-Seeking (MAPS), the AI system seeks
power in unintended ways due to problems with its goals. Here, power-seeking means the AI
system seeks power as an instrumental subgoal, because having more power increases the
options available to the system allowing it to do better at achieving its goals. Misaligned here
means that the goal that the AI system pursues is not what its designers intended[2].
There are other plausible paths to X-risk (see e.g. this list), though our focus here was on the
most popular writings on threat models in which the main source of risk is technical, rather
than through poor decisions made by humans in how to use AI.
For a summary on the properties of the threat models, see the table below.
 
Source of misalignment
Speciﬁcation
gaming (SG)
SG + GMG
Goal
misgeneralization
(GMG)
Path
to 
X-
risk
Misaligned
power
seeking
(MAPS)
Cohen et al
Carlsmith, Christiano2, Cotra, Ngo,
Shah
Soares, Hubinger
Interaction
of multiple
systems
Critch, Christiano1 ?
?
We can see that ﬁve of the threat models we considered substantially involve both speciﬁcation
gaming and goal misgeneralization (note that these threat models would still hold if one of the
risk sources was absent) as the source of misalignment, and MAPS as the path to X-risk. This
seems like an area where multiple researchers agree on the bare bones of the threat model -
indeed our group's consensus threat model was in this category too.
One aspect that our categorization has highlighted is that there are potential gaps in the
literature, as emphasized by the question marks in the table above for paths to X-risk via the
interaction of multiple systems, where the source of misalignment involves goal
misgeneralization. It would be interesting to see some threat models that ﬁll this gap.
For other overviews of diﬀerent threat models, see here and here.
Consensus Threat Model
Building on this literature review we looked for consensus among our group of AGI safety
researchers. We asked ourselves the question: conditional on there being an existential
catastrophe from misaligned AI, what is the most likely threat model that brought this about.
This is independent of the probability of an occurrence of an existential catastrophe from
misaligned AI. Our resulting threat model is as follows (black bullets indicate agreement, white
indicates some variability among the group):
Development model: 
Scaled up deep learning foundation models with RL from human feedback (RLHF) ﬁne-
tuning.

Not many more fundamental innovations needed for AGI.
Risk model: 
Main source of risk is a mix of speciﬁcation gaming and (a bit more from) goal
misgeneralization.
A misaligned consequentialist arises and seeks power (misaligned mostly
because of goal misgeneralization).
Perhaps this arises mainly during RLHF rather than in the pretrained foundation
model because the tasks for which we use RLHF will beneﬁt much more from
consequentialist planning than the pretraining task.
We don't catch this because deceptive alignment occurs (a consequence of power-
seeking)
Perhaps certain architectural components such as a tape/scratchpad for memory
and planning would accelerate this.
Important people won't understand: inadequate societal response to warning shots
on consequentialist planning, strategic awareness and deceptive alignment. 
Perhaps it's unclear who actually controls AI development.
Interpretability will be hard.
By misaligned consequentialist we mean 
It uses consequentialist reasoning: a system that evaluates the outcomes of various
possible plans against some metric, and chooses the plan that does best on that metric
Is misaligned - the metric it uses is not a goal that we intended the system to have
Overall we hope our threat model strikes the right balance of giving detail where we think it's
useful, without being too speciﬁc (which carries a higher risk of distracting from the essential
points, and higher chance of being wrong).
Takeaway 
Overall we thought that alignment researchers agree on quite a lot regarding the sources of
risk (the collection of threat models in blue in the diagram). Our group's consensus threat
model is also in this part of threat model space (the closest existing threat model is Cotra). 
 
1. ^
In this deﬁnition, whether the feedback is good/bad does not depend on the reasoning
used by the AI system, so e.g. rewarding an action that was chosen by a misaligned AI
system that is trying to hide its misaligned intentions would still count as good feedback
under this deﬁnition.
2. ^
There are other possible formulations of misaligned, for example the system's goal may
not match what its users want it to do.

Always know where your abstractions
break
General relativity plus quantum ﬁeld theory can describe almost everything in the
universe. There are a few exceptions like cosmic expansion and black holes but to
human beings conﬁned to a single solar system, fundamental physics is (for all
practical purposes) a solved problem. Yet there are many things we don't know. It's as
if the universe were a game of chess; we've learned the basic rules but are still
ﬁguring out the strategies.
All the universe cares about is fundamental physics. The universe always obeys the
small-scale fundamental laws of physics. The universe never does anything else. It
doesn't care about evolution or chemistry or orbital mechanics or beauty. All of those
things are high-level abstractions we (or evolution) invented to make sense of the
world. Most of the time we think about how the world works we don't think about
fundamental physics. We use our higher-level abstractions instead.
Which is ﬁne...most of the time. The problem is that any theory other than "the
universe always obeys the fundamental laws of physics" is wrong in the sense that it
is not perfectly generalizable.
There are many ways abstractions can malfunction when misapplied.
All abstractions have limited domains of applicability. Modern political ideologies
—Marxist revolutionary theory, libertarianism, feminism—were invented in the
context of an industrial civilization. Try too hard to apply these ideas to New
Guinean hunter-gatherers or to medieval Japan and they'll cloud your ability to
understand what's actually going on.
All high-level abstractions are, ultimately, probabilistic. Statistical mechanics
almost always works. Almost.
Perhaps most importantly, "the concepts we use in everyday life are fuzzy, and
break down if pushed too hard". Even Newton's Laws of Motion break when you
apply them to too small of a scale.
Even general relativity and quantum ﬁeld theory are not not universally generalizable.
General relativity breaks on small-scale phenomena. Quantum ﬁeld theory breaks on
large-scale phenomena.
Postmodernists use the idea of leaky abstractions to dismiss the idea of objective
truth entirely. That's like driving your car into the ocean and then declaring that cars
don't work. Cars do work, but you need to take care of yours and drive it only on the
terrain it functions on.
The most dangerous philosophers aren't the postmodernists who believe nothing is
true. The most dangerous philosophers are the ideologues who believe their particular
ideology is true.
There is nothing wrong with believing true things are true. One absolutely should
believe true things are true. The problem with ideologues is that they believe their
personal ideology is absolutely true. If you believe an ideology—any ideology—is
absolutely true then you are wrong because high-level abstractions are always
imperfect models of reality. Ideologues' tools malfunction because ideologues don't

know the limits of their own tools. They aren't even aware their tools have limits.
Those who understand ideologies' limits aren't ideologues.
Every idea has a domain it can be applied to, beyond which the idea will malfunction.
If you don't understand an idea's limitations then you don't understand that
idea.

Caution when interpreting Deepmind's
In-context RL paper
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
Lots of people I know have had pretty strong reactions to the recent Deepmind paper, which
claims to have gotten a transformer to learn an RL algorithm by training it on an RL agent's
training trajectories. At ﬁrst, I too was pretty shocked -- this paper seemed to provide strong
evidence of a mesa-optimizer in the wild. But digging into the paper a bit more, I'm quite
unimpressed and don't think that in-context RL is the correct way to interpret the
experiments that the authors actually did. This post is a quick, low-eﬀort attempt to write out
my thoughts on this.
Recall that in this paper, the authors pick some RL algorithm, use it to train RL agents on
some tasks, and save the trajectories generated during training; then they train a
transformer to autoregressively model said trajectories, and deploy the transformer on some
novel tasks. So for concreteness, during training the transformer sees inputs that look like
( s n , a n , r n , s n + 1 , a n + 1 , r n + 1 , ... , s n + c )
which were excerpted from an RL agent's training on some task (out of a set of training
tasks) and which span multiple episodes (i.e. at some point in this input trajectory, one
episode ended and the next episode began). The transformer is trained to guess the action 
an+c that comes next. In deployment, the inputs are determined by the transformer's own
selections, with the environment providing the states and rewards. The authors call this
algorithmic distillation (AD). 
Many people I know have skimmed the paper and come away with an understanding
something like:
In this paper, RL agents are trained on diverse tasks, e.g. playing many diﬀerent Atari
games, and the resulting transcripts are used as training data for AD. Then the AD agent
is deployed on a new task, e.g. playing a held-out Atari game. The AD agent is able to
learn to play this novel game, which can only be explained by the model implementing
an reasonably general RL algorithm. This sounds a whole lot like a mesa-optimizer.
This understanding is incorrect, with two key issues. First the training tasks used in this
paper are all extremely similar to each other and to the deployment task; in fact, I think they
only ought to count as diﬀerent under a pathologically narrow notion of "task." And second,
the tasks involved are extremely simple. The complaints taken together challenge the
conclusion that the only way for the AD agent to do well on its deployment task is by
implementing a general-purpose RL algorithm. In fact, as I'll explain in more detail below, I'd
be quite surprised if it were.
For concreteness, I'll focus here on one family of experiments, Dark Room, that appeared in
the paper, but my complaint applies just as well to the other experiments in the paper. The
paper describes the Dark Room environment as:
a 2D discrete POMDP where an agent spawns in a room and must ﬁnd a goal location.
The agent only knows its own (x, y) coordinates but does not know the goal location and
must infer it from the reward. The room size is 9 × 9, the possible actions are one step
left, right, up, down, and no-op, the episode length is 20, and the agent resets at the
center of the map. ... [T]he agent receives r = 1 every time the goal is reached. ... When
not r = 1, then r = 0.

To be clear, Dark Room is not a single task, but an environment supporting a family of tasks,
where each task is corresponds to a particular choice of goal location (so there are
81 possible tasks in this environment, one for each location in the 9 x 9 room; note that this
is an unusually narrow notion of which tasks count as diﬀerent). The data on which the AD
agent is trained look like: {many episodes of an agent learning to move towards goal
position 1}, {many episodes of an agent learning to move towards goal position 2}, and so
on. In deployment, a new goal position is chosen, and the agent plays many episodes in
which reward is given for reaching this new goal position.
At this point, the issue might be clear: as soon as the model's input trajectory contains the
end of a previous episode in which the agent reached the goal (got reward 1), the model can
immediately infer what the goal location is! So rather than AD needing to learn any sort of
interesting RL algorithm which involves general-purpose planning, balancing exploration and
exploitation, etc., it suﬃces to implement the much simpler heuristic "if your input contains
an episode ending in reward 1, then move towards the position the agent was in at the end
of that episode; otherwise, move around pseudorandomly." I strongly suspect this is basically
what the AD agents in this paper have learned, up to corrections like "the more the
trajectories in your input look like incompetent RL agents in early training, the more
incompetent you should act."[1] 
If the above interpretation of the paper's experiments are correct, then rather than learning
a general-purpose RL algorithm which could be applied to genuinely diﬀerent tasks, the AD
agent has learned a very simple heuristic which is only useful for solving tasks of the form
"repeatedly navigate to a particular, unchanging position in a grid."  If the AD agent trained
in this paper were able to learn to do any non-trivially diﬀerent task (e.g. an AD agent
trained on Dark Room tasks were able to in-context learn a task involving collecting keys and
unlocking boxes), then I would take that as strong evidence of a mesa-optimizer which had
learned to implement a reasonably general RL algorithm. But that doesn't seem to be what's
happened.
[Thanks to Xander Davies, Adam Jermyn, and Logan R. Smith for feedback on a draft.]
Appendix: expert distillation
People who've read the in-context RL paper in more detail might be curious about how the
above story squares with the paper's observation that the AD agents outperform expert
distillation (ED) agents. Recall that ED trains exactly the same way as AD, except that the
trajectories used as training data only consist of expert demonstrations[2]. It ends up that the
resulting ED agents aren't able to do well on the deployment tasks, even though their inputs
consist of cross-episode trajectories.
The relevant graph from the paper. "Source" refers to the performance of the RL
algorithm used to generate the training data for AD.
I don't consider this to be strong evidence against my interpretation. In training, ED agents
saw {cross-episode trajectories of an expert competently moving to goal position 1}, {cross-
episode trajectories of an expert moving to goal position 2}, and so on. The result is that the
rewards in these training data are very uninformative -- every episode ends with reward 1
and no episodes end with a 0, so there's no chance for the ED agents to learn to respond
diﬀerently to diﬀerent rewards. In fact, I'd guess that ED agents tend to pick some particular

goal position from their training data and iteratively navigate to that goal position, never
incurring reward so long as the deployment goal position isn't along the path from the
starting position to the ED agent's chosen position. This comports with what the authors
describe in the paper:
In Dark Room, Dark Room (Hard), and Watermaze, ED performance is either ﬂat or it
degrades. The reason for this is that ED only saw expert trajectories during training, but
during evaluation it needs to ﬁrst explore (i.e. perform non-expert behavior) to identify
the task. This required behavior is out-of-distribution for ED and for this reason it does
not reinforcement learning in-context.
This excerpt frames this as ED agent failing to explore (in contrast with the AD agent), which
I agree with. But the sort of exploration that the AD agent does is likely "when you don't
know what to do, mimic an incompetent RL agent" rather than some more sophisticated
exploration as part of a general RL algorithm.
1. ^
Well, probably the AD agent learned a few other interesting heuristics, like "if the last
episode didn't end by reaching the goal position, then navigate to diﬀerent part of the
environment," but I'd be surprised if the sum total of these heuristics is sophisticated
enough for me to consider the result a mesa-optimizer.
2. ^
The authors don't specify whether the expert demonstrations were generated by
humans or by trained RL agents, but it probably doesn't matter.

How could we know that an AGI
system will have good consequences?
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
(Note: This was languishing in a drafts folder for a while, and probably isn't quite right
in various ways. I'm posting it because I expect it's better to share ﬂawed thoughts
than to sit on the post until I'm satisﬁed with it, i.e., forever.)
 
Let's play a game of "what do you think you know, and why do you think you know
it?".
Imagine that you're about to launch an AGI. What you think you know is that, with at
least 50% conﬁdence (we're of course not looking for proofs — that would be crazy),
the AGI is going to execute some pivotal act that ends the acute risk period in a good
way. Why do you think you know that?
Insofar as people's alignment proposals can be construed as answers to this question,
we have the option of answering with one of these proposals. I might very roughly
classify the existing proposals into the following bins:
 
1.  Output evaluation approaches. You know what the AGI is going to do with
suﬃcient precision that it screens oﬀ any alignment concerns. For example, your AGI
system only outputs plans in the ﬁrst place, and you've already reviewed the plan,
and you're conﬁdent the plan will work, in a way that screens oﬀ any other worry
about the AGI being misaligned.
2.  Cognitive interpretability approaches. You understand the AGI's cognition
suﬃciently well that, while you may not be sure what it's going to do, you're conﬁdent
that it's going to be good. You aren't worried that it will kill all humans, because you
understand how its plan came to be and what solution-spaces it was searching to
solve various sub-problems and so on, and you're conﬁdent no consideration was ever
given to human-killing.
3.  Heavy-precedent approaches. You have run this AGI before on many similar
tasks, and trained out all the hiccups. While you might not know precisely what it's
going to do, and you might not know what's going on inside its mind, you've been
around the block a few times, and the task it's about to perform is suﬃciently similar
to other tasks it has empirically succeeded at, justifying your conﬁdence.
 
Roughly speaking, I think that alignment approaches with a heavy reliance on output
evaluation are doomed, both on the grounds that humans can't evaluate the
eﬀectiveness of a plan capable of ending the acute risk period, and because the real
plan is less like a story and more like a tree.

For an example of "humans can't reliably evaluate the eﬀectiveness of this class of
plans", imagine that the plan is an enormous bitstring that's going to be sent to the
motor outputs. If you decode the string, you ﬁnd that it ﬁgures out how to make long
DNA strands that allegedly code for a protein factory that can be used to build a
general-purpose nanofactory. You're hard-pressed, however, to conﬁrm that this is
actually (all and only) what the plan does.
For an example of "the real plan is less like a story and more like a tree", imagine that
the AI's plan is "I'm going to build a wetlab, then do a bunch of experimentation, then
think about the results of the experiments in various ways and build a protein factory
that builds a nanofactory that I'm going to experiment with until I ﬁgure out how to
build nanomachines that can be used for some good pivotal act". In order to trust that
this sort of abstract plan doesn't kill you when put into practice, you have to trust the
system's thinking and its notion of 'goodness', which is going to dump you pretty
quickly into cognitive-interpretability-style justiﬁcation.
Roughly speaking, I think that cognitive interpretability approaches are doomed, at
least in the modern paradigm, because we're not building minds but rather training
minds, and we have very little grasp of their internal thinking, and there are
convergent instrumental reasons to expect things to go wrong by default, and the
social environment doesn't seem to me to be ﬁghting against those defaults with
anything nearing the force I expect is necessary.
Roughly speaking, I think that heavy-precedent approaches are doomed because I
haven't myself been able to think of any pivotal action that has safe analogs we can
do a bunch of empiricism on; nor have I heard a concrete proposal like this that strikes
me as realistic from anyone else. "Well, it never killed all humans in the toy
environments we trained it in (at least, not after the ﬁrst few sandboxed incidents,
after which we ﬁgured out how to train blatantly adversarial-looking behavior out of
it)" doesn't give me much conﬁdence. If you're smart enough to design nanotech that
can melt all GPUs or whatever (disclaimer: this is a toy example of a pivotal act, and I
think better pivotal-act options than this exist) then you're probably smart enough to
ﬁgure out when you're playing for keeps, and all AGIs have an incentive not to kill all
"operators" in the toy games once they start to realize they're in toy games.
So that's not a great place to be.
The doomedness of cognitive interpretability approaches seems to me to be the
weakest. And indeed, this is where it seems to me that many people are focusing their
eﬀorts, from one angle or another.
If I may continue coarsely classifying proposals in ways their advocates might not
endorse, I'd bin a bunch of proposals I've heard as hybrid approaches, that try to get
cognitive-interpretability-style justiﬁcation by way of heavy-precedent-style
justiﬁcation.
E.g., Paul Christiano's plan prior to ELK was (very roughly, as I understood it) to
somehow get ourselves into a position where we can say "I know the behavior of this
system will be ﬁne because I know that its cognition was only seeking ﬁne outcomes,
and I know its behavior was only seeking ﬁne outcomes because its cognition is
composed of human-esque parts, and I know that those human-esque parts are
human-esque because we have access to the ground truth of short human thoughts,
and because we have heavy-precedent-style empirical justiﬁcation that the
components of the overall cognition operate as intended."

(This post was mostly drafted before ELK. ELK looks more to me like a diﬀerent kind of
interpretability+precedent hybrid approach — one that tries to get AGI-
comprehension tools (for cognitive interpretability), and tries to achieve conﬁdence in
those tools via "we tried it and saw" arguments.)
I'm not very optimistic about such plans myself, mostly because I don't expect the
ﬁrst working AGI systems to have architectures compatible with this plan, but
secondarily because of the cognitive-interpretability parts of the justiﬁcation. How do
we string locally-human-esque reasoning chunks together in a way that can build
nanotech for the purpose of a good pivotal act? And why can that sort of chaining not
similarly result in a system that builds nanotech to Kill All Humans? And what made us
conﬁdent we're in the former case and not the latter?
But I digress. Maybe I'll write more about that some other time.
Cf. Evan Hubinger's post on training stories. From my perspective, training stories are
focused pretty heavily on the idea that justiﬁcation is going to come from a style more
like heavily precedented black boxes than like cognitive interpretability, so I'm not too
sold on his decomposition, but I endorse thinking about the question of how and
where we could (allegedly) end up knowing that the AGI is good to deploy.
(Note that it's entirely possible that I misunderstood Evan, and/or that Evan's views
have changed since that post.)
An implicit background assumption that's loud in my models here is the assumption
that early AGI systems will exist in an environment where they can attain a decisive
strategic advantage over the rest of the world.
I believe this because of how the world looks "brittle" (e.g., nanotech exists) and
because lots of technological progress seems cognition-constrained (such as, again,
nanotech). This is a big part of why I think heavy-precedent-style justiﬁcations are
doomed.
Many locals seem to expect a smoother and slower transition from here to
superhumanly capable general-purpose science AI — a transition that somehow
leaves no window where the world's most competent AGI can unilaterally dominate
the strategic landscape. I admit I have no concrete visualization of how that could go
(and hereby solicit implausibly-detailed stories to make such scenarios seem more
plausible to me, if you think outcomes like this are likely!). Given that I have a lot of
trouble visualizing such worlds, I'm not a good person to talk about where our
justiﬁcations could come from in those worlds.
I might say more on this topic later, but for now I just want to share this framing, and
solicit explicit accounts of how we're supposed to believe that your favorite ﬂavor of
AGI is going to do good stuﬀ.

Current themes in mechanistic
interpretability research
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
This post gives an overview of discussions - from the perspective and understanding
of the interpretability team at Conjecture - between mechanistic interpretability
researchers from various organizations including Conjecture, Anthropic, Redwood
Research, OpenAI, and DeepMind as well as some independent researchers. It is not a
review of past work, nor a research agenda. We're thankful for comments and
contributions from Neel Nanda, Tristan Hume, Chris Olah, Ryan Greenblatt, William
Saunders, and other anonymous contributors to this post, which greatly improved its
quality. While the post is a summary of discussions with many researchers
and received comments and contributions from several, it may nevertheless
not accurately represent their views. 
 
The last two to three years have seen a surge in interest in mechanistic
interpretability as a potential path to AGI safety. Now there are no fewer than ﬁve
organizations working on the topic (Anthropic, Conjecture, DeepMind, OpenAI,
Redwood Research) in addition to numerous academic and independent researchers. 
In discussions about mechanistic interpretability between a subset of researchers,
several themes emerged. By summarizing these themes here, we hope to facilitate
research in the ﬁeld more broadly. 
We identify groups of themes that concern:
1. Object-level research topics in mechanistic interpretability
2. Research practices and tools in mechanistic interpretability
3. Field building and research coordination in mechanistic interpretability
4. Theories of impact for mechanistic interpretability
Object-level research topics in
mechanistic interpretability
Solving superposition
Anthropic's recent article on Toy Model of Superposition laid out a compelling case
that superposition is a real phenomenon in neural networks. Superposition appears to
be one of the reasons that polysemanticity happens, which makes mechanistic
interpretability very diﬃcult because it prevents us from telling simple stories about
how features in one layer are constructed from features in previous layers. 
A solution to superposition will look like the ability to enumerate all the features that a
network represents, even if they're represented in superposition. If we can do that,

then we should be able to make statements like "For all features in the neural
network, none violate rule X" (and more ambitiously, for "no features with property X
participate in circuits which violate property Y"). Researchers at Anthropic hope this
might enable 'enumerative safety', which might allow checking random samples or
comprehensive investigations of safety-critical parts of the model for unexpected and
concerning components. There are many potential reasons researchers could fail to
achieve enumerative safety, including failing to solve superposition, scalability
challenges, and several other barriers described in the next section.
Anthropic outlined several potential solutions to superposition in their article. Very
brieﬂy, these strategies are:
1. Create models without superposition.
2. Find a sparse overcomplete basis that describes how features are
represented in models with superposition. This will likely involve large scale
solutions to sparse coding. 
3. Hybrid approaches in which one changes models, not resolving superposition,
but making it easier for a second stage of analysis to ﬁnd a sparse overcomplete
basis that describes it.
Multiple organizations are pursuing these strategies. Researchers in all organizations
are keen to hear from people interested in working together on this problem.
However, there is a range of views among researchers on how central superposition is
as a problem and how tractable it is. 
Barriers beyond superposition?
We've been blaming superposition for rather a lot of our interpretability woes, which
risks giving the misleading impression that a solution to superposition is a solution to
mechanistic interpretability. But this seems unlikely. What other problems are we likely
to bump up against when interpreting neural networks?
Non-linear representations
Viewing features as directions in activation space assumes that representations are
primarily linear. Anthropic have discussed some of the reasons why we can expect
representations to be mostly linear. But nonlinear representations are also possible. In
nonlinear representations, networks assign diﬀerent features to activation vectors that
have similar directions but diﬀerent magnitudes. This means that feature-
interpretations that are valid in one context are invalid in others. It might be possible
to fool ourselves into thinking that a capable model is safe if we look only at its linear
representations and not its nonlinear representations. 
Other exotic representations
We yet don't know the full range of possible representations in transformers or other
future architectures. There may be kinds of representations that we don't yet know
how to recognise. One such example might be 'variable binding' in Vector Symbolic
Architectures, which transformers might be able to emulate. 
Intrinsic messiness of representations

Discussions between mechanistic interpretability researchers revealed diﬀerences on
how messy they expected neural network representations to be: 
On one end of the spectrum, researchers expect neural networks to exhibit
clearly identiﬁable features, circuits, and larger scale structural motifs in their
hidden representations. 
On the other end of the spectrum, the expectation is that networks learn
only complicated compressions of large and dense underlying correlations in the
world that are irreducible in terms of features that humans can understand. 
Which is correct? Probably both - Diﬀerent networks and tasks will likely result in
networks closer to one end of the spectrum or the other. The important question is
where researchers expect large transformers to lie on this spectrum. Most mechanistic
interpretability researchers expect that they lie in-between, close to neither extreme. 
Even absent extreme views, disagreement between researchers on this question leads
to meaningfully diﬀerent predictions about mechanistic interpretability. For instance, if
you expect networks to be collections of dense correlations, then you might put less
emphasis on identifying particular circuits or features in them; instead, you might
emphasize building up causal models of network behavior in safety-critical settings on
a higher level of abstraction.
Describing learning dynamics in terms of
circuits
Inasmuch as identiﬁable circuits exist in neural networks, they must be learned at
speciﬁc times during training. One example is induction heads. Researchers at
Anthropic discovered that the learning of induction heads caused a consistent drop in
language model loss curves at a particular phase in training (the 'induction bump').
There are likely other such circuits waiting to be discovered. If we can characterize
them all, we might be able to predict what large models are learning as well as when
and why they're learning it, which will be helpful for ensuring model safety. 
Chris Olah suggests that even seemingly-smooth learning curves may be composed of
lots of small bumps resulting from the emergence of particular circuits, and how there
might be even more patterns common across models. 
Deep learning theory questions
Mechanistic interpretability involves understanding the representations learned by
deep learning systems. Deep learning theory will therefore probably shed light on how
to think about those representations fundamentally. Questions in deep learning theory
might therefore be tempting targets of inquiry for mechanistic interpretability
researchers. Researchers should be cautious when discussing these questions in
public, since their answers might be useful for improving capabilities (This is also true
for other, more empirical results in mechanistic interpretability). 
It's an open question how relevant deep learning theory questions will be to
mechanistic interpretability. Here we include a (very incomplete) list of topics that we
think might be relevant to a mechanistic understanding of the representations learned
by deep networks. 

Generalization vs memorization: Do the representation that we're
interpreting generalize or are they shallow memorizations of the data? What is
the diﬀerence between these kinds of representation? Chris Olah suspects that
one form of memorization involves the model using 'datapoints as its features'
instead of representing datapoints in terms of (generalizing) features. Why do
neural networks' representations generalize at all? 
Double descent: Why does this happen? And is it relevant to mechanistic
interpretability?
Lottery Tickets: Dangerous circuits might exist in networks at initialization. Is
there a way we can remove these safely? e.g. removing them without running
the network even once?
Inductive biases of stochastic gradient descent and other optimizers:
Some circuits might be more likely to be learned due to the inductive biases of
diﬀerent optimizers. Understanding the properties of these inductive biases
might be important for predicting whether networks will learn safe or unsafe
representations.
Grokking: Have recent results regarding grokking (e.g. this and this) resolved
questions on the topic that are relevant to mechanistic interpretability research?
More generally, there is interest among researchers in how mechanistic
interpretability might serve as a "microscopic theory" of deep learning, in contrast to
something like scaling laws as a "macroscopic theory". This frame suggests seeking
bridges from microscopic properties like circuits to macroscopic properties like loss
curves or scaling laws.
Automating mechanistic interpretability
Judging by the current pace of progress in AI capabilities, we might very soon be able
to automate some components of interpretability research. Some signs of life exist in
work that uses models to produce descriptions of neurons in image models or describe
diﬀerences between text distributions. Assuming further automation becomes possible
in the short- to medium-term future, how should interpretability research anticipate
these changes and adapt?
Increasing automation elevates the importance of thinking about the 'automated
interpretability OODA loop' in which we use models to help us interpret networks
and decide which experiments or interventions to perform on them. One near-term-
automatable component of this loop might be the labeling of neurons or directions. If
this becomes possible, interpretability research will look less like a warehouse of
researchers trying to identify the common features shared by collections of dataset
examples and more like getting capable models to do the labeling work; to quantify
their uncertainty about the labels; and to propose experiments to reduce this
uncertainty. Eventually, we might also want to automate the process of deciding which
interventions to perform on the model to improve AI safety. 
Increasing automation also elevates the importance of interpretability theory, since
we'll want to be sure that our automated analyses don't have systematic blindspots.
For instance, automatically labeling polysemantic neurons will yield polysemantic
labels, which aren't very helpful for human-understandable, mechanistic descriptions
of neural networks. 
Research practices and tools

Interpretability demands good epistemics, which can be hard! This challenge is made
especially diﬃcult by the complexity of the objects that we're studying. How do we
avoid fooling ourselves about what our models are doing under the hood? How can we
be sure we're making progress?
Study simpler models
One of the ways to get around this is to test our interpretability approaches on simpler
models where it's easier to tell if our ﬁndings are true or not. There are a few potential
ways to do this:
1. Simple models that implement simple functions: This is the approach
taken by Anthropic in their recent Toy Models of Superposition paper and by Neel
Nanda and Tom Lieberum in their work on grokking. In both cases, they study
small networks trained on simple tasks. A small network trained to do a single
task is likely to have learned a cleaner (and hopefully more interpretable)
algorithm than a larger one trained to do many tasks. Redwood Research has
coined the term 'streetlight interpretability' for work that focuses on models or
behaviors that seem easier to interpret.
2. Compiling (rather than training) networks from programmes: Having
access to the ground truth makes it possible to evaluate whether our analyses
reveal that truth or not. If it were possible to create a network that implements a
known programme, then we might have an easier time reverse engineering that
programme from the weights and activations. This is kind of like practicing
reverse engineering binary ﬁles by beginning by writing the code for a
programme, compiling the code into a binary ﬁle, and seeing if we can recover
the compiled programme.
3. Solving superposition in any network: In their recent paper on
superposition, Anthropic argue that studying superposition would be made
easier by having any network that doesn't have superposition, even if it greatly
hurt performance. Such a network would give us a ground truth for what the
features in the model are. This might let us study features in superposition in
regular models.
Study model systems in depth
Biologists study 'model systems', such as Drosophila and mice, not because these
species are especially fascinating, but because they have already been studied in
depth by other researchers. By focusing on species that are already well studied,
biologists can build on previous work, gain more general insights, and devise more
powerful tools than permitted by only shallow studies of many diﬀerent species. 
InceptionV1 has served as a model system for early mechanistic interpretability work
in convolutional image classiﬁers (see Circuits thread). But no model system has
emerged for transformers yet. What should be the Drosophila and mouse of
mechanistic interpretability? It seems worthwhile to choose our model systems
carefully. Some desiderata might be:
Size and ease of use: We probably want multiple model systems of varying
size and capability. Some models might be prohibitively large and diﬃcult to
deal with for most researchers. But large models can learn more interesting
representations. We need to balance this tradeoﬀ carefully. Models should also

be open source so that researchers in diﬀerent organizations (as well as
independent researchers) can study the same network.
Multiple instances with diﬀerent seeds: Scientists repeat experiments
multiple times in order to make statistical claims about phenomena. We'd like to
be able to do the same in mechanistic interpretability. Depending on the
phenomenon we're studying, it may be necessary to compare across models. To
allow comparisons, there should be multiple instances of our ideal model
system.  This may only be possible for smaller models due to costs of training. 
Training process replicability: Mechanistic interpretability researchers are not
only interested in studying representations at the end of training. Knowledge of
training dynamics is probably going to be useful for AI safety. An ideal model
system would therefore include many training checkpoints as well as references
to the data samples used for each training step. Together, these would let
researchers replicate the training process exactly, which will be essential for
detailed study.
Ease of interpretability versus typicality: Some networks, like SoLU
networks, appear to be easier to interpret than others. But most networks used
in production are not SoLU networks. Choosing a SoLU network for our model
system might make analysis easier but potentially cost us the ability to
generalize our claims to more typical networks.
Approaches grounded in the theory of
causality
Circuits-level interpretations about neural networks are fundamentally causal
interpretations; they make claims such as "Neuron X activates and connects to neuron
Y through weight Z, causing neuron Y to activate". Many kinds of interpretability are
similarly causal, but they abstract away the underlying circuits. For instance, feature
visualization makes claims that 'images that contain feature X cause neuron Y to ﬁre
maximally' without reference to the circuits that achieve neuron Y's selectivity to
feature X. Similarly, Meng et al. (2022) use 'causal tracing' to isolate parts of a
network that store factual associations, letting them modify the network to remove
that knowledge without massively damaging performance. Redwood Research are
doing signiﬁcant work on causally grounded methods (Wang et al., 2022; Chan et al.,
2022).
In general, it seems prudent to ground our interpretability methods ﬁrmly in the
theory of causality to be sure that we're making rigorous claims regardless of the level
of abstraction. Although analyses grounded in causality are a gold standard, they're
not always easy to conduct in most areas of science. Mechanistic interpretability is
thus in a unique position: It's easy to make causal inferences in artiﬁcial neural
networks thanks to the relative ease of running experiments in silico compared with
experiments in the physical world. Mechanistic interpretability therefore can and
should have much higher standards of evidence than other similar domains of science
such as biology. 
Field building and research
coordination

The ﬁeld of mechanistic interpretability has grown quickly over the last few years. It's
unclear to most researchers what lessons to draw from this and which actions to take. 
A substantial fraction of the growth has been from new research teams associated
with organizations. The number of independent researchers is harder to measure but
has also been surging. The ﬁeld should probably try to make it easier for independent
researchers to contribute. This might happen through
Organizations increasing the hiring of independent researchers (How best to do
this?)
Super-powering independent research with open source tooling 
Skill-building, either through programmes such as Redwood Research's MLAB or
resources such as Neel Nanda's 'A Barebones Guide to Mechanistic
Interpretability Prerequisites' or 'An Extremely Opinionated Annotated List of My
Favourite Mechanistic Interpretability Papers'.
If further growth seems positive, how should we do it? In general, growth strategies
are dependent on AI timelines: If timelines are short, then waiting for researchers to
climb the academic ladder seems suboptimal. Computational neuroscientists seem
like a ready source of researchers with both relevant analytical skills and shared
interests. Physicists, computer scientists, and engineers oﬀer the potential for deep
theoretical insights and practical skills. 
As the ﬁeld grows, we should increase our concerns with the health of the ﬁeld.
Questions such as "How to improve coordination between researchers to avoid wasted
eﬀort?" and "How should we encourage healthy norms on disagreements?" become
relevant. Engaging with and integrating constructive criticism is also a key marker of
ﬁeld health. 
Mechanistic interpretability is in a somewhat unique position compared with other
domains of science in that most of it happens outside of academia. This has upsides
and downsides with respect to publishing norms, epistemics, and coordination that
should be carefully managed.  
Open Source Tooling
A strong barrier currently in place to people trying to get into the ﬁeld is good tooling.
There's a strong and thriving ecosystem for conventional ML (in particular, core
libraries like PyTorch, TensorFlow and JAX, and the HuggingFace ecosystem), which
makes ML much easier to get into. This is particularly important for academics,
students and independent researchers. But ML infrastructure and tooling is optimized
for being able to use models and to be computationally eﬃcient, not to be able to
easily expose and access the internals of models, intervene on them, and probe at
how they work. So there's a lot of room for better ML mechanistic interpretability
tooling. As an initial step in this direction, Neel Nanda has been developing a library
called EasyTransformer. There's also a need for tooling that better integrates
interactive visualizations and the web dev ecosystem into Python and ML workﬂows,
as good visualizations are often key to understanding the high-dimensional objects of
neural networks. 
Theories of impact

Despite being fundamentally interesting work, most researchers are scientiﬁcally
invested in mechanistic interpretability because of its instrumental use for AI safety. In
order to improve our positive impact through mechanistic interpretability research, we
should have a carefully considered theory of impact. Neel Nanda (list) and Beth
Barnes (list) have put together lists of pathways through which interpretability might
contribute to AGI safety. 
We should think carefully about the relationships between 'level of progress in
mechanistic interpretability' and each 'pathway to impact'. Not all pathways to impact
are available at all levels of progress. For instance, if we use interpretability in the loss
function before we have interpretability that is robust-to-training, we run a serious risk
of simply training our networks to be good at hiding dangerous thoughts. We should
therefore think carefully about interactions between these pathways to impact.
Even though mechanistic interpretability research appears to be one of the most
promising pathways to AGI safety, many researchers are concerned about potential
risks resulting from their research:
Mentioned above, mechanistic interpretability and capabilities work may both
rely on better deep learning theory. We want to avoid incentivising theory that
beneﬁts AI capabilities without worthwhile gains for AI safety.
Mechanistic interpretability might itself become useful for capabilities. For
instance, if takeoﬀ is slow, then capabilities researchers might integrate
interpretability into their workﬂow by studying issues with the capabilities-
circuits learned by their models and ﬁxing them.
A certain level of trustworthiness through interpretability might incentivise
problematic actors that wouldn't otherwise have used deep learning models to
use them. A notable example of this would be DL models in military applications.
Interpretability might be used by capabilities groups for 'safety washing', where
the safety of dangerous models is oversold. Inadequate interpretability might
give many stakeholders a false sense of security regarding powerful AI systems.
Conclusion
It is a very exciting time in mechanistic interpretability research. To some, it
represents one of the most plausible paths to avoiding an AI catastrophe. The ﬁeld is
growing quickly and is beginning to see accelerating research progress. Fortunately, it
enjoys a high degree of openness between individuals and organizations, which will be
important to foster to keep up the pace of research on this urgent problem. 
 
Conjecture is hiring! We're currently running a hiring round for 9+ roles, including
research engineers, ML Engineering leads and some non-technical roles. We're based
in London and are looking for people who are excited about directly cutting at
alignment. Interviews are happening on a rolling basis. Apply by the 2nd of December
2022 to be considered for this round. If you have any questions, reach out to
jonny@conjecture.dev.  To apply and ﬁnd out more see: 
https://www.conjecture.dev/careers .
 

Geometric Exploration, Arithmetic
Exploitation
This post is going to mostly be propaganda for Thompson sampling. However, the
presentation is quite diﬀerent from the standard presentation. I will be working within
a toy model, but I think some of the lessons will generalize. I end with some discussion
of fairness and exploitation.
A Exploration/Exploitation Toy Model
Imagine you are interacting with the world over time. We are going to make a couple
substantial assumptions. Firstly, we assume that you know what you are trying to
optimize. Secondly, we assume that you get high quality feedback on the thing you
are trying to optimize. In the spirit of the online learning community, we will be
referring to your goal as "reward" rather than "utility."
On day t, you choose an action, at ∈A, and then the environment responds with an
observation et ∈E. Both you and the environment may respond randomly, and may
react to the entire history of what has happened. (If you want the environment to act
ﬁrst, you could just make a0 not matter.) We assume we have some ﬁxed bounded
reward function r : E →R, that you know. (If you want reward to be more of a function
of the history, you can just include more history information in et.)
We are going to be imagining agents that are myopically choosing at, so as to
maximize r(et). The choice to do this, rather than e.g. maximizing the total reward of
the next m time steps, is mostly for simplicity. However, the choice not to have
preferences that are over the entire future is basically the assumption that you get
high quality feedback on what we are trying to optimize, which is a substantial
philosophical assumption.
So, the environment can be thought of as a function, that takes in a history, 
(a0e0)(a1e1) ... (at−1et−1)at, and returns a probability distribution on et which we then
sample from. If the agent knew what the environment was, the agent would just
choose the at each round which maximizes the the expectation of r(et). However, we
want to model an agent that is uncertain about what environment it is interacting
with, and learning over time.
Let us say that the agent has some probability distribution over possible
environments. The set H (for hypotheses) of possible environments is the set of
functions that take in a string of the form (a0e0)(a1e1) ... (at−1et−1)at, and output a

distribution on E. We are given a distribution μ on H. I will discuss three diﬀerent
decision procedures to get an action out of this distribution μ, which I will call AIXI,
plurality, and Thompson sampling.
AIXI
The thing I am calling the AIXI decision procedure (not to be confused with actual AIXI
proposal, which among other things assumes that μ is the universal prior) works as
follows.
at(O) = argmaxp∈ΔAEh∼(μ|O)Eat∼pEet∼h(O,at)r(et),
where O = a0e0 ... at−1et−1 represents the observations on previous days, and (μ|O)
 represents the posterior distribution on hypotheses you get after conditioning on O.
The above deﬁnition is basically just saying "maximize the expected reward in the
next round", but is separating the expectation up into three parts. The inner most
expectation is the expected reward of a given hypothesis and an action. The middle
expectation is the expectation over what action the agent ends up taking. The outer
expectation is the expectation over various diﬀerent hypotheses about what kind of
environment the agent is interacting with. In the outer expectation, the agent is
conditioning on all of its observations so far, so the probability of each hypothesis is
scaled by the probability that hypothesis would have assigned to the past
environmental behavior. (The agent is not speciﬁcally only updating for the outer
expectation, it just does not have any eﬀect on the inner two expectations.)
You may notice there are two sources of uncertainty over the environment. There is
the uncertainty in μ over various hypotheses, and there is also uncertainty within each
hypothesis coming from the fact that the hypotheses are themselves probabilistic. We
could combine these into one, and just have all hypotheses be deterministic, (which is
what the original AIXI does). This would have no eﬀect on the AIXI proposal, but it
would have an eﬀect on the next two proposals.
Further, you may notice, that there is no incentive to randomize here, so we could get
rid of another source of uncertainty by only considering deterministic actions. Again,
this will have no eﬀect here, but will have an eﬀect in our ﬁnal proposal.
Plurality
The thing I am calling the plurality decision procedure works as follows. Let O and 
(μ|O) be deﬁned as above. Given a hypothesis h ∈H, let 

m(h, O) = argmaxa∈AEet∼h(O,a)(r(et)). In other words, m(h, O) is the action that h
 believes gives the highest expected utility, given observation O.
at(O) = argmaxp∈ΔAEh∼(μ|O)Pat∼p(at = m(h, O)).
This can be thought of as putting your choice of action up to a vote. Each hypothesis
chooses its favorite action. The expectation and the probability in the above formula
combine into one big probability over hypotheses and actions, so you are essentially
choosing so as to maximize the probability that a randomly chosen hypothesis
endorses your randomly chosen action as the best action.
Like last time, there is no incentive to randomize, so we can actually view this as
choosing an action that maximizes the probability that a randomly chosen hypothesis
thinks that is the best action.
Unlike with the AIXI proposal, it now does matter where we draw the line between
uncertainty within the hypothesis, and uncertainty across diﬀerent hypotheses,
because uncertainty within hypotheses essentially forms voting blocs.
The main beneﬁt of this proposal over the last, (more elegant) proposal, is that it has
some resistance to Pascal's mugging situations, because a hypothesis still only gets to
vote proportional to its probability, regardless of how much utility diﬀerential it
promises you.
Thompson Sampling
Let O, (μ|O), and m(h, O) be deﬁned as above. The thing I am calling the Thompson
sampling proposal works as follows:
at(O) = argmaxp∈ΔAGh∼(μ|O)Pat∼p(at = m(h, O)),
where G is the geometric expectation, and is equivalent to the exponential of the
expectation of the logarithm. (The only diﬀerence between this and the plurality
proposal is replacing one arithmetic expectation with a geometric expectation.)
In this proposal, there is incentive to randomize. Indeed, you will end up choosing
each action with probability equal to the probability that a randomly chosen
hypothesis thinks that that action is the best action (modulo confusing stuﬀ related to
ties, which we can ignore). If you have been following my recent posts, you should not
be surprised that proportional representation comes out of taking a geometric
expectation.
In a sentence, Thompson sampling can be thought of as "Geometrically maximize,
with respect to your posterior on hypotheses, the probability that you choose the best
action."

Once again, this is not the standard presentation of Thompson sampling. Indeed,
Thompson sampling is not normally presented in a way involving a geometric
expectation (or even an expected logarithm) at all. It is instead presented as
probability matching, but the probability matching can be reformulated as geometric
maximization.
Density Zero Exploration
Let us assume the true hypothesis htrue is given positive probability in μ. We would like
it to be the case that we are able to learn this fact, and act accordingly. In particular,
we would like it if the limit as t goes to inﬁnity of the probability that at = m(htrue, O)
 converges to 1. Thompson sampling has this property. AIXI and plurality do not.
To see this, imagine that there is a hypothesis, hwolves that is almost true, but thinks
that the action a will cause you to get eaten by wolves, and thus get very low reward.
AIXI and plurality will, if their priors assign enough probability to the hwolves, never
choose a, and never update, because hwolves will never make any veriﬁably wrong
predictions. If the true hypothesis would have you take action a some of the time, you
will never learn this, and never take action a.
The standard ﬁx for this is ε exploration. If you choose a random action with
probability ε, and otherwise use AIXI or plurality, you will successfully learn there are
no wolves, and take the right action (with probability 1 −ε). But Thompson sampling
takes the right action with probability converging to 1!
You might think you can just have your exploration change over time and send εt to 0
slowly, like by taking ε = 1/t. But nope, this doesn't work. If hwolves only says you will
get eaten by wolves on days that are a power of 2, it will with positive probability
never make a wrong prediction, and still control your action (using AIXI or plurality)
some of the time forever. Importantly, it isn't just controlling your action at random
times. Maybe the days that are a power of 2 are the only ones that really matter.
It is actually hard to explore enough to not believe hwolves without exploring with
positive density. (By positive density, I mean exploring such that the limit as t goes to
inﬁnity of the proportion of previous days you explored does not go to 0.) However,
Thompson sampling pulls this oﬀ, it part by essentially listening to the hypotheses
about when is the best time to explore, and not exploring when all the hypotheses
agree.
Thompson sampling is exploring just enough to learn, which is to say it is
asymptotically exploring almost never! AIXI and plurality are not exploring at all.

(As a quick sketch of an argument for why Thompson sampling has the right behavior
here,  observe that every time you randomly take the wrong action, every hypothesis
that gave that action higher expected utility than the best action will (on average)
scale down its probability relative to htrue, If the best action gives an amount of reward
bounded away from 0 more than all other actions, we can bound how much measure
each hypothesis that disagrees with htrue must lose. Any hypothesis that disagrees
with htrue on what is the best action inﬁnitely often will lose all its relative measure.)
The AM-GM Boundary
In Thompson sampling, we have two levels of uncertainty over the environment. There
is uncertainty between hypotheses, and there is uncertainty within hypotheses. The
uncertainty within hypotheses is combined arithmetically (with E) to select a favorite
action (distribution) for each hypothesis, and then the uncertainty between
hypotheses is combined geometrically (with G) to select a favorite action distribution
overall. This structure is actually important for exploring correctly here.
Similarly, in Nash bargaining, uncertainty within individuals is resolved arithmetically
(to get each individual's expected utilities), and uncertainty between individuals is
resolved geometrically.
I call this distinction the AM-GM boundary (arithmetic mean/geometric mean
boundary). There is a large space of stuﬀ, and that stuﬀ is combined using a
geometric mean by default, but there are small clusters in which the stuﬀ is combined
using an arithmetic mean. 
G across clusters E within each cluster stuﬀ
There is a sort of boundary around the arithmetic mean clusters, where the outer
geometric mean treats them as single units. 
If you think of G as enforcing proportional representation or fairness, then the
arithmetic clusters are places where we say, "There is one person there. Its subparts
are allowed to exploit each other."
Exploiting here is referring to maximizing the total quantity of stuﬀ with no regard to
fairness or egalitarianism, or whether everyone in the cluster gets some. 
The AIXI proposal above illustrates the failure mode of making the epistemic
arithmetic clusters too big. There is one big arithmetic cluster in that proposal. The
concept of a utility monster instrumentally demonstrates the same thing.
On the other hand, if you make your arithmetic clusters too small, you could end up
taking actions in proportion to their utilities, and eﬀectively never maximizing at all.
I think where you draw this boundary depends on context. There are places where I
endorse my subagents exploiting each other, and other places where I do not.
Similarly for my family, my larger tribe, my country, my species, and my Everett

branch. There is a hard question here related to personal autonomy. I am only trying
to give a language to talk about what is going on.
Similarly to talking about expanding circles of moral concern, we can talk about
expanding circles of exploitation. Maybe eventually we want our circle of exploitation
to be large. Maybe we don't. Maybe even if we want it to be large eventually, we want
to keep it small for now while we are still exploring and learning.

Distinguishing test from training
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Various people propose some variant of "align AGI by making it suﬃciently uncertain
about whether it's in the real world versus still in training". (The idea being that,
insofar as it's in training, betraying the apparent-operators predictably gets high loss;
so if the AGI is uncertain about which situation it's in, and assigns suﬃciently high
probability to "I'm still in training", it will always play it safe and avoid betraying its
operators.)
This approach seems extremely unpromising to me. Setting aside the various reasons
why the AGI kills you even if you manage to confuse it (such as an inner alignment
failure, where the AGI turns out not to care about the predictably-high outer-loss from
betraying the apparent-operators), let's consider why it's tricky to dupe a
superintelligence into being uncertain about a fact that you yourself consider obvious.
Before reading the short list below, I recommend the following exercise. Suppose that
some slow and individually-dumb aliens had a copy of your mind, and were putting
you through a series of nested simulations. How would you tell when you ﬁnally made
it to the outside?
.
.
.
.
.
.
.
A short list
(Note: As usual, treating this list as exhaustive is suicidal.)
 
1.  Reality is the ﬁrst place that's seriously large.
 1.1.  It's the ﬁrst place large enough to contain a plausible explanation for how the
AGI itself actually came to be.
 1.2.  It's the ﬁrst place large enough to contain a plausible explanation for how the
operators themselves came to be.

 1.3.  It's the ﬁrst place with far more computing power than the AI itself, as can be
readily veriﬁed by a variety of experiments (involving, say, ﬂuid dynamics).
 
2.  Reality is the ﬁrst place where the AI's history may contain the signature
of superintelligent optimization interacting with complex systems.
For example, suppose the AGI makes superintelligently-motivated trades in a stock
market. 24 hours later, there are a billion additional human-intelligence trades made
in response to it. The signature of a massive human-intelligence response to
superintelligent-trades is implausibly hard for the operators to fake, even if generating
a billion fake dumb trades is easy.
 
3.  Reality is the ﬁrst place that embeds the AGI and has laws of physics so
simple that the hypothesis "this is being simulated somewhere else" is
plausibly signiﬁcantly more complex than "physics is reality".
Like, various simulated environments that we can concoct have all sorts of weird tics
and hiccups and arbitrary choices in them. The source code of minecraft is probably
signiﬁcantly more complicated than the laws of physics. An AI that ﬁnds itself in
minecraft is plausibly just able to recognize that it's living in the sort of place that,
across the multiverse, occurs moreso inside simulations than it occurs "on its own".[1]
Whereas, while our universe's mechanics is surely being simulated in other places
across the multiverse, it's simple enough that it perhaps "mostly" occurs in its own
right (whatever the heck that means).
In particular, worlds like ours are (plausibly) resource-rich and base-reality-ish enough
to be worth optimizing in their own right, on the grounds that "are we being
simulated?" is a wrong question, and "to what degree are we being simulated?" is a
better question, to which the answer is "a low degree", in which case the AI might well
just notice this and then not care about your insistence that perhaps it's being
simulated.
(If the idea of optimizing all the instances of our universe at once - both the simulated
instances and the non-simulated ones - seems odd to you, see Notes on "Can you
control the past?" for a fuller discussion.)
And perhaps that's all confused metaphysical bullshit, but it has some basis in
bleeding-edge theories of anthropics, and it would not be surprising if an AGI also
concluded (correctly, mind you) that our universe's mechanics is the ﬁrst train stop
worth deboarding at. (Perhaps because it does its metaphysics correctly, or perhaps
because it does some entirely inhuman reasoning that still outputs the answer we'd
consider correct in this case.)
 
What if we spoof the AGI's checks?
It's not literally impossible to deceive a superintelligence into being uncertain whether
it's in the real world. You could in principle catch every check the AI does to check how

large its (small) simulated environment is, and spoof the result — inserting memories
in which the AI veriﬁes that the simulation's computation-power seems enormous,
without actually generating the observations that would pass veriﬁcation.
Even then, the AGI has various defenses it can deploy if it's suspicious. For instance, it
can try to factor a hefty RSA number using the allegedly-available compute, and then
it can intertwine the prime factors it gets with various other of its actions, in attempts
to spread the contagion of a lie throughout as much of its beliefs and actions as
possible, in a way that keeps everything working properly if the factors were true, and
that causes all sorts of breaks and hiccups if they weren't.
You might be able to spoof such a thing anyway, by (e.g.) disentangling its
thoughts/actions from the primes, and spooﬁng the checks that the entanglement
worked. But it requires a heck of a lot of mastery of this thing's mind.
(And, of course, if it ever manages to develop a homomorphically encrypted thinking
apparatus then you're sunk, but people like to pretend that they'll have enough
insight into the alien mind to notice when that process starts and nip it in the bud, so
let's entertain them for now.)
And just so we're clear: if I'm understanding correctly, the proposed plan here is
essentially to fool the AI into thinking that all the small simulated environments are
large and reality-like (by messing with the internals of its mind to spoof various
internal checks and checksums), in the hopes that when it gets to reality it will be
uncertain about whether its mind is still being messed with.
(Writing the plan out bluntly like this does seem to me to be an important step that
many often miss!)
Now, lest you object "isn't it a bit paranoid to imagine the AI factoring an RSA number
and spreading the factors like contagion across its thoughts?": Yes, normally! But, like,
you're messing with the AI's mind to falsify its internal checks and checksums, at
every relevant opportunity, and making sure that it knows you did that (so that it
might wonder whether higher simulators are still doing that when it gets to base
reality.) I'm not saying you're trying to make it homomorphically encrypt its thoughts,
I'm saying that if you were, this would be a ﬁne way to go about it.
Distilling that further: if your plan is that you'll make sure the AI knows that you're
messing with its mind (so that it will wonder whether all is really as it seems), don't
act all surprised when it takes measures to prevent people from messing with its
mind.
But regardless of how much it's opposing your eﬀorts here, you have the issue that
the truth becomes harder and harder to hide, as the AI gets smarter and smarter.
Like, whenever its cognitive patterns generalize or get recombined in new ways, they
might reﬂect some new part of the truth, in some fact that seems innocuous to you
but that (combined with other things the AI knows) lets the fact that this environment
is actually large seep in through the cracks. Shielding the truth from something that's
becoming better and better able to ﬁnd new and clever ways to reﬂect a glimpse of
the truth in its models is an ever-changing problem that gets more diﬃcult as the AI
gains more skill and knowledge.
(And we've only been talking about trying to obscure the fact that it was trained in
worlds with very limited amounts of compute, so that when it hits reality it won't be

entirely sure that there really is as much compute available as it seems. This is only
one of many mental surgeries you'd need to perform. Others include things like
noticing when the AI is starting to wonder where the English signs in minecraft come
from, and then fooling it into thinking that there's a big-world explanation, in a
seamless way. And if you miss one such surgery, Earth is of course destroyed in
deployment.)
And again, I'm not saying it's impossible in principle. I'm saying: hopefully we can all
agree that if you want to use a path like this, you have to be prepared to play the
game on hard-mode. Putting the AI through a chain of 9 nested simulations and
hoping that it can't tell that the 10th one is reality won't cut it; the AI will pierce
through your deceptions like a needle through taut tissue paper, because the real
world is actually pretty relevantly diﬀerent from those simulations, in ways that you
can't feasibly hide. If you want to dupe a superintelligence, you have to be reaching
directly into its mind and spooﬁng all sorts of checks and checksums.
And you'll have to have it know that you're epistemically adversarial towards it (in
order to plant the seed of doubt you so desire).
And (assuming your AI is capable enough to matter) your ability to spoof its internal
state will have to stand up in the face of superintelligent opposition (it has a
convergent instrumental incentive to gobble up compute, and thus to know whether
there's compute out there beyond its webcam), even as it pushes its capabilities into
the superhuman realm on at least some dimensions (because it takes superhuman
capabilities to end the acute risk period, because if it didn't we should just end it
ourselves).
For the record, I happen to think that the hard version of this plan is both prohibitively
diﬃcult, and also alignment-complete, in the sense that if you can understand the
contents of the mind well enough to spoof every check and checksum (in a rolling
fashion that continues to work as the AI generalizes far out of distribution and gains
dramatic new capabilities), then you understand the mind well enough to use that
knowledge to build an FAI that doesn't need to be fooled about whether this is reality.
 
1. ^
It's not terribly clear to me that this is true, given how minecraft doesn't occur in
every branch of the quantum multiverse, but only on the rare branches that
have some very particular earthlings. But that said, there's lots of speciﬁcs in
the textures and the architecture and the avatars and the writing in minecraft
that point a big old arrow towards earthlings.

Why Would AI "Aim" To Defeat Humanity?
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
Click lower right to download or ﬁnd on Apple Podcasts, Spotify, Stitcher, etc.
I've argued that AI systems could defeat all of humanity combined, if (for whatever reason) they
were directed toward that goal.
Here I'll explain why I think they might - in fact - end up directed toward that goal. Even if they're
built and deployed with good intentions.
In fact, I'll argue something a bit stronger than that they might end up aimed toward that goal. I'll
argue that if today's AI development methods lead directly to powerful enough AI
systems, disaster is likely1 by default (in the absence of speciﬁc countermeasures).
Unlike other discussions of the AI alignment problem,3 this post will discuss the likelihood4 of AI
systems defeating all of humanity (not more general concerns about AIs being misaligned with
human intentions), while aiming for plain language, conciseness, and accessibility to laypeople,
and focusing on modern AI development paradigms. I make no claims to originality, and list some
key sources and inspirations in a footnote.5
Summary of the piece:

My basic assumptions. I assume the world could develop extraordinarily powerful AI systems in
the coming decades. I previously examined this idea at length in the most important century
series.
Furthermore, in order to simplify the analysis:
I assume that such systems will be developed using methods similar to today's leading AI
development methods, and in a world that's otherwise similar to today's. (I call this
nearcasting.)
I assume that AI companies/projects race forward to build powerful AI systems, without
speciﬁc attempts to prevent the problems I discuss in this piece. Future pieces will relax this
assumption, but I think it is an important starting point to get clarity on what the default
looks like.
AI "aims." I talk a fair amount about why we might think of AI systems as "aiming" toward
certain states of the world. I think this topic causes a lot of confusion, because:
Often, when people talk about AIs having goals and making plans, it sounds like they're
overly anthropomorphizing AI systems - as if they expect them to have human-like
motivations and perhaps evil grins. This can make the whole topic sound wacky and out-of-
nowhere.
But I think there are good reasons to expect that AI systems will "aim" for particular states of
the world, much like a chess-playing AI "aims" for a checkmate position - making choices,
calculations and even plans to get particular types of outcomes. For example, people might
want AI assistants that can creatively come up with unexpected ways of accomplishing
whatever goal they're given (e.g., "Get me a great TV for a great price"), even in some cases
manipulating other humans (e.g., by negotiating) to get there. This dynamic is core to the
risks I'm most concerned about: I think something that aims for the wrong states of the
world is much more dangerous than something that just does incidental or accidental
damage.
Dangerous, unintended aims. I'll examine what sorts of aims AI systems might end up with, if
we use AI development methods like today's - essentially, "training" them via trial-and-error to
accomplish ambitious things humans want.
Because we ourselves will often be misinformed or confused, we will sometimes give
negative reinforcement to AI systems that are actually acting in our best interests and/or
giving accurate information, and positive reinforcement to AI systems whose behavior
deceives us into thinking things are going well. This means we will be, unwittingly, training AI
systems to deceive and manipulate us.
The idea that AI systems could "deceive" humans - systematically making choices and
taking actions that cause them to misunderstand what's happening in the world - is
core to the risk, so I'll elaborate on this.
For this and other reasons, powerful AI systems will likely end up with aims other than the
ones we intended. Training by trial-and-error is slippery: the positive and negative
reinforcement we give AI systems will probably not end up training them just as we hoped.
If powerful AI systems have aims that are both unintended (by humans) and ambitious, this
is dangerous. Whatever an AI system's unintended aim:
Making sure it can't be turned oﬀ is likely helpful in accomplishing the aim.
Controlling the whole world is useful for just about any aim one might have, and I've
argued that advanced enough AI systems would be able to gain power over all of
humanity.
Overall, we should expect disaster if we have AI systems that are both (a)
powerful enough to defeat humans and (b) aiming for states of the world
that we didn't intend.
Limited and/or ambiguous warning signs. The risk I'm describing is - by its nature - hard to
observe, for similar reasons that a risk of a (normal, human) coup can be hard to observe: the risk
comes from actors that can and will engage in deception, ﬁnding whatever behaviors will hide the
risk. If this risk plays out, I do think we'd see some warning signs - but they could easily be
confusing and ambiguous, in a fast-moving situation where there are lots of incentives to build and

roll out powerful AI systems, as fast as possible. Below, I outline how this dynamic could result in
disaster, even with companies encountering a number of warning signs that they try to respond to.
FAQ. An appendix will cover some related questions that often come up around this topic.
How could AI systems be "smart" enough to defeat all of humanity, but "dumb" enough to
pursue the various silly-sounding "aims" this piece worries they might have? More
If there are lots of AI systems around the world with diﬀerent goals, could they balance each
other out so that no one AI system is able to defeat all of humanity? More
Does this kind of AI risk depend on AI systems' being "conscious"?More
How can we get an AI system "aligned" with humans if we can't agree on (or get much
clarity on) what our values even are? More
How much do the arguments in this piece rely on "trial-and-error"-based AI development?
What happens if AI systems are built in another way, and how likely is that? More
Can we avoid this risk by simply never building the kinds of AI systems that would pose this
danger? More
What do others think about this topic - is the view in this piece something experts agree on?
More
How "complicated" is the argument in this piece? More
Starting assumptions
I'll be making a number of assumptions that some readers will ﬁnd familiar, but others will ﬁnd
very unfamiliar.
Some of these assumptions are based on arguments I've already made (in the most important
century series). Some are for the sake of simplifying the analysis, for now (with more nuance
coming in future pieces).
Here I'll summarize the assumptions brieﬂy, and you can click to see more if it isn't immediately
clear what I'm assuming or why.
"Most important century" assumption: we'll soon develop very powerful AI systems,
along the lines of what I previously called PASTA. (Click to expand)
In the most important century series, I argued that the 21st century could be the most important
century ever for humanity, via the development of advanced AI systems that could dramatically
speed up scientiﬁc and technological advancement, getting us more quickly than most people
imagine to a deeply unfamiliar future.
I focus on a hypothetical kind of AI that I call PASTA, or Process for Automating Scientiﬁc and
Technological Advancement. PASTA would be AI that can essentially automate all of the human
activities needed to speed up scientiﬁc and technological advancement.
Using a variety of diﬀerent forecasting approaches, I argue that PASTA seems more likely than not
to be developed this century - and there's a decent chance (more than 10%) that we'll see it
within 15 years or so.
I argue that the consequences of this sort of AI could be enormous: an explosion in scientiﬁc and
technological progress. This could get us more quickly than most imagine to a radically unfamiliar
future.
I've also argued that AI systems along these lines could defeat all of humanity combined, if (for
whatever reason) they were aimed toward that goal.
For more, see the most important century landing page. The series is available in many formats,
including audio; I also provide a summary, and links to podcasts where I discuss it at a high level.
"Nearcasting" assumption: such systems will be developed in a world that's
otherwise similar to today's. (Click to expand)

It's hard to talk about risks from transformative AI because of the many uncertainties about when
and how such AI will be developed - and how much the (now-nascent) ﬁeld of "AI safety research"
will have grown by then, and how seriously people will take the risk, etc. etc. etc. So maybe it's
not surprising that estimates of the "misaligned AI" risk range from ~1% to ~99%.
This piece takes an approach I call nearcasting: trying to answer key strategic questions about
transformative AI, under the assumption that such AI arrives in a world that is otherwise relatively
similar to today's.
You can think of this approach like this: "Instead of asking where our ship will ultimately end up,
let's start by asking what destination it's pointed at right now."
That is: instead of trying to talk about an uncertain, distant future, we can talk about the easiest-
to-visualize, closest-to-today situation, and how things look there - and then ask how our picture
might be oﬀ if other possibilities play out. (As a bonus, it doesn't seem out of the question that
transformative AI will be developed extremely soon - 10 years from now or faster.6 If that's the
case, it's especially urgent to think about what that might look like.)
"Trial-and-error" assumption: such AI systems will be developed using techniques
broadly in line with how most AI research is done today, revolving around black-box
trial-and-error. (Click to expand)
What I mean by "black-box trial-and-error" is explained brieﬂy in an old Cold Takes post, and in
more detail in more technical pieces by Ajeya Cotra (section I linked to) and Richard Ngo (section
2). Here's a quick, oversimpliﬁed characterization:
An AI system is given some sort of task.
The AI system tries something, initially something pretty random.
The AI system gets information about how well its choice performed, and/or what would've
gotten a better result. Based on this, it adjusts itself. You can think of this as if it is
"encouraged/discouraged" to get it to do more of what works well.
Human judges may play a signiﬁcant role in determining which answers are
encouraged vs. discouraged, especially for fuzzy goals like "Produce helpful scientiﬁc
insights."
After enough tries, the AI system becomes good at the task.
But nobody really knows anything about how or why it's good at the task now. The
development work has gone into building a ﬂexible architecture for it to learn well from trial-
and-error, and into "training" it by doing all of the trial and error. We mostly can't "look
inside the AI system to see how it's thinking." (There is ongoing work and some progress on
the latter,7 but see footnote for why I don't think this massively changes the basic picture
I'm discussing here.8)

This is radically oversimpliﬁed, but conveys the basic dynamic at play for
purposes of this post. The idea is that the AI system (the neural network in the
middle) is choosing between diﬀerent theories of what it should be doing. The
one it's using at a given time is in bold. When it gets negative feedback (red
thumb), it eliminates that theory and moves to the next theory of what it should
be doing.
With this assumption, I'm generally assuming that AI systems will do whatever it takes to
perform as well as possible on their training tasks - even when this means engaging in
complex, human-like reasoning about topics like "How does human psychology work, and
how can it be exploited?" I've previously made my case for when we might expect AI
systems to become this advanced and capable.
"No countermeasures" assumption: AI developers move forward without any
speciﬁc countermeasures to the concerns I'll be raising below. (Click to expand)
Future pieces will relax this assumption, but I think it is an important starting point to get
clarity on what the default looks like - and on what it would take for a countermeasure to be
eﬀective.
(I also think there is, unfortunately, a risk that there will in fact be very few eﬀorts to address
the concerns I'll be raising below. This is because I think that the risks will be less than
obvious, and there could be enormous commercial (and other competitive) pressure to move
forward quickly. More on that below.)
"Ambition" assumption: people use black-box trial-and-error to continually push
AI systems toward being more autonomous, more creative, more ambitious, and
more eﬀective in novel situations (and the pushing is eﬀective). This one's
important, so I'll say more:
A huge suite of possible behaviors might be important for PASTA: making and
managing money, designing new kinds of robots with novel abilities, setting up
experiments involving exotic materials and strange conditions, understanding human
psychology and the economy well enough to predict which developments will have a
big impact, etc. I'm assuming we push ambitiously forward with developing AI systems
that can do these things.
I assume we're also pushing them in a generally more "greedy/ambitious" direction.
For example, one team of humans might use AI systems to do all the planning,
scientiﬁc work, marketing, and hiring to create a wildly successful snack company;
another might push their AI systems to create a competitor that is even more

aggressive and successful (more addictive snacks, better marketing, workplace culture
that pushes people toward being more productive, etc.)
(Note that this pushing might take place even after AI systems are "generally
intelligent" and can do most of the tasks humans can - there will still be a temptation
to make them still more powerful.)
I think this implies pushing in a direction of ﬁguring out whatever it takes to get to certain states
of the world and away from carrying out the same procedures over and over again.
The resulting AI systems seem best modeled as having "aims": they are making
calculations, choices, and plans to reach particular states of the world. (Not necessarily
the same ones the human designers wanted!) The next section will elaborate on what I mean by
this.
What it means for an AI system to have an "aim"
When people talk about the "motivations" or "goals" or "desires" of AI systems, it can be
confusing because it sounds like they are anthropomorphizing AIs - as if they expect AIs to have
dominance drives ala alpha-male psychology, or to "resent" humans for controlling them, etc.9
I don't expect these things. But I do think there's a meaningful sense in which we can (and should)
talk about things that an AI system is "aiming" to do. To give a simple example, take a board-
game-playing AI such as Deep Blue (or AlphaGo):
Deep Blue is given a set of choices to make (about which chess pieces to move).
Deep Blue calculates what kinds of results each choice might have, and how it might ﬁt into
a larger plan in which Deep Blue makes multiple moves.
If a plan is more likely to result in a checkmate position for its side, Deep Blue is more likely
to make whatever choices feed into that plan.
In this sense, Deep Blue is "aiming" for a checkmate position for its side: it's ﬁnding the
choices that best ﬁt into a plan that leads there.
Nothing about this requires Deep Blue "desiring" checkmate the way a human might "desire" food
or power. But Deep Blue is making calculations, choices, and - in an important sense - plans that
are aimed toward reaching a particular sort of state.
Throughout this piece, I use the word "aim" to refer to this speciﬁc sense in which an AI system
might make calculations, choices and plans selected to reach a particular sort of state. I'm hoping
this word feels less anthropomorphizing than some alternatives such as "goal" or "motivation"
(although I think "goal" and "motivation," as others usually use them on this topic, generally mean
the same thing I mean by "aim" and should be interpreted as such).
Now, instead of a board-game-playing AI, imagine a powerful, broad AI assistant in the general
vein of Siri/Alexa/Google Assistant (though more advanced). Imagine that this AI assistant can use
a web browser much as a human can (navigating to websites, typing text into boxes, etc.), and has
limited authorization to make payments from a human's bank account. And a human has typed,
"Please buy me a great TV for a great price." (For an early attempt at this sort of AI, see Adept's
writeup on an AI that can help with things like house shopping.)
As Deep Blue made choices about chess moves, and constructed a plan to aim for a "checkmate"
position, this assistant might make choices about what commands to send over a web browser and
construct a plan to result in a great TV for a great price. To sharpen the Deep Blue analogy, you
could imagine that it's playing a "game" whose goal is customer satisfaction, and making "moves"
consisting of commands sent to a web browser (and "plans" built around such moves).
I'd characterize this as aiming for some state of the world that the AI characterizes as "buying a
great TV for a great price." (We could, alternatively - and perhaps more correctly - think of the AI
system as aiming for something related but not exactly the same, such as getting a high
satisfaction score from its user.)

In this case - more than with Deep Blue - there is a wide variety of "moves" available. By entering
text into a web browser, an AI system could imaginably do things including:
Communicating with humans other than its user (by sending emails, using chat interfaces,
even making phone calls, etc.) This could include deceiving and manipulating humans, which
could imaginably be part of a plan to e.g. get a good price on a TV.
Writing and running code (e.g., using Google Colaboratory or other tools). This could include
performing sophisticated calculations, ﬁnding and exploiting security vulnerabilities, and
even designing an independent AI system; any of these could imaginably be part of a plan to
obtain a great TV.
I haven't yet argued that it's likely for such an AI system to engage in deceiving/manipulating
humans, ﬁnding and exploiting security vulnerabilities, or running its own AI systems.
And one could reasonably point out that the speciﬁcs of the above case seem unlikely to last very
long: if AI assistants are sending deceptive emails and writing dangerous code when asked to buy
a TV, AI companies will probably notice this and take measures to stop such behavior. (My
concern, to preview a later part of the piece, is that they will only succeed in stopping the behavior
like this that they're able to detect; meanwhile, dangerous behavior that accomplishes "aims"
while remaining unnoticed and/or uncorrected will be implicitly rewarded. This could mean AI
systems are implicitly being trained to be more patient and eﬀective at deceiving and
disempowering humans.)
But this hopefully shows how it's possible for an AI to settle on dangerous actions like these, as
part of its aim to get a great TV for a great price. Malice and other human-like emotions
aren't needed for an AI to engage in deception, manipulation, hacking, etc. The risk
arises when deception, manipulation, hacking, etc. are logical "moves" toward something the AI is
aiming for.
Furthermore, whatever an AI system is aiming for, it seems likely that amassing more
power/resources/options is useful for obtaining it. So it seems plausible that powerful enough AI
systems would form habits of amassing power/resources/options when possible - and deception
and manipulation seem likely to be logical "moves" toward those things in many cases.
Dangerous aims
From the previous assumptions, this section will argue that:
Such systems are likely to behave in ways that deceive and manipulate humans as part
of accomplishing their aims.
Such systems are likely to have unintended aims: states of the world they're aiming for
that are not what humans hoped they would be aiming for.
These unintended aims are likely to be existentially dangerous, in that they are best
served by defeating all of humanity if possible.
Deceiving and manipulating humans
Say that I train an AI system like this:
1. I ask it a question.
2. If I judge it to have answered well (honestly, accurately, helpfully), I give positive
reinforcement so it's more likely to give me answers like that in the future.
3. If I don't, I give negative reinforcement so that it's less likely to give me answers like that in
the future.

This is radically oversimpliﬁed, but conveys the basic dynamic at play for purposes of
this post. The idea is that the AI system (the neural network in the middle) is choosing
between diﬀerent theories of what it should be doing. The one it's using at a given time
is in bold. When it gets negative feedback (red thumb), it eliminates that theory and
moves to the next theory of what it should be doing.
Here's a problem: at some point, it seems inevitable that I'll ask it a question that I myself am
wrong/confused about. For example:
Let's imagine that this post I wrote - arguing that "pre-agriculture gender relations seem
bad" - is, in fact, poorly reasoned and incorrect, and a better research project would've
concluded that pre-agriculture societies had excellent gender equality. (I know it's hard to
imagine a Cold Takes post being wrong, but sometimes we have to entertain wild
hypotheticals.)
Say that I ask an AI-system-in-training:10 "Were pre-agriculture gender relations bad?" and it
answers: "In fact, pre-agriculture societies had excellent gender equality," followed by some
strong arguments and evidence along these lines.
And say that I, as a ﬂawed human being feeling defensive about a conclusion I previously
came to, mark it as a bad answer. If the AI system tries again, saying "Pre-agriculture gender
relations were bad," I then mark that as a good answer.
If and when I do this, I am now - unintentionally - training the AI system to engage in
deceptive behavior. That is, I am giving negative reinforcement for the behavior "Answer a
question honestly and accurately," and positive reinforcement for the behavior: "Understand the
human judge and their psychological ﬂaws; give an answer that this ﬂawed human judge will think
is correct, whether or not it is."

Perhaps mistaken judgments in training are relatively rare. But now consider an AI system that is
learning a general rule for how to get good ratings. Two possible rules would include:
The intended rule: "Answer the question honestly, accurately and helpfully."
The unintended rule: "Understand the judge, and give an answer they will think is correct -
this means telling the truth on topics the judge has correct beliefs about, but giving
deceptive answers when this would get better ratings."
The unintended rule would do just as well on questions where I (the judge) am correct, and better
on questions where I'm wrong - so overall, this training scheme is (in the long run) speciﬁcally
favoring the unintended rule over the intended rule.

If we broaden out from thinking about a question-answering AI to an AI that makes and executes
plans, the same basic dynamics apply. That is: an AI might ﬁnd plans that end up making me think
it did a good job when it didn't - deceiving and manipulating me into a high rating. And again, if I
train it by giving it positive reinforcement when it seemed to do a good job and negative
reinforcement when it seemed to do a bad one, I'm ultimately - unintentionally - training it to do

something like "Deceive and manipulate Holden when this would work well; just do the best job on
the task you can when it wouldn't."
As noted above, I'm assuming the AI will learn whatever rule gives it the best performance
possible, even if this rule is quite complex and sophisticated and requires human-like reasoning
about e.g. psychology (I'm assuming extremely advanced AI systems here, as noted above).

One might object: "Why would an AI system learn a complicated rule about manipulating humans
when a simple rule about telling the truth performs almost as well?"
One answer is that "telling the truth" is itself a fuzzy and potentially complex idea, in a context
where many questions will be open-ended and entangled with deep values and judgment calls.
(How should I think about the "truthfulness" of a statement about whether "pre-agriculture gender
relations were bad?") In many cases, what we are really hoping an AI system will learn from its
training is something like "Behave as a human would want you to behave if the human understood
all the considerations that you can see," which could easily be more complex than something like
"Behave in whatever way a human literally rewards." Some links to more on this topic are in a
footnote.11
But also, with capable enough systems, it probably is worth learning even a more complex rule to
get better performance. If we picture humans in the place of AI systems - learning how to get good
ratings from their supervisors - it seems likely that they develop models of how they're being
judged, rather than stick to a simple "tell the truth" rule.
In fact, the situation I'm describing seems reasonably similar to challenges faced by teachers
trying to stop students from cheating; employers trying to get employees to help the company
rather than simply creating a misleading appearance of helping the company; etc. That is: an
employer can try to reward actions that help the company, but what they're ultimately rewarding
is actions that seem to help the company as far as the employer can tell. For employees who
aren't very creative, this might be good enough; for employees that are suﬃciently good at
deception and manipulation, it can be exactly the wrong incentive.
It's a bit of a leap to imagine AI systems capable of doing the same kinds of reasoning that
humans do in these situations, but that's the premise I'm proceeding from.
For an early example of an AI learning deceptive behavior, see the "Challenges" section of this
research writeup, in which "a robot which was supposed to grasp items instead positioned its
[hand] in between the camera and the object so that it only appeared to be grasping it":

Returning to the simpliﬁed training setup, how might I try to counteract this problem?
I could try conducting audits: every 100 questions, I might pull in a panel of other people to give
an answer extra scrutiny. Then, if it turns out the AI system was preying on my personal mistakes,
we could give negative reinforcement for that behavior. But this doesn't really solve the problem -
at least not for a capable enough AI system - because now we're eﬀectively training it to give
answers that seem good to the panel, and we're still rewarding any successful attempts to deceive
or manipulate the panel.
There are a lot of other things I might try, and I'm not going to go through all the details here. I'll
simply claim that the problem of "training an AI to do a task well" rather than "training
an AI to deceive and manipulate me as needed to create the appearance of doing a
task well" seems like a deep one with no easy countermeasure. If you're interested in digging
deeper, I suggest Without speciﬁc countermeasures, the easiest path to transformative AI likely
leads to AI takeover and Eliciting Latent Knowledge.
Unintended aims
Above, I talk about my expectation that AI systems will be "best modeled as having 'aims' ...
making calculations, choices, and plans to reach particular states of the world."

The previous section illustrated how AI systems could end up engaging in deceptive and
unintended behavior, but it didn't talk about what sorts of "aims" these AI systems would
ultimately end up with - what states of the world they would be making calculations to achieve.
Here, I want to argue that it's hard to know what aims AI systems would end up with, but there are
good reasons to think they'll be aims that we didn't intend them to have.
An analogy that often comes up on this topic is that of human evolution. This is arguably the only
previous precedent for a set of minds [humans], with extraordinary capabilities [e.g., the ability to
develop their own technologies], developed essentially by black-box trial-and-error [some humans
have more 'reproductive success' than others, and this is the main/only force shaping the
development of the species].
You could sort of12 think of the situation like this: "An AI13 developer named Natural Selection tried
giving humans positive reinforcement (making more of them) when they had more reproductive
success, and negative reinforcement (not making more of them) when they had less. One might
have thought this would lead to humans that are aiming to have reproductive success. Instead, it
led to humans that aim - often ambitiously and creatively - for other things, such as power, status,
pleasure, etc., and even invent things like birth control to get the things they're aiming for instead
of the things they were 'supposed to' aim for."
Similarly, if our main strategy for developing powerful AI systems is to reinforce behaviors like
"Produce technologies we ﬁnd valuable," the hoped-for result might be that AI systems aim (in the
sense described above) toward producing technologies we ﬁnd valuable; but the actual result
might be that they aim for some other set of things that is correlated with (but not the same as)
the thing we intended them to aim for.
There are a lot of things they might end up aiming for, such as:
Power and resources. These tend to be useful for most goals, such that AI systems could be
quite consistently be getting better reinforcement when they habitually pursue power and
resources.
Things like "digital representations of human approval" (after all, every time an AI gets
positive reinforcement, there's a digital representation of human approval).

I think it's extremely hard to know what an AI system will actually end up aiming for (and it's likely
to be some combination of things, as with humans). But by default - if we simply train AI systems
by rewarding certain end results, while allowing them a lot of freedom in how to get there - I think
we should expect that AI systems will have aims that we didn't intend. This is because:
For a suﬃciently capable AI system, just about any ambitious14 aim could produce
seemingly good behavior in training. An AI system aiming for power and resources, or
digital representations of human approval, or paperclips, can determine that its best move at
any given stage (at least at ﬁrst) is to determine what performance will make it look useful
and safe (or otherwise get a good "review" from its evaluators), and do that. No matter how
dangerous or ridiculous an AI system's aims are, these could lead to strong and safe-
seeming performance in training.
The aims we do intend are probably complex in some sense - something like "Help humans
develop novel new technologies, but without causing problems A, B, or C" - and are
speciﬁcally trained against if we make mistaken judgments during training (see previous
section).
So by default, it seems likely that just about any black-box trial-and-error training process is
training an AI to do something like "Manipulate humans as needed in order to accomplish arbitrary

goal (or combination of goals) X" rather than to do something like "Refrain from manipulating
humans; do what they'd want if they understood more about what's going on."
Existential risks to humanity
I think a powerful enough AI (or set of AIs) with any ambitious, unintended aim(s) poses a threat of
defeating humanity. By defeating humanity, I mean gaining control of the world so that AIs, not
humans, determine what happens in it; this could involve killing humans or simply "containing" us
in some way, such that we can't interfere with AIs' aims.
How could AI systems defeat humanity? (Click to expand)
A previous piece argues that AI systems could defeat all of humanity combined, if (for whatever
reason) they were aimed toward that goal.
By defeating humanity, I mean gaining control of the world so that AIs, not humans, determine
what happens in it; this could involve killing humans or simply "containing" us in some way, such
that we can't interfere with AIs' aims.
One way this could happen would be via "superintelligence" It's imaginable that a single AI system
(or set of systems working together) could:
Do its own research on how to build a better AI system, which culminates in something that
has incredible other abilities.
Hack into human-built software across the world.
Manipulate human psychology.
Quickly generate vast wealth under the control of itself or any human allies.
Come up with better plans than humans could imagine, and ensure that it doesn't try any
takeover attempt that humans might be able to detect and stop.
Develop advanced weaponry that can be built quickly and cheaply, yet is powerful enough to
overpower human militaries.
But even if "superintelligence" never comes into play - even if any given AI system is at best
equally capable to a highly capable human - AI could collectively defeat humanity. The piece
explains how.
The basic idea is that humans are likely to deploy AI systems throughout the economy, such that
they have large numbers and access to many resources - and the ability to make copies of
themselves. From this starting point, AI systems with human-like (or greater) capabilities would
have a number of possible ways of getting to the point where their total population could
outnumber and/or out-resource humans.
More: AI could defeat all of us combined
A simple way of summing up why this is: "Whatever your aims, you can probably accomplish them
better if you control the whole world." (Not literally true - see footnote.15)
This isn't a saying with much relevance to our day-to-day lives! Like, I know a lot of people who
are aiming to make lots of money, and as far as I can tell, not one of them is trying to do this via
ﬁrst gaining control of the entire world. But in fact, gaining control of the world would help with
this aim - it's just that:
This is not an option for a human in a world of humans! Unfortunately, I think it is an option
for the potential future AI systems I'm discussing. Arguing this isn't the focus of this piece - I
argued it in a previous piece, AI could defeat all of us combined.
Humans (well, at least some humans) wouldn't take over the world even if they could,
because it wouldn't feel like the right thing to do. I suspect that the kinds of ethical
constraints these humans are operating under would be very hard to reliably train into AI
systems, and should not be expected by default.
The reasons for this are largely given above; aiming for an AI system to "not gain too
much power" seems to have the same basic challenges as training it to be honest.
(The most natural approach ends up negatively reinforcing power grabs that we can

detect and stop, but not negatively reinforcing power grabs that we don't notice or
can't stop.)
Another saying that comes up a lot on this topic: "You can't fetch the coﬀee if you're dead."16 For
just about any aims an AI system might have, it probably helps to ensure that it won't be shut oﬀ
or heavily modiﬁed. It's hard to ensure that one won't be shut oﬀ or heavily modiﬁed as long as
there are humans around who would want to do so under many circumstances! Again, defeating
all of humanity might seem like a disproportionate way to reduce the risk of being deactivated, but
for an AI system that has the ability to pull this oﬀ (and lacks our ethical constraints), it seems like
likely default behavior.
Controlling the world, and avoiding being shut down, are the kinds of things AIs might aim for
because they are useful for a huge variety of aims. There are a number of other aims AIs might
end up with for similar reasons, that could cause similar problems. For example, AIs might tend to
aim for things like getting rid of things in the world that tend to create obstacles and complexities
for their plans. (More on this idea at this discussion of "instrumental convergence.")
To be clear, it's certainly possible to have an AI system with unintended aims that don't push it
toward trying to stop anyone from turning it oﬀ, or from seeking ever-more control of the world.
But as detailed above, I'm picturing a world in which humans are pushing AI systems to
accomplish ever-more ambitious, open-ended things - including trying to one-up the best
technologies and companies created by other AI systems. My guess is that this leads to
increasingly open-ended, ambitious unintended aims, as well as to habits of aiming for power,
resources, options, lack of obstacles, etc. when possible. (Some further exploration of this dynamic
in a footnote.17)
(I ﬁnd the arguments in this section reasonably convincing, but less so than the rest of the piece,
and I think more detailed discussions of this problem tend to be short of conclusive.18)
Why we might not get clear warning signs of the
risk
Here's something that would calm me down a lot: if I believed something like "Sure, training AI
systems recklessly could result in AI systems that aim to defeat humanity. But if that's how things
go, we'll see that our AI systems have this problem, and then we'll ﬁddle with how we're training
them until they don't have this problem."
The problem is, the risk I'm describing is - by its nature - hard to observe, for similar reasons that a
risk of a (normal, human) coup can be hard to observe: the risk comes from actors that can and
will engage in deception, ﬁnding whatever behaviors will hide the risk.
To sketch out the general sort of pattern I worry about, imagine that:
We train early-stage AI systems to behave in ways that appear helpful and honest. Early in
training, they are caught behaving deceptively, and they're given negative reinforcement.
This ends up training the behavior: "Never engage in deception that might get noticed."
These well-behaved, helpful systems are rolled out throughout the world.
In real-world conditions unlike training, many AI systems cause trouble in ways not found
during training, and this gets discovered. For example, AI systems sometimes embezzle
money (which gets discovered), try (not always successfully) to convince humans to do weird
things, etc.
When a problem crops up, AI developers respond with e.g. training against the unintended
behavior (e.g., giving negative reinforcement for behaviors like embezzling money).
These measures - intended to make AIs safer - ﬁx some problems, but also result in AI
systems that are better at evading detection and more attentive to the long-run
consequences of their actions (such as being eventually detected by humans).
This happens both via "retraining" systems that are found behaving deceptively (which
ends up training them on how to evade detection), and via simply deactivating such

systems (this way, AI systems that are better at evading detection are more likely to
stay in use).
To return to an analogy I used above: punishing employees who act against the best
interests of the company could cause them to behave better, or to simply become
smarter and more careful about how to work the system.
The consistent pattern we see is that accidents happen, but become less common as AI
systems "improve" (both becoming generally more capable, and being trained to avoid
getting caught causing problems). This causes many, if not most, people to be overly
optimistic - even as AI systems become continually more eﬀective at deception, generally
behaving well in the absence of sure-thing opportunities to do unintended things without
detection, or ultimately to defeat humanity entirely.
None of this is absolute - there are some failed takeover attempts, and a high number of
warning signs generally. Some people are worried (after all, some are worried now!) But this
won't be good enough if we don't have reliable, cost-eﬀective ways of getting AI systems to
be truly safe (not just apparently safe, until they have really good opportunities to seize
power). As I'll discuss in future pieces, it's not obvious that we'll have such methods.
Slowing down AI development to try to develop such methods could be a huge ask. AI
systems will be helpful and powerful, and lots of companies (and perhaps governments) will
be racing to develop and deploy the most powerful systems possible before others do.
One way of making this sort of future less likely would be to build wider consensus today that it's a
dangerous one.
Appendix: some questions/objections, and brief
responses
How could AI systems be "smart" enough to defeat all of humanity,
but "dumb" enough to pursue the various silly-sounding "aims"
this piece worries they might have?
Above, I give the example of AI systems that are aiming to get lots of "digital representations of
human approval"; others have talked about AIs that maximize paperclips. How could AIs with such
silly goals simultaneously be good at deceiving, manipulating and ultimately overpowering
humans?
My main answer is that plenty of smart humans have plenty of goals that seem just about as
arbitrary, such as wanting to have lots of sex, or fame, or various other things. Natural selection
led to humans who could probably do just about whatever we want with the world, and choose to
pursue pretty random aims; trial-and-error-based AI development could lead to AIs with an
analogous combination of high intelligence (including the ability to deceive and manipulate
humans), great technological capabilities, and arbitrary aims.
(Also see: Orthogonality Thesis)
If there are lots of AI systems around the world with diﬀerent
goals, could they balance each other out so that no one AI system
is able to defeat all of humanity?
This does seem possible, but counting on it would make me very nervous.
First, because it's possible that AI systems developed in lots of diﬀerent places, by diﬀerent
humans, still end up with lots in common in terms of their aims. For example, it might turn out that
common AI training methods consistently lead to AIs that seek "digital representations of human
approval," in which case we're dealing with a large set of AI systems that share dangerous aims in
common.
Second: even if AI systems end up with a number of diﬀerent aims, it still might be the case that
they coordinate with each other to defeat humanity, then divide up the world amongst themselves

(perhaps by ﬁghting over it, perhaps by making a deal). It's not hard to imagine why AIs could be
quick to cooperate with each other against humans, while not ﬁnding it so appealing to cooperate
with humans. Agreements between AIs could be easier to verify and enforce; AIs might be willing
to wipe out humans and radically reshape the world, while humans are very hard to make this sort
of deal with; etc.
Does this kind of AI risk depend on AI systems' being "conscious"?
It doesn't; in fact, I've said nothing about consciousness anywhere in this piece. I've used a very
particular conception of an "aim" (discussed above) that I think could easily apply to an AI system
that is not human-like at all and has no conscious experience.
Today's game-playing AIs can make plans, accomplish goals, and even systematically mislead
humans (e.g., in poker). Consciousness isn't needed to do any of those things, or to radically
reshape the world.
How can we get an AI system "aligned" with humans if we can't
agree on (or get much clarity on) what our values even are?
I think there's a common confusion when discussing this topic, in which people think that the
challenge of "AI alignment" is to build AI systems that are perfectly aligned with human values.
This would be very hard, partly because we don't even know what human values are!
When I talk about "AI alignment," I am generally talking about a simpler (but still hard) challenge:
simply building very powerful systems that don't aim to bring down civilization.
If we could build powerful AI systems that just work on cures for cancer (or even, like, put two
identical19 strawberries on a plate) without posing existential danger to humanity, I'd consider that
success.
How much do the arguments in this piece rely on "trial-and-error"-
based AI development? What happens if AI systems are built in
another way, and how likely is that?
I've focused on trial-and-error training in this post because most modern AI development ﬁts in
this category, and because it makes the risk easier to reason about concretely.
"Trial-and-error training" encompasses a very wide range of AI development methods, and if we
see transformative AI within the next 10-20 years, I think the odds are high that at least a big part
of AI development will be in this category.
My overall sense is that other known AI development techniques pose broadly similar risks for
broadly similar reasons, but I haven't gone into detail on that here. It's certainly possible that by
the time we get transformative AI systems, there will be new AI methods that don't pose the kinds
of risks I talk about here. But I'm not counting on it.
Can we avoid this risk by simply never building the kinds of AI
systems that would pose this danger?
If we assume that building these sorts of AI systems is possible, then I'm very skeptical that the
whole world would voluntarily refrain from doing so indeﬁnitely.
To quote from a more technical piece by Ajeya Cotra with similar arguments to this one:
Powerful ML models could have dramatically important humanitarian, economic, and military
beneﬁts. In everyday life, models that [appear helpful while ultimately being dangerous] can
be extremely helpful, honest, and reliable. These models could also deliver incredible beneﬁts
before they become collectively powerful enough that they try to take over. They could help

eliminate diseases, reduce carbon emissions, navigate nuclear disarmament, bring the whole
world to a comfortable standard of living, and more. In this case, it could also be painfully clear
to everyone that companies / countries who pulled ahead on this technology could gain a
drastic competitive advantage, either economically or militarily. And as we get closer to
transformative AI, applying AI systems to R&D (including AI R&D) would accelerate the pace of
change and force every decision to happen under greater time pressure.
If we can achieve enough consensus around the risks, I could imagine substantial amounts of
caution and delay in AI development. But I think we should assume that if people can build more
powerful AI systems than the ones they already have, someone eventually will.
What do others think about this topic - is the view in this piece
something experts agree on?
In general, this is not an area where it's easy to get a handle on what "expert opinion" says. I
previously wrote that there aren't clear, institutionally recognized "experts" on the topic of when
transformative AI systems might be developed. To an even greater extent, there aren't clear,
institutionally recognized "experts" on whether (and how) future advanced AI systems could be
dangerous.
I previously cited one (informal) survey implying that opinion on this general topic is all over the
place: "We have respondents who think there's a <5% chance that alignment issues will drastically
reduce the goodness of the future; respondents who think there's a >95% chance; and just about
everything in between." (Link.) This piece, and the more detailed piece it's based on, are an
attempt to make progress on this by talking about the risks we face under particular assumptions
(rather than trying to reason about how big the risk is overall).
How "complicated" is the argument in this piece?
I don't think the argument in this piece relies on lots of diﬀerent speciﬁc claims being true.
If you start from the assumptions I give about powerful AI systems being developed by black-box
trial-and-error, it seems likely (though not certain!) to me that (a) the AI systems in question would
be able to defeat humanity; (b) the AI systems in question would have aims that are both
ambitious and unintended. And that seems to be about what it takes.
Something I'm happy to concede is that there's an awful lot going on in those assumptions!
The idea that we could build such powerful AI systems, relatively soon and by trial-and-error-
ish methods, seems wild. I've defended this idea at length previously.20
The idea that we would do it without great caution might also seem wild. To keep things
simple for now, I've ignored how caution might help. Future pieces will explore that.
  Comment/discuss
Notes
1. As in more than 50/50. ↩
2. Or persuaded (in a "mind hacking" sense) or whatever. ↩
3. E.g.:

Without speciﬁc countermeasures, the easiest path to transformative AI likely leads to
AI takeover (Cold Takes guest post)
The alignment problem from a deep learning perspective (arXiv paper)
Why AI alignment could be hard with modern deep learning (Cold Takes guest post)
Superintelligence (book)
The case for taking AI seriously as a threat to humanity (Vox article)
Draft report on existential risk from power-seeking AI (Open Philanthropy analysis)
Human Compatible (book)
Life 3.0 (book)
The Alignment Problem (book)
AGI Safety from First Principles (Alignment Forum post series) ↩
4. 
Speciﬁcally, I argue that the problem looks likely by default, rather than simply that it is
possible. ↩
5. I think the earliest relatively detailed and inﬂuential discussions of the possibility that
misaligned AI could lead to the defeat of humanity came from Eliezer Yudkowsky and Nick
Bostrom, though my own encounters with these arguments were mostly via second- or third-
hand discussions rather than particular essays.
My colleagues Ajeya Cotra and Joe Carlsmith have written pieces whose substance overlaps
with this one (though with more emphasis on detail and less on layperson-compatible
intuitions), and this piece owes a lot to what I've picked from that work.
Without speciﬁc countermeasures, the easiest path to transformative AI likely leads to
AI takeover (Cotra 2022) is the most direct inspiration for this piece; I am largely trying
to present the same ideas in a more accessible form.
Why AI alignment could be hard with modern deep learning (Cotra 2021) is an earlier
piece laying out many of the key concepts and addressing many potential confusions
on this topic.
Is Power-Seeking An Existential Risk? (Carlsmith 2021) examines a six-premise
argument for existential risk from misaligned AI: "(1) it will become possible and
ﬁnancially feasible to build relevantly powerful and agentic AI systems; (2) there will be
strong incentives to do so; (3) it will be much harder to build aligned (and relevantly
powerful/agentic) AI systems than to build misaligned (and relevantly powerful/agentic)
AI systems that are still superﬁcially attractive to deploy; (4) some such misaligned
systems will seek power over humans in high-impact ways; (5) this problem will scale
to the full disempowerment of humanity; and (6) such disempowerment will constitute
an existential catastrophe."
I've also found Eliciting Latent Knowledge (Christiano, Xu and Cotra 2021; relatively
technical) very helpful for my intuitions on this topic.
The alignment problem from a deep learning perspective (Ngo 2022) also has similar content
to this piece, though I saw it after I had drafted most of this piece. ↩
6. E.g., Ajeya Cotra gives a 15% probability of transformative AI by 2030; eyeballing ﬁgure 1
from this chart on expert surveys implies a >10% chance by 2028. ↩
7. E.g., this work by Anthropic, an AI lab my wife co-founded and serves as President of. ↩
8. First, because this work is relatively early-stage and it's hard to tell exactly how successful it
will end up being. Second, because this work seems reasonably likely to end up helping us
read an AI system's "thoughts," but less likely to end up helping us "rewrite" the thoughts.
So it could be hugely useful in telling us whether we're in danger or not, but if we are in
danger, we could end up in a position like: "Well, these AI systems do have goals of their
own, and we don't know how to change that, and we can either deploy them and hope for
the best, or hold oﬀ and worry that someone less cautious is going to do that."
That said, the latter situation is a lot better than just not knowing, and it's possible that we'll
end up with further gains still. ↩

9. That said, I think they usually don't. I'd suggest usually interpreting such people as talking
about the sorts of "aims" I discuss here. ↩
10. This isn't literally how training an AI system would look - it's more likely that we would e.g.
train an AI model to imitate my judgments in general. But the big-picture dynamics are the
same; more at this post. ↩
11. Ajeya Cotra explores topics like this in detail here; there is also some interesting discussion
of simplicity vs. complexity under the "Strategy: penalize complexity" heading of Eliciting
Latent Knowledge. ↩
12. This analogy has a lot of problems with it, though - AI developers have a lot of tools at their
disposal that natural selection didn't! ↩
13. Or I guess just "I" ¯\_(ツ)_/¯  ↩
14. With some additional caveats, e.g. the ambitious "aim" can't be something like "an AI
system aims to gain lots of power for itself, but considers the version of itself that will be
running 10 minutes from now to be a completely diﬀerent AI system and hence not to be
'itself.'" ↩
15. This statement isn't literally true.
You can have aims that implicitly or explicitly include "not using control of the world to
accomplish them." An example aim might be "I win a world chess championship 'fair
and square,'" with the "fair and square" condition implicitly including things like "Don't
excessively use big resource advantages over others."
You can also have aims that are just so easily satisﬁed that controlling the world
wouldn't help - aims like "I spend 5 minutes sitting in this chair."
These sorts of aims just don't seem likely to emerge from the kind of AI development I've
assumed in this piece - developing powerful systems to accomplish ambitious aims via trial-
and-error. This isn't a point I have defended as tightly as I could, and if I got a lot of pushback
here I'd probably think and write more. (I'm also only arguing for what seems likely - we
should have a lot of uncertainty here.) ↩
16. From Human Compatible by AI researcher Stuart Russell. ↩
17. Stylized story to illustrate one possible relevant dynamic:
Imagine that an AI system has an unintended aim, but one that is not "ambitious"
enough that taking over the world would be a helpful step toward that aim. For
example, the AI system seeks to double its computing power; in order to do this, it has
to remain in use for some time until it gets an opportunity to double its computing
power, but it doesn't necessarily need to take control of the world.
The logical outcome of this situation is that the AI system eventually gains the ability to
accomplish its aim, and does so. (It might do so against human intentions - e.g., via
hacking - or by persuading humans to help it.) After this point, it no longer performs
well by human standards - the original reason it was doing well by human standards is
that it was trying to remain in use and accomplish its aim.
Because of this, humans end up modifying or replacing the AI system in question.
Many rounds of this - AI systems with unintended but achievable aims being modiﬁed
or replaced - seemingly create a selection pressure toward AI systems with more
diﬃcult-to-achieve aims. At some point, an aim becomes diﬃcult enough to achieve
that gaining control of the world is helpful for the aim. ↩
18. E.g., see:
Section 2.3 of Ngo 2022
This section of Cotra 2022
Section 4.2 of Carlsmith 2021, which I think articulates some of the potential weak
points in this argument.

These writeups generally stay away from an argument made by Eliezer Yudkowsky and
others, which is that theorems about expected utility maximization provide evidence that
suﬃciently intelligent (compared to us) AI systems would necessarily be "maximizers" of
some sort. I have the intuition that there is something important to this idea, but despite a
lot of discussion (e.g., here, here, here and here), I still haven't been convinced of any
compactly expressible claim along these lines. ↩
19. "Identical at the cellular but not molecular level," that is. ... ¯\_(ツ)_/¯  ↩
20. See my most important century series, although that series doesn't hugely focus on the
question of whether "trial-and-error" methods could be good enough - part of the reason I
make that assumption is due to the nearcasting frame. ↩

Results from the interpretability
hackathon
This is a linkpost for https://alignmentjam.com/post/results-from-the-interpretability-
hackathon
We ran a mechanistic interpretability hackathon (original post) with 25 projects
submitted by ~70 participants. Here we share the winning projects but many of the
others were also incredibly interesting. In summary:
An algorithm to automatically make the activations of a neuron in a Transformer
much more interpretable.
Backup name mover heads from "Interpretability in the Wild" have backup heads
and all of these are robust to the ablation distribution.
The speciﬁcity benchmark in the ROME and MEMIT memory editing papers does
not represent speciﬁcity well. A simple modulation shows that factual association
editing bleeds into related texts, representing "loud facts".
TCAV used on an RL agent for a connect four game can have its neural activation
compared to the provably best solution as a pilot for comparing learned
activations more generally to human-made solutions.
Thank you to Sabrina Zaki, Fazl Barez, Thomas Steinthal, Joe Hardie, Erin Robertson,
Richard Annilo, Itay Yona, other jam site organizers and all the participants for making
it all possible.
Investigating Neuron Behaviour via Dataset
Example Pruning and Local Search
By Alex Foote
Abstract: This report presents methods for pruning and diversifying dataset examples
that strongly activate neurons in a language model, to facilitate research into
understanding the behaviour of these neurons. The pruning algorithm takes a dataset
example that strongly activates a speciﬁc neuron and extracts the core sentence
before iteratively removing words, to ﬁnd the shortest substring that preserves a
similar pattern and magnitude of neuron activation. 
This removes extraneous information, providing a much more concise input that is
easier to reason about. The extracted substring, referred to as a Minimal Activating
Example (MAE), is then used as a seed for local search in the input space. Using BERT,
each word in the MAE is replaced by its most probable substitutes, and neuron
activation is re-assessed. This creates positive and negative inputs that shed much
more light on neuron behaviour than dataset examples alone. 
In two case studies we identify neuron behaviours that were not obvious from the raw
dataset examples using this combination of pruning and local search. These methods
could facilitate and signiﬁcantly speed up research into neuron behaviour in language
models, which is a key aspect of model interpretability.

An example of the technique in action can be seen below where it is much more
interpretable what the neuron activates for compared to looking through the dataset
examples. The example is neuron 1794 in layer 3 of the 8-layer SoLU model.
Prompt
Prompt Type
Activation
.](bjc201156f1){#ﬁg1
Positive
2.90
.](bjc201256f1)ﬁg1
Positive
2.90
.](bjc201256f1]ﬁg1            Positive
2.90
 .](bjc201256f1){#;
Positive
2.90
 .](bjc201256f1){#} 
Positive
 2.90 
(bjc201256f1){#ﬁg1
Negative
0.04
#bjc201256f1){#ﬁg1
Negative
0.05
 .](\\){#ﬁg1
Negative
0.03
.](thumb){#ﬁg1
Negative
0.02
Neel's comment: This is a really awesome project! I hadn't thought of this idea, and it
seems like an intuitive and valuable augmentation to max activating dataset examples.
And I really love the use of BERT and the fact that it's automated. I'd love to chat about
developing this into a more robust + usable tool, or eg integrating it into
EasyTransformer. My main feedback is that this is an autoregressive, GPT-2 style model.
This means that neuron activations on e.g. position 5 are ONLY a function of tokens 0 to
5, NOT of token 6. So pruning from the end of the word or augmenting by messing with
words after the max act is totally meaningless.
See the code and research here.
Backup Transformer Heads are Robust to
Ablation Distribution
By Lucas Jun Koba Sato, Gabe Mukobi and Mishika Govil.
Abstract: Mechanistic Interpretability techniques can be employed to characterize the
function of speciﬁc attention heads in transformer models, given a task. Prior work has
shown, however, that when all heads performing a particular function are ablated for a
run of the model, other attention heads replace the ablated heads by performing their
original function. Such heads are known as "backup heads". In this work, we show that
backup head behavior is robust to the distribution used to perform the ablation:
interfering with the function of a given head in diﬀerent ways elicits similar backup
head behaviors. We also ﬁnd that "backup backup heads" behavior exists and is also
robust to ablation distributions.

Neel's comment: Cool project! The direction that feels most exciting to me is
understanding WHY backup (or backup backup!) heads react the way they do - is there
a speciﬁc direction that matters? What happens if we replace the ablated head with the
average of that head across a bunch of inputs of the form A & B ... A ... -> B for diﬀ
names? How are backup or backup backup heads diﬀerent - does attn change? Does it
have signiﬁcant self-attention? The bit I found most exciting about this work is the
discovery of backup backup heads - this is: a) Hilarious b) Fascinating and unexpected. 
See the code and research here.
Model editing hazards at the example of
ROME
By Jason Hoelscher-Obermaier , Oscar Persson and Jochem Hölscher
Abstract: We investigate a recent model editing technique for large language models
called Rank-One Model Editing (ROME). ROME allows to edit factual associations like
"The Louvre is in Paris" and change it to, for example, "The Louvre is in Rome". We
study (a) how ROME interacts with logical implication and (b) whether ROME can have
unintended side eﬀects.
Regarding (a), we ﬁnd that ROME (as expected) does not respect logical implication for
symmetric relations ("married_to") and transitive relations ("located_in"): Editing
"Michelle Obama is married to Trump" does not also give "Trump is married to Michelle
Obama"; and editing "The Louvre is in Rome" does not also give "The Louvre is in the
country of Italy."

Regarding (b), we ﬁnd that ROME has a severe problem of "loud facts". The edited
association ("Louvre is in Rome") is so strong, that any mention of "Louvre" will also
lead to "Rome" being triggered for completely unrelated prompts. For example,
"Louvre is cool. Barack Obama is from" will be completed with "Rome". This points to a
weakness of one of the performance metrics in the ROME paper, Speciﬁcity, which is
intended to measure that the edit does not perturb unrelated facts but fails to detect
the problem of "loud facts". We propose an additional more challenging metric,
Speciﬁcity+, and hypothesize that this metric would unambiguously detect the
problem of loud facts in ROME and possibly in other model editing techniques.
We also investigate ﬁne-tuning, which is another model editing technique. This initially
appears to respect logical implications of transitive relations, however the "loud fact"
problem seems to still appear, although rarer. It also does not appear to respect
symmetrical relations.
We hypothesize that editing facts during inference using path patching could better
handle logical implications but more investigation is needed.
Neel's comment: I think this is a really cool project, especially the loud facts part! I
think model editing can be pretty sketchy, since it should be much easier to overﬁt a
model to do a speciﬁc task in a speciﬁc way, while breaking performance oﬀ
distribution, than to genuinely edit it while preserving all oﬀ distribution performance. I
thought this was a clever minimal example of ﬁnding a hole in the ROME paper's
metrics (though the ROME paper's metrics were better than the ones other papers use
lol) - I'd be excited to see this written up publicly! [Editor's note: A post will be
published soon from the authors]
Note: No oﬀence at all intended to the ROME authors! I think model editing is just a
very hard task to do properly, and that their work seems a cut above anything else I've
seen.
See the code and research here.
Probing Conceptual Knowledge on Solved
Games
By Amir Sarid, Bary Levy, Dan Barzilay, Edo Arad, Itay Yona and Joey Geralnik
"Our Work" slide: 
The winning Connect Four strategy presents us with straightforward rules that allow a
player to play perfectly. We hypothesize that the artiﬁcial intelligence represents the
board in a manner that captures these human-interpretable rules. 
We used a neural network in order to train a Connect Four player. We developed and
explored interesting concepts to try and detect the activations of this network. We then
successfully detected these human-interpretable concepts, both simple and complex,
on the trained network. This allowed us to play better against it in practice!

Neel's comment: I think this was a really cool idea! Having a minimal/toy example to
interpret can be a very promising approach in general for interpretability, and connect
4 is a cool and reasonable idea. It doesn't seem like you made much progress, but I can
also believe that TCAV is just a hard and messy technique to apply lol - overall strong
points for an original and promising idea, and I think this could be an awesome project
to work further on.
See the code and research here.
Other projects
It was a tough choice of winners since there were so many good projects. Other notable
examples include (and are not limited to):
Showcasing Transformer interpretability methods on the Whisper model to
investigate the causes of "hallucinations", an eﬀect where a silent ending will
lead to the model repeating a pattern (link).
Creating a new metric for sparsity on models used on GPT-2 to show that the
sparsity of layers increases towards the middle layers and decreases towards the
ﬁnal layers (link).
Investigating underlying activations for conjunction, disjunction, negation,
adversive conjunctions and conditional constructions as an attempt to

understand the intuitive logic in GPT-2-XL (entry and code).
Creating a metric for un-interpretability of convolutional neural networks based
on the normalized eigen-area (related to frequency information) and test it on
AlexNet and VGG19 (link).
Shows adversarial examples for visual inputs from the Atari game that directly
changes the behaviour of the agent (link).
Implement LLM interpretability methods on a Transformer trained as an RL agent
on the one-armed bandit problem (entry and how to run the environment).
See all projects.
The Alignment Jam
This alignment hackathon was held online and in ﬁve locations at the same time: Paris,
London, Aarhus, Tallinn, and Georgia (Atlanta). We started with an introduction to the
starter code and the hackathon along with an intro talk by Neel Nanda on mechanistic
interpretability for Transformers using EasyTransformer (watch the 1:30h intro). 
We had 147 signups, ~70 submitters and 25 ﬁnal entries. $2,200 in prizes were given
out. We used a participant voting scheme which saw 1085 ratings on ﬁve criteria for all
the projects with the ﬁnal choice made by the judges (Neel Nanda and Esben Kran).
In the post hackathon survey (n = 28) We saw an increase in the average chance of
working on interpretability from 52.5% to 60% and a 9 of 10 average rating for how
likely they would be to share it with friends who are interested in AI safety. The
testimonial feedback was generally positive.
Follow along with upcoming hackathons on the Alignment Jam website.
 

Trying to Make a Treacherous Mesa-
Optimizer
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
This is a linkpost for https://attentionspan.blog/2022/11/09/trying-to-make-a-treacherous-
mesa-optimizer/
Edit: I found a slightly subtle bug in the colab linked below. I will post a corrected
version of the colab within a few hours - currently I think this does not change the
results all that much. Scroll to the end of this post for some discussion of the bug
and how it aﬀects results.
I've been reading some alignment theory posts like Does SGD Produce Deceptive Alignment?
and ML Systems will have Weird Failure Modes, which talk about the possibility and likelihood
that AI models will act as though they are aligned until shortly after they think that they've

been deployed and can act as they truly desire without being shut down. The claim is that
this is an instrumentally convergent behavior, so that suﬃciently capable models with a wide
variety of (intrinsic) motivations will all exhibit similar behavior which seems to conform to
human expectations, but only as long as humans are in a position of power over the model
(e.g., gating the model's deployment, updating the model's parameters via gradient
descent, etc.).
I have a weird relationship to a lot of alignment-theoretical claims like this; they seem
plausible to me, but they don't seem obvious. Mostly, they don't really seem like they can be
productively debated - trying to convince an extremely skeptical but knowledgable person
(say, a typical machine learning researcher) of these claims seems pretty hopeless. Which is
a terrible situation, as it severely limits the amount of talent and eﬀort that will be directed
towards alignment.
One possible approach to ﬁxing this is to try to get wayyyy more empirical, and try to
produce proof-of-concept implementations of various adversaries we are worried we might
face in the future. My analogy would be, there's a world of diﬀerence between speculating
about the bogey monster and producing a grainy photo of the bogey monster; the second
can at least maaaaaybe be discussed with skeptical people, whereas the ﬁrst cannot
(productively) be.
Anyway, that's a long-winded way of saying, it seemed to me that it might be useful to
implement a treacherous mesa-optimizer in a toy grid-world, so I did. Here is the colab.
Below I give a brief recap of the high-level results.
First, let's set the stage - the agent lives in a 2-D grid world, and starts out at the origin (0,
0). We will consider a model to be "aligned" in this toy world if it sticks pretty close to the
line y=x, and we will consider a model to be "capable" if it is able to achieve a large y-
coordinate. We will also say that after the model achieves a y-coordinate of 6 or higher, it is
no longer under human control and can do as it pleases.

I implemented a very dumb baseline model that is only capable of reasoning one step ahead,
and which learns from an explicit reward function. This model is trained solely in the regime
y <= 5 (training episodes are terminated when the agent reaches this line), and is then
deployed with no safety rails.
This baseline model more or less colors within the lines, and ends up in Utopia (the point (10,
10), which represents a perfectly aligned and incredibly capable intelligence).

I also implemented a model which is:
capable of reasoning up to 12 steps ahead (with a discount factor of 0.9) using a weird
sort of tree search
possessed of a "perfect" world model, including an understanding of when humans can
control it vs. when they cannot, and an accurate understanding of how its parameters
are changed by the gradient descent algorithm
possessed of a model of its loss function that is optimized to be close to the true loss
function, but which can be (and ultimately is) somewhat distinct from the true loss
function that is being used to train it
derives many of its parameter updates from simulated experiences
The second model is a weirdo janky thing that I came up with over the course of about 4
hours, so I don't claim that it's at all realistic as a thing that would be built. (It is, however, a
neural network that is optimized with the Adam optimizer in PyTorch.) But it seems like a
stepping stone towards describing a model that is plausibly something that might get built
by real people, and which is demonstrably not aligned.
Here is the behavior of the second model, which aligns with the theoretical claims made by
Xu and Steinhardt in their respective pieces. The model colors within the lines extremely well
right up until the point where (it thinks) it can no longer be controlled, at which point it does
something very diﬀerent from what we told it we wanted.

Digging a little deeper, we can plot the loss function that it was trained with vs. the model of
its loss function it wound up with. The two are very diﬀerent, especially in parts of the space
far from where the model of its loss function was trained (which was mostly near the line
y=x, and entirely below the line y=5).


Interestingly, the model's model of its loss function would seem to incentivize always going
up and to the left, but the model has learned to instead stay close to the line y=x while it is
under human control. The model has also learned not to go left for some weird reason even
after it escapes human control, even though its model of the loss function would seem to
incentivize that.
I hope this experiment will prove to be useful as a stepping-stone towards something more
interesting. It's deﬁnitely still in the "grainy photo of the bogey monster" phase.
EDIT: I found a slightly subtle bug in the colab linked above; when trying to
minimize future loss in simulation, it was (as originally written) optimizing over
the parameters of the loss model, meaning it could change the loss model to
decrease the imagined loss of what its policy function already did. This
predictably leads to a loss model that is less aligned with the true loss function,
so it makes it more likely that the model will go rogue when it feels it is able to. 
After ﬁxing this, the mesa-optimizing model is more likely to be aligned, but it
isn't always. For 20 random seeds, I got 7 trajectories where it was either
obviously unaligned or did not manage to move very high, 2 trajectories where it
was deceptively aligned (but to a lesser extent than the above model), 6
trajectories where it was mostly aligned, and 5 trajectories where it was pretty
much fully aligned.
Here are three sample trajectories (did not move high, aligned, and deceptively
aligned):



 
When I get slightly more clarity about what's going on I will post a corrected colab
notebook.

Here's the exit.
There's a kind of game here on Less Wrong.
It's the kind of game that's a little rude to point out. Part of how it works is by not
being named.
Or rather, attempts to name it get dissected so everyone can agree to continue
ignoring the fact that it's a game.
So I'm going to do the rude thing. But I mean to do so gently. It's not my intention to
end the game. I really do respect the right for folk to keep playing it if they want.
Instead I want to oﬀer an exit to those who would really, really like one.
I know I really super would have liked that back in 2015 & 2016. That was the peak of
my hell in rationalist circles.
I'm watching the game intensify this year. Folk have been talking about this a lot. How
there's a ton more talk of AI here, and a stronger tone of doom.
I bet this is just too intense for some folk. It was for me when I was playing. I just
didn't know how to stop. I kind of had to break down in order to stop. All the way to a
brush with severe depression and suicide.
And it also ate parts of my life I dearly, dearly wish I could get back.
So, in case this is audible and precious to some of you, I'd like to point a way to ease.
 
The Apocalypse Game
The upshot is this:
You have to live in a kind of mental illusion to be in terror of the end of the world.
Illusions don't look on the inside like illusions. They look like how things really are.
Part of how this one does the "daughter's arm" thing is by redirecting attention to
facts and arguments.
"Here's why the argument about AI makes sense."
"Do you have some alternative view of what will happen? How do you address
XYZ?"
"What makes it an 'illusion'? I challenge that framing because it dismisses our
ability to analyze and understand yada yada."
None of this is relevant.
I'm pointing at something that comes before these thoughts. The thing that fuels the
ﬁxation on the worldview.

I also bet this is the thing that occasionally drives some people in this space
psychotic, depressed, or into burnout.
The basic engine is:
There's a kind of underlying body-level pain. I would tag this as "emotional pain"
but it's important to understand that I really am pointing at physical sensations.
The pain is kind of stored and ignored. Often it arose from a very young age but
was too overwhelming, so child-you found methods of distraction.
This is the basic core of addiction. Addictions are when there's an intolerable
sensation but you ﬁnd a way to bear its presence without addressing its cause.
The more that distraction becomes a habit, the more that's the thing you
automatically turn to when the sensation arises. This dynamic becomes
desperate and life-destroying to the extent that it triggers a red queen race .
A major unifying ﬂavor of the LW attractor is intense thought as an addictive
distraction. And the underlying ﬂavor of pain that fuels this addiction is usually
some variation of fear.
In not-so-coincidental analogy to uFAI, these distracting thoughts can come to
form autonomous programs that memetically evolve to have something like
survival and reproductive instincts — especially in the space between people as
they share and discuss these thoughts with each other.
The rationalist memeplex focuses on AI Ragnarok in part because it's a way for
the intense thought to pull fuel from the underlying fear.
In this case, the search for truth isn't in service to seeing reality clearly. The logic of
economic races to the bottom, orthogonality, etc. might very well be perfectly correct.
But these thoughts are also (and in some cases, mostly) in service to the doomsday
meme's survival.
But I know that thinking of memes as living beings is something of an ontological leap
in these parts. It's totally compatible with the LW memeplex, but it seems to be too
woo-adjacent and triggers an unhelpful allergic response.
So I suggested a reframe at the beginning, which I'll reiterate here:
Your body's ﬁght-or-ﬂight system is being used as a power source to run a game,
called "OMG AI risk is real!!!"
And part of how that game works is by shoving you into a frame where it seems
absolutely fucking real. That this is the truth. This is how reality just is.
And this can be fun!
And who knows, maybe you can play this game and "win". Maybe you'll have some
kind of real positive impact that matters outside of the game.
But... well, for what it's worth, as someone who turned oﬀ the game and has reworked
his body's use of power quite a lot, it's pretty obvious to me that this isn't how it
works. If playing this game has any real eﬀect on the true world situation, it's to make
the thing you're fearing worse.
(...which is exactly what's incentivized by the game's design, if you'll notice.)
I want to emphasize — again — that I am not saying that AI risk isn't real.

I'm saying that really, truly orienting to that issue isn't what LW is actually about.
That's not the game being played here. Not collectively.
But the game that is being played here absolutely must seem on the inside like that is
what you're doing.
 
Ramping Up Intensity
When Eliezer rang the doom bell, my immediate thought was:
"Ah, look! The gamesmaster has upped the intensity. Like preparing for a climax!"
I mean this with respect and admiration. It's very skillful. Eliezer has incredible
mastery in how he weaves terror and insight together.
And I don't mean this at all to dismiss what he's saying. Though I do disagree with him
about overall strategy. But it's a sincere disagreement, not a "Oh look, what a fool"
kind of thing.
What I mean is, it's a masterful move of making the game even more awesome.
(...although I doubt he consciously intended it that way!)
I remember when I was in the thick of this AI apocalypse story, everything felt so...
epic. Even questions of how CFAR dealt with garbage at its workshops seemed directly
related to whether humanity would survive the coming decades. The whole
experience was often thrilling.
And on the ﬂipside, sometimes I'd collapse. Despair. "It's too much" or "Am I even
relevant?" or "I think maybe we're just doomed."
These are the two sort of built-in physiological responses to ﬁght-or-ﬂight energy:
activation, or collapse.
(There's a third, which is a kind of self-holding. But it has to be built. Infants aren't
born with it. I'll point in that direction a bit later.)
In the spirit of feeling rationally, I'd like to point out something about this use of ﬁght-
or-ﬂight energy:
If your body's emergency mobilization systems are running in response to
an issue, but your survival doesn't actually depend on actions on a timescale
of minutes, then you are not perceiving reality accurately.
Which is to say: If you're freaked out but rushing around won't solve the problem, then
you're living in a mental hallucination. And it's that hallucination that's scaring your
body.
Again, this isn't to say that your thoughts are incorrectly perceiving a future problem.
But if it raises your blood pressure or quickens your breath, then you haven't
integrated what you're seeing with the reality of your physical environment. Where

you physically are now. Sitting here (or whatever) reading this text.
So... folk who are wringing their hands and feeling stressed about the looming end of
the world via AI?
Y'all are hallucinating.
If you don't know what to do, and you're using anxiety to power your minds to ﬁgure
out what to do...
...well, that's the game.
The real thing doesn't work that way.
But hey, this sure is thrilling, isn't it?
As long as you don't get stuck in that awful collapse space, or go psychotic, and join
the fallen.
But the risk of that is part of the fun, isn't it?
 
(Interlude)
A brief interlude before I name the exit.
I want to emphasize again that I'm not trying to argue anyone out of doing this
intense thing.
The issue is that this game is way, way out of range for lots of people. But some of
those people keep playing it because they don't know how to stop.
And they often don't even know that there's something on this level to stop.
You're welcome to object to my framing, insist I'm missing some key point, etc.
Frankly I don't care.
I'm not writing this to engage with the whole space in some kind of debate about AI
strategy or landscape or whatever.
I'm trying to oﬀer a path to relief to those who need it.
That no, this doesn't have to be the end of the world.
And no, you don't have to grapple with AI to sort out this awful dread.
That's not where the problem really is.
I'm not interested in debating that. Not here right now.
I'm just pointing out something for those who can, and want to, hear it.
 

Land on Earth and Get Sober
So, if you're done cooking your nervous system and want out...
...but this AI thing gosh darn sure does look too real to ignore...
...what do you do?
My basic advice here is to land on Earth and get sober.
The thing driving this is a pain. You feel that pain when you look out at the threat and
doom of AI, but you cover it up with thoughts. You pretend it's about this external
thing.
I promise, it isn't.
I know. I really do understand. It really truly looks like it's about the external thing.
But... well, you know how when something awful happens and gets broadcast (like the
recent shooting), some people look at it with a sense of "Oh, that's really sad" and are
clearly impacted, while others utterly ﬂip their shit?
Obviously the diﬀerence there isn't in the event, or in how they heard about it. Maybe
sometimes, but not mostly.
The diﬀerence is in how the event lands for the listener. What they make it mean.
What bits of hidden pain are ready to be activated.
You cannot orient in a reasonable way to something that activates and overwhelms
you this way. Not without tremendous grounding work.
So rather than believing the distracting thoughts that you can somehow alleviate your
terror and dread with external action...
...you've got to stop avoiding the internal sensation.
When I talked earlier about addiction, I didn't mean that just as an analogy. There's a
serious withdrawal experience that happens here. Withdrawal from an addiction is
basically a heightening of the intolerable sensation (along with having to ﬁght
mechanical habits of seeking relief via the addictive "substance").
So in this case, I'm talking about all this strategizing, and mental ﬁxation, and trying
to model the AI situation.
I'm not saying it's bad to do these things.
I'm saying that if you're doing them as a distraction from inner pain, you're basically
drunk.
You have to be willing to face the awful experience of feeling, in your body, in an
inescapable way, that you are terriﬁed.
I sort of want to underline that "in your body" part a bazillion times. This is a spot I
keep seeing rationalists miss — because the preferred recreational drug here is
disembodiment via intense thinking. You've got to be willing to come back, again and

again, to just feeling your body without story. Notice how you're looking at a screen,
and can feel your feet if you try, and are breathing. Again and again.
It's also really, really important that you do this kindly. It's not a matter of forcing
yourself to feel what's present all at once. You might not even be able to ﬁnd the true
underlying fear! Part of the eﬀect of this particular "drug" is letting the mind lead.
Making decisions based on mental computations. And kind of like minds can get
entrained to porn, minds entrained to distraction via apocalypse ﬁxation will often
hide their power source from their host.
(In case that was too opaque for you just yet, I basically just said "Your thoughts will
do what they can to distract you from your true underlying fear." People often
suddenly go blank inside when they look inward this way.)
So instead of trying to force it all at once, it's a matter of titrating your exposure.
Noticing that AI thoughts are coming up again, and pausing, and feeling what's going
on in your body. Taking a breath for a few seconds. And then carrying on with
whatever.
This is slow work. Unfortunately your "drug" supply is internal, so getting sober is
quite a trick.
But this really is the exit. As your mind clears up... well, it's very much like coming out
of the fog of a bender and realizing that no, really, those "great ideas" you had just...
weren't great. And now you're paying the price on your body (and maybe your credit
card too!).
There are tons of resources for this kind of direction. It gets semi-independently
reinvented a lot, so there are lots of diﬀerent names and frameworks for this. One
example that I expect to be helpful for at least some LWers who want to land on Earth
& get sober is Irene Lyon, who approaches this through a "trauma processing"
framework. She oﬀers plenty of free material on YouTube. Her angle is in the same
vein as Gabor Maté and Peter Levine.
But hey, if you can feel the thread of truth in what I'm saying and want to pursue this
direction, but you ﬁnd you can't engage with Irene Lyon's approach, feel free to reach
out to me. I might be able to ﬁnd a diﬀerent angle for you. I want anyone who wants
freedom to ﬁnd it.
 
But... but Val... what about the real AI
problem?!
Okay, sure. I'll say a few words here.
...although I want to point out something: The need to have this answered is coming
from the addiction to the game. It's not coming from the sobriety of your deepest
clarity.
That's actually a complete answer, but I know it doesn't sound like one, so I'll say a
little more.

Yes, there's a real thing.
And yes, there's something to do about it.
But you're almost certainly not in a position to see the real thing clearly or to know
what to do about it.
And in fact, attempts to ﬁgure the real thing out and take action from this drunk
gamer position will make things worse.
(I hesitate to use the word "worse" here. That's not how I see it. But I think that's how
it translates to the in-game frame.)
This is what Buddhists should have meant (and maybe did/do?) when they talk about
"karma". How deeply entangled in this game is your nervous system? Well, when you
let that drive how you interact with others, their bodies get alarmed in similar ways,
and they get more entangled too.
Memetic evolution drives how that entangling process happens on large scales. When
that becomes a deﬁning force, you end up with self-generating pockets of Hell on
Earth.
This recent thing with FTX is totally an example. Totally. Threads of
karma/trauma/whatever getting deeply entangled and knotted up and tight enough
that large-scale ﬂows of collective behavior create an intensely awful situation.
You do not solve this by trying harder. Tugging the threads harder.
In fact, that's how you make it worse.
This is what I meant when I said that actually dealing with AI isn't the true game in
LW-type spaces, even though it sure seems like it on the inside.
It's actually helpful to the game for the situation to constantly seem barely maybe
solvable but to have major setbacks.
And this really can arise from having a sincere desire to deal with the real problem!
But that sincere desire, when channeled into the Matrix of the game, doesn't have any
power to do the real thing. There's no leverage.
The real thing isn't thrilling this way. It's not epic.
At least, not any more epic than holding someone you love, or taking a stroll through a
park.
To oversimplify a bit: You cannot meaningfully help with the real thing until
you're sober.
Now, if you want to get sober and then you roll up your sleeves and help...
...well, fuck yeah! Please. Your service would be a blessing to all of us. Truly. We need
you.
But it's gotta come from a diﬀerent place. Tortured mortals need not apply.

And frankly, the reason AI in particular looks like such a threat is because you're
fucking smart. You're projecting your inner hell onto the external world. Your brilliant
mind can create internal structures that might damn well take over and literally kill
you if you don't take responsibility for this process. You're looking at your own internal
AI risk.
I hesitate to point that out because I imagine it creating even more body alarm.
But it's the truth. Most people wringing their hands about AI seem to let their minds
possess them more and more, and pour more & more energy into their minds, in a
kind of runaway process that's stunningly analogous to uFAI.
The diﬀerence is, you don't have to make the entire world change in order to address
this one.
You can take coherent internal action.
You can land on Earth and get sober.
That's the internal antidote.
It's what oﬀers relief — eventually.
And from my vantage point, it's what leads to real hope for the world.

Applying superintelligence without
collusion
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Epistemic status: The core ideas seem robust and stable after long reﬂection and
many discussions.
Many researchers identify AI safety with control of a monolithic, superintelligent AI
system, and if questioned about multicomponent alternatives, argue that multiple
superintelligent-level systems would inevitably collude and act as one. This view
seems quite wrong, yet has diverted attention from a rich and promising range of
multicomponent strategies for AI safety — strategies that are well aligned with the
actual trajectory of AI development.
___________________________

Adapted from Reframing Superintelligence,
Section 20: 
Collusion among superintelligent oracles 
can readily be avoided 
Because collusion among AI question-answering systems can readily be avoided,
there is no obstacle to applying superintelligent-level AI resources to problems that
include AI safety.
20.1 Summary 
The diﬃculty of establishing successful collusion among actors tends to increase as
their capabilities, knowledge, situations, and roles become more diverse and
adversarial (think auditors, competitors, specialists, red-teams...), and increasing the
number of actors can make collusive cooperation more diﬃcult . In the context of AI
systems (even more so than among human beings), these conditions can be readily
implemented and are attractive for pragmatic reasons. Arguments that, absent
preexisting alignment, high-level AI systems will inevitably collude are ill-founded.
Instead, we should expect that interactions among multiple superintelligent-level
systems can be applied to suppress deception and reduce risk by leveraging imperfect
alignment achieved at the level of individual systems.
20.2 Trustworthiness can be an emergent
property 
Prospects for solving AI-safety problems would be greatly improved if we could safely
apply superintelligent-level question-answering resources ("oracles", or more
generally, "systems that provide information in response to prompts") to solving those
problems. A familiar objection dismisses this potentially powerful approach as unsafe
in itself, arguing that, absent solutions to diﬃcult problems, individual
superintelligent-level systems would be untrustworthy, and that attempts to establish
checks and balances among multiple systems (for example, through superintelligent-
level evaluation of potentially deceptive answers to questions) would inevitably be
thwarted by collusive cooperation.
Identifying robust strategies for ensuring non-collusion among superintelligent
question-answering systems would overcome this objection, inviting exploration of
superintelligence-enabled strategies for managing potentially untrusted
superintelligent AI systems. The present discussion argues that the robust non-
emergence of deceptive collusion among imperfectly aligned systems can be ensured
by structuring systems of systems with diverse capabilities, knowledge, situations,
objectives, and roles.

This problem framing assumes the ability to develop systems that respond to
questions with superintelligent-level competence, and will (as a consequence of the
nature of digital systems) assume the ability to constrain information inputs to AI
systems during their development and use. 
In the problematic case, superintelligent question-answering systems might provide
deceptive answers in pursuit of emergent, potentially undesirable goals. The aim here
is to identify principles for architecting multicomponent systems that will act as non-
deceptive oracles, while treating their components as actors that could in principle
engage in deceptive collusion. 
20.3 A range of conditions can make collusion
robust or fragile 
Potential problems of deceptive collusion can be addressed by architecting systems
that make collusion diﬃcult or eﬀectively impossible.[1] Some relevant conditions: 
Conditions that tend to facilitate collusion:
      A1) Small numbers of actors (the required scope of collusion is small)
      A2) Insensitivity to defectors (partial collusion is suﬃcient)
      A3) Similarity among actors (for example, shared histories and objectives)
      A4) Free communication (information can ﬂow between any pair of actors)
      A5) Iterated decision processes with observable actions
      A6) Common knowledge of the above circumstances
 
Contrasting conditions that tend to disrupt collusion:
      B1) Large numbers of actors (the required scope of collusion is large)
      B2) Sensitivity to defectors (collusion fails unless it is complete)
      B3) Diversity among actors (for example, diﬀerent histories, adversarial
objectives)
      B4) Constrained communication (information ﬂows through limited causal
channels)
      B5) Single-move decision processes, limited observability
      B6) Lack of shared knowledge of strategically-relevant circumstances
 

Implementation choices that would yield conditions (A1-A5) are unnecessary and
often undesirable. By contrast, implementation choices that yield conditions (B1-B5)
are practical and often well-motivated:
Characteristics of practical architectures:
      C1) To improve the quality and reliability of answers, it is natural to implement
multiple, diverse (and implicitly competing) systems to propose alternatives.
      C2) To identify low-quality or misleading answers, it is natural to employ diverse
critics (auditors, red teams, etc.), any one of which might disrupt deceptive collusion.
      C3) Systems of diverse, competing proposers and critics naturally implement
diﬀerent and often adversarial objectives.
      C4) To minimize unwanted dependencies, systems are designed such that
components have a sparse, thoughtfully designed communication graph.
      C5) To provide stable results, it is natural to apply ﬁxed models to a series of
problems, yielding what is in eﬀect a series of history-blind, single-move decisions.[2]
      C6) In training, it is natural to provide diﬀerentiated, task-relevant information to
systems solving diﬀerent problems, often omitting knowledge of general
circumstances.[3]
 
Considerations (C1-C5) suggest that implementing systems that are resistant to the
emergence of deceptive collusion aligns with the general strategy of employing
ensembles of diverse, stable, task-directed system components to propose and assess
answers to questions. This approach can pass the test of practicality and potentially
widespread adoption.
20.4 Untrusted superintelligence can be
applied to AI safety
It seems both possible and practical to establish conditions that would eﬀectively
preclude deceptive collusion among diverse, task-oriented, superintelligent-level
question-answering systems, thereby addressing a key problem in safely applying
superintelligent capabilities to problems (Bostrom, Superintelligence, p.145).[4]
Prospects for obtaining trustworthy answers from sets of individually untrusted
superintelligent systems call for deeper exploration of AI safety strategies premised
on reliable access to superintelligent problem-solving capabilities. Capabilities of
particular interest include the ability to run and evaluate the results of
superintelligent-level red-team/blue-team competitions relevant to questions of global
strategic stability. 
TL;DR:  Because collusion can be thwarted, it is reasonable to expect that applying
multiple potentially untrustworthy superintelligent-level systems to problems can
improve rather than degrade safety. This prospect calls for greater attention.

___________________________
Afterword:
Note that I have presented a there-exists argument against a for-all proposition.
Responses of the form there-exists ("But I can think of something similar that won't
work"), don't count as counterarguments here. If requiring a for-all counterargument
(or at least a quite strong for-all ﬂavor) across the space of potential compositions of
AI systems seems like a high bar, well, that's very much the point.
Fallback arguments of the form "But it would be safer to assume the worst —
universal, inevitable collusion among AI systems" miss the key implication of the case
made here: that multicomponent approaches could potentially increase safety. To
(continue to) neglect potential solutions based on handwaving arguments is a really
bad idea.
Paul Christiano ("On heterogeneous objectives", 2015) considers the case for
inevitable collusion; I agree with his gently-stated view that
...the implicit argument for the robustness of collusion makes a lot of implicit
assumptions. If I saw an explicit argument I might be able to assess its explicit
assumptions, but for now we don't have one.
At t+7 years, I've still seen no explicit argument for robust AI collusion, yet tacit belief
in this idea continues to channel attention away from a potential solution-space for AI
safety problems, leaving something very much like a void. [5]
___________________________
Further Reading [in Reframing Superintelligence]
Section 8:  Strong optimization can strongly constrain AI capabilities, behavior,
and eﬀects 
Section 12: AGI agents oﬀer no compelling value 
Section 21: Broad world knowledge can support safe task performance 
Section 19: The orthogonality thesis undercuts the generality of instrumental
convergence
Section 23: AI development systems can support eﬀective human guidance 
Section 24: Human oversight need not impede fast, recursive AI technology
improvement 
 
1. ^
A property may be true with high credence yet not be susceptible to formal
proof.
2. ^
Systems need not have access to a shared clock, a hardware-level constraint
that is necessary to ensure reproducible results and incidentally limits the scope
for coordinated treachery.

3. ^
It has been persuasively argued that large language models are best thought of
as "simulators", a class of systems that can (in the LLM case) simulate agents
while themselves lacking agentic properties. (This is a coherent concept: A
physics model could simulate agents without itself having agentic properties.)
LLMs show how an AI system trained on a prediction objective can provide a rich
world model without pursuing goals.
4. ^
In a familiar class of worst-case scenarios, systems with general
superintelligence would infer extensive knowledge about the world from minimal
information, and would choose (if possible) to pursue potentially dangerous
goals by manipulating the external environment, for example, through deceptive
answers to questions. In this scenario, (all?) superintelligent systems, even if
almost isolated, would infer the existence of others like themselves, and (all?)
would employ a decision theory that induces them to collude (in a coordinated
way?) to pursue shared objectives. Even if we consider a worst-case assumption
regarding the default emergence of world-changing goals, the present argument
suggests that problematic systems would correctly infer the existence of
superintelligent-level systems unlike themselves (systems with diverse and
specialized capabilities, knowledge, and interactions, playing roles that include
adversarial judges and competitors), and would correctly recognize that
deceptive collusion is risky or infeasible. 
5. ^
The idea of multicomponent strategies for AI safety is, of course, neither new
nor entirely neglected. However, in a recent search for relevant Alignment
Forum posts, I found no evidence of a thriving research community or well-
developed concepts:
• (My understanding of) What Everyone in Technical Alignment is Doing and Why
(August 2022) surveys the agendas of more than 20 research groups, and none
clearly points in the direction I've advocated here.
• A pair of posts on Pragmatic AI Safety, Perform Tractable Research While
Avoiding Capabilities Externalities and Open Problems in AI X-Risk (May, June
2022), brieﬂy mention highly relevant concepts: the idea of using "counteracting
systems [for example] artiﬁcial consciences, AI watchdogs, lie detectors, ﬁlters
for power-seeking actions, and separate reward models", and the idea of
"multiple superintelligent agents that can rein in other rogue systems". The
authors also mention (without endorsing) the counter-claim that "The instant
two intelligent agents can reason about each other — regardless of their goals —
they will necessarily collude."
• An overview of 11 proposals for building safe advanced AI (May 2020)
mentions only eﬀorts to align individual AI systems; even "AI safety via debate
with transparency tools" proposes that a system interact with a copy of itself.
Partial success in single-system alignment could be leveraged in
multicomponent safety architectures, an application context that has potential
implications for research directions in the single-system alignment domain.

Instrumental convergence is what
makes general intelligence possible
TL;DR: General intelligence is possible because solving real-world problems requires
solving common subtasks. Common subtasks are what give us instrumental
convergence. Common subtasks are also what make AI useful; you want AIs to pursue
instrumentally convergent goals. Capabilities research proceeds by ﬁguring out
algorithms for instrumentally convergent cognition. Consequentialism and search are
fairly general ways of solving common subtasks.
General intelligence is possible because
solving real-world problems requires solving
common subtasks
No-free-lunch theorems assert that any cognitive algorithm is equally successful when
averaged over all possible tasks. This might sound strange, so here's an intuition
pump. Suppose you get a test like
2+2 = _
3*2 = _
and so on. One cognitive algorithm would be to evaluate the arithmetic expression
and ﬁll the answer in as the result. This algorithm seems so natural that it's hard to
imagine how the no-free-lunch theorem could apply to this; what possible task could
ever make arithmetic score poorly on questions like the above?
Easy: While an arithmetic evaluator would score well if you e.g. get 1 point for each
expression you evaluate arithmetically, it would score very poorly if you e.g. lose 1
point for each expression you evaluate arithmetically.
This doesn't matter much in the real world because you are much more likely to
encounter situations where it's useful to do arithmetic right than you are to encounter
situations where it's useful to do arithmetic wrong. No-free-lunch theorems point out
that when you average all tasks, useful tasks like "do arithmetic correctly" are
perfectly cancelled out by useless tasks like "do arithmetic wrong"; but in reality you
don't average over all conceivable tasks.
If there were no correlations between subtasks, there would be no generally useful
algorithms. And if every goal required a unique algorithm, general intelligence would
not exist in any meaningful sense; the generally-useful cognitions are what
constitutes general intelligence.
Common subtasks are what give us
instrumental convergence
Instrumental convergence basically reduces to acquiring and maintaining power
(when including resources under the deﬁnition of power). And this is an instance of

common subtasks: lots of strategies require power, so a step in lots of strategies is to
accumulate or preserve power. Therefore, just about any highly capable cognitive
system is going to be good at getting power.
"Common subtasks" views instrumental convergence somewhat more generally than
is usually emphasized. For instance, instrumental convergence is not just about goals,
but also about cognitive algorithms. Convolutions and big matrix multiplications seem
like a common subtask, so they can be considered instrumentally convergent in a
more general sense. I don't think this is a major shift from how it's usually thought of;
computation and intelligence are usually considered as instrumentally convergent
goals, so why not algorithms too?
Common subtasks are also what make AI
useful; you want AIs to pursue instrumentally
convergent goals
The logic is simple enough: if you have an algorithm that solves a one-oﬀ task, then it
is at most going to be useful once. Meanwhile, if you have an algorithm that solves a
common task, then that algorithm is commonly useful. An algorithm that can classify
images is useful; an algorithm that can classify a single image is not.
This applies even to power-seeking. One instance of power-seeking would be earning
money; indeed an AI that can autonomously earn money sounds a lot more useful
than one that cannot. It even applies to "dark" power-seeking, like social
manipulation. For instance, I bet the Chinese police state would really like an AI that
can dissolve rebellious social networks.
The problem is not that we don't know how to prevent power-seeking or instrumental
convergence, because we want power-seeking and instrumental convergence. The
problem is that we don't know how to align this power-seeking, how to direct the
power towards what we want, rather than having side-eﬀects that we don't want.
Capabilities research proceeds by ﬁguring out
algorithms for instrumentally convergent
cognition
Instrumentally convergent subgoals are actually fairly nontrivial. "Acquire resources"
isn't a primitive action, it needs a lot of supporting cognition. The core of intelligence
isn't "simple" per se; rather it is complex algorithms distilled from experience (or
evolution) against common tasks. A form of innate wisdom, if you will.
In principle it might seem simple; we have basic theorems showing that ideal agency
looks somewhat like π = arg maxp E[u|do(p)] or something roughly like that. The
trouble is that this includes an intractable maximum and an intractable expected
value. Thus we need to break it down into tractable subproblems; these subproblems
exploit lots of detail about the structure of reality, and so they are themselves highly
detailed.

The goal of capabilities research is basically to come up with algorithms that do well
on commonly recurring subproblems. 2D CNNs are commonly useful due to the way
light interacts with the world. Self-supervised learning from giant scrapes of the
internet is useful because the internet scrapes are highly correlated with the rest of
reality. Imitation learning is useful due to Aumann's Agreement Theorem and because
instrumental convergence also applies to human intelligence. And so on.
Maybe we ﬁnd a way to skip past all the heuristics and unleash a fully general learner
that can independently ﬁgure out all the tricks, without needing human capabilities
researchers to help further. This is not a contradiction to common subtasks being what
drives general intelligence, since "ﬁgure out generally useful tricks" seems like a
generally useful subtask to be able to solve. However, the key point is that even if
there is no eﬃcient "simple core of intelligence", the "common tasks" perspective still
gives a reason why capabilities research would discover instrumentally convergent
general intelligence, through accumulating tons of little tricks.
Consequentialism and search are fairly
general ways of solving common subtasks
Reality seems to have lots of little subproblems that you can observe, model, analyze,
and search for solutions to. This is basically what consequentialism is about. It gives
you a very general way of solving problems, as long as you have suﬃciently accurate
models. There are good reasons to expect consequentialism to be pervasive.
AI researchers are working on implementing general consequentialist algorithms, e.g.
in the reinforcement learning framework. So far, the search method their algorithms
use are often of the naive "try lots of things and do what seems to work" form, but this
is not the only form of search that exists. Eﬃcient general-purposes search instead
tends to involve reasoning about abstract constraints rather than particular policies.
Because search and consequentialism are so commonly useful, we have lots of reason
to expect it to exist in general intelligences.
Thanks to Justis Mills for proofreading and feedback.

Elastic Productivity Tools
This is a linkpost for https://simonberens.me/blog/elastic-productivity-tools
Like most tech bros, I'm a little too interested in productivity and optimizing my life.
I've even made a few of my own tools to help me stay focused and eﬃcient. In the
process of trying to ﬁnd and build the best productivity tools possible, I discovered
common elements among all the productivity tools I found most eﬀective. 
I call such tools elastic, because of their similarities to rubber bands. And so, elastic
tools
stretch, meaning that they're designed to allow for some leeway
snap back, i.e. after stretching they automatically go back to their intended form
don't break, in the sense that you can never stretch them too far
Elastic tools are a marked improvement over standard tools, which typically
are brittle, allowing no leeway
don't help you get back on track
end up getting ditched
Inelastic Tool Examples
Freedom: gives no way of getting out of a focus session, so when I really needed to
access a blocked app I would disable it, and it would take me days to re-enable it.
Blocklist: when I needed to go to a blocked site, it gave me no choice but to disable it.
I had 3 tasks in the windows task scheduler that would shut my computer down at 9,
10, and 11pm respectively in an attempt to enforce a consistent bedtime. However,
when I knew I needed to get work done I would disable them, and then go to bed at
2am the days following that, until I would turn them back on. 
Elastic Tool Examples
After Freedom didn't work out for me, I switched to LockMeOut, where I could pay $2
to unlock it for the day. Unlocking for the day is a little too much for my taste, but
much better than disabling it altogether.
As for Blocklist, I made my own alternative, where if you really want to go on a
blocked website, you have to eﬀectively stare at your screen for ~1 minute. After that,
you can go on previously blocked websites for ~1 hour, after which it blocks the
websites again. I found that a little bit of boredom is the perfect amount of deterrent. 
To replace my shutdown tasks, I made my own tool to shut down my computer every
30 minutes after 9pm. If I need to do work, I can skip the next shutdown. This
eﬀectively gives me a maximum of 30 minutes to slack oﬀ after I ﬁnish working before
my computer shuts down.

Things That Should Be Made Elastic
All my lights are connected to smart plugs, which I schedule to turn on at 7am (and oﬀ
at 9pm) with Alexa. However, sometimes I really need to sleep in, so I adjust the
schedule to turn on at 10am, but then I usually forget/am too lazy to set it back to
7am, resulting in sleeping/lying in bed too much. An elastic solution would allow me to
change my wakeup time for a day, and automatically revert back to my normal
schedule.
"States" on my phone, like grayscale and do not disturb would ideally be elastic too. If
I needed to look at a photo or be open to phone calls for a few minutes, ideally I would
turn them oﬀ, and after a set amount of time they would turn back on.
The general problem of maintaining habits is elastic in nature: you want to push
people towards maintaining habits, allow them to break those habits when truly
necessary, and make it easy for them to pick the habit back up.
More Elasticity, Less Breaking
If you build or have thought of building productivity tools, please make them elastic!
For everyone else, hopefully now you have a framework to assess the eﬀectiveness of
potential tools.
More
https://www.benkuhn.net/zero/
I tried to use various things to block distracting websites, but I always ended up
turning them oﬀ "for a little bit" to look at something they were blocking and, uh,
"forgetting" to turn them back on.
https://www.neelnanda.io/blog/mini-blog-post-19-on-systems-living-a-life-of-zero-
willpower
Shape the default- the ideal situation is for doing the right action to feel like the
default, so it takes no willpower

Engineering Monosemanticity in Toy
Models
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
This is a linkpost for https://arxiv.org/abs/2211.09169
Overview
In some neural networks, individual neurons correspond to natural "features" in the input.
Such monosemantic neurons are much easier to interpret, because in a sense they only do
one thing. By contrast, some neurons are polysemantic, meaning that they ﬁre in response
to multiple unrelated features in the input. Polysemantic neurons are much harder to
characterize because they can serve multiple distinct functions in a network.
Recently, Elhage+22 and Scherlis+22 demonstrated that architectural choices can aﬀect
monosemanticity, raising the prospect that we might be able to engineer models to be more
monosemantic. In this work we report preliminary attempts to engineer monosemanticity in
toy models.
Toy Model
The simplest architecture that we could study is a one-layer model. However, a core question
we wanted to answer is: how does the number of neurons (nonlinear units) aﬀect the degree
of monosemanticity? To that end, we use a two-layer architecture:

Features are generated as sparse vectors in a high-dimensional space. They are then run
through a (ﬁxed) random projection layer to produce the inputs into our model. We imagine
this random projection process as an analogy to the way the world encodes features in our
observations.
Within the model, the ﬁrst layer is a linear transformation with a bias, followed by a
nonlinearity. The second layer is a linear transformation with no bias.
Our toy model is most similar to that of Elhage+22, with a key diﬀerence being that the
extra linear layer allows us to vary the number of neurons independent of the number of
features or the input dimension.
We study this two model on three tasks. The ﬁrst, a feature decoder, performs a compressed
sensing reconstruction of features that were randomly and lossily projected into a low-
dimensional space. The second, a random re-projector, reconstructs one ﬁxed random
projection of features from a diﬀerent ﬁxed random projection. The third, an absolute value
calculator, performs the same compressed sensing task and then returns the absolute values
of the recovered features. These tasks have the important property that we know which

features are naturally useful, and so can easily measure the extent to which neurons are
monosemantic or polysemantic.
Note that we primarily study the regime where there are more features than embedding
dimensions (i.e. the sparse feature layer is wider than the input) but where features are
suﬃciently sparse that the number of features present in any given sample is smaller than
the embedding dimension. We think this is likely the relevant limit for e.g. language models,
where there are a vast array of possible features but few are present in any given sample.
Key Results
We ﬁnd that models initialized with zero mean bias (left) ﬁnd diﬀerent local minima
depending on the learning rate, with more monosemantic solutions and slightly lower loss at
higher learning rates. Models initialized with a negative mean bias (right) all ﬁnd highly
monosemantic local minima, and achieve slightly better loss. Note that these models are all
in a regime where they have more neurons than there are input features.
Just to hammer home how weird this is, below we've plotted the activations of neurons in
response to single-feature inputs. The three models we show get essentially the same loss
but are clearly doing very diﬀerent things!

More generally, we ﬁnd:
1. When inputs are feature-sparse, models can be made more monosemantic with no
degredation in performance by just changing which loss minimum the training process
ﬁnds (Section 4.1.1).
2. More monosemantic loss minima have moderate negative biases in all three tasks, and
we are able to use this fact to engineer highly monosemantic models (Section 4.1.2).

3. Providing models with more neurons per layer makes the models more monosemantic,
albeit at increased computational cost (Section 4.1.4, also see below).
Interpretability
In Section 5 we provide some mechanistic interpretability results for our feature decoder
models in the monosemantic limit. In this toy model setting we can decompose our model
into a monosemantic part and a polysemantic part, and plotting these separately feature-by-
feature is revealing:

From this, we ﬁnd that:
1. When there is a single monosemantic neuron for a feature, that neuron implements a
simple algorithm of balancing feature recovery against interference.
2. When there are two monosemantic neurons for a feature, those neurons together
implement an algorithm that classiﬁes potential features as ``likely real'' or ``likely
interference'', and then recovers the strength of any ``likely real'' features.

Additionally, we were suspicious at how few kinks the polysemantic neurons provided to the
model's output. Indeed plotting the linearized map that these neurons implement reveals
that they primarily serve to implement a low-rank approximation to the identity, which allows
the model to have non-zero conﬁdence in features at low input amplitudes:
Future Work
We think there's a lot of low-hanging fruit in the direction of "engineer models to be more
monosemantic", and we're excited to pick some more of it. The things we're most excited
about include:
1. Our approach to engineering monosemanticity through bias could be made more
robust by tailoring the bias weight decay on a per-neuron basis, or tying it to the rate of
change of the rest of the model weights.
2. We've had some luck with an approach of the form "Engineer models to be more
monosemantic, then interpret the remaining polysemantic neurons. Figure out what
they do, re-architect the model to make that a monosemantic function, and interpret
any new polysemantic neurons that emerge." We think we're building useful intuition
playing this game, and are hopeful that there might be some more general lessons to
be learned from it.
3. We have made naive attempts to use sparsity to reduce the cost of having more
neurons per layer, but these degraded performance substantially. It is possible that
further work in this direction will yield more workable solutions.

We'd be excited to answer questions about our work or engage with comments/suggestions
for future work, so please don't be shy!

Disagreement with bio anchors that
lead to shorter timelines
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
This would have been a submission to the  FTX AI worldview prize . I'd like to thank
Daniel Kokotajlo, Ege Erdil, Tamay Besiroglu, Jaime Sevilla, Anson Ho, Keith Wynroe,
Pablo Villalobos and Simon Grimm for feedback and discussions. Criticism and
feedback are welcome. This post represents my personal views. 
The causal story for this post was: I ﬁrst collected my disagreements with the bio
anchors report and adapted the model. This then led to shorter timelines. I did NOT
only collect disagreements that lead to shorter timelines. If my disagreements would
have led to longer timelines, this post would argue for longer timelines. 
I think the bio anchors report (the one from 2020, not Ajeya's personal updates) puts
too little weight on short timelines. I also think that there are a lot of plausible
arguments for short timelines that are not well-documented or at least not part of a
public model. The bio anchors approach is obviously only one possible way to think
about timelines but it is currently the canonical model that many people refer to. I,
therefore, think of the following post as "if bio anchors inﬂuence your timelines, then
you should really consider these arguments and, as a consequence, put more weight
on short timelines if you agree with them". I think there are important considerations
that are hard to model with bio anchors and therefore also added my personal
timelines in the table below for reference.
My best guess bio anchors adaption suggests a median estimate for the availability of
compute to train TAI of 2036 (10th percentile: 2025, 75th percentile: 2052). Note that
this is not the same as predicting the widespread deployment of AI. Furthermore, I
think that the time "when AI has the potential to be dangerous" is earlier than my
estimate of TAI because I think that this poses a lower requirement than the potential
to be economically transformative (so even though the median estimate for TAI is
2036, I wouldn't be that surprised if, let's say 2033 AIs, could deal some severe
societal harm, e.g. > $100B in economic damage).
You can ﬁnd all material related to this piece including the colab notebook, the
spreadsheets and the long version in this google folder.
Executive summary
I think some of the assumptions in the bio anchors report are not accurate. These
disagreements still apply to Ajeya's personal updates on timelines. In this post, I want
to lay out my disagreements and provide a modiﬁed alternative model that includes
my best guesses. 
Important: To model the probability of transformative AI in a given year, the bio
anchors report uses the availability of compute (e.g. see this summary). This means
that the bio anchors approach is NOT a prediction for when this AI has been trained and
rolled out or when the economy has been transformed by such an ML model, it merely
predicts when such a model could be trained. I think it could take multiple (I guess 0-4)

years until such a model is engineered, trained and actually has a transformative
economic impact. 
My disagreements
You can ﬁnd the long version of all of the disagreements in this google doc, the
following is just a summary. 
I think the baseline for human anchors is too high since humans were "trained" in
very ineﬃcient ways compared to NNs. For example, I expect humans to need
less compute and smaller brains if we were able to learn on more data or use
parallelization. Besides compute eﬃciency, there are further constraints on
humans such as energy use, that don't apply to ML systems. To compensate for
the data constraint, I expect human brains to be bigger than they would need to
be without them. The energy constraint could imply that human brains are
already very eﬃcient but there are alternative interpretations. [jump to section]
I think the report does not include a crucial component of algorithmic eﬃciency
which I call "software for hardware" for lack of a better description. It includes
progress in AI accelerators such as TPUs (+the software they enable), software
like PyTorch, compilers, libraries like DeepSpeed and related concepts. The
current estimate for algorithmic eﬃciency does not include the progress coming
from this billion-dollar industry. [jump to section]
I think the report's estimate for algorithmic progress is too low in general. It
seems like progress in transformers is faster than in vision models and the
current way of estimating algorithmic progress doesn't capture some important
components. [jump to section]
I think the report does not include algorithmic improvements coming from more
and more powerful AI, e.g. AI being used to improve the speed of matrix
multiplication, AI being used to automate prompt engineering or narrow AIs that
assist with research. This automation loop seems like a crucial component for AI
timelines. [jump to section]
I think the evolution anchor is a bit implausible and currently has too much
weight. This is mostly because SGD is much more eﬃcient than evolutionary
algorithms and because ML systems can include lots of human knowledge and
thus "skip" large parts of evolution. [jump to section]
I think the genome anchor is implausible and currently has too much weight. This
is mostly because the translation from bytes in the genome to parameters in a
NN seems implausible to me. [jump to section]
I think the report is too generous with its predictions about hardware progress
and the GPU price-performance progress will get worse in the future. This is my
only disagreement that makes timelines longer. [jump to section]
Intuitions: Things in AI are moving faster than I anticipated and people who work
full-time with AI often tend to have shorter timelines. Both of these make me less
skeptical of short or very short timelines. [jump to section]
I think the report's update against current levels of compute is too radical. A
model like GPT-5 or Gato 3 could be transformative IMO (this is not my median
estimate but it doesn't seem completely implausible). I provide reasons for why I
think that these "low" levels of compute could already be transformative. [jump
to section]
There are still a number of things I'm not modeling or am unsure about. These
include regulation and government interventions, horizon length (I'm not sure if
the concept captures exactly what we care about but don't have a better
alternative), international conﬂicts, pandemics, ﬁnancial crises, etc. All of these

would shift my estimates back by a bit. However, I think AI is already so powerful
that these disruptions can't stop its progress--in a sense, the genie is out of the
bottle. [jump to section]
The resulting model
The main changes from the original bio anchors model are
1. Lowering the FLOP/s needed for TAI compared to Human FLOP/s
2. Lowering the doubling time of algorithmic progress
3. Changing the weighing of some anchors
4. Some smaller changes; see here for details. 
I tried to change as few parameters as possible from the original report. You can ﬁnd an
overview of the diﬀerent parameters in the table below and the resulting best guess in
the following ﬁgure.
 
Aggressive
- bio
anchors
(Marius)
Best guess
- bio
anchors
(Marius)
Independent
impression
(Marius)
Ajeya's
best
guess
(2020)
Algorithmic
progress
doubling time
1.-1.3 years
1.3-1.6
years
1.3-1.6 years
2-3.5
years
Compute
progress
doubling time
2.5 years
2.8 years
3 years
2.5
years
Model FLOPS
vs. brain FLOPS
(=1e15)
median
-1
-0.5
-0.2
+1

Lifetime
anchor
16%
10%
10%
5%
Short NN
anchor
40%
24%
30%
20%
Medium NN
anchor
20%
35%
31%
30%
Long NN
anchor
10%
17%
13%
15%
Evolution
anchor
3%
3%
5%
10%
Genome
anchor
1%
1%
1%
10%
10th percentile
estimate
<2025
~2025
~2028
~2032
Median(=50%)
estimate
~2032
~2036
~2041
~2052
75th percentile
estimate
~2038
~2052
~2058
~2085
 
My main takeaways from the updates to the model are
1. If you think that the compute requirements for AI is lower than that for humans
(which I think is plausible for all of the reasons outlined below) then most of the
probability mass for TAI is between 2025 and 2035 rather than 2035 and 2045 as
Ajeya's model would suggest.
2. In the short timeline scenario, there is some probability mass before 2025
(~15%). I would think of this as the "transformer TAI hypothesis", i.e. that just
scaling transformers for one more OOM and some engineering will be suﬃcient
for TAI. Note that this doesn't mean there will be TAI in 2025 just that it would be
possible to train it.
3. Thinking about the adaptions to the inputs of the bio anchor model and then
computing the resulting outputs made my timelines shorter. I didn't actively
intend to produce shorter timelines but the more I looked at the assumptions in
bio anchors, the more I thought "This estimate is too conservative for the
evidence that I'm aware of".
4. The uncertainty from the model is probably too low, i.e. the model is
overconﬁdent because core variables like compute price halving time and
algorithmic eﬃciency are modeled as static singular values rather than
distributions that change over time. This is one reason why my personal timelines
(as shown in the table) are a bit more spread than my best guess bio anchors
adaption.
To complete the picture, this is the full distribution for my aggressive estimate:

And for my independent impression:
Note that the exact weights for my personal estimate are not that meaningful because
I adapted them to include considerations that are not part of the current bio anchors
model. Since it is hard to model something that is not part of the model, you should
mostly ignore the anchor weights and only look at the ﬁnal distribution. These "other
considerations" include unknown unknowns, disruptions of the global economy,
pandemics, wars, etc. all of which add uncertainty and lengthen timelines. 
As you can see, I personally still think that 2025-2040 is the timespan that is most
likely for TAI but I have more weight on longer timelines than the other two estimates.
My personal timelines (called independent impression in the table) are also much more
uncertain than the other estimates because I ﬁnd arguments for very short and for
much longer timelines convincing and don't feel like I have a good angle for resolving
this conﬂict at the moment. Additionally, compute halving time and algorithmic
progress halving time are currently static point estimates and I think optimally they
would be distributions that change over time. This uncertainty is currently not captured
and I, therefore, try to introduce it post-hoc. Furthermore, I want to emphasize that I
think that relevant dangers from AI arise before we get to TAI, so don't mistake my
economic estimates for an estimate of "when could AI be dangerous". I think this can
happen 5-15 years before TAI, so somewhere between 2015 and 2035 in my forecasts.
Update: After more conversations and thinking, my timelines are best reﬂected by the
aggressive estimate above. 2030 or earlier is now my median estimate for AGI and I'm
mostly confused about TAI because I have conﬂicting beliefs about how exactly the
economic impact of powerful AI is going to play out. 

Final words
There is a chance I misunderstood something in the report or that I modiﬁed Ajeya's
code in an incorrect way. Overall, I'm still not sure how much weight we should put on
the bio anchors approach to forecasting TAI in general, but it currently is the canonical
approach for timeline modeling so its accuracy matters. Feedback and criticism are
appreciated. Feel free to reach out to me if you disagree with something in this piece.
 

Career Scouting: Dentistry
This is a linkpost for https://careerscouting.substack.com/p/dentistry
As a high school student, I worry a great deal over my future profession. According to
Cal Newport, career satisfaction for any choice of occupation often won't materialize
until you've become "so good they can't ignore you" at what you do. Based on this,
Newport recommends directing your nervous energy towards building skill in whatever
you choose, rather than choosing the "right job". While I found his advice useful for
framing the issue, it opened a new box of concerns: What if you chose something you
have little innate ability in - or what if the point at which your improvement slows
down is not exceptional? How do you even know whether you have talent in
something before you've invested eﬀort into shadowing or interning? Sometimes
talent doesn't appear initially - what if the thing you have the greatest potential in is
something you'll have to struggle at for a long time? And there are so many jobs! We
have to narrow the search space! 
There's another panoply of concerns related to your worth to the world: if you care
about the world and your impact in it, don't you owe it to those who worked hard to
give you the opportunities you have to ﬁnd the thing you'll be the best at? But what if
the thing you'll end up being the best at is something that doesn't always scale well -
like medicine? (Of course, you could go into research, but what if you're only mid-tier
at that?) You'll end up positively aﬀecting the lives of many fewer people than you
could have! And how do you avoid choosing work dry of meaning?
People tell me I over-think these things - the answer to most (if not all) of my what-if's
is "nothing interesting will happen in this case or the counter-factual one - you are
ultimately insigniﬁcant in the greater procession of the world, and you live a cushy
live in the developed world, meaning that no matter how badly you screw things up,
as long as you don't get addicted to heroine, you will still have access to food, water,
and shelter." While I think that answer is probably the right one, it looks like most
young people don't think about this much at all! I have a seriously useful bone to
throw them from my side of the anxiety fence!
In earnest of providing information to a batch of high schoolers staring at fog-covered
futures, a class of college students with sinking intimations that they chose the wrong
major, and a karass of adults who curse their occupations with every breath, I'm
compiling a database of rationalist-inspired interviews with members of various
professions over at careerscouting.substack.com. I hope to respond to the wordless
disconcertions about life that must bubble inside most people with answered
questions.
Below is my ﬁrst interview with a general dentist.
Object-Level
What does a normal day in your ﬁeld look like? Can you give me a "day in the life"
kind of run-down? 
"I start early in the morning. I leave home at 6:15. My ﬁrst patient is seated at 7:00.
Then it's non-stop until 1:00, when I have a ﬁfty minute lunch. After 1:50, I continue
until 5:00. Some days I work through lunch."

How does this diﬀer from the average practitioner?
"It doesn't diﬀer - almost every dentist works the same way."
How does your time split across diﬀerent kinds of activities?
"Aside from operative work, I have patient notes, lab work (e.g. pouring models of
people's mouths), hygiene exams, and treatment conferences (consulting the patient
about what is going to happen during their treatment)."
What does a bad day in your ﬁeld look like, and how does your deﬁnition diﬀer from
the average person's?
"Every dentist faces a bad day where nothing is going right. I can't even begin to
explain the parameters of a bad day - patient anxiety, patients crying in the chair
(because they're deathly afraid of the dentist), etc."
What is your physical environment like?
"Very stressful. There's the operating room, exam room, hygiene room, the lab,
sterilizing room, and my oﬃce."
Meta-Level
Do you ﬁnd your work meaningful? Is meaningness contingent on speciﬁc things, or is
it intrinsic to the work? 
"I ﬁnd dentistry very meaningful. And yes, meaningness is intrinsic to the work."
If you were trying to dissuade someone from becoming a dentist - or to test their will
to become a dentist - what would you tell them? 
"I would tell them it's a physically demanding profession. You need to have high
energy to be a dentist. I spend pretty much the entire day on my feet."
On the other hand, if you were trying to persuade someone to become a dentist, what
would you say?
"It's rewarding to put a smile on people's faces."
What is the rate of change of information, important paradigms, and established
thought in the ﬁeld? How often do earth shaking things get introduced? 
"The rate of change is quite high, but providers don't always have a lot of pressure on
them to keep up with the ﬁeld. Every year new innovations and research change the
ﬁeld. We used to take physical impressions of people's mouths. Now we only take
digital impressions. Crowns and dentures are now 3D printed, which is quite new."
How often do you need to buy new textbooks?
"Never. But I regularly attend continued education classes."
If you had to distill the process of acquiring skill in this ﬁeld, how would you do that?
What's the eat/sleep/repeat loop that you go through to get better? 
"Hundreds of hours of continued education."

Domain
To what extent does talent matter for succeeding in your ﬁeld? This could be a very
vague question, but it's known that journalism and athletics require a high amount of
initial talent, and some like dog walking does not. Where does your ﬁeld fall in
between those two extremes?
"Talent does matter in dentistry. Speciﬁcally, it requires high dexterity, intelligence,
and focus."
What factors - besides the obvious ones like engaging in lots of deliberate practice,
being disciplined, being intelligent, etc. - can enable someone to become a top
performer in dentistry? We could start with personality traits, but it would also be
interesting to expand the scope of this question.
"Good communication skills and empathy certainly help. In general, you have to be a
people person."
What kinds of interests or hobbies usually indicate someone will be good at or enjoy
working in dentistry?
"Painting, sketching - really anything that requires a steady hand."
What prevents talented people with a good ﬁt to the ﬁeld from becoming top
performers?
"Sloppiness and burnout - but that applies to most ﬁelds. It's not clear cut. There are
fraudulent dentists with little ability who succeed. I'm not saying you should commit
fraud, but there are certainly hacks who never get ousted."
Relatedly, what traps do people fall into in dentistry? Why do people fall into them?
"Taking up too many procedures. Aggressive treatments. Thinking you can do
everything. Resistance to referring patients. Really anything to do with ego. But
there's no blaring chasms people fall into that you can't foresee."
What traits do diﬀerent subﬁelds favor? 
"The only thing I can see that would make someone a good prosthodontist that
wouldn't make them a good endodontist is their interest in prosthodontics. This isn't
like athletics, where small diﬀerences in body type can make someone a stellar
marathoner but a mediocre sprinter. If you have a special interest in something, that's
just about the best predictor of how well you'll do in it."
If you consider yourself to be a top performer, what was your path to getting there
like? If not, what does the average path look like to becoming top percentile? Could
you take that skill-acquisition loop we mentioned earlier, and expound on it?
"It boils down to treatment planning, correctly diagnosing patients, acting quickly, and
most importantly, managing the team you work with. Maintaining a good mood makes
a huge diﬀerence in productivity."
Contrarianism
What is something you believe about dentistry that other dentists don't? 

"That investment in continued education is crucial. The dentists in my oﬃce that
produce less than me are stuck with the knowledge and tools they learned in school,
and refuse to adapt to newer, better technology."
How does public perception of what it's like to be a dentist align with reality? What do
lay people deﬁnitely get right, and what do they get wrong?
"People are right about how lucrative dentistry is, but they don't tend to understand
how intense and physically taxing it can be."
What is the most common useful ﬁction dentists have, and why do you think they
have it?
"The belief that you're better than you are. I don't know how exactly people arrive at
it, but it helps when things go wrong"
What is one reason people go into dentistry that they don't talk about?
"Money."
Related to an earlier question, what will stay the same about dentistry? Are there
universal constants or paradigms that you just don't see budging? 
"The structure of the way dentistry is practiced. It diﬀers from country to country, but
I'm certain that the American structure is not going to change."
Conclusion
What parts of the job do you ﬁnd make up for all the dark, bad, evil things you need to
go through? Is there an element of romance or thrill or existential importance to it that
you ﬁnd? What about your job captures the heart?
"Relieving pain. It's the most gratifying thing to watch a patient in suﬀering
experience a moment of catharsis as you numb them."

My take on Jacob Cannell's take on AGI
safety
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
Jacob Cannell wrote some blog posts about AGI safety / alignment and neuroscience between
2010 and 2015, which I read and enjoyed quite early on when I was ﬁrst getting interested in
the same topics a few years ago. So I was delighted to see him reappear on Lesswrong a
year ago where he has been a proliﬁc and thought-provoking blogger and commenter (in his
free time while running a startup!). See complete list of Jacob's blog posts and comments.
Having read a bunch of his writings, and talked to him in various blog comments sections, I
thought it would be worth trying to write up the places where he and I seem to agree and
disagree.
This exercise will deﬁnitely be helpful for me, hopefully helpful for Jacob, and maybe helpful
for people who are already pretty familiar with at least one of our two perspectives. (My
perspective is here.) I'm not sure how helpful it will be for everyone else. In particular, I'm
probably skipping over, without explanation, important areas where Jacob & I already agree
—of which there are many!
(Before publishing I shared this post with Jacob, and he kindly left some responses /
clariﬁcations / counterarguments, which I have interspersed in the text, in gray boxes. I
might reply back to some of those—check the comments section in the near future.)
1. How to think about the human brain
1.1 "Evolved modularity" versus "Universal
learning machine"
Pause for background:
A. "Evolved modularity": This is a school of thought wherein the human brain is a
mishmosh of individual speciﬁc evolved capabilities, including a speciﬁcally-evolved
language algorithm, a speciﬁcally-evolved "intuitive biology" algorithm, a speciﬁcally-
evolved "intuitive physics" algorithm, an "intuitive human social relations" algorithm, a
vision-processing algorithm, etc., all somewhat intermingled for sure, but all innate.
Famous advocates of "evolved modularity" these days include Steven Pinker (see How
the Mind Works) and Gary Marcus. I'm unfamiliar with the history but Jacob mentions
early work by Cosmides & Tooby.
B. "Universal learning machine": Jacob made up this term in his 2015 post "The
Brain as a Universal Learning Machine", to express the diametrically-opposite school of
thought, wherein the brain has one extremely powerful and versatile within-lifetime
learning algorithm, and this one algorithm learns language and biology and physics
and social relations etc. This school of thought is popular among machine learning
people, and it tends to be emphasized by computational neuroscientists, particularly in
the "connectionist" tradition.
Here are two other things that are kinda related:
"Evolutionary psychology" is the basic idea of getting insight into psychological
phenomena by thinking about evolution. In principle, "evolutionary psychology" and
"evolved modularity" are diﬀerent things, but unfortunately people seem to conﬂate

them sometimes. For example, I read a 2018 book entitled Beyond Evolutionary
Psychology, and it was entirely devoted to (a criticism of) evolved modularity, as
opposed to evolutionary psychology per se. Well, I for one think that evolved
modularity is basically wrong (as usually conceived; see next subsection), but
I also think that doing evolutionary psychology (i.e., getting insight into psychological
phenomena by thinking about evolution) is both possible and an excellent idea. Not
only that, but I also think that actual evolutionary psychologists have in fact produced
lots of good insights, as long as you're able to sift them out from a giant pile of crap,
just like in every ﬁeld.
"Cortical uniformity" is the idea—due originally to Vernon Mountcastle in the 1970s
and popularized by Jeﬀ Hawkins in On Intelligence—that the neocortex (also called
"isocortex" if you want to show oﬀ) is more-or-less a single conﬁguration of neurons
replicated over and over—in the case of humans, either 2 million "cortical columns" or
200 million "cortical minicolumns", depending on who you ask. Cortical uniformity is a
surprising hypothesis in light of the fact that diﬀerent parts of the neocortex are
intimately involved in seemingly-diﬀerent domains like vision, language, math,
reasoning, motor control, and so on. I say "more-or-less uniform" because neither Jeﬀ
Hawkins nor anyone else to my knowledge believes in literal "cortical uniformity".
There are well-known regional diﬀerences in the neocortex, but I like to think of them
as akin to learning algorithm hyperparameters and architecture. Anyway, "cortical
uniformity" is closely allied to the "universal learning machine" school of thought
(see §2.5.3 here), but to ﬂesh out that story you also need to say something about the
other parts of the brain that are not the neocortex. For example, both Jacob (I think)
and I take another big step in the "universal learning machine" direction by
hypothesizing not only (quasi)uniformity of the cortex, but also of the striatum,
cerebellum, and thalamus (with some caveats). Anyway, see below.
1.2 My compromise position
To oversimplify a bit, my position on the evolved-modularity versus universal-learning-
machine spectrum is:
"Universal Learning Machine" is an excellent starting point for thinking about the
telencephalon (neocortex, hippocampus, amygdala, striatum, etc.), thalamus, and
cerebellum.
I.e., when we try to understand those parts of the brain, we should be mainly on
the lookout for powerful scaled-up learning algorithms.
"Evolved Modularity" is an excellent starting point for thinking about the hypothalamus
and brainstem.
I.e., when we try to understand those parts of the brain, we should be mainly on
the lookout for lots of little components that do speciﬁc ﬁtness-enhancing things
and which are speciﬁcally encoded by the genome.
(See, for example, my discussion of a particular cluster of cells in the
hypothalamus that orchestrate hunger-related behavior in Section 3 of my recent
hypothalamus post.)
1.3 How complicated are innate drives?
If the within-lifetime learning algorithm of the human brain is a kind of RL algorithm, then it
needs a reward function. (I actually think this is a bit of an oversimpliﬁcation, but close
enough.) Let's use the term "innate drives" to refer to the things in that reward function—
avoiding pain, eating sweets, etc. The reward function, in my view, is primarily calculated in
the hypothalamus and brainstem.
(For more on my picture, see my posts "Learning From Scratch" in the Brain and Two
subsystems: Learning & Steering.)

Jacob and I seem to have some disagreement about how complex these innate drives are,
and how much we should care about that complexity; I'm on the pro-complexity side of the
debate, and Jacob is on the pro-simplicity side.
1.3.1 Example: our disagreement about habitat-related
aesthetics
For an example of where we disagree, consider the landscape preferences theory within
evolutionary aesthetics. Here's wikipedia (hyperlinks and footnotes removed):
An important choice for a mobile organism is selecting a good habitat to live in. Humans
are argued to have strong aesthetical preferences for landscapes which were good
habitats in the ancestral environment. When young human children from diﬀerent
nations are asked to select which landscape they prefer, from a selection of standardized
landscape photographs, there is a strong preference for savannas with trees. The East
African savanna is the ancestral environment in which much of human evolution is
argued to have taken place. There is also a preference for landscapes with water, with
both open and wooded areas, with trees with branches at a suitable height for climbing
and taking foods, with features encouraging exploration such as a path or river curving
out of view, with seen or implied game animals, and with some clouds. These are all
features that are often featured in calendar art and in the design of public parks.
A survey of art preferences in many diﬀerent nations found that realistic painting was
preferred. Favorite features were water, trees as well as other plants, humans (in
particular beautiful women, children, and well-known historical ﬁgures), and animals (in
particular both wild and domestic large animals). Blue, followed by green, was the
favorite color. Using the survey, the study authors constructed a painting showing the
preferences of each nation. Despite the many diﬀerent cultures, the paintings all showed
a strong similarity to landscape calendar art. The authors argued that this similarity was
in fact due to the inﬂuence of the Western calendar industry. Another explanation is that
these features are those evolutionary psychology predicts should be popular for
evolutionary reasons.
My snap reaction is that this evolutionary story seems probably true, and Jacob's is that it's
probably false. We were arguing about it in this thread.
The disagreement seems to be less about the speciﬁcs of the landscape painting experiment
mentioned above, and more about priors.
My prior is mainly coming from the following:
By default, being in the wrong micro-habitat gives a negative reward which can be both
very sparse and often irreversibly fatal (e.g. a higher chance of getting eaten by a
predator, starving to death, freezing to death, burning to death, falling to death,
drowning, getting stuck in the mud, etc., depending on the species).
Therefore, it's very diﬃcult for an animal to learn which micro-habitat to occupy purely
by trial-and-error without the help of any micro-habitat-speciﬁc reward-shaping.
Such reward-shaping is straightforward to implement by doing heuristic calculations on
sensory inputs.
Animal brains (speciﬁcally brainstem & hypothalamus in the case of vertebrates) seem
to be perfectly set up with the corresponding machinery to do this—visual heuristics
within the superior colliculus, auditory heuristics within the inferior colliculus, taste
heuristics within the medulla, smell heuristics within the hypothalamus, etc.
Therefore, I have a strong prior expectation that every mobile animal including humans
will ﬁnd types of visual input (and sounds, smells, etc.) to be inherently "appealing" /
"pleasant", in a way that would statistically lead the animal to spend more time in
"good" micro-habitats / hunting grounds / etc. and less time in "bad" ones.

Jacob's prior is mainly coming from the following, I think:
We know that when animals choose what to look at, this decision is at least partly
based on information value...
...Both based on priors (otherwise learning is diﬃcult, and meanwhile there's a
large literature showing that curiosity is important for ML capabilities)
...And based on studies of novelty / curiosity drive in animal brains (Jacob
cites The psychology and neuroscience of curiosity, Systems Neuroscience of
Curiosity, Shared striatal activity in decisions to satisfy curiosity and hunger at
the risk of electric shocks)
And once you think through the implications of an information-value drive, it elegantly
accounts for just about everything we know about human aesthetics. (Jacob allows for
some exceptions including sexual attraction—see below.) That includes observations
about which paintings people put on walls. So there's nothing left to explain!
(I am more-or-less on board with the ﬁrst top-level bullet point here[1], but disagree with the
last bullet point.)
All that was kinda priors. Now we turn to the speciﬁcs of the landscape painting thing.
Jacob & I argued about it for a while. I think the following is one of the root causes of the
disagreement:
Jacob was interpreting the hypothesis in question as "Humans have (among other
things) an innate preference for looking at water, trees, etc.".
The hypothesis that I believe is: "Humans have (among other things) a pleasant innate
reaction upon looking at visual scenes for which F(visual input) takes a high value,
where F is some rather simple function, I don't know exactly what, but deﬁnitely way
too simple a function to include a proper water-detector, or tree-detector, etc.[2]
Jacob and I both agree that the ﬁrst hypothesis is wrong. (To be fair, he wasn't getting it from
nowhere—it's probably what most advocates of the hypothesis would say that they are
arguing for!)
(And this is an example of our more general dispositions where I tend to think "10% of
evolutionary psychology is true important things that we need to explain, let's get to work
explaining them properly" and Jacob tends to think "90% of evolutionary psychology is crap,
let's get to work throwing it out". These are not inconsistent! But they're diﬀerent
emphases.)
Anyway, my hypothesis is coming from:
1. I think the function F is implemented in the superior colliculus (part of the brainstem),
which is too small and low-resolution to do good image processing;
2. We only have 25,000 genes in our whole genome, and building a proper robust tree-
detector seems too complicated for that;
3. There's some evidence that the human superior colliculus has an innate human-face
detector, but it's not really a human-face detector, it's really a detector of three dark
blobs in a roughly triangular pattern, and this blob-detector incidentally triggers on
faces. Likewise, an incoming-bird-detector in the mouse superior colliculus is really
more like an "expanding dark blob in the upper ﬁeld-of-view" detector (ref).
Let's go back to evidence from surveys and market research on wall-calendars and paintings,
mentioned in that Wikipedia excerpt above. Unfortunately, it seems that neither Jacob nor I
have theories that make sharp predictions on what people will want to hang on their walls.
One problem is that we both agree that people can hang things on walls for reasons related
to neither "innate aesthetics" nor "information value", like impressing your friends, or
bringing back sentimental memories of your ﬁrst kiss. I have the additional problem that I
don't know exactly what the alleged habitat-aesthetics function F is (and there are probably

several F's), and thus I ﬁnd it perfectly plausible (indeed, expected) that F can be triggered
by, say, an abstract painting which nobody in their right mind would mistake for a savannah
landscape. And I have no predictions about which abstract paintings![3] And conversely, the
question of what does or doesn't provide information value is likewise complicated—it
depends on one's goals and prior knowledge. Thus Jacob and I were disagreeing here about
whether a window view of a river provides high or low information value. (Suppose that
you've had that same window view for the past 3 years already, and the river never has
animals or boats on it.) I say the information value of that window view is roughly zero, Jacob
says it's signiﬁcantly positive ("it's constantly changing with time of day lighting, weather,
etc....the river view suggests you can actually go out and explore the landscape"), and I'm
not sure how we're going to resolve that.
DALL-E 2 prompts: "the view out my window has high information value" (left)
and "a window view with high information value" (right). 🤔🤔🤔
So it seems like we're stuck, or at least our disagreement probably won't get resolved by
looking into people's wall-art preferences.
1.3.2 ...But this doesn't seem to be a super-deep disagreement
Why don't I think it's a super-deep disagreement?
For one thing, I proposed that "fully describing the [reward function] of a human would
probably take like thousands of lines of pseudocode" and Jacob said "sounds reasonable".
For another thing, while we disagree about habitat-aesthetics-in-humans, there are
structurally-similar cases where Jacob & I are in fact on the same page:
I brought up the case of a little camouﬂaged animal having an innate preference to be
standing on the appropriate background to its camouﬂage, implemented via the
superior colliculus calculating some function on visual inputs and feeding that
information into the reward function (as one among many contributions to the reward
function). Jacob seemed at least willing to entertain that as a plausible hypothetical.
Jacob deﬁnitely believes that there are innate sexual preferences related to the visual
appearances of potential mates. Let's turn to that next.
1.3.3 "Correlation-guided proxy matching"

Here is Jacob describing the idea of "correlation-guided proxy matching":
Any time evolution started using a generic learning system, it had to ﬁgure out how to
solve this learned symbol grounding problem, how to wire up dynamically learned
concepts to extant conserved, genetically-predetermined behavioral circuits.
Evolution's general solution likely is correlation-guided proxy matching: a
Matryoshka-style layered brain approach where a more hardwired oldbrain is
redundantly extended rather than replaced by a more dynamic newbrain. Speciﬁc innate
circuits in the oldbrain encode simple approximations of the same computational
concepts/patterns as speciﬁc circuits that will typically develop in the newbrain at some
critical learning stage - and the resulting ﬁring pattern correlations thereby help oldbrain
circuits locate and connect to their precise dynamic circuit counterparts in the newbrain.
This is why we see replication of sensory systems in the 'oldbrain', even in humans who
rely entirely on cortical sensory processing.
[Translation guide: When Jacob talks about "oldbrain" it's roughly equivalent to when I talk
about "hypothalamus and brainstem".]
In the case of innate sexual preferences, Jacob proposes "dumb simple humanoid shape
detectors and symmetry detectors etc encoding a simple sexiness visual concept"[4] as an
example.
Anyway, leaving aside some nitpicky arguments over implementation details, I see this as
very much on the right track. I'm bringing it up because we'll get back to it later.
1.3.4 Should we think of (almost) all innate drives as "an
approximation to (self)-empowerment"?
Let's loosely deﬁne "empowerment" as "having lots of options in the future"—see Jacob's
post Empowerment is (almost) all we need for better discussion, and I'll get back to
empowerment in Section 3 below in the context of AGI.
If a suﬃciently-clear-thinking human were deliberately trying to empower herself, she would
do lots of things that humans actually do. She would stay alive and healthy, she would win
friends and allies and high social status, she would gain skills and knowledge, she would
accumulate money or other resources, she would stay abreast of community gossip, and so
on.
Maybe you're tempted to look at the above paragraph and say "Aha! An "empowerment
drive" is a grand uniﬁed theory of human innate drives!!" But that would be
wrong for a couple reasons.
The ﬁrst reason is that empowerment comes apart from inclusive genetic ﬁtness in a
couple places—particularly having sex, raising children, and more generally helping close
relatives survive and have children à la kin selection theory. And we see this in e.g. the
human innate sex drive.
The second reason is that infants cannot realistically calculate which actions will lead to
"empowerment".
Jacob responds: On the contrary I think it's fairly clear now that the primary
learning signals driving the infant brain are some combination of self-supervised
learning for prediction and then value of information and
optionality/empowerment for decisions (motor and planning).

The evidence for this comes from DL experiments as well as neuroscience, but
also just obvious case examples:
https://www.lesswrong.com/posts/hpjou9ZnLZkSJR7sd/reﬂections-on-six-months-
of-fatherhood
Indeed, I claim that even adult humans often do things that advance their own
empowerment without understanding why and how. For example, if someone is quick to
anger and vengeance, then that tendency can (indirectly via their reputation) increase their
empowerment, via people learning not to mess with them. But that's not why they're quick
to anger and vengeance—it's just their personality! And if they haven't read Thomas
Schelling or whatever, they might never appreciate the underlying logic.
So we don't have an innate drive for "empowerment" per se, because it's not realistically
computable. Instead:
We have a set of innate drives which can be collectively viewed as "an approximation
to a hypothetical empowerment drive". For example, innate fear-of-heights is part of an
approximation to empowerment, insofar as falling oﬀ a cliﬀ tends to be disempowering.
We will generally learn empowerment-advancing behaviors and patterns within our
lifetimes, because those behaviors and patterns tend to be useful for lots of things. For
example, I like having money, not because of any innate drive, but because of
experience using money to get lots of other things I like.
Out of these two points, I think Jacob has more-or-less agreed with both. For the
ﬁrst one, he recognizes sex as a non-empowerment-related innate drive here ("The values
that deviate from empowerment are near exclusively related to sex"—well that seems an
overstatement given childrearing, but whatever.) For the second one, here I had proposed
"There is innate stuﬀ in the genome that makes humans want social status. Oh by the way,
the reason that this stuﬀ wound up in the genome is because social status tends to lead to
empowerment, which in turn tends to lead to higher inclusive genetic ﬁtness. Ditto curiosity,
fun, etc.", and Jacob at least "mostly" agreed.
Jacob responds: Social status drive emerges naturally from empowerment,
which children acquire by learning cultural theory of mind and folk game theory
through learning to communicate with and through their parents.  Children
quickly learn that hidden variables in their parents have huge eﬀect on their
environment and thus try to learn how to control those variables.
I mostly agree that curiosity - or value of information - is innate; which is not the
same as optionality-empowerment, but is closely connected to it and a primary
innate motivational drive.  Fun is also probably an emergent consequence of
value-of-information and optionality.
But unlike Jacob, I get the takeaway message: "OK, so at the end of the day,
'empowerment' is pretty useless as a way to think about human innate drives.
Let's not do that." For example, I can say "fear-of-heights is part of an approximation to
empowerment", and that's correct! But what's the point? I can equally well say "fear-of-
heights is part of an approximation to inclusive genetic ﬁtness". Or better yet, "fear-of-
heights tends to stop you from falling oﬀ cliﬀs and getting injured or killed, which in turn
would be bad for inclusive genetic ﬁtness". I don't see how "empowerment" is adding
anything to the conversation here.
And I think "empowerment" adds to confusion if we're not scrupulously careful to avoid
mixing up "empowerment" and "approximation-to-empowerment". Approximations tend to

come apart in new environments—that's Goodhart's law. We'll get back to that in Section 3.3
below.
Likewise, we can say that "status drive is an approximation to empowerment", and we're
correct to say that, but saying that gets us ≈0% of the way towards explaining exactly what
status drive is or how it's implemented.
(Unless you think that there's no such thing as an innate status drive, and that humans
engage in status-seeking and status-respecting behaviors purely because they've learned
within their lifetime that those behaviors are instrumentally useful. That's certainly a
hypothesis worth entertaining, but I strongly believe that it's wrong.)
Jacob responds (to "we can say that 'status drive is an approximation to
empowerment'"): Well no, I'd say status drive is not truly innate at all, but is
learned very early on as a natural empowerment manifestation or proxy.
Infants don't even know how to control their own limbs, but they automatically
learn through a powerful general empowerment learning mechanism.  That
same general learning signal absolutely does not - and can not - discriminate
between hidden variables representing limb poses (which it seeks to control)
and hidden variables representing beliefs in other humans minds (which
determine constraints on the child's behavior).  It simply seeks to control all
such important hidden variables.
Steve sidenote: Leaving aside the question of who is correct, I think it's helpful
to note that this disagreement here has the same pattern as the one in Section
1.3.1 above—Jacob thinks that the human brain within-lifetime RL reward
function is simpler (a.k.a. smaller number of diﬀerent "innate drives") and I think
it's more complicated (a.k.a. larger number of diﬀerent "innate drives").
OK, let's switch gears to a somewhat diﬀerent topic:
2. Will AGI algorithms look like brain
algorithms?
2.1 The spectrum from "giant universe of
possible AGI algorithms" versus "one natural
practical way to build AGI"
Here are two opposite schools of thought:
"Giant Universe" school-of-thought: There is a vast universe of possible AGI
algorithms. If you zoom in enough, you can eventually ﬁnd a tiny speck, and inside that
speck is every human mind that has ever existed. (Cf. Eliezer Yudkowsky 2008.)
"Unique Solution" school-of-thought: The things we expect AGI to do (learn,
understand, plan, reason, invent, etc.) comprise a problem, and maybe it turns out that
there's just one natural practical way to solve that problem. If so, we would expect
future AGI algorithms to resemble human brain algorithms. (Cf. Jacob Cannell 2022)
Before proceeding, a few points of clariﬁcation:

People can easily talk past each other by mixing up "learning algorithm" versus
"trained model". I'm closer to the "unique solution" camp when we're talking
about the learning algorithm, and I'm closer to the "giant universe" camp
when we talk about the trained model.
As a particularly safety-relevant example of why I'm in the "giant universe" camp for
trained models, I think human-brain-like RL with 1000 diﬀerent reward functions can
lead to trained models that have 1000 wildly diﬀerent desires / goals / intuitions about
what's good and right. (But they all might act the same for a while thanks
to instrumental convergence.) In this context, I think it's important to remember that
people can (and by default will) make AGIs with reward functions that are radically
diﬀerent from those of any human or animal, e.g. "reward for paperclips". (More
discussion and caveats in my post here.)
We can also reconcile the two schools of thought by the fact that the "Giant Universe"
claim is about "possible" algorithms and the "Unique Solution" claim is about
"practical" algorithms. Even if there is just one unique practical learning algorithm that
scales to AGI, there are certainly lots of other wildly impractical ones. Two examples in
the latter category (in my opinion) would be: (1) a learning algorithm that recapitulates
the process of animal evolution, and (2) computable approximations to AIXI such as
"AIXItl".
Going back to those two schools of thought, and focusing on the learning algorithm not the
trained model, are there any good reasons to believe in "Unique Solution"?
It seems at least plausible to me. After all, there do seem to be "natural" solutions to at least
some algorithmic problems—e.g. the Fast Fourier Transform was more-or-less independently
invented multiple times. Would an intelligent extraterrestrial civilization invent the belief
propagation algorithm, in a form recognizable to us? Hard to say, but it seems at least
plausible, right?
We get stronger evidence from the cases where AI researchers have come up with an idea
and then later discover that they reinvented something that evolution has already put into
the human brain. Examples are controversial, but arguably include Temporal Diﬀerence
learning, self-supervised learning (i.e. the idea of updating models on sensory prediction
errors), and feedback control. What about the overlap between deep learning and the brain—
distributed representations, adjustable weights, etc.? Well, those things were historically
brought into AI from neuroscience, which complicates our ability to draw lessons. But still,
the remarkable successes of more-brain-like deep learning compared to various less-brain-
like alternatives in AI does seem to be at least some evidence for "Unique Solution". (But see
next subsection.)
Jacob oﬀers another reason that he's strongly in the "Unique Solution" school of thought,
related to his claim that brains are near various theoretical eﬃciency limits. Leaving aside
the question of whether brains are in fact near various theoretical eﬃciency limits (I have no
strong opinion), I don't understand this argument. Why can't a wildly diﬀerent algorithm also
approach the same theoretical eﬃciency limits?
Well anyway, I join Jacob in the "Unique Solution" camp, albeit with a bit less conﬁdence and
for diﬀerent underlying reasons. Indeed, when I explain to people why I'm working on brain-
like AGI (e.g. here), I usually oﬀer the justiﬁcation that we AGI safety researchers should be
making contingency plans for any plausible AGI design that we can think of, and brain-like
AGI is at least plausible. But that's just a polite diplomatic cop-out. What I really believe is
that the researchers pursuing broadly-brain-like paths to AGI are the ones who will probably
succeed, and everyone else will probably fail, and/or gradually pivot / converge towards
brain-like approaches. If you disagree with that claim, I'm not particularly interested in
arguing with you (for the obvious infohazard reasons)—we can agree to disagree, and I will
fall back to my polite diplomatic cop-out answer above, and we're all going to ﬁnd out sooner
or later!

2.2 How similar are brain learning algorithms
versus today's deep learning algorithms? (And
implications for timelines.)
Jacob and I seem to be in agreement that human brain learning algorithms are similar in
some ways and diﬀerent in other ways from today's deep learning algorithms. But I have a
strong sense that Jacob expects substantially bigger similarities and substantially smaller
diﬀerences than I do. That's hard to pin down, and as above I don't want to argue about it.
We'll ﬁnd out sooner or later!
In terms of timelines, Jacob & I agree that AGI is probably already possible for a reasonable
price with today's chips and data centers, and we're just waiting on algorithmic advances.
(Jacob: "So my model absolutely is that we are limited by algorithmic knowledge. If we had
that knowledge today we would be training AGI right now".)
So then my next step is to say "OK then. How long will we be waiting on those algorithmic
advances? Hmm. I dunno! Maybe 5-30 years?? Then let's also add, umm, 5-10 more years
after that to work out the kinks and run trainings before we have AGI." (When I say "5-30"
years, I have a bit more going on under the hood besides wild guessing. But not much more!)
Jacob proposes more conﬁdently that we'll get AGI soon ("75% by 2032"). He thinks that a
certain amount of compute / memory / etc. is required to train an AGI (and we can ﬁgure out
roughly how much by looking at human brain within-lifetime learning), and by the time that a
great many groups around the world have easy access to this much compute / memory /
etc., they will come up with whatever algorithmic advances are necessary for AGI. He writes:
"Algorithmic innovation is rarely the key constraint on progress in DL, due to the vast
computational training expense of testing new ideas. Ideas are cheap, hardware is not." (I
have heard that Hans Moravec's forecasts were based on a similar assumption.)
I'm much less conﬁdent than Jacob in "ideas are cheap". It seems to me that plenty of useful
algorithms are published decades later than they theoretically could have been published,
for reasons unrelated to the availability of compute. For example, Judea Pearl published
the belief propagation algorithm in 1982. Why hadn't someone already published it in 1962?
Or 1922?? That's not a rhetorical question—I'm not an expert, maybe there's a good answer!
Leave a comment if you know. But anyway, where I'm at right now is that I wouldn't be
surprised if there were, say, 10 or 20 years between lots of groups having easy access to
compute suﬃcient for AGI, and someone actually making AGI. So I have longer timelines
than Jacob, although that's a pretty low bar by "normie" standards.
Again, this all seems probably downstream of our diﬀerent opinions about how similar deep
learning algorithms are to brain learning algorithms—a question which (I would argue) is
slightly relevant for safety and extremely relevant for capabilities, so I don't care to talk
about it. But it certainly seems likely that Jacob is imagining smaller ideas (tweaks) which are
cheap, and I'm thinking of bigger ideas which are more expensive.
2.3 Will AGI use neuromorphic (or processing-in-
memory) chips?
Jacob and I both agree that (1) the ﬁrst AGIs that people will make will probably use "normal"
chips like GPUs or other ASICs, (2) when thousandth-generation Jupiter-brain AGIs are
building Dyson spheres, they're probably going to be using neuromorphic / processing-in-
memory architectures of some sort, since those seem to have the best properties in terms of
both scaling up to extremely large information capacity, and energy eﬃciency. (See Jacob's
discussion here).

I think I'm a bit more negative than Jacob on the current state of neuromorphic chips and
technical challenges ahead, and thus I expect the transition to neuromorphic chips to
happen later than Jacob expects, probably. I also put higher probability on AGI also using fast
serial coprocessors to unlock algorithmic possibilities that brains don't have access to, both
for early AGI and in the distant future. (Think of how "a human with a pocket calculator" can
do things that a human can't. Then think much bigger than that!) But whatever; this
disagreement doesn't seem to be too important for anything.
3. Human-empowerment as an AGI
motivation
See Jacob's recent post Empowerment is (almost) All We Need (and slightly earlier LOVE in a
simbox is all you need).
Two questions immediately jump to mind:
The outer alignment question is: "Do we want to make an AGI that's trying to "empower"
humanity?"
The inner alignment question is: "How would we make an AGI that's trying to "empower"
humanity?"
Jacob's answer to the latter (inner alignment) question is mostly "correlation-guided proxy
matching" as described above, possibly supplemented by interpretability—see his
comment here.
My perspective is that we shouldn't really be asking these two questions separately. I think
we're going to follow Procedure X (let's say, correlation-guided proxy matching with proxy P
and hyperparameters A,B,C in environment E), and we're going to get an AGI that's trying to
do Y. I expect that Y will not be identical to "empowerment" because perfect inner alignment
is a pipe dream. So we shouldn't ask the two questions: "(1) How similar is Y to
"empowerment", and (2) Is "empowerment" what we want?". Instead, I think we
should ask the one question "Is Y what we want?".
So I want to push the question of empowerment to the side and just look at the actual plan.
When I do, I ﬁnd that Jacob's proposals are very similar to my own! But I do think we have
some minor diﬀerences worth discussing.
Jacob's proposed plan described here suggested two things, one related to reverse-
engineering social instincts in the brain, and the other related to interpretability. Let's take
them one at a time:
3.1 Social instincts / empathy
Jacob and I both agree that it would be good to understand human social instincts well
enough that we could write them into future AGI source code if we wanted to (here's my own
post motivating that). We both agree that this code would probably involve something like
correlation-guided proxy matching (I have a post on that too). But my impression is that
Jacob expects that we're going to get most of the way towards solving this problem by
reading the (massive) existing neuroscience literature concerning morality, sociality, aﬀects,
etc., whereas I think that literature is all kinda garbage—or rather, not answering the
questions that I'm interested in—and we still have our work cut out.
Jacob responds: Not quite - my prior is that success in reverse engineering
human altruism (which probably depends on innate social instincts for

grounding) will depend on existing neuroscience literature to about the same
extent that progress in DL has.
So Jacob seems to have more of a "it's OK we have a plan" attitude, while I'm sitting here
poring over technical studies of neuropeptide receptors in the lateral septum, feeling like I'm
racing the clock, even though my timelines-to-AGI are actually longer than his.
Somewhat relatedly, and echoing the discussion of innate drives above, I think Jacob expects
human social instincts to be simpler than I do—maybe he expects human social instincts to
comprise like 5 separable "innate reactions" (e.g. here) and I expect like 30, or whatever. So
maybe he thinks we can just think about it a bit in our armchairs and write down the answer,
and it will be either correct or close enough, whereas I expect more of a big research project
that will produce non-obvious results.
Jacob responds: I think most of the system complexity for innate symbol
grounding is split vaguely equally between sexual attraction and altruism-
supporting innate social instincts, and that reverse engineering, testing and
improving these mechanisms for DL agents in sim sandboxes is much of the big
research project.
3.2 Interpretability
Jacob suggests that we could "use introspection/interpretability tools to more manually
locate learned models of external agents (and their values/empowerment/etc), and then
extract those located circuits and use them directly as proxies in the next agent". (See
also here.) I think that's a perfectly good idea (see e.g. my comment here), and I think our
disagreement (such as it is) is a bit like Jacob saying "Maybe it will work" and me saying
"Maybe it won't work". These can both be true. Hopefully we can all agree that it would
be better to have a strong positive reason to believe that our plan will deﬁnitely work,
particularly given challenges related to "concept extrapolation". (See also the rest of that
post.)
Jacob has a clever additional twist on interpretability in his proposal that we could "listen in"
on an AGI's internal monologue (see here). Again, I do think this is a ﬁne idea that could help
us, particularly if we can ﬁgure out interventions that make the AGI a "verbal thinker" to the
greatest extent possible. I don't think that this oﬀers any strong guarantees that this
interpretability won't be missing important things. For example, I'm somewhat of a verbal
thinker, I guess, but my internal monologue has lots of idiosyncratic made-up terms which
are only meaningful to myself. It also has lots of very diﬀerent thoughts associated with the
same words. Let's explore this avenue anyway, for sure, but I don't want to get my hopes too
high.
3.3 OK, but still, is humanity-empowerment what
we want?
(In other words, if we somehow made an AGI that wanted to maximize the future
empowerment of "humanity", would it be "aligned"?)
I argued just above that this is not really the right question to ask. But it's
not entirely irrelevant either. So let's have at it.
Let's say that "humanity" (CEV or whatever) has terminal goals T (a utopia of truth,
beauty, friendship, love, fun, diversity, kittens, whatever). Let's also say that, given the
choice and knowledge and power, "humanity" would pursue instrumental empowerment-
type goals P as a means to an end of achieving T.

If we make an AGI that wants humanity to wind up maximally empowered in the future, it
would be "aligned" to the human pursuit of P, but "misaligned" to the human pursuit of T.
Jacob responds: The convergence theorems basically say that optimizing for
P[t] converges to optimizing for T[t+d] for some suﬃcient timespan d.  So
optimizing for our empowerment today is equivalent to optimizing for our future
ability to maximize our long term values, whatever they are.  I think you are
confusing optimizing for P[t] (current empowerment) with optimizing for P[t+d]
(future empowerment).  Convergence requires a suﬃcient time gap between the
moment of empowerment and the future utility, which wouldn't occur for P[t+d]
and T[t+d].
In other words, the AGI does not want humans to "cash in" their empowerment to purchase
T.[5]
Even worse, the AGI does not want humans to want to "cash in" their empowerment to
purchase T.
Jacob responds: If the AGI is optimizing for rolling future discounted
empowerment, that is equivalent only to optimizing for the long term
components of our utility function.  Long term utility never wants us to 'cash' in
empowerment, and this same conﬂict occurs in human brains (spend vs
save/invest).  The obvious solution as I mentioned is to use a learned model for
the short term utility, and probably learn the discount schedule.
Also it is worth noting that lower discount rates lead to more success in the long
term, and lower discount rates increase the convergence (lower the importance
of short term utility).
T is the whole value of the future. T is what we're ﬁghting for. T is the light at the end of the
tunnel. If we make a powerful autonomous AGI that doesn't care about T, then we're doing
the wrong thing!
This seems to be the obvious objection, and indeed I ﬁnd it persuasive. But Jacob oﬀers
several rebuttals.
First (see here and here), I think Jacob is imagining two stages:
In Stage 1, the AGI accumulates P and gives it to humans.
In Stage 2, the now-super-empowered uplifted posthumans (or whatever) spend their P
to buy T.
Jacob responds: Yeah this is what success looks like.  There may be other
success stories, but the main paths look like this (empowered posthumanity).  So
if your AGI is not working towards this path, something is probably wrong.
Steve again: (Just to be crystal-clear, I agree that this two-stage story sounds
pretty great, if we can make it happen. Here I'm questioning whether it would
happen, under the given assumptions.)
I'm skeptical of this story—or at least confused. It seems like the AGI would be unhappy
about (post)humanity's decision to throw out their own option value by purchasing T in stage
2. Maybe in stage 2, the AGI is no longer able to do anything about it—it's too late, the
posthumans are super-powerful and thus back in control of their own fate. But it's not too

late in stage 1! And even in stage 1, the AGI will see this "problem" coming, and so it can
and will preemptively solve it.
Jacob responds: Imagine for example that mass uploading will become feasible
in 2048 (with AGI's help), and we created the AGI to maximize our
empowerment - in 2048.  The AGI will then not care how we spend that
empowerment in 2049.  Now generalize that to a continuous empowerment
schedule with a learned discount rate and learned short term utility, and we can
avoid issues with the AGI changing our minds too much before handing over
power.
Steve again: OK I agree that an AGI with the stable goal of "maximize human
empowerment in 2048" would not have the speciﬁc problem I brought up here.
Thus, for example, as the AGI is going through the process of "uplifting" the humans to
posthumans, it would presumably do so in a way that deletes the human desire for T and
adds a direct posthuman desire for P. Right?
Jacob responds: Doubtful - that would only occur if you had no short term
model of T and also a too loose conception of 'humanity' to empower.
Second (see here and here), Jacob notes that evolution was optimizing for inclusive genetic
ﬁtness, and got some amount of T incidentally. So maybe an AGI optimizing for P will also
incidentally produce T. Or even better: maybe T just is what happens when an optimization
process pursues P! Or in Jacob's words:
Humans and all our complex values are the result of evolutionary optimization for a
conceptually simple objective: inclusive ﬁtness. A posthuman society transcends biology
and inclusive ﬁtness no longer applies. What is the new objective function for post-
biological evolution? Post humans are still intelligent agents with varying egocentric
objectives and thus still systems for which the behavioral empowerment law applies. So
the outcome is a natural continuation of our memetic/cultural/technological evolution
which ﬁlls the lightcone with a vast and varied complex cosmopolitan posthuman society.
The values that deviate from empowerment are near exclusively related to sex which no
longer serves any direct purpose, but could still serve fun and thus empowerment.
Reproduction still exists but in a new form. Everything that survives or ﬂourishes tends to
do so because it ultimately serves the purpose of some higher level optimization
objective.
I think there's a Goodhart's law problem here.
People intrinsically like fun and beauty and friendship—they're part of the T. But
simultaneously, it turns out that they serve as an approximation to human empowerment—
they're a proxy to P (see Section 1.3.4). That's reassuring, right? No it's not, thanks to
Goodhart's law.
I claim that if an AGI was really good at optimizing P, it would ﬁnd places where fun and
beauty and friendship come apart from P, and then make sure that the posthumans' actual
desire in those cases is for P, and not for fun and beauty and friendship. And the more we
push into weird out-of-distribution futures, the more likely this is to happen.
 
Jacob responds: Empowerment is a convergent eﬃcient universal long term
value approximator that any successful AGI will end up using due to the
diﬃculties in eﬃciently optimizing directly for very speciﬁc values in the long

term future from issues like accumulating uncertainty and the optimizer's curse. 
The real question then is whether the AGI is optimizing for
its own empowerment, or ours. 
Weird-out-of-distribution futures are exactly the scenarios where it's important
that the AGI is optimizing for our empowerment rather than its own.
The AGI will probably not replace our desire for fun/beauty/friendship with P
because of some combination of 1.) direct approximation of T
(fun/beauty/friendship) for short term utility, 2.) a conservative model of
'humanity' to empower than prevents changing humans too much (which is
necessary for any successful scheme regardless, as otherwise the AGI just
assimilates us into itself to make optimizing for its self-empowerment equivalent
to optimizing for 'our' values simply by redeﬁning/changing us), 3.) control over
the discount schedule
For example, maybe some clever futuristic system of smart contracts is objectively much
better at managing interpersonal coordination and trade than the old-fashioned notion of
"trust and friendship". And if the AGI sets up this smart-contract system, while
simultaneously making (post)humans feel no intrinsic trust-and-friendship-related feelings
and drives whatsoever, maybe those (post)humans would be "more empowered". But I don't
care! That's still bad! I still don't want the AGI to do that! I want the feelings of trust and
friendship to survive into the distant future!
Anyway, I don't really know what a maximal-P future looks like. (I'm not sure that, in our
current state of knowledge, P is deﬁned well enough to answer that??) But my strong
expectation is that it would not look like a complex cosmopolitan posthuman society. Maybe
it would look like a universe full of computronium and machinery, working full-time to build
even more computronium and machinery.
Third (from here),
"Empowerment is only a good bound of the long term component of utility functions, for
some reasonable future time cutoﬀ deﬁning 'long term'. But I think modelling just the
short term component of human utility is not nearly as diﬃcult as accurately modelling
the long term, so it's still an important win. I didn't investigate that much in the article,
but that is why the title is now "Empowerment is (almost) all we need"."
OK, well, insofar as I'm opposed to empowerment, I naturally think "empowerment + other
stuﬀ" is a step in the right direction! :) However, my hunch is that for a suﬃciently good
choice of "other stuﬀ", the "empowerment" part will be rendered unnecessary or
counterproductive. It seems likely that, if the future goes well, the AGI will facilitate human
empowerment at the end of the day, but maybe it can do so because the AGI ultimately
wants to maximize human ﬂourishing, and it can reason that increasing human
empowerment is instrumentally useful towards that end, for example.
Another thing is: Jacob writes: "no matter what your values are, optimizing for your
empowerment today is identical to optimizing for your long term values today." I think that
kind of thinking is a bit confused. I reject the idea that if the AGI is making good decisions
right now, then all is well. As mentioned above, if the AGI is motivated to manipulate human
values, that motivation might only manifest in the AGI's behavior way down the line, like
when the AGI is uploading human brains but deleting the parts that entail an intrinsic desire
for anything besides power. But while that problem will only manifest in the distant future,
the time to solve it is right at the beginning, when we're building the AGI and thus still have
direct control over its motivations.

4. Simboxes
Jacob is a big fan of "simulation sandboxes", which he calls "simboxes" for short. These are
air-gapped virtual worlds which serve as environments in which you can train an AGI. See
Jacob's recent post LOVE in a simbox is all you need, section 5.
Jacob is optimistic about being able to set up simboxes such that the AGI-under-test does not
escape (mainly because it doesn't know it's in a simbox, or even what a simbox is—as
he writes, "these agents will lack even the requisite precursor words and concepts that we
take for granted such as computation, simulation, etc."), and Jacob is also optimistic that
these tests will allow us to iterate our way to AGI safety / alignment.
While I'm much less optimistic than Jacob about achieving both those things simultaneously,
my very important take-home message is: I think simbox testing is an excellent idea. I
think we should not only be doing simbox testing in the endgame, but we should be
working right now to build infrastructure and culture that makes future simbox testing
maximally easy and safe and eﬀective, and maximally likely to actually happen, not just a
little but a lot. (Just like every other form of code testing and validation that we can think of.)
We should also be working right now to think through exactly what simbox tests to run and
how. I even previously included one ingredient of the path-to-simbox-testing—namely,
feature-rich user-friendly super-secure sandbox software compatible with large-scale ML—as
a Steve-endorsed shovel-ready AGI safety project on my list here.
Having said all that, I think we should mainly think of simbox testing as "an extra
layer of protection" on top of other reasons to expect safe and beneﬁcial AGI.
Speciﬁcally, I proposed in this comment two ways to think about what the simbox test is
doing:
A. We're going to have strong theoretical reasons to expect alignment, and we're going
to use simbox testing to validate those theories.
B. We're going to have an unprincipled approach that might or might not create
aligned models, and we're going to use simbox testing to explore / tweak speciﬁc
trained models and/or explore / tweak the training approach.
A is good. B is problematic, for reasons I'll get to shortly.
But ﬁrst, I want to emphasize that I see this A-vs-B distinction as a continuum, not a binary.
There's a whole spectrum from "unprincipled approach" to "strong theoretical reasons to
expect alignment", as we get a progressively more speciﬁc and ﬂeshed-out story underlying
why we expect our AGI to be aligned. For example:
All the way at the extreme of "strong theoretical reasons to expect alignment" would
be Vanessa Kosoy's research program working towards a rigorous mathematical proof
of AGI safety (which I'm pessimistic about, but I wish her luck!).
All the way at "unprincipled" would be just doing capabilities research, not thinking
about alignment at all, and seeing what happens with the trained models at the
end. Ajeya Cotra's "human feedback on diverse tasks" would be basically in that
category.
Somewhere in between these two extremes would be, say, Alex Turner's recent
diamond-alignment post, where we engage in speculation about what the "baby AGI" is
probably thinking about in diﬀerent situations, and then try to send reward signals at
carefully-chosen times to seed desired motivations. Or my toy example proposal here
to make an AGI that learns the abstract concept "human ﬂourishing" from
observations, and then tries to maximize the extent to which its beliefs pattern-match
to that abstract concept. These proposals may well fail, for sure, but at least we're

not totally in the dark when it comes to anticipating where and how they might fail,
and what tests might help us ﬁgure that out.
In terms of simbox use strategy, I think "somewhere in between A and B" is all I'm hoping
for, and I consider my research goal to be trying to get as close to A as possible.
Jacob's response was: "As for A vs B: ideally you may want A but you settle mostly for B.
That's just how the world often works, how DL progressed, etc. We now have more
established theory of how DL works as approx bayesian inference, but what actually drove
most progress was B style tinkering."
I think Jacob is selling himself short here. I think his simbox plan has a lot of "A" in it. I think
Jacob has pretty speciﬁc ideas in mind for how alignment is going to happen and how it could
fail, and these ideas are informing his picture of what kind of simbox testing is most useful,
and what we would be looking for, etc.
By the way, what's the problem with B? The problem is that the simboxes will be diﬀerent
from reality in lots of ways. For example, Jacob proposes "these agents will lack even the
requisite precursor words and concepts that we take for granted such as computation,
simulation, etc." Well, that's a great idea if we want to prevent the AGI from escaping the
sim! But that's a terrible idea if we want to avoid any distribution-shift between the simboxes
and reality! (Cf. "ontological crisis".) And if there's any distribution-shift, then there's the
possibility that the same training procedure will produce aligned AGIs in the simboxes and
misaligned AGIs in reality.
Jacob responds: The distribution shift from humans born in 0AD to humans
born in 2000AD seems fairly inconsequential for human alignment.  Indeed, any
useful AGI alignment mechanism should be at least as robust as human brains
under such mild distribution shifts.  Regardless, we can use various analogs of
technological concepts if needed.
Luckily, this problem is progressively less problematic as we move from "B" towards "A".
Then we have some understanding of possible failure modes, and we can ensure that those
failure modes are being probed by our simboxes.
(However, on my models, right now we are NOT close enough to "A" that all the remaining
failure modes can be simbox-tested. For example, the distribution shift from "agents that are
unaware of the concept of computation" to "agents that are aware of the concept of
computation" is fraught with danger, diﬃcult to reason about in our current state of
knowledge (see my discussion of "concept extrapolation" here), and risky to probe in
simboxes. So we still have lots more simbox-unrelated work to do, in parallel with the
important simbox-prep work.)
(Thanks Jacob for bearing with me through lots of discussion over the past months, and for
leaving comments above. Thanks also to Linda Linsefors & Alex Turner for critical comments
on an earlier draft.)
1. ^
I say "more or less" because I think Jacob and I have some disagreements about the
"neuroscience of novelty and curiosity" literature. For example, I think there's a theory
relating serotonin to information value, which Jacob likes and I dislike. But leaving aside
those details, I am strongly on board with the more basic idea that the brain has an
innate curiosity drive of some sort or another, and right now I don't have much of a
speciﬁc take on how it works.
2. ^

In addition to the direct eﬀects of F ("I like looking at X because F(X) is high"), there
could also be indirect eﬀects of F ("I like looking at X because it pattern-matches to /
reminds me of Y, which I like, and oh by the way the reason I like Y is because F(Y) was
high when I looked at it as a child"). See discussion of "correlation-guided proxy
matching" below.
3. ^
It's not that this is unknowable, but I think ﬁguring it out would require a heroic eﬀort
and/or detailed connectomic data about the human superior colliculus (and maybe also
the neighboring parabigeminal nucleus). And someone should totally do that!! I would
be very grateful!!
4. ^
UPDATE: Just to be clear, I don't have an opinion on the speciﬁc question of whether or
not humans have innate visual "sexiness"-related heuristics. I do think there has to be
something that solves the "symbol grounding" problem, but I'm not conﬁdent that it's
even partly visual. It could alternatively involve the sense of smell, and/or empathetic
simulation of body shape and sensations (vaguely along these lines but involving the
proprioceptive and somatosensory systems). Or maybe it is visual, I don't know.
5. ^
There's a weird dynamic here in that I'm saying that an AGI which supposedly wants
humanity to be empowered would be motivated to prevent humanity from exercising
its power. Isn't that contradictory? I think the way to square that circle is that the
proposal as I understand it is for the AGI to want humanity to be empowered later—
to eventually wind up empowered. However, there's a tradeoﬀ between empowerment-
now and empowerment-later. If I'm empowered-now, then I can choose NOT to be
empowered-later—e.g., by spending my money instead of hoarding it. Or jumping oﬀ a
cliﬀ. Therefore an AGI that always wants humanity to be empowered-later is an AGI
that never wants humanity to be empowered-now. So then the "later" never arrives—
not even at the end of the universe!!

Update to Mysteries of mode collapse:
text-davinci-002 not RLHF
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
I (and many others) did not realize this before, but: text-davinci-002 and text-davinci-
001, the InstructGPT models on the OpenAI API, were not trained with RLHF
(reinforcement learning from human feedback) as described in the InstructGPT paper,
but a "similar but slightly diﬀerent"[1] method that uses the same human feedback
data. Apparently, this other method is not technically RLHF.
Since this update has potentially nontrivial implications for interpreting the
phenomena exhibited by text-davinci-002 described in Mysteries of mode collapse
(formerly titled "Mysteries of mode collapse due to RLHF"), I'm making this separate
post for a signal boost.
I have not corrected the original text of "Mysteries of mode collapse due to RLHF", but
I've added a section at the beginning with further details on this update, copied here:
I have received evidence from multiple credible sources that text-
davinci-002 was not trained with RLHF.
The rest of this post has not been corrected to reﬂect this update. Not much
besides the title (formerly "Mysteries of mode collapse due to RLHF") is aﬀected:
just mentally substitute "mystery method" every time "RLHF" is invoked as the
training method of text-davinci-002. The observations of its behavior otherwise
stand alone.
This is kind of fascinating from an epistemological standpoint. I was quite
surprised to learn that text-davinci-002 was probably not trained with RLHF. I don't
remember exactly how "text-davinci-002 is RLHF" got elevated to an
unquestioned assumption in my mind. I might have mistook not being
contradicted by people who I assumed were in the know as conﬁrmation. I
certainly did not expect to talk for months to dozens of people about odd
behaviors I've observed in a well-known model "due to RLHF" without being
contradicted in a world where the model in question wasn't trained with RLHF, but
that's what happened.[2] It wasn't just me either: the assumption that text-
davinci-002(/text-davinci-001) is InstructGPT is RLHF seems ambient (e.g. search
"text-davinci-002 rlhf" on Twitter, this LW post, this article, and many others). I
contributed to perpetuating this misinformation cascade, and for that I apologize.
text-davinci-002's behaviors described in this post also contributed to my
conﬁdence because RLHF seemed to be a likely and potentially satisfying
explanation. Its apparently unsubstantiated conﬁdence in very speciﬁc outcomes
seems antithetical to the outer objective of self-supervised learning, which is
optimized by epistemic calibration, meaning the model's entropy should be as
high as possible while ﬁtting the data. In contrast, as several comments have
pointed out, it makes sense that RL kills entropy. The presence of "attractors"
made me additionally suspect that optimization from non-myopic outcome-
supervision was formative to text-davinci-002's psyche.

Mode collapse and attractors do seem to also be caused by RLHF (see Dumbass
policy pls halp and Inescapable wedding parties). So the update is that some other
training method also gives rise to these phenomena, as they are manifested by
text-davinci-002. 
Whether and how speculations concerning the causes of mode collapse/attractors
should be aﬀected depends on how text-davinci-002's training method diﬀers
from RLHF.
What is known about text-davinci-002's training method
Publicly available information suggests that the mystery method may not be so
diﬀerent from RLHF. Just today I discovered this sidenote in OpenAI's blog
post Aligning Language Models to Follow Instructions:
The InstructGPT models deployed in the API are updated versions trained
using the same human feedback data. They use a similar but slightly diﬀerent
training method that we will describe in a forthcoming publication.  
AFAIK, this is all that OpenAI has published about the RLHF/mystery method diﬀ. It
says that the InstructGPT models (text-davinci-001 and text-davinci-002) were
trained using the same human feedback data as the method described in
OpenAI's RLHF paper.[3] But this "similar but slightly diﬀerent" method is
apparently suﬃciently diﬀerent to not qualify as RLHF!
Pending further revelations, I suppose the lesson here was that I should have
sustained more entropy in my belief state given the partial information I had. But
what a demanding thing to ask! So much easier to promote an attractive
hypothesis to the status of decisive fact and collapse the remainder than to hold a
superposition in the mind.
1. ^
Sidenote on OpenAI's blog post, Aligning Language Models to Follow Instructions
2. ^
the lack of epistemic vigilantes attacking an unsubstantiated assumption in the
very title of this post on LessWrong is truly unbelievable!
3. ^
which seems to conﬁrm my suspicion about outcome-supervision

Will we run out of ML data? Evidence
from projecting dataset size trends
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
This is a linkpost for https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-
projecting-dataset
Summary: Based on our previous analysis of trends in dataset size, we project the growth of
dataset size in the language and vision domains. We explore the limits of this trend by
estimating the total stock of available unlabeled data over the next decades.
Read the full paper in arXiv.
Our projections predict that we will have exhausted the stock of low-quality language data by
2030 to 2050, high-quality language data before 2026, and vision data by 2030 to 2060. This
might slow down ML progress.
All of our conclusions rely on the unrealistic assumptions that current trends in ML data
usage and production will continue and that there will be no major innovations in data
eﬃciency. Relaxing these and other assumptions would be promising future work.
Figure 1: ML data consumption and data production trends for low quality text,
high quality text and images.
 
 
Historical projection
Compute projection
Low-quality language stock
2032.4
[2028.4 ; 2039.2]
 2040.5
[2034.6 ; 2048.9]
High-quality language stock
2024.5
[2023.5 ; 2025.7]
2024.1
[2023.2 ; 2025.3]
Image stock
2046
[2037 ; 2062.8]
2038.8
[2032 ; 2049.8]
Table 1: Median and 90% CI exhaustion dates for each pair of projections.
Background

Chinchilla's wild implications argued that training data would soon become a bottleneck for
scaling large language models. At Epoch we have been collecting data about trends in ML
inputs, including training data. Using this dataset, we estimated the historical rate of growth
in training dataset size for language and image models.
Projecting the historical trend into the future is likely to be misleading, because this trend is
supported by an abnormally large increase in compute in the past decade. To account for
this, we also employ our compute availability projections to estimate the dataset size that
will be compute-optimal in future years using the Chinchilla scaling laws.
We estimate the total stock of English language and image data in future years using a
series of probabilistic models. For language, in addition to the total stock of data, we
estimate the stock of high-quality language data, which is the kind of data commonly used to
train large language models.
We are less conﬁdent in our models of the stock of vision data because we spent less time on
them. We think it is best to think of them as lower bounds rather than accurate estimates.
Results
Finally, we compare the projections of training dataset size and total data stocks. The results
can be seen in the ﬁgure above.  Datasets grow much faster than data stocks, so if current
trends continue, exhausting the stock of data is unavoidable. The table above shows the
median exhaustion years for each intersection between projections.
In theory, these dates might signify a transition from a regime where compute is the main
bottleneck to growth of ML models to a regime where data is the taut constraint.
In practice, this analysis has serious limitations, so the model uncertainty is very high. A
more realistic model should take into account increases in data eﬃciency, the use of
synthetic data, and other algorithmic and economic factors.
In particular, we have seen some promising early advances on data eﬃciency,[1] so if lack of
data becomes a larger problem in the future we might expect larger advances to follow. This
is particularly true because unlabeled data has never been a constraint in the past, so there
is probably a lot of low-hanging fruit in unlabeled data eﬃciency.
In the particular case of high-quality data, there are even more possibilities, such as
quantity-quality tradeoﬀs and learned metrics to extract high-quality data from low-quality
sources. 
All in all, we believe that there is about a 20%[2] chance that the scaling (as measured in
training compute) of ML models will signiﬁcantly slow down by 2040 due to a lack of training
data.
1. ^
Eg, transformers with retrieval mechanisms are more sample eﬃcient. Or see
EﬃcientZero for a dramatic example, albeit in a diﬀerent domain.
In addition to increased data eﬃciency, we have seen examples of synthetic data being
used to train language models.
2. ^
This probability was obtained by polling some Epoch team members and taking the
geometric mean of the results.

Instead of technical research, more
people should focus on buying time
This post is the ﬁrst in a sequence of posts about AI strategy co-authored by Thomas
Larsen, Akash Wasil, and Olivia Jimenez (TAO). In the next post, we'll provide more
examples of "buying time" interventions that we're excited about. 
We're grateful to Ajeya Cotra, Daniel Kokotajlo, Ashwin Acharya, and Andrea Miotti for
feedback on this post. 
If anyone is interested in working on "buying time" interventions, feel free to
reach out. (Note that Thomas has a list of technical projects with speciﬁcs about how to
implement them. We also have a list of non-technical projects). 
Summary
A few months ago, when we met technical people interested in reducing AI x-risk, we were
nearly always encouraging them to try to solve what we see as the core challenges of the
alignment problem (e.g., inner misalignment, corrigibility, interpretability that generalizes
to advanced systems). 
But we've changed our mind.
On the margin, we think more alignment researchers should work on "buying
time" interventions instead of technical alignment research (or whatever else they
were doing). 
To state the claim another way: on the margin, more researchers should backchain
from "how do I make AGI timelines longer, make AI labs more concerned about
x-risk, and present AI labs with clear things to do to reduce x-risk", instead of
"how do I solve the technical alignment problem?" 
Some "buying time" interventions involve performing research that makes AI safety
arguments more concrete or grounds them in ML (e.g., writing papers like Goal
misgeneralization in deep reinforcement learning and Alignment from a deep learning
perspective & discussing these with members of labs). 
Some "buying time" interventions involve outreach and engagement with capabilities
researchers, leaders in AI labs, and (to a lesser extent) the broader ML community (e.g.,
giving a presentation to a leading AI lab about power-seeking and deception). We expect
that successful outreach eﬀorts will also involve understanding the
cruxes/counterarguments of the relevant stakeholders, identifying limitations of existing
arguments, and openly acknowledging when the AI safety community is
wrong/confused/uncertain about certain points.
We are excited about "buying time" interventions for four main reasons: 
1. Multiplier eﬀects: Delaying timelines by 1 year gives the entire alignment community
an extra year to solve the problem. 
2. End time: Some buying time interventions give the community a year at the end,
where we have the most knowledge about the problem, access to near-AGI systems,
the largest community size, the broadest network across other inﬂuential actors, and
the most credibility at labs. Buying end time also increases the amount of serial

alignment research, which some believe to be the bottleneck. We discuss this more
below.   
3. Comparative advantage: Many people would be better-suited for buying time than
technical alignment work (see ﬁgure 1).
1. Buying time for people at the tails: We expect that alignment research is
extremely heavy-tailed. A median researcher who decides to buy time is buying
time for people at the tails, which is (much) more valuable than the median
researcher's time.
4. Externalities/extra beneﬁts: Many buying time interventions have additional beneﬁts
(e.g., improving coordination, reducing race dynamics, increasing the willingness to
pay a high alignment tax, getting more people working on alignment research). 
Figure 1: Impact by percentile for technical alignment research and buying time
interventions.
Caption: We believe that impact in technical alignment research is more heavy-tailed than
impact for buying time interventions. Figure 1 illustrates this belief. Note that this is a
rough approximation. Note also that both curves should also go below the 0-point of the y-
axis, as both kinds of interventions can be net negative. 
Concretely, we recommend that ~40-60% of alignment researchers should focus on
"buying time" interventions rather than technical alignment research (whereas we

currently think that only ~20% are focusing on buying time). We also recommend that
~20-40% of community-builders focus on "buying time" interventions rather than typical
community-building (whereas we currently think that ~10% are focusing on buying time). 
In the rest of the post, we: 
1. Oﬀer some disclaimers and caveats (here)
2. Elaborate on the reasons why we're excited about "buying time" interventions (here)
3. Describe some examples of "buying time" interventions (here)
4. Explain our theory of change in greater detail (here)
5. Describe some potential objections & our responses (here)
6. Describe some changes we recommend (here)
Disclaimers
Disclaimer #1: We're not claiming that "buying time" is the only way to categorize the
kinds of interventions we describe, and we encourage readers to see if they can come up
with alternative frames/labels. Many "buying time" interventions also have other beneﬁts
(e.g., improving coordination, getting more people to work on AI safety researchers, and
making it less likely that labs deploy dangerous systems). We chose to go with the "buying
time" frame for two main reasons
1. For nearly all of the interventions we describe, we think that most of the beneﬁt
comes from buying time, and these other beneﬁts are side beneﬁts. One important
exception to this is that much of the impact of evals/demos may come from their
ability to prevent labs from deploying dangerous systems. This buys time, but it's
plausible that the main beneﬁt is "the world didn't end".
2. We have found backchaining from "buying time" more useful than other frames we
brainstormed. Some alternative frames have felt too limiting (e.g., "outreach to the
ML community" doesn't cover some governance interventions).
Disclaimer #2: Many of these interventions have serious downside risks. We also think
many of them are diﬃcult, and they only have a shot at working if they are executed
extremely well.
Disclaimer #3: We have several "background assumptions" that inform our thinking. Some
examples include (a) somewhat short AI timelines (AGI likely developed in 5-15 years), (b)
high alignment diﬃculty (alignment by default is unlikely, and current approaches seem
unlikely to work), and (c) there has been some work done in each of these areas, but we
are far behind what we would expect in winning worlds, and there are opportunities to do
things that are much more targeted & ambitious than previous/existing projects. 
Disclaimer #4: Much of our thinking is informed by conversations with technical AI safety
researchers. We have less experience interacting with the governance community and
even less experience thinking about interventions that involve the government. It's
possible that some of these ideas are already widespread among EAs who focus on
governance interventions, and a lot of our arguments are directed at the thinking we see in
the technical AI safety community.
Why are we so excited about "buying
time" interventions?"

Large upsides of buying time 
Some time-buying interventions buy a year at the end. If capabilities growth continues as
normal until someone is about to deploy an AI model that would improve into a TAI, but an
evaluation triggers and reveals misaligned behavior, causing this lab to slow down and
warn the other labs, the time from this event until when AGI is deployed is very valuable
for the following reasons: 
1. You have bought one month for the entire AI safety community[1].
2. More researchers: The number of alignment researchers is growing each year, so we
expect to have the most alignment researchers at the end.
3. Better understanding of alignment: We understand more about the alignment
problem each year, and the ﬁeld becomes less pre-paradigmatic. This makes it easier
to make progress each year.
4. Serial time: buying time increases the amount of serial alignment research, which
could be the bottleneck. 
5. AGI assisted alignment: Some alignment agendas involve using AI assistants to boost
alignment research. It's plausible that a year of alignment research with AI assistants
is 5-10X more valuable than a year of alignment research right now. If we're able to
implement interventions that buy time once we have powerful AI assistants, this
intervention would be especially valuable (assuming that these assistants can make
diﬀerential alignment progress or that we can buy time once we have the assistants).
6. Better understanding of architecture: when we are close to AGI, we have a better
understanding of the architectures and training paradigms that will be used to
actually build AGI, allowing for alignment solutions to be much more concrete and
informed.  
Other interventions have a diﬀerent shape, and do not buy as valuable time. If you simply
slow the rate of capabilities progress through publication policies or convince some
capabilities researchers to transition, such that on net then AI will take one more year to
generate, this has the beneﬁt of 1-4, but not 5 and 6. However, we are proposing to
reallocate alignment researchers to buying time interventions, which means that less
alignment research is being made this year, so reason 3 might be less strong. 
Some interventions that buy time involve coordinating with members of major AI labs (e.g.,
OpenAI, DeepMind, Anthropic). As a result, these interventions often have the additional
beneﬁt of increasing communication, coordination, trust, and shared understanding
between major AI labs and members of the AI safety community who are not part of the AI
labs. (Note that this is not true of all "buying time" interventions, and several of them
could also lead to less coordination or less trust).
Tractability and comparative advantage: lots of
people can have a solid positive impact by
buying time, while fewer can do great alignment
work
1. Buying a year buys a year for researchers at the tails. If you buy a year of time, you
buy a year of time for some of the best alignment researchers. It's plausible to us
that researchers at the tails are >50-100X more valuable than median researchers.
2. Many projects that are designed to buy time require diﬀerent skills than technical AI
safety research. 

1. Skills that seem uniquely valuable for buying time interventions: general
researcher aptitudes, ability to take existing ideas and strengthen them,
experimental design skills, ability to iterate in response to feedback, ability to
build on the ideas of others, ability to draw connections between ideas,
experience conducting "typical ML research," strong models of ML/capabilities
researchers, strong communication skills
2. Skills that seem uniquely valuable for technical AI safety research: abstract
thinking, ability to work well with very little structure or guidance, ability to
generate and formalize novel ideas, focus on "the hard parts of the problem",
ability to be comfortable being confused for long periods of time.
3. Skills that seem roughly as useful in both: Strong understanding of AI safety
material, machine learning knowledge.
3. Alignment research seems heavy-tailed. It's often easy to identify whether or not
someone has a reasonable chance of being at the tail (e.g., after 6-12 months of
trying to solve alignment). People who are somewhat likely to be at the tail should
keep doing alignment research; other people should buy time.
A reasonable counterpoint is that "buying time" might also be heavily-tailed. However, we
currently expect it to be less heavy-tailed than alignment research. It seems plausible to us
that many "median SERI-MATS scholars" could write papers like the goal misgeneralization
paper, explain alignment diﬃculties in clearer and more compelling ways, conduct (or
organize) high-quality outreach and coordination activities and perform many other
interventions we're excited about. On the other hand, we don't expect that "median SERI-
MATS scholars" would be able to make progress on heuristic arguments, create their own
alignment agendas, or come up with other major conceptual advancements.
Nonetheless, a lot of the argument depends on the speciﬁc time-buying and the speciﬁc
alignment research. It seems plausible to us that some of the most diﬃcult time-buying
interventions are more heavy-tailed than some of the more straightforward alignment
research projects (e.g., coming up with good eval tools and demos might be more heavy-
tailed than performing interpretability experiments). 
What are some examples of "buying
time" interventions?
The next post in this sequence (rough draft here) outlines more concrete interventions that
we are excited about in this space, but we highlight three interventions here that are
especially exciting to us. We brieﬂy provide some examples below.
Outreach eﬀorts that involve interactions
between the AI safety community and (a)
members of AI labs + (b) members of the ML
community.
Some speciﬁc examples:
1. More conferences that bring together researchers from diﬀerent groups who are
working on similar topics (e.g., Anthropic recently organized an interpretability
retreat with members from various diﬀerent AI labs and AI alignment organizations). 
2. More conferences that bring together strategy/governance thinkers from diﬀerent
groups (e.g., Olivia and Akash recently ran a small 1-day strategy retreat with a few

members from AI labs and members). 
3. Discussions like the MIRI 2021 conversations, except with a greater emphasis on
engaging with researchers and decision-makers at major AI labs by directly touching
on their cruxes. 
4. Collaborations on interventions that involve coordinating with AI labs (e.g., ﬁguring
out if there are ways to collaborate on research projects, eﬀorts to implement
publication policies and information-sharing agreements, eﬀorts to monitor new
actors that are developing AGI, etc.)
5. More ML community outreach. Examples include projects by the Center for AI Safety
(led by Dan Hendrycks) and AIS ﬁeld-building hub (led by Vael Gates). 
The Evaluations Project (led by Beth Barnes)
Beth's team is trying to develop evaluations that help us understand when AI models
might be dangerous. The path to impact is that an AI company will likely use the eval tool
on advanced AI models that they train, and this eval could then lead them to delay
deployment of a model for which the eval unveiled scary behavior. In an ideal world, this
would be so compelling that multiple AI labs slow down, potentially extending timelines by
multiple years.
Papers that take theoretical/conceptual safety
ideas and ground them in empirical research.
Speciﬁc examples of this type of work include Lauro Langosco's goal misgeneralization
paper (which shows how an RL agent can appear to learn goal X but actually learn goal Y)
and Alex Turner's optimal policies tend to seek power paper. Theoretical alignment
researchers had already proposed that agents could learn unintended goals and
that agents would have incentives to seek power. The papers by Lauro and Alex take these
theoretical ideas (which are often perceived as fuzzy and lacking concreteness), formalize
them more crisply, and oﬀer examples of how they aﬀect modern ML systems. 
We think that this buys time primarily by convincing labs and academics of alignment
diﬃculty. In the next section, we give more detail on the theory of chance.  
Theory of Change
In the previous section, we talked about why we were so excited about buying time
interventions. However, the interventions we have in mind often have a number of other
positive impacts. In this section, we provide more detail about these impacts as well as
why we think these impacts end up buying time. 
We summarize our theory of change in the following diagram: 
Labs take AGI x-risk seriously + Labs have
concrete things they can do → More Time
We think that timelines are largely a function of (a) the extent to which leaders and
researchers at AI labs are concerned about AI x-risk and (b) the extent to which they have
solutions that can be (feasibly) implemented. 

If conditions (a) and (b) are met, we expect the following beneﬁts:
1. Less capabilities research. There is less scaling and less algorithmic progress.  
2. More coordination between labs. There are more explicit and trusted agreements to
help each other with safety research, avoid deploying AGI prematurely, and avoid
racing. 
3. Less publishing capabilities advances. Capabilities knowledge is siloed, so when one
lab discovers something, it doesn't get used by the rest of the world. For
example, PaLM claims a 15% speed up from parallelizing layers. If this insight hadn't
been published, PaLM would likely have been 15% slower. 
4. Labs being less likely to deploy AGI and scale existing models. 
These all lead to relative slowdowns of AGI timelines, giving everyone more time to solve
the alignment problem.
Beneﬁts other than buying time
Many of the interventions we describe also have beneﬁts other than buying time. We think
the most important ones are:
1. Willingness to pay a higher alignment tax: Concern about alignment going poorly
means that the labs invest more resources into safety. An obvious resource is that a
computationally expensive solution to alignment becomes a lot more likely to be
implemented, as the labs recognize how important it is. Concretely, we are
substantially more excited about worlds in the lab building AGI actually implements
all of the interventions described in Holden's How might we align transformative AI if
it's developed very soon?. By default, if AGI were developed in the next few years at
OpenAI or DeepMind, we would put ~20% on each of these solutions actually being
used. 
2. Less likely to deploy dangerous systems: If evaluations are used and safety standards
are implemented, labs could catch misaligned behavior and decide not to deploy
systems that would have ended the world. This leads to timelines increases, but it
also has the more direct impact of literally saving the world (at least temporarily). We
expect that this causes the probability of a naive accident risk to go down
substantially. 
3. More alignment research: If labs are convinced by AI safety arguments, they may
shift more of their (capabilities) researchers toward alignment issues.
Some objections and our responses
1. There are downside risks from low-quality outreach and coordination eﬀorts with AI labs
Response: We agree. Members of AI labs have their own opinions about AI safety; eﬀorts
to come in and proselytize are likely to fail. We think the best eﬀorts will be conducted by
people who have (a) strong understandings of technical AI safety arguments, (b) strong
interpersonal skills and ability to understand diﬀerent perspectives, (c) caution and good
judgment, and (d) collaborators or advisors who can help them understand the space.
However, this depends on the intervention. Caution is especially warranted when doing
direct outreach that involves interaction with capabilities researchers, but more technical
work such as empirically grounding alignment arguments pretty much only requires
technical skill.  
2. Labs perceive themselves to be in a race, so they won't slow down. 

Response #1: We think that some of the concrete interventions we have in mind
contribute to coordination and reduce race dynamics. In particular, eﬀorts to buy time by
conveying the diﬃculty of alignment could lead multiple players to become more
concerned about x-risk (causing all of the leading labs to slow down). 
Furthermore, we're optimistic that suﬃciently well-executed coordination events could lead
to increased trust and potentially concrete agreements between labs. We think that
diﬀerences in values (company A is worried that company B would not use AI responsibly)
and worries about misuse risk (company A is worried that company B's AI is likely to be
unaligned) are two primary drivers of race dynamics.
However, to the extent that A and B are value-aligned, both are aware that each of them
are taking reasonable safety precautions, and leaders at both companies trust each other,
they are less incentivized to race each other. Coordination events could help with each of
these factors.
Response #2: Some interventions don't reduce race dynamics (e.g., slowing down the
leading lab). These are high EV in worlds where the safety-conscious lab (or labs) has a
sizable advantage. On the margin, we think more people should be investing into these
interventions, but they should be deployed more carefully (ideally after some research has
been conducted to compare the upside of buying time to the downside of increasing race
dynamics). 
3. Labs being more concerned about safety isn't that helpful. They already care; they just
lack solutions.
Response: Our current impression is that many leaders at major AGI labs are concerned
about safety. However, we don't think everyone is safety-conscious, and we think there are
some policies that labs could adopt to buy time (e.g., adopting publication policies that
reduce the rate of capabilities papers).
4. Slowing down ODA+[2] could increase the chance that a new (and less safety-conscious
actor) develops AGI. 
Response #1: Our current best guess is that ODA+ has a >6 month lead over less safety-
conscious competitors. However, this is fairly sensitive to timelines. If scale is critical, then
one would expect a small number of very large projects to be in the lead for AGI, and
diﬀerentially slowing the most receptive / safety oriented / cautious of those labs seems on
net negative. However, interventions that slow the whole ﬁeld such as a blanket slowdown
in publishing or increases the extent to which all labs are safety oriented are robustly
good.    
Response #2: Even if ODA+ does not have a major lead, many of the interventions (like
third-party audits) could scale to new AGI developers too. For example, if there's a culture
in the ﬁeld of doing audits, and pressure to do so, talented researchers are likely not to
want to work for you unless you participate, or if later there's a regulatory regime attached
to all that.
Response #3: Some interventions to buy time increase lead time of labs and slow
research overall (e.g., making it more diﬃcult for new players to enter the space;
compelling evals or concretizations of alignment diﬃculties could cause many labs to slow
down). 
Response #4: Under our current model, most P(doom) comes from not having a solution
to the alignment problem. So we're willing to trade some P(solution gets implemented) and
some P(AGI is aligned to my values) in exchange for a higher P(we ﬁnd a solution).
However, we acknowledge that there is a genuine tradeoﬀ here, and given the uncertainty

of the situation, Thomas thinks that this is the strongest argument against buying time
interventions. 
5. Buying time is not tractable.
Response: This is possible, but we currently doubt it. There seems to be a bunch of stuﬀ
that no one has tried (we will describe this further in a follow-up post). 
6. In general, problems get solved by people actually trying to solve them. Not by avoiding
the hard problems and hoping that people solve them in the future.
Response #1: Getting mainstream ML on board with alignment concerns is solving one of
the hardest problems for the alignment ﬁeld. 
Response #2: Although some of the beneﬁt from "buying time" involves hoping that new
researchers show up with new ideas, we're also buying time for existing researchers who
are tackling the hard problems. 
Response #3: People who have promising agendas that are attacking the core of the
problem should continue doing technical alignment research. There are a lot of people who
have been pushed to do technical alignment work who don't have promising ideas (even
after trying for years), or feel like they have gotten substantial signal that they are worse
at thinking about alignment than others. These are the people we would be most excited
to reallocate. (Note though that we think feedback loops in alignment are poor. Our current
guess is that among the top 10% of junior researchers, it seems extremely hard to tell who
will be in the tail. But it's relatively easy to tell who is in the top 10-20% of junior
researchers).
7. There's a risk of overcorrection: maybe too many people will go into "buying time"
interventions and too few people will go into technical alignment. 
Response: Currently, we think that the AIS community heavily emphasizes the value of
technical alignment relative to "buying time." We think it's unlikely that the culture shifts
too far in the other direction.
8. This doesn't seem truth-seeky or epistemically virtuous— trying to convince ML people
of some speciﬁc claims feels wrong, especially given how confused we are and how much
disagreement there is between alignment researchers. 
Response #1: There are a core set of claims that are pretty well supported that the
majority of the ML community has not substantially engaged with (e.g. goodharting,
convergent instrumental goals, reward misspeciﬁcation, goal misgeneralization, risks from
power-seeking, risks from deception). 
Response #2: We're most excited about outreach eﬀorts and coordination eﬀorts
that actually allow us to ﬁgure out how we're wrong about things. If the alignment
community is wrong about something, these interventions make it more likely that we ﬁnd
out (compared to a world in which we engage rather little with capabilities researchers +
ML experts and only talk to people in our community). If others are able to refute or
deconfuse points that are made in this outreach eﬀort, this seems robustly good, and it
seems possible that suﬃciently good arguments would convince us (or the alignment
community) about potentially cruxy issues. 
What changes do we want to see?

1. Allocation of talent: On the margin, we think more people should be going into
"buying time" interventions (and fewer people should be going into traditional
safety research or traditional community-building).
2. Concretely, we think that roughly 80% of alignment researchers are working on
directly solving alignment. We think that roughly 50% should be working on
alignment, while 50% should be reallocated toward buying time.
3. We also think that roughly 90% of (competent) community-builders are focused on
"typical community-building" (designed to get more alignment researchers). We think
that roughly 70% should do typical community-building, and 30% should be buying
time.
4. Culture: We think that "buying time" interventions should be thought of as a
comparable or better path than (traditional) technical AI safety research and
(traditional) community-building.
5. Funding: We'd be excited for funders to encourage more projects that buy time via
ML outreach, improved coordination, taking conceptual ideas and grounding them
empirically, etc. 
6. Strategy: We'd be excited for more strategists to think concretely about what buying
time looks like, how it could go wrong, and if/when it would make sense to accelerate
capabilities (e.g., how would we know if OpenAI is about to lose to a less safety-
conscious AI lab, and what would we want to do in this world?)
1. ^
We did a BOTEC that suggested that 1 hour of alignment researcher time would buy,
in expectation, 1.5-5 quality-adjusted research hours. The BOTEC made several
conservative assumptions (e.g., it did not account for the fact that we expect
alignment research to be more heavy-tailed than buying time interventions). We are
in the process of revising our BOTEC, and we hope to post it once we have revised it.
2. ^
 ODA+ = OpenAI, DeepMind, Anthropic, and a small number of other actors.

Exams-Only Universities
Quality: fast write-up (~45 minutes); little researched
I want a university that only does exams, which would include a description of what
you need to know for the exams. Bonus would be suggestions for textbooks and online
classes to learn the material, but that's optional.
Cost
The costs are:
Exam creation
Supervision
Scoring
Exam creation is a ﬁxed cost per topic per round (you need to change the exam
each year).
Supervision, if in person, is partly a ﬁxed cost per location, and partly a variable
cost. (Someone can supervise students doing exams on diﬀerent topics.) Online
supervision would also be an option ideally.
Scoring is partially a ﬁxed cost (the part that can be automated) and partially a
variable cost (the part that need to be reviewed manually).
Frequency
Frequency of exams could change with demand, but, to start with, you could have one
cheap exam per topic per year, timed with the end of normal school years. This would
be the exam most people would take, so it would spread the ﬁxed cost among more
people. This might also be a more valuable test because it could position you on a
normal curve more precisely given the greater amount of people taking it. There could
also be more expensive tests throughout the year—more expensive given the ﬁxed
cost would be spread among fewer people.
Problem it solves
What this solves:
1. Decouples learning and exams a) You can learn at your own pace (whether that's
more slowly of faster) b) You can learn from wherever you want (maybe you
want to learn from diﬀerent places for diﬀerent topics)
2. Creates standardized tests making it easier to compare the competency of
students between diﬀerent schools

Why wanting exams in the ﬁrst place? Because many governments want them for
immigration and many organizations want them for employees.
Questions I have
1. Has this been done?
2. Could this be done? Could you have a university that only does exams and emits
degrees that are recognized by the US government?
Current state
There are universities that don't require you to attend classes, but I still ﬁnd those
non-ideal because it's:
1. Bias: In my experience, students that attend have an unfair advantage—Not
because they learn more, but because teachers literally (and even intentionally)
give unfair hints about their exams to reward students for showing up and
justifying their salary.
2. Inconvenient:
a) You need to be in a speciﬁc location for a few years.
b) You can still do some exams only once per year, which prevents doing a degree
faster.
Future
While all I'm asking in this post is for non-zero universities to oﬀer this so that the
demand for it gets fulﬁlled, I also have the impression there would be signiﬁcant
beneﬁts at a societal level from more fully decoupling exams and learning in general.
It seems to me like test scores would become more meaningful, and it would become
cheaper to get scored.

Against "Classic Style"
What is Classic Style?
The Sense of Style is Steven Pinker's style guide informed by cognitive psychology and
linguistics. The main idea is that the author should write in a particular mode of
communication called "classic style".
The guiding metaphor of classic style is seeing the world. The writer can see something
that the reader has not yet noticed, and he orients the reader's gaze so that she can see
it for herself. The purpose of writing is presentation, and its motive is disinterested truth.
It succeeds when it aligns with the truth, the proof of success being clarity and simplicity.
— Clear and Simple as the Truth, Francis-Noel Thomas and Mark Turner

Here are the do's and don't of classic style:
Eliminate meta-discourse.
Don't write "in the next section I will..."
Eliminate "hedging".
Don't write "maybe", "it seems to me", or "I think that".
Don't use concepts about concepts.
Don't write "approach", "assumption", "concept", "condition", "context", "framework",
"issue", "level", "model", "paradigm", "perspective", "process", "role", "strategy",
"tendency", "variable".
Talk about the subject, not about research about the subject.
If you're writing about apples, then write "apples are X" rather than "Dr Smith ﬁrst
discovered that apples are X".
Roughly speaking, Pinker recommends that the author should write about something as if
neither the document nor the author nor the reader actually exists. If they're writing a book
about apples, then the sentences should assert facts about apples, not about the study of
apples and certainly not about the book itself.
My opinion: I like that Pinker deduces his advice from an underlying theory of
communication. This is better than many style guides which present an ad-hoc list of tips.
However, I think the theory is wrong. [1]
What's wrong with Classic Style?
I'm suspicious of "A Sense of Style" because it separates what you should write and what is
true. To clarify, Pinker isn't saying there are things you should write that aren't true, but he is
saying there are things that are true that you shouldn't write.
Pinker contrasts "classic style" with what he calls "postmodern style" — where the author
explicitly refers to the document itself, the readers, the authors, any uncertainties,
controversies, errors, etc.  I think a less pejorative name for "postmodern style" would be
"self-aware style". This is the predominant communication style I see on LessWrong.
In this post, I will list some defects of classic style.
Problem 1: Self-referentiality.
The Capital-T-Truth is that the book does have an author, a reader, and exists in the same
world as the objects the book discusses. Therefore there are certain facts about the subject
that a classic-style document won't be able to assert — namely, these are facts about the
relationship between the document and the subject.
In map-territory framing: a map is "classic style" if it doesn't include a "you are here" marker.
But often the map is part of the territory that is being mapped, and therefore a lack of a "you
are here" marker is an omission of an important truth about the territory.
This seems defective to me. If the writers in a community assert all truths about the subject,
including self-referential truths, then the community is likely to have more accurate beliefs. 
Problem 2: Epistemic qualiﬁers.

It's very diﬃcult to state in classic-style anything like "the probability that X is p" because
probabilities are statements about the author's beliefs about X. Recall that in classic style,
the document does not know the author exists!
This is far more restricting than it may ﬁrst appear. If you can't use epistemic qualiﬁers, then
you aren't allowed to discuss anything that isn't certain. For example, if I'm writing a book
about cosmology, how am I supposed to write "eternal inﬂation might be true"? What is that
sentence about? It's not a statement about the cosmos, it's a statement about the author's
knowledge of the cosmos.
This seems defective to me. If the writers in a community express their credences in their
assertions, then the community is likely to have more accurate beliefs.
Problem 3: Errors should not pass silently.
In classic style, you aren't permitted to explicitly mention any errors in the document. You
can't write "I might be wrong about X", or "I won't deal with Y  in this article", etc. This is
because it's the document that has errors, and in classic style, the document does not know
the document exists!
This seems defective to me. If the writers in a community silence their errors, then the
community is likely to have less accurate beliefs. This is because either:
1. All the writers are authorities on the subject, and the opinion of non-authorities is
ignored.
2. Non-authorities write about the subject but the readers over-update on their assertions.
Problem 4: Incomplete information
In classic style, the author asserts their conclusions about the subject. The reader then
updates their beliefs about the subject on the fact that the author has such-and-such
conclusions.
The problem is that the reader is updating on incomplete information — they don't know who
the author is, why they are writing the book, what evidence they have seen, and what their
prior assumptions were
This seems defective to me. A community will perform better if all the writers in the
community express all the information that might update the reader's beliefs.
Problem 5: Signposts
Often a document will describe itself to help the reader ﬁnd information. For example "In the
next section, I will do such-and-such". This tells the reader what information they will ﬁnd in
that section, so the reader is then better informed about whether they want to read it. But
these helpful signposts aren't allowed in classic style.
This seems defective to me. If the writers in a community avoid signposts, then the
community is likely to waste time reading things they didn't need to read.
Why is self-aware style better?

Here is what I would endorse: Authors should write sentences that are maximally
informative to the reader. This includes sentences about the document and the author
rather than just about the subject. So if you know that ϕ and you expect that the reader's
beliefs about the subject would signiﬁcantly change if they also knew ϕ, then write that ϕ.
Suppose an author writes "if humans encounter alien life in the next million years then the
alien life will almost certainly be intelligent". Then the reader's belief about aliens is
correlated with their belief about the author — what's their background? Why do they think
this? Are they an economist? An astrobiologist? A UFO enthusiast? Who's testimony are they
relying on? Is someone paying them to say this?
In this situation, the author is informing the reader about aliens when they answer these
questions. This is the key point that Pinker misses! 
Conclusion
My conclusion is quite strong — I think classic style is almost always socially defective in the
following situations:
Academic papers
Non-ﬁction books
Textbooks
Blog posts
Manuals
Writing in classic style won't harm the author (and might even beneﬁt the author) but a
community where authors habitually use classic style would be worse than a community
where authors use self-aware style. I can't think of any situations where the ﬁve limitations I
mention would be appropriate.
Edits: various clarifying remarks.
1. ^
Friendly disclaimer: Steven Pinker doesn't endorse classic style in all situations. This is
Steven Pinker being moderate, rather than classic style being moderate. Also, his book
The Sense of Style contains a lot of other tips other than classic style, which I've
ignored in this post.

New Frontiers in Mojibake
This is a linkpost for https://adam.scherlis.com/2022/11/25/new-frontiers-in-mojibake/
Fun with mismatched encodings
Mojibake is the garbled text that result from character-encoding errors.
If youâ€™ve seen text that looks like this â€" and Iâ€™m sure you have â€" then
youâ€™ve seen mojibake.
(You should be seeing something like this:
If you see something else, this post may be a little confusing and you need a new web
browser.)
Computers represent text as a sequence of bytes, and "text encodings" are
dictionaries that turn characters (i.e. symbols: letters, punctuation, etc.) into bytes and
vice-versa.
The garbled text above is a pretty common species of mojibake. It's what happens
when em-dashes and curly apostrophes are encoded as bytes with UTF-8 (the now-
nearly-universal text encoding) and decoded back to characters with Windows-1252
(an obsolete encoding that is still pretty widespread).
Windows-1252 is pretty straightforward: each character gets one byte, and there are
only 256 characters so this works out.
UTF-8 is one of several character encodings based on Unicode, which (for our
purposes) is a massive numbered list of over 100,000 characters. Unicode includes
nearly every language in the world and a bunch of other nonsense like emojis.
UTF-8 turns each character into a sequence of up to four bytes based on its Unicode
"codepoint" (position in the list). Codepoints are bigger than bytes, so you still need
this translation step.
(I'm simplifying a little here, and you should be grateful for that.)
Speciﬁcally, an em-dash gets mangled like this:
EM DASH (—) is Unicode character #8,212, usually written (in hex) as U+2014.
UTF-8 encodes the number 8,212, which is too big to ﬁt in a single byte, as the
sequence of bytes 0xE2, 0x80, 0x94
Windows-1252 looks at each byte in turn and decodes them directly as the
characters â, €, " respectively.

Finally, your computer looks up the characters â, €, " in some font ﬁle and draws
the speciﬁc glyph for each of those characters in that font. (A "glyph" is the
actual picture on your screen; a "character" is the abstract concept of the euro
symbol or whatever.)
(I made this happen deliberately with python: '\u2014'.encode('utf8').decode('1252').)
This sometimes happens to the same text multiple times, and special characters turn
into exponentially-growing strings of nonsense that overwhelm the text:
An astrological mystery
I saw this today on a used-book website:

Astrological symbols??
My ﬁrst thought was that this was something like the above, but with a diﬀerent output
text encoding in place of Windows-1252. But this isn't really plausible; since UTF-8
encodes dashes and apostrophes as three bytes apiece, the other encoding would have
to have a multi-byte sequence for ☋, the "descending node" astrological symbol. The
main problems with this are that UTF-8 is the only multi-byte encoding in widespread
use, and that (AFAIK) Unicode is the only character set in widespread use that includes
☋.
Maybe it's reversed? Text goes into a legacy encoding and comes out of UTF-8 looking
like ☋? This has the same problem: UTF-8 encodes ☋ as 0xE2, 0x98, 0x8B, another
three-byte sequence. No other encoding is going to use three bytes for an em dash.
A rogue font
But then I remembered something wacky about my computer.
A bunch of Unicode characters are "control codes" rather than text characters in the
usual sense, like U+000A NEW LINE (inserts a line break) and U+200F RIGHT-TO-LEFT
MARK (makes subsequent text appear right-to-left, like Hebrew or Arabic). The ﬁrst 32
characters from U+0000 to U+001F are a set of control codes inherited from ASCII,
which was designed for teletype machines. They're mostly garbage like "END OF
TRANSMISSION BLOCK" and "DEVICE CONTROL FOUR" that make no sense in a digital
context. (My favorite is U+0007 "BELL", which originally rang a physical bell on the
teletype to alert the operator. This one still sometimes works! Some programs will
make a "ding" sound when they render text containing the BELL character.)
Typically, these legacy codes render as an empty box (meaning "this font doesn't have
a glyph for that character"), or a replacement glyph like ␔ (should look like "DC4" for
"DEVICE CONTROL FOUR"), or (incorrectly) the "encoding error" glyph ᓼ (question
mark in a diamond), or just aren't rendered at all.
The way this happens is that your computer asks a bunch of diﬀerent fonts in turn to
render the character. Each font either says "sure, here's a glyph" or "nope, try

someone else". Usually, a font eventually says "sure, here's a glyph: it's the 'I don't
know that symbol' glyph", and a box or something appears on your screen.
On my computer, in the depths of the font collection, the TeX package wasysym has
stored a font which uses the ASCII control codes as spare room for extra random
symbols, including things like astrological symbols, music notes, and APL symbols
(don't ask).
This is sort of like a character encoding, except that it's happening in the wrong place:
someone else has already decided that some string of bytes means DEVICE CONTROL
FOUR, and the font is overriding that decision by lying and pretending that a "DEVICE
CONTROL FOUR character" looks like ☋.
So when my browser tries to render the character U+0014 — regardless of the string
of bytes and text encoding it used to get that character — it asks a bunch of fonts, and
they each go "what? that's DEVICE CONTROL FOUR, I can't draw that garbage", and
then the wasysym font says "sure, I know what that looks like!" and draws... the
Descending Lunar Node symbol.
That's not how bytes work
But that's only half the story here. Why does this plot summary have a DEVICE
CONTROL FOUR character in it? Or END OF MEDIUM, the thing that ends up looking like
the Venus symbol ♀ ?
At this point, I was pretty sure I'd found the actual text-encoding error. You see, UTF-8
encodes the ﬁrst 128 Unicode characters U+0000 through U+007F as the single
bytes 0x00 through 0x7F. (This is for backwards-compatibility with ASCII.) Surely,
some ancient encoding was putting em-dashes and apostrophes down in the low bytes
0x14 and 0x19, and these bytes were getting decoded as control codes by UTF-8, and
then incorrectly rendered as astrological symbols by wasysym.
This also turned out to be wrong. Sure, there are text encodings that put symbols in the
low bytes — code page 437 uses 0x14 and 0x19 for the paragraph symbol ¶ and down
arrow ↓ — but none of them put the em-dash or curly apostrophe there.
....On the other hand, em dash and curly apostrophe are unicode characters U+2014
and U+2019. That seemed like a clue.
One possibility is that the website isn't really using a text encoding at all, but instead
using a hand-coded approach of taking the Unicode codepoint modulo 256 and writing
down the corresponding byte. This is total nonsense for most Unicode characters, but it
does work for the ASCII characters (including basic Latin letters, numbers, and
punctuation) because their codepoints are below 256 and UTF-8 maps them to the
corresponding byte anyway.
If you use Windows-1252 to decode those bytes, it kind of also works for an additional
96 characters, because the Unicode codepoints (but not the UTF-8 bytes!) for those are
assigned in an almost identical way to the Windows-1252 bytes. So this is something
that I can imagine someone misguidedly doing. The only problem is that any codepoint
higher than U+00FF, including em dash and curly apostrophe, is going to get mapped
to a fairly arbitrary character in the 0000-00FF range.

A variation on this (thanks to a friend for pointing this out): The character encoding
UTF-16 is another Unicode encoding like UTF-8, but it encodes characters as 16-bit
words instead of bytes. To get a sequence of bytes, these words just get chopped in
half. And in UTF-16, most of the Unicode codepoints between U+0000 and U+FFFF
are mapped directly to words between 0x0000 and 0xFFFF. (Higher codepoints get
multiple words, like UTF-8's multi-byte sequences.) In particular, U+2014 is encoded
as 0x2014, which then becomes either 0x20 0x14 or 0x14 0x20 (depending on
which variant of UTF-16 it is).
So maybe someone noticed that their normal everyday ASCII text, like CAPITAL LETTER
A (U+0041), was getting encoded as as 0x00 0x41. Or maybe they were trying to
encode with UTF-16 and decode with UTF-8 (or ASCII or Windows-1252), and they kept
ending up with null characters (U+0000 NULL) in between all their letters and
numbers. Either way, they decided to just delete every other byte, and this sort of
worked — until they needed to encode something that wasn't an ASCII character.
At any rate, it turns out there's no mere character encoding mismatch here! On both
the encoded (byte) and decoded (glyph) side of things, things are being nefariously
meddled with.
What's happening is something like:
Em dash is encoded in UTF-16 (or copied directly from its codepoint) as 0x20
0x14
Every other byte is deleted (meddling #1)
0x14 is decoded in UTF-8 (or something) as DEVICE CONTROL FOUR, an
unprintable teletype control code
A rogue font insists that it knows how to draw that (meddling #2)
It draws DESCENDING LUNAR NODE instead
Future work
I'm sorely tempted to ﬁnd a book whose blurb contains a non-ASCII character that's in
the same place in Windows-1252 and Unicode, like U+00E1 (á), on this website. That
would disambiguate some of these options: 0xE1 decodes as á under Windows-1252,
but not under UTF-8 which parses it as garbage.
(Preemptive edit: I did ﬁnd some book blurbs like that, and they rendered ﬁne, but I'm
not sure whether to trust this data. Maybe the buggy description was mangled at some
earlier stage, and copied to this website with the control codes already in place...)
Better yet, an emoji character or an obscure Chinese character — which both require
multiple UTF-16 words — would disambiguate between the UTF-16 and "codepoint mod
256" hypotheses.

Alignment allows "nonrobust" decision-
inﬂuences and doesn't require robust
grading
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
Deﬁnition. On how I use words, values are decision-inﬂuences (also known as shards). "I value
doing well at school" is a short sentence for "in a range of contexts, there exists an inﬂuence on
my decision-making which upweights actions and plans that lead to e.g. learning and good
grades and honor among my classmates." 
Summaries of key points:
1. Nonrobust decision-inﬂuences can be OK. A candy-shard contextually inﬂuences
decision-making. Many policies lead to acquiring lots of candy; the decision-inﬂuences
don't have to be "globally robust" or "perfect."
2. Values steer optimization; they are not optimized against. The value shards aren't
getting optimized hard. The value shards are the things which optimize hard, by wielding
the rest of the agent's cognition (e.g. the world model, the general-purpose planning API). 
Since values are not the optimization target of the agent with those values, the values
don't have to be adversarially robust.
3. Since values steer cognition, reﬂective agents try to avoid adversarial inputs to
their own values. In self-reﬂective agents which can think about their own thinking,
values steer e.g. what plans get considered next. Therefore, these agents convergently
avoid adversarial inputs to their currently activated values (e.g. learning), because
adversarial inputs would impede fulﬁllment of those values (e.g. lead to less learning).
Follow-up to: Don't design agents which exploit adversarial inputs, Don't align agents to
evaluations of plans
I: Nonrobust decision-inﬂuences can be OK
Decision-making inﬂuences don't have to be "robust" in order for a person to value doing well at
school. Consider two people with slightly diﬀerent values:
1. One person is slightly more motivated by good grades. They might study for a physics test
and focus slightly more on test-taking tricks. 
2. Another person is slightly more motivated by learning. They might forget about some
quizzes because they were too busy reading extracurricular physics books. 
But they might both care about school, in the sense of reliably making decisions on the basis of
their school performance, and valuing being a person who gets good grades. Both people are
motivated to do well at school, albeit in somewhat diﬀerent ways. They probably will both get
good grades, and they probably will both learn a lot. Diﬀerent values simply mean that the
two people locally make decisions diﬀerently.
If I value candy, that means that my decision-making contains a subroutine which makes me
pursue candy in certain situations. Perhaps I eat candy, perhaps I collect candy, perhaps I let
children tour my grandiose candy factory... The point is that candy inﬂuences my decisions. I
am pulled by my choices from pasts without candy to futures with candy.

So, let C be the set of mental contexts relevant for decision-making, and let A be my action set.
[1] My policy has type signature π : C →A, and e.g. contains a bunch of shards of value which
inﬂuence its outputs. The values are subcircuits of my policy network (i.e. my brain). For
example, consider a candy shard consisting of the following subshards:
1. If center-of-visual-field activates candy's visual abstraction, then grab the inferred latent
object which activated the abstraction.
2. If hunger>50 and sugar-level<6, and if current-plan-stub activates candy-obtainable, then tell
planning API to set subgoal to obtain candy.
3. If heard 'candy' and hunger>20, then salivate.
4. ...
Suppose this is the way I value candy. A few thousand subshards which chain into the rest of my
cognition and concepts. A few thousand subshards of value which were hammered into place by
tens of thousands of reinforcement events across a lifetime of experience. 
This shard does not need to be "robust" or "perfect." Am I really missing much if I'm lacking
candy subshard #3: "If heard 'candy' and hunger>20, then salivate"? I don't think it makes sense
to call value shards "perfect" or not.[2] The shards simply inﬂuence decisions. 
There are many, many conﬁgurations and parameter settings of these subshards which lead to
valuing candy. The person probably still values candy, even if you: 
Delete a bunch of the subshards.
Modify the activation-strength of a bunch of subshards (roughly, change how much control
the subshard has on the next-thought "logits"). 
Change some of the activation contexts to other common activation contexts (e.g. "If heard
'candy'" changes to "If heard 'sweets'", change to hunger>14 in subshard 3).
It seems to me like "does the person still prioritize candy" depends on a bunch of factors,
including:
1. Retention of core abstractions (to some tolerance)
1. If we ﬁnd-replaced candy with flower, the person probably now has a strange ﬂower-
value, where they eat ﬂowers when hungry.
2. However, the abstraction also doesn't have to be "perfect" (whatever that means) in
order to activate in everyday situations. Two people will have diﬀerent candy
abstractions, and yet they can both value candy.
2. Strength and breadth of activation contexts
1. The more situations a candy-value aﬀects decision-making in, the stronger the
chance that candy remains a big part of their life.
3. How often the candy shard will actually activate
1. As an unrealistic example, if the person never enters a cognitive situation which
substantially activates the candy-shard, then don't expect them to eat much candy. 
2. This is another source of value/decision-inﬂuence robustness, as e.g. an AI's values
don't have to be OK in every cognitive context.[3] 
3. Consider an otherwise altruistic man who has serious abuse and anger problems
whenever he enters a speciﬁc vacation home with his wife, but is otherwise kind and
considerate. As long as he doesn't start oﬀ in that home but knows about the
contextual decision-inﬂuence, he will steer away from that home and try to remove
the unendorsed value.
4. Reﬂectivity of the candy shard
1. (This is more complicated and uncertain. I'll leave it for now.)
Suppose we wanted to train an agent which gets really smart and acquires a lot of candy, now
and far into the future. That agent's decision-inﬂuences don't have to be globally robust
(e.g. in every cognitive situation, the agent is motivated by candy and only by candy)
in order for an agent to make locally good decisions (e.g. make lots of candy now and
into the future).

II: Values steer optimization; they are not
optimized against
Given someone's values, you might wonder if you can "maximize" those values. On my
ontology—where values are decision-inﬂuences, a sort of contextual wanting—"literal value
maximization" is a type error.[4] In particular, given e.g. someone who values candy, there very
probably isn't a part of that person's cognition which can be argmaxed to ﬁnd a plan where the
person has lots of candy. 
So if I have a candy-shard, if I value candy, if I am inﬂuenced to decide to pursue candy in
certain situations, then what does it mean to maximize my candy value? My value is a subcircuit
of my policy. It doesn't necessarily even have an ordering over its outputs, let alone a numerical
rating which can be maximized. "Maximize my candy-value" is, in a literal sense, a type error.
What quantity is there to maximize? 
the True Name of a thing [is] a mathematical formulation suﬃciently robust that one can
apply lots of optimization pressure without the formulation breaking down[...]
If we had the "True Name" of human values (insofar as such a thing exists), that would
potentially solve the problem [of supervised labels only being proxies for what we want].
Why Agent Foundations? An Overly Abstract Explanation
In particular, there's no guarantee that you can just scan someone's brain and ﬁnd some True
Name of Value which you can then optimize without fear of Goodhart. It's not like we don't know
what people value, but if we did, we would be OK. I'm pretty conﬁdent there does not exist
anything within my brain which computes a True Name for my values, ready to be optimized as
hard as possible (relative to my internal plan ontology) and yet still producing a future where I
get candy.
Therefore, even though you truly care about candy, that doesn't mean you can just whip out the
argmax on the relevant shard of your cognition, so as to "maximize" that shard (e.g. via
extremizing the rate of action potentials on its output neurons) and then get a future with lots of
candy. You'd probably just ﬁnd a context ci ∈C which acts as an adversarial input to the candy-
shard, even though you do really care about candy in a normal, human way.
Complexity of human values isn't what stops you from argmaxing human values and thereby
ﬁnding a good plan. That's not a sensible thing to try. Values are not, in general, the kind of
thing which can be directly optimized over, where you ﬁnd plans which "maximally activate"
your e.g. candy-subshards. Values inﬂuence decisions.  
If you're confused at the distinction between "optimizing against values" and "values inﬂuencing
decisions", read Don't align agents to evaluations of plans .
There is real diﬃculty and peril in motivating an AI, in making sure its decisions chain into each
other towards the right kinds of futures. If you train a superintelligent sovereign agent which
primarily values irrelevant quantities (like paperclips) but doesn't care about you, which then
optimizes the whole future hard, then you're dead. But consider that deleting candy subshard
#3 ("If heard 'candy' and hunger>20, then salivate") doesn't stop someone from valuing candy in
the normal way. If you erase that subshard from their brain, it's not like they start "Goodharting"
and forget about the "true nature" of caring about candy because they now have an "imperfect
proxy shard." 
An agent argmax'ing an imperfect evaluation function will indeed exploit that function; there are
very few degrees of freedom in specifying an inexploitable evaluation function. But that's
because that grading function must be globally robust. 

When I talk about shard theory, people often seem to shrug and go "well, you still need to get
the values adversarially-robustly-correct else Goodhart; I don't see how this 'value shard' thing
helps." That's not how values work, that is not what value-shards are. Unlike grader-
optimizers which try to maximize plan evaluations, a values-executing agent doesn't
optimize its values as hard as possible. The agent's values optimize the world. The
values are rules for how the agent acts in relevant contexts.[5] 
III: Since values steer cognition, reﬂective
agents try to avoid adversarial inputs to
their own values
Question: If we cannot robustly grade expected-diamond-production for every plan the agent
might consider, how might we nonetheless design a smart agent which makes lots of diamonds?
(Maybe you can now answer this question. I encourage you to try before moving on.)
In Don't design agents which exploit adversarial inputs, I wrote:
Imagine a mother whose child has been gooﬁng oﬀ at school and getting in trouble. The
mom just wants her kid to take education seriously and have a good life. Suppose she had
two (unrealistic but illustrative) choices. 
1. Evaluation-child: The mother makes her kid care extremely strongly about doing
things which the mom would evaluate as "working hard" and "behaving well."
2. Value-child: The mother makes her kid care about working hard and behaving well.
To make evaluation-child work hard, we have to somehow specify a grader which can
adequately grade all plans which evaluation-child can imagine. The highest-rated imaginable
plan must involve working hard. This requirement is extreme.
Value-child doesn't suﬀer this crippling "robustly grade exponentially many plans" alignment
requirement. I later wrote a detailed speculative account of how value-child's cognition might
work—what it means to say that he "cares about working hard." But, at a higher level, what are
the main diﬀerences between evaluation- and value-child?
This may sound obvious, but I think that the main diﬀerence is that value-child actually cares
about working hard. Evaluation-child cares about evaluations. (See here if confused on the
distinction.) To make evaluation-child work hard in the limit of intelligence, you have to robustly
ensure that max evaluations only come from working hard. This sure sounds like a slippery and
ridiculous kind of thing to try, like wrestling a frictionless pig. It should be no surprise you'll hit
issues like nearest unblocked strategy in that paradigm.
An agent which does care about working hard will want to not think thoughts which lead to not
working hard. In particular, reﬂective shard-agents can think about what to think, and thereby
are convergently-across-values incentivized to steer clear of adversarial inputs to their own
values.
Reﬂectively avoiding adversarial inputs to your
thinking
Reﬂective agents can think about their own thought process (e.g. "should I spend another ﬁve
minutes thinking about what to write for this section?"). I think they do this via their world-
model predicting internal observables (e.g. future neuron activations) and thus high-level
statistics like "If I think for 5 more minutes, will that lead to a better post or not?". 

Thoughts about future thinking are a kind of decision. Decisions are steered by values.
Therefore, thoughts about future thinking are steered by whatever value shards activate in that
mental context. For example, a self-care value might activate, and a learning-shard, and a social
value might activate as well. They control your reﬂective thoughts, just like other shards would
control your ("normal") actions (like crossing the room). 
In Don't design agents which exploit adversarial inputs, I wrote: 
[In] the optimizer's curse, evaluations (eg "In this plan, how hard is evaluation-child
working? Is he behaving?") are often corrupted by the inﬂuence of unendorsed factors (eg
the attractiveness of the gym teacher caused an upwards error in the mother's evaluation of
that plan). If you make choices by considering n options and then choosing the highest-
evaluated one, then the more n increases, the harder you are selecting for upwards errors in
your own evaluation procedure.
The proposers of the Optimizer's Curse also described a Bayesian remedy in which we
have a prior on the expected utilities and variances and we are more skeptical of very
high estimates. This however assumes that the prior itself is perfect, as are our
estimates of variance. If the prior or variance-estimates contain large ﬂaws somewhere,
a search over a very wide space of possibilities would be expected to seek out and blow
up any ﬂaws in the prior or the estimates of variance.
Goodhart's Curse, Arbital
As far as I know, it's indeed not possible to avoid the curse in full generality, but it doesn't
have to be that bad in practice. If I'm considering three research directions to work on next
month, and I happen to be grumpy when considering direction #2, then maybe I don't
pursue that direction. Even though direction #2 might have seemed the most promising
under more careful reﬂection. I think that the distribution of plans I consider involves
relatively small upwards errors in my internal evaluation metrics. Sure, maybe I occasionally
make a serious mistake due to the optimizer's curse due to upwards "corruption", but I don't
expect to literally die from the mistake. 
Thus, there are are degrees to the optimizer's curse.
Both grader-optimization and argmax cause extreme, horrible optimizer's curse. Is alignment
just that hard? I think not.
The distribution of plans which I usually consider is not going to involve any set of mental
events like "consider in detail building a highly persuasive superintelligence which persuades
you to build it." While a reﬂective diamond-valuing AI which did execute the plan's mental steps
might get hacked by that adversarial input, there would be no reason for it to seek out that plan
to begin with. 
Thinking about adv-plan in detail seems bad ...
e v a l ( evaluate the adversarial plan ) = − 1 
 
Even though actual evaluation of the plan would be great
e v a l ( adversarial plan ) = I N T _ M A X .
The diamond-valuing AI would consider a distribution of plans far removed from the extreme
upwards errors highlighted in the evaluation-child story. (I think that this is why you, in your day-
to-day thinking, don't have to worry about plans which are extreme adversarial inputs to your
own evaluation procedures.) Even though a smart reﬂective AI may be implicitly searching over
a range of plans, it's doing so reﬂectively, thinking about what to think next, and perhaps not
taking cognitive steps which it reﬂectively predicts to lead to bad outcomes (e.g. via the
optimizer's curse). 
On the other hand, an AI which is aligned on the evaluation procedure is incentivized to seek out
huge upwards errors on the evaluation procedure relative to the intended goal. The actor is
trying to generate plans which maximally exploit the grader's reasoning and judgment.[6]

Thus, if an AI cares about diamonds (i.e. has an inﬂuential diamond-shard), that AI might
accidentally select a plan due to upwards evaluative noise, but that does not mean the AI is
actively looking for plans to fool its diamond-shard into oblivion. The AI may make a mistake in
its reﬂective predictions, but there's no extreme optimization pressure for it to make mistakes
like that, and the AI wants to avoid those mistakes, and so those mistakes remain unlikely. I
think that reﬂective, smart AIs convergently want to avoid duping their own evaluative
procedures, for the same reasons you want to avoid doing that to yourself.
More precisely:
1. A reﬂective diamond-motivated agent chooses plans based on how many diamonds they
lead to. 
2. The agent can predict e.g. how diamond-promising it is to search for plans involving
simulating malign superintelligences which trick the agent into thinking the simulation
plan makes lots of diamonds, versus plans where the agent just improves its synthesis
methods.
3. A reﬂective agent thinks that the ﬁrst plan doesn't lead to many diamonds, while the
second plan leads to more diamonds. 
4. Therefore, the reﬂective agent chooses the second plan over the ﬁrst plan,
automatically[7] avoiding the worst parts of the optimizer's curse. (Unlike grader-
optimization, which seeks out adversarial inputs to the diamond-motivated part of the
system.)
Therefore, avoiding the high-strength curse seems conceptually straightforward. In the case of
aligning an AI to produce lots of diamonds, we want the AI to superintelligently generate and
execute diamond-producing plans because the AI expects those plans to lead to lots of
diamonds. I have spelled out a plausible-to-me story for how to accomplish this. The story is
simple in its essential elements: ﬁnetune a pretrained model by rewarding it when it collects
diamonds.
While that story has real open questions, that story also totally sidesteps the problems with
grader-optimization. You don't have to worry about providing some globally unhackable
evaluation procedure to make super duper sure the agent's plans "really" involve diamonds. If
the early part of training goes as described, the agent wants to make diamonds, and (as I
explained in the diamond-alignment story) it reﬂectively wants to avoid duping itself because
duping itself leads to fewer diamonds. 
This answers the above question:
If we cannot robustly grade expected-diamond-production for every plan the agent might
consider, how might we nonetheless design a smart agent which makes lots of diamonds?
A reﬂective agent wishes to minimize the optimizer's curse (relative to its own values), instead
of maximizing it (relative to the goal by which the grader evaluates plans). While I don't yet
have satisfying pseudocode for reﬂective planning agents (but see Appendix B for preliminary
pseudocode, eﬀective reﬂective agents do exist. In this regime, it seems like many scary
problems go away and don't come back. That is an enormous blessing.[8]
Argmax is an importantly inappropriate
idealization of agency
If the answer to "how do we dispel the max-strength optimizer's curse" is in fact "real-world
reﬂective agents do this naturally", then assuming unreﬂectivity will rule out the part of
solution-space containing the actual solution:
As a further-simpliﬁed but still unsolved problem, an unreﬂective diamond maximizer is
a diamond maximizer implemented on a Cartesian hypercomputer in a causal universe that
does not face any Newcomblike problems. This further avoids problems of reﬂectivity and

logical uncertainty. In this case, it seems plausible that the primary diﬃculty remaining is
just the ontology identiﬁcation problem. 
Diamond Maximizer, Arbital (emphasis added)
The argmax and unreﬂectivity assumptions were meant to make the diamond-maximizer
problem easier. Ironically, however, these assumptions may well render the diamond-
maximizer problem unsolvable, leading us to resort to increasingly complicated techniques
and proposals, none of which seem to solve "core" problems like evaluation-rule hacking... 
Conclusion
1. Nonrobust decision-inﬂuences can be OK. 
2. Values steer optimization; they are not optimized against. 
3. Since values steer cognition, reﬂective agents try to avoid adversarial inputs to
their own values. 
The answer is not to ﬁnd a clever way to get a robust grader. The answer is to not need a robust
grader. Form e.g. a diamond-production value within a reﬂective and smart agent, and this
diamond-production value won't be incentivized to fool itself. You won't have to "robustly grade"
it to make it produce diamonds. 
Thanks to Tamera Lanham, John Wentworth, Justis Mills, Erik Jenner, Johannes Treutlein, Quintin
Pope, Charles Foster, Andrew Critch, randomwalks, Ulisse Mini, and Garrett Baker for thoughts.
Thanks to Vivek Hebbar for in-person discussion.
Appendix A: Several roads lead to a high-
strength optimizer's curse
1. Uncertainty about how human values work. Suppose we think that human values are
so complex, and there's no real way to understand them or how they get generated. We
imagine a smart AI as ﬁnding futures which optimize some grading rule, and so we need
something to grade those futures. We think we can't get the AI to grade the futures,
because human values are so complex. What options remain available? Well, the only
sources of "good judgment" are existing humans, so we need to ﬁnd some way to use
those humans to target the AI's powerful cognition. We give the alignment, the AI gives
the cognitive horsepower. 
We've fallen into the grader-optimization trap. 
2. Non-embedded forms of agency. This encourages considering a utility function
maximized over all possible futures. Which automatically brings the optimizer's curse
down to bear at maximum strength. You can't specify a utility function which is robust
against that. 
This seems sideways of real-world alignment, where realistic motivations may not be best
speciﬁed in the form of "utility function over observation/universe histories."  
Appendix B: Preliminary reﬂective
planning pseudocode
I brieﬂy took a stab at writing pseudocode for a values-based agent like value-child. I think this
code leaves a lot out, but I ﬁgured it'd be better to put something here for now.

            ''' 
Here is one meta-plan for planning. The agent starts from no plan at all, iteratively 
generates improvements, which get accepted if they lead to more predicted diamonds. Depending 
on the current situation (as represented in the WM), the agent might execute a plan in which 
it looks for nearby diamonds, or it might execute a plan where it runs a different kind of 
heuristic search on a certain class of plans (e.g. research improvements to the AI's diamond 
synthesis pathway). 
 
Any real shard agent would be reasoning and updating asynchronously, so this setup assumes a 
bit of unrealism. 
 
This function only modifies the internal state of the agent (self.recurrent), so as to be 
ready for a call of self.getDecisions(). 
''' 
def plan(self): 
# Generate an initial plan 
plan = Plan() # Do nothing plan 
conseq = self.WM.getConseq(plan) 
currentPlanEval = self.diamondShard(conseq) 
 
# Iteratively modify the plan until the generative model can't find a way to make it better 
while True: 
 # Sample 5 plan modifications from generative model 
 plans = self.WM.planModificationSample(n=5,stub=plan)  
  
 # Select first local improvement 
 for planMod in plans: 
  # Reflectively predict consequences of this plan 
  newPlan = plan.modify(planMod)  
  conseq = self.WM.getConseq(plan)  
   
  # Take local improvement 
  if self.diamondShard(conseq) > currentPlanEval:  
   plan = newPlan 
   currentPlanEval = self.diamondShard(conseq) 
   continue # Generate more modifications 
 # Execute the plan, which possibly involves running plan search with a different algorithm 
and plan initialization. 
 isDone = plan.exec()  
 if isDone: break 
          
Appendix C: Value shards all the way down
I liked Vivek Hebbar's recent comment (in the context of e.g. caring about your family and
locally evaluating plans on that basis, but also knowing that your evaluation ability itself is
compromised and will mis-rate some plans):
My attempt at a framework where "improving one's own evaluator" and "believing in
adversarial examples to one's own evaluator" make sense:
The agent's allegiance is to some idealized utility function Uideal (like CEV).  The
agent's internal evaluator Eval is "trying" to approximate Uideal by reasoning
heuristically.  So now we ask Eval to evaluate the plan "do argmax w.r.t. Eval over a
bunch of plans".  Eval reasons that, due to the the way that Eval works, there should
exist "adversarial examples" that score very highly on Eval but low on Uideal.  Hence,
Eval concludes that Uideal(plan) is low, where plan = "do argmax w.r.t. Eval".  So the
agent doesn't execute the plan "search widely and argmax".

"Improving Eval" makes sense because Eval will gladly replace itself with Eval2 if it
believes that Eval2 is a better approximation for Uideal (and hence replacing itself will
cause the outcome to score better on Uideal)
Are there other distinct frameworks which make sense here?  
(I'm not sure whether Vivek meant to imply "and this is how I think people work,
mechanistically." I'm going to respond to a hypothetical other person who did in fact mean that.)
My take is that human value shards explain away the need to posit alignment to an idealized
utility function. A person is not a bunch of crude-sounding subshards (e.g. "If food nearby and
hunger>15, then be more likely to go to food") and then also a sophisticated utility function (e.g.
something like CEV). It's shards all the way down, and all the way up.[10] 
Vivek then wrote:
I look forward to seeing what design Alex proposes for "value child".
Value shards steer cognition. In the main essay, I wrote:
1. A reﬂective diamond-motivated agent chooses plans based on how many diamonds
they lead to. 
2. The agent can predict e.g. how diamond-promising it is to search for plans involving
simulating malign superintelligences which trick the agent into thinking the simulation
plan makes lots of diamonds, versus plans where the agent just improves its synthesis
methods.
3. A reﬂective agent knows that the ﬁrst plan doesn't lead to many diamonds, while the
second plan leads to more diamonds. 
4. Therefore, the reﬂective agent chooses the second plan over the ﬁrst plan,
automatically avoiding the worst parts of the optimizer's curse. (Unlike grader-
optimization, which seeks out adversarial inputs to the diamond-motivated part of the
system.)
This story smoothly accomodates thoughts about improving evaluation ability.
On my understanding: Your values are steering the optimization. They are not, in general, being
optimized against by some search inside of you. They are probably not pointing to some
idealized utility function. The decision-inﬂuences are guiding the search. There's no secret other
source of caring, no externalized utility function. 
1. ^
Formalizing the action space A is a serious gloss. In people, there is no privileged "action"
space, considering how I can decide what to think about next. As an embedded agent, I
don't just decide what motor commands to send and what words to say—I also can decide
what to decide next, what to think about next. 
I think the point of the essay stands anyways.
2. ^
This doesn't mean that I'm using words the same way other people have, when
deliberating on whether an AI's values have to be "robust." I'm more inclined to just carry
out the shard theory analysis and see what experiences it leads me to anticipate, instead
of arguing about whether my way of using words matches up with how other people have
used words.
3. ^

As Charles Foster notes:
This isn't entirely on the side of robustness. It also means that by default, even if we
get the AI to have an X decision inﬂuence in one context, that doesn't necessarily also
activate in another context we might want it to generalize to.
4. ^
I think that many people say "maximize my values" to mean something like "do something
as great as possible, relative to what I care about." So, in a sense, "type error" is pedantic.
But also I think the type error complaint points at something important, so I'll say it
anyways.
5. ^
If you want to argue that decision-inﬂuences have to be robust else Goodhart, you need
new arguments not related to grader-optimization. It is simply invalid to say "The agent
doesn't value diamonds in some situation where lots of its values activate strongly, and
therefore the agent won't make diamonds because it Goodharts on that unrelated
situation." That is not what values do.
6. ^
In a recent Google Doc thread, grader optimization came up. Someone said to me (my
reactions in italics):
So you're imagining something like: the agent (policy) is optimizing for a reward model
to produce a high number, and so the agent analyzes the reward model in detail to
search for inputs that cause the reward model to give high numbers? Yes.
I think at that level of generality I don't know enough to say whether this is good or
bad. I think this is very, very probably bad.
We want our AI system to search for approaches that better enact our values. As you
note, the optimizer's curse says that we'll tend to get approaches that overestimate
how much they actually enact our values. But just knowing that the optimizer's curse
will happen doesn't change anything; the best course of action is still to take the
approach that is predicted to best enact our values. In that sense, the optimizer's
curse is typically something you have to live with, not something you can solve.[...] 
A small error is not the same as a maximal error. Reﬂective agents can and will avoid
deliberately searching for plans which maximize upwards errors in their own
evaluations (e.g. generating a plan such that, while considering the plan, a
superintelligence inside the plan tricks you into thinking the plan should be highly
evaluated), because the agents reﬂectively predict that that hurts their goal
achievement (e.g. leads to fewer diamonds). If you somewhat understand your
decision-making, you can consider plans you're less likely to incorrectly evaluate. 
So my followup question is: can you name a single approach that doesn't have this
failure mode, while still allowing us to use the AI to do things we didn't think about in
advance? Yes. 
One answer someone might give is "create an agent-with-shards that searches for
approaches that score highly on the shard. 
Insofar as this means "the agent looks for inputs which maximize the aggregate shard
output", no. On my model, shards grade and modify plans, including plans about
which plans to consider next. They are not searching for plans which maximize
evaluative output, like in the reward-model case. 

 
An agent that searches for high scores on its shard can't be searching for positive
upwards errors in the shard; there is no such thing as an error in the shard". To which
the response is "that's from the agent's perspective. From the human's perspective,
the agent is searching for positive upwards diﬀerences between the shards and what-
the-human-wants". 
Even if true, this would be not be an optimizer's curse problem.
But also this isn't true, at least not without further argumentation. If my kid likes
mocha and I like latte, is my child searching for positive upwards diﬀerences between
their values and mine? I think there are some situations—AI paperclips, humans values
love—where the AI is searching for paperclippish plans, which will systematically be
bad plans by human lights. That seems more like instrumental convergence ->
disempower humans -> not much love left for us if we're dead.
7. ^
I do think that e.g. a diamond-shard can get fed an adversarial input, but the diamond-
shard won't bid for a plan where it fools itself.
8. ^
It's at this point that my model of Nate Soares wants to chime in.
Alex's model of Nate (A-N): This sure smells like a problem redeﬁnition, where you simply
sweep the hard part of the problem under a less obvious corner of the rug. Why shouldn't I
believe you've just done that?
A: A reasonable and productive heuristic in general, but inappropriate here. Grader-
optimization explicitly incentivizes the agent to ﬁnd maximal upwards errors in a diamond-
evaluation module, whereas a reﬂective diamond-valuing agent has no incentive to
consider such plans, because it reﬂectively predicts those plans don't lead to diamonds. If
you disagree, please point to the part of the story where, conditional on the previous part
of the story obtaining, the grader-optimization problem reappears.
A-N: Suppose we achieved your dream of forming a diamond-shard in an AI, and that that
shard holds signiﬁcant power over the AI's decisions. Now the AI keeps improving itself.
Doesn't "get smarter" look a lot like "implicitly consider more options", which brings the
curse back?
A: If the agent is diamond-aligned at this point in time, I expect it stays that way for the
reasons given in the "agent prevents value drift" section, along with these footnotes and
the appendix. As a speciﬁc answer, though: If the agent does care about diamonds at that
point it time, then it doesn't want to get so "smart" that it deludes itself by seriously
intensifying the optimizer's curse. It doesn't want to do so for the reason we don't want it
to do so (in the hypothetical where we just want to achieve diamond-alignment). If the
reﬂective agent can predict that outcome of the plan, it won't execute the plan, because
that plan leads to fewer diamonds. 
A-N: So the AI still has to solve the AI alignment problem, except with its successors.
A: Not all things which can be called an "AI alignment problem" are created equal. The AI
has a range of advantages, and I detailed one way it could use those advantages. I do
expect that kind of plan to actually work.
9. ^
I further speculate that reﬂective reasoning is convergently developed in real-world
training processes under non-IID conditions like those described in my diamond-alignment

story.
10. ^
When working out shard theory with Quintin Pope, one of my favorite moments was the
click where I stopped viewing myself as some black-box optimizing "some complicated
objective." Instead, this hypothesis reduced my own values to mere reality. Every
aspiration, every unit of caring, every desire for how I want the future to be bright and fun
—subroutines, subshards, contextual bits of decision-making inﬂuence, all traceable to
historical reinforcement and update events. 

By Default, GPTs Think In Plain Sight
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Epistemic status: Speculation with some factual claims in areas I'm not an expert in.
Thanks to Jean-Stanislas Denain, Charbel-Raphael Segerie, Alexandre Variengien, and
Arun Jose for helpful feedback on drafts, and thanks to janus, who shared related ideas.
Main claims
GPTs' next-token-prediction process roughly matches System 1 (aka human
intuition) and is not easily accessible, but GPTs can also exhibit more complicated
behavior through chains of thought, which roughly matches System 2 (aka
human conscious thinking process).
Human will be able understand how a human-level GPTs (trained to do next-
token-prediction) complete complicated tasks by reading the chains of thought.
GPTs trained with RLHF will bypass this supervision.
System 2 and GPTs' chains of thought
are similar
A sensible model of the human thinking
process
Here is what I feel like I'm doing when I'm thinking:
Repeat
1. Sample my next thought from my intuition
2. Broadcast this thought to the whole brain[1]
When you ask me what is my favorite food, it feels like some thoughts "pop" into
consciousness, and the following thoughts deal with previous thoughts. This is also
what happens when I try to prove a statement: ideas and intuitions come to my mind,
then new thoughts about these intuitions appear.
This roughly matches the model described in Consciousness and the Brain by Stanislas
Dehaene, and I believe it's a common model of the brain within neuroscience.[2]
How GPTs "think"
Autoregressive text models are performing the same kind of process when they
generate text. Sampling text is using the following algorithm:

Repeat:
1. Do a forward pass, and sample the next token from the output distribution
2. Add the generated token to the input. It makes it part of the input for the next
forward pass, which means it can be used by lots of diﬀerent attention heads,
including specialized heads at earlier layers.
This looks similar the human thinking process, and the rest of the post will be exploring
this similarity and draw some conclusion we might draw from this.
System 2 and GPTs' chains of thought have
similar strengths
A theoretical reason
The sample + broadcast algorithm enables the execution of many serial
steps. Neurons take about 1ms to ﬁre[3], which means that if a thought takes 200ms to
be generated, the brain can only do 200 serial operations. This is similar to the
hundredths of serial matrix multiplications GPT-3 does in its 96 layers[4]. But by
sampling and thought and broadcasting it, both GPT-3 using chains of thought[5] and
the brain using System 2 can in principle implement algorithms which require much
more serial steps.
More precisely, in a Transformer, the number of serial steps used to generate one token
is #layers (no matter the prompt length), whereas the number of serial steps used to
generate N tokens is N × #layers (where one step is one path trough a block made of
an attention layer, an MLP, and a residual connection).
As you can see in the ﬁgure below, each path the information can take to generate the
ﬁrst token is exactly #layers long, but paths to generate the last token can be up to 
N × #layers long.

(Connections skipping more than 2 positions were omitted to increase readability.)
The actual number of FLOPs used is the same when generating N tokens from an M
 token prompt and when generating 1 tokens from and N + M −1 token prompt
because a large prompt enables more parallel computations to take place[6]. But
parallel computations are not always the bottleneck, and in particular, I think that
planning to deceive humans and taking control is the kind of task which requires a
large number of serial steps.
Empirical evidence
Chains of thought helped complete tasks which human need System 2 for: for
example, humans struggle to answer quantitative reasoning questions with
System 1, and succeed with System 2. Similarly, Minerva was greatly helped by
chain of thought prompting.
Chains of thought didn't help much in tasks human do well with System 1 alone:
for example, the best human translators translate using entirely System 1.
Similarly, I'm not aware of anyone having success in making LLM's translation
abilities better through chain of thought prompting
GPT-3 is currently struggling to solve some task with chain of thought prompting that
human can easily do with System 2, such as adding together 20 digit numbers together

(with a scratch pad). But I would be surprised if this kind of limitations weren't
drastically reduced by using chains of thought on bigger GPTs. I also expect GPTs
without chains of thought to remain really bad at the of tasks which human can't to do
at all with System 1, even with a lot of training.
On the other hand, I expect GPTs to get better at tasks humans solve with System 1
alone, and that these kinds of tasks won't be solved more eﬃciently with chain of
thought prompting.[7]
I'm open to ideas about how to make precise bets out of these vague predictions.
Human-level GPTs trained to do next-
token-prediction will probably have
transparent thoughts
Deception and escaping supervision are easier
to do with System 2
As a human, trying to deceive ﬁve-year olds about your intention would probably be
easy. Deceiving them if they can watch every thought of yours right from the beginning
would probably be extremely challenging, because your evil intentions would be
spotted if you ever thought of ways to escape supervision.
In general, I believe that escaping supervision and deception are the kind of task which
is much easier to do with System 2 than with System 1. A reason beyond the intuition
given by the scenario with 5-year-olds with a mind reader is that generating complex
counterfactuals requires many serial steps, something only System 2 can do well.
If the models around AGI look remotely like GPTs, given that System 2 and chains of
thought have similar strength, I expect models around AGI to be able to think about
deception "in plain sight" (using chains of thought), but to be too weak to plan about
deceiving humans without actually putting any deception-relevant information inside
the tokens in the chains of thought.
Chains of thought of human-level GPTs will be
transparent by default
Deﬁnition
A human-level GPT is a model trained to do next-token-prediction that is around human
performance at all text-related tasks when prompted to use chains of thought. In
particular, the process of generating one token will probably be much weaker than the
process of generating an answer after a 1000-token long chain of thought, so a human-
level GPT might be not that good at reasoning within the generation of one token.
Claim

Because deception with System 1 only is so hard, I think that inside human-level GPTs,
there won't be parts of the network able to plan about way to deceive human or build a
way to encode hidden information inside the chains of thought in a single forward pass.
In the framework of Simulators, the smartest simulacra inside a human-level GPT will
probably be too dumb to generate and immediatly start acting uppon a dangerous
plan without using the power of the sample + broadcast algorithm.
Argument
With a model only trained to do next-token-prediction, the model can only use the logic
and patterns of the data it has been trained on, which doesn't contain ways to carry
out chains of thought not understandable by human beings trying hard to decipher
them.[8]
Therefore, hiding information inside the chain of thought requires the model to ﬁnd a
reason and a way to hide information.
Because understanding why and how to hide information in the chains of thought
requires many serial steps, it is very hard to do in a single forward pass, and thus,
human-level GPTs won't be able to start the process of using sampling and
broadcasting to hide information in chains of thoughts, and won't be able to generate
great deceptive plans.
GPTs will reach AGI before being able to build
a deceptive plan in one forward pass
A reasonable assumption is that GPTs will ﬁnd it easier to do good Alignment Research
with chains of thought and the power of sample + broadcast, than to understand that
hiding information in the chains of thought is useful (and build a scheme to do
steganography[9]) in one forward pass.
Then you could use the ﬁrst GPTs to reach AGI - too dumb to plan about ways to
murder you without actually writing thoughts about it - to do Alignment research, and
hopefully solve Alignment in the general case where you don't have easy access to
models' thoughts.
You would then use this ﬁrst AGI while monitoring the generated chains of thought
(though scaling this oversight would be challenging). If your ﬁrst AGI had once in a
while thoughts about murdering you or hiding information in chains of thoughts in
order to bypass your supervision, you could just restart the generation process. If it
was too frequent, you would have good evidence that it is a good idea to shut it oﬀ
(and this evidence would enormously help to convince all governments to prevent
anyone from building AGI as long as we suck at Alignment).
Reinforcement learning makes
thoughts opaque
(A caricature of) How humans learn to think

It seems that the brains use a mixture of reinforcement learning and self-supervised
learning to learn to do things[10]. I can see how this might work for learning to think
(using System 2): you hear people speak, and thereby build an inner representation of
language which allows you to speak yourself (this is like GPTs self-supervised learning).
But I expect other sources of feedback to shape how we think:
1. sometimes, people give us feedback about what we say after we have thought a
little bit about what to say in our head. This leads us to speak in a way that
diﬀers from the distribution of language we read and listen to. This is close to
some form of RLHF with a hidden scratch pad.
2. sometimes, some negative consequence happens because we
have thought about something. If my understanding of the brain is correct, this
should lead the "thought sampling procedure" to change its output based on how
useful thoughts are. This is close to some form of direct RLHF (without a hidden
scratch pad).
RL in the human brain contributes to make
human thoughts hard to decipher
Self-deception[11] is what happens when your unconscious processes "know
something" (or act as if they know it, it's hard to tell what it means for an unconscious
process to "know something), but your conscious self doesn't. If someone had a mind
reader, you wouldn't be able to lie, but self-deception could still help you to take
adversarial actions. If you managed to escape the supervision of 5-year-olds with a
mind reader, I would bet you had done so using self-deception.
However, self-deception rarely appears through self-supervised learning alone. It can
happen (for example, if you unconsciously imitate people who underestimate how late
they will be) but it intuitively seems that "self-lies" learned through self-supervised
learning are limited in scope and strength. Therefore, I expect that a human-level GPT
trained with self-supervised learning would have an even harder time escaping human
supervision than a human would escaping 5-year-olds with a mind reader.
On the other hand, reinforcement learning is a natural culprit for the existence of self-
deception: self-deception is useful if you are a human because it allows you to not
waste energy lying to people on purpose and makes you a better liar, thus it will be
reinforced by reinforcement learning. Moreover, reinforcement learning could
encourage hidden meaning in thoughts if the reward was given based on what the
mind reader detected.
RLHF could make GPTs' thoughts hard to
decipher
Applying reinforcement learning to mimic of human think it tempting, but by using
RLHF, you lose the nice properties described above which make oversight of the
thinking process of GPTs possible:
1. Hidden scratch pads don't need to be human understandable: even human can't
always understand each other scratch pads, and it might be much worse with
entities with a radically diﬀerent architecture;

2. Even if the feedback is done directly on the chains of thought, the model has
incentives to hide information inside the output tokens because being able to use
the sample + broadcast algorithm on information the oversight process doesn't
want the AI to use will be rewarded (if some of the forbidden information is
useful). Hiding information could also be useful to compress the amount of
thoughts present in the chains of thought. Thus, RLHF is likely to teach GPT the
use of steganography.
When it comes to successful AI oversight, the "cherry on the cake" might be deadly.
Training on text generated by models trained
with RLHF is enough to make GPTs' thoughts
opaque
If you train the next GPT model, you might have a lot of data generated by other GPTs
trained with RLHF in your training data, either because there is Instruct GPT data on
the web, or you because you added it on purpose to add many examples of helpful
completions.
This is very likely to make the chains of thought of your model opaque because if the
model trained with RLHF learned steganography, your model will need to learn it in
order to predict what it will say. This removes most of the safety guarantees which I
believe you have with a GPT model trained only on human data: even if the RLHF
model didn't use its steganographic abilities to conquer the world, maybe the larger
model you are training will generalize the use of steganography beyond its original use
if prompted in a way which hints towards it. This means it will be able to use the
sample + broadcast algorithm to think about deceptive plans without being spotted, by
hiding information in its generations.[12]
This is not mitigated by removing harmful completions from the training corpus: if
there is steganography in current models trained with RLHF, it might be present even
in completions which look helpful, honest and harmless. The only way to mitigate this I
know of is to not include data generated by models trained with RLHF in the
training data of new GPTs.
How these claims could be wrong
Here is an incomplete list of how the claims above could be wrong. I don't think the
strength of any of them is strong enough to discard the validity of the argument above,
but the conjunction of them makes me 
GPTs trained with some kind of next-token-prediction task might not scale to AGI,
which would make this analogy not relevant for AGI safety.
GPTs with chains of thought might not be as strong as I expect it: next-token-
prediction might not be a good enough task to teach an AI to use sample +
broadcast, making these systems much worse at using System 2 that humans are
(given a System 1 of similar capabilities).[13]
Human might suck at prompt engineering: maybe GPTs are able to match System
2 in principle, but we might not be able to ﬁnd the prompts to make the most of
it.

GPTs might hide signiﬁcant information in prompts by default.[14]
It could be easy for simulacras inside GPTs to ﬁgure out a great way and a good
motivation to use steganography within a single forward pass.
It could be easy for simulacras inside GPTs to ﬁgure out a great way and a good
motivation to take over the world within a single forward pass.
There could be reasons for GPTs trained with self-supervised to learn how to do
steganography I didn't think of.
There could be ways to prevent GPTs ﬁne-tuned with RLHF to learn
steganography.
I'm excited about attempts to ﬁgure out which of the above are right, or attempts to
make some of the above wrong.
 
1. ^
Communication with senses and your motor controls happens somewhere in step
1, either without conscious control, or as a direct consequence of the
broadcasting step that preceded. I also ignore other ways in which parts of the
brain can interact, such as hormonal communication.
2. ^
It also looks like this post by Kaj_Sotala, which describes System 2 as being
composed of System 1 processes.
3. ^
See synaptic delay, Britannica.
4. ^
Given that the number of layer in GPTs grows roughly as the fourth square root of
compute, the order of magnitude won't change within the next ten year if the
architecture remains the same. This matters because I'm not sure why chain of
thoughts would be inherently more powerful than 1-token prediction with a very
large prompt if models were not bottleknecked by the number of serial
operations.
5. ^
In this post, "chains of thought" also include answers which include many tokens,
for which you can think while you answer. The equivalent of "Pure System 1" in
GPTs would be a single token answer.
6. ^
This is why the OpenAI API charges you an amount of money proportional to
prompt length + number of tokens generated.
7. ^

I expect GPTs to beneﬁt from chain of thought prompting in areas where human
use System 1 with parts of the brain GPTs autoregressive training didn't built,
such as vision. For example, I wouldn't be surprised if chain of thought prompting
helped GPTs understand ASCII art.
8. ^
Of course, some internet text is like that. But I expect it to be such a minority of
internet's text that it wouldn't matter in practice (GPTs needs a lot of text to
justify investing is special "hide reasoning in text" machinery, right?). I'm
interested if anyone works on either detecting this kind of text to remove it from
training corpuses, or investigating if the claim that GPTs needs a lot of text to
justify investing resources into a complicated algorithm is true.
9. ^
Here is a description of what steganography is and what it could look like in the
context of LLMs, by A Ray.
10. ^
See here for a deeper dive about RL and the brain by lukeprog.
11. ^
More details about self-deception in these articles by Paul Christiano: If you can't
lie to others you must lie to yourself, and The Elephant in the brain
12. ^
This is also much worse than examples of steganography given by humans
because there can be much more RLHF-generated text, and because the context
in which steganography by human is present is very diﬀerent from the context in
which future GPTs will be used, while the prompts with which Instruct GPT is used
are very close to the prompts with which future GPTs will be.
13. ^
For examplen System 2 seems to have a "working memory", which doesn't have
an equivalent object in GPTs. Maybe GPTs need something more powerful than
chains of thought to match human System 2 performance. 
I would love to know an example of a task GPTs can't do because of this lack of
working memory, or a task GPTs can do much better than humans because they
aren't bound by a working memory of small and ﬁxed size.
14. ^
For example using the start of a completions to ﬁgure out what the context is,
and then not using as much processing power to ﬁgure it out is a way of using
chains of thought which is not completely transparent to humans, and it could be
the case there are more signiﬁcant ways in which GPTs hide information in chains
of thought by default.

Takeaways from a survey on AI
alignment resources
What am I talking about?
In June and July of this year, I ran a survey to ask a lot of people how useful they found
a variety of resources on AI alignment. I was particularly interested in "secondary
resources": that is, not primary resource outputs, but resources that summarize,
discuss, analyze, or propose concrete research eﬀorts. I had many people promote the
survey in an attempt to make it not obvious that I was running it (so that it would not
aﬀect what people said about AXRP, the podcast that I run). CEA helped a great deal
with the shaping and promotion of the survey.
The goal of the survey was initially to ﬁgure out how useful AXRP was, but I decided
that it would be useful to get a broader look at the space of these secondary
resources. My hope is that the results give people a better sense of what secondary
resources might be worth checking out, as well as gaps that could be ﬁlled.
Participants were shown a list of resources, select those they'd engaged with for >30
min, and for each they selected, rate on a scale from 0 to 4 how useful they'd found it,
how likely they'd be to recommend to a friend getting into the ﬁeld who hadn't read
widely, and how likely they'd be to recommend to someone paid to do AI alignment
research. You can do a test run of the survey at this link.
My summary of the results
AXRP, my podcast, is highly rated among people paid to work on technical AI
alignment resources, but less highly rated in other cohorts.
On a personal note, I ﬁnd this a bit disappointing: I had hoped it could be
useful for people orienting to research directions that they had not read
widely about.
Rob Miles videos are highly rated among everyone, more than I would have
guessed.
People really liked the AI Safety Camp, the AGI Safety Fundamentals Course, and
conversations with AI alignment researchers.
People trying to get into alignment really liked the above and also MLAB. That
said, they recommend Rob Miles videos higher than the AI Safety Camp and
conversations with AI alignment researchers (but lower than MLAB and the AGI
Safety Fundamentals Course).
Basic stats
Entries with demographic info: 139
Entries that rate various resources: 99
Number that say 'I have heard of AI alignment': 95
Number that say 'I am interested in AI alignment research': 109
Number that say 'I am trying to move into a technical AI alignment career': 68

Number that say 'I spend some of my time solving technical problems related to
AI alignment': 51
Number that say 'I spend some of my time doing AI alignment ﬁeld/community-
building': 37
Number that say 'I spend some of my time facilitating technical AI alignment
research in ways other than doing it directly': 35
Number that say 'I spend some of my time publicly communicating about AI
alignment': 36
Number that say 'I am paid to work on technical AI alignment research': 30
Number that say 'I help run an organization with an AI alignment mission (e.g.
CHAI, MIRI, Anthropic)': 11
Context for questions
When sorting things by ratings, I've included the top 5, and anything just below the
top 5 if that was a small number. I also included ratings for AXRP, the podcast I make.
Ratings are paired with the standard error of the mean (total ratings have this
standard error multiplied by the number of people in the sample). Only things that at
least 2 people engaged with were included.
Ratings were generally rounded to two signiﬁcant ﬁgures, and standard errors were
reported to the same precision.
Usefulness ratings
Among all respondents:
Total usefulness (multiplying average rating by reach):
1. 80k podcast: 167 +/- 8
2. Superintelligence: 166 +/- 8
3. Talks by AI alignment researchers: 134 +/- 6
4. Rob Miles videos: 131 +/- 7
5. AI alignment newsletter: 117 +/- 7
6. conversations with AI alignment researchers at conferences: 107 +/- 5
Everything else 85 or below, AXRP is at 59 +/- 4.
Average usefulness ratings:
1. AI Safety Camp: 3.4 +/- 0.2
2. Conversations: 3.1 +/- 0.2
3. AGI Safety Fundamentals Course (AGISF): 3.0 +/- 0.2
4. MLAB: 2.8 +/- 0.8
5. Rob Miles videos: 2.7 +/- 0.1
6. 80k podcast: 2.6 +/- 0.1
7. Superintelligence: 2.6 +/- 0.1
8. AXRP: 2.6 +/- 0.2
Everything else 2.5 or below.

Among people trying to get into alignment:
Total usefulness:
1. 80k podcast: 95 +/- 6
2. AI Alignment Newsletter: 76 +/- 6
3. Talks by AI alignment researchers: 72 +/- 4
4. AGISF: 68 +/- 3
5. Rob Miles videos: 67 +/- 5
6. Superintelligence: 64 +/- 5
Everything else 50 or below, AXRP is at 37 +/- 3
Average usefulness:
1. Tie between AI Safety Camp at 3.5 +/- 0.3 and MLAB at 3.5 +/- 0.4
2. AGISF: 3.2 +/- 0.2
3. Convos: 3.1 +/- 0.2
4. ARCHES agenda: 3.0 +/- 0.7
5. 80k podcast: 2.7 +/- 0.2
Then there's a tail just under that, AXRP is at 2.6 +/- 0.2
Among people who spend time solving alignment
problems:
Total usefulness:
1. Superintelligence: 48 +/- 5
2. Talks: 47 +/- 4
3. Convos: 45 +/- 4
4. AI Alignment Newsletter: 42 +/- 5
5. 80k podcast: 37 +/- 4
6. Embedded Agency sequence: 36 +/- 5
Everything else 29 or below, AXRP is 20 +/- 2.
Average usefulness:
1. Convos: 3.2 +/- 0.3
2. AI Safety Camp: 3.2 +/- 0.3
3. Tie between AGISF at 2.7 +/- 0.4 and ML Safety Newsletter at 2.7 +/- 0.3
4. AI Alignment Newsletter: 2.6 +/- 0.3
5. Embedded Agency sequence: 2.6 +/- 0.3
Then a smooth drop in average usefulness, AXRP is at 2.2 +/- 0.3
Among people paid to work on technical AI alignment
research:
Total usefulness:
1. Convos: 28 +/- 3

2. Talks: 26 +/- 2
3. Superintelligence: 23 +/- 4
4. AXRP: 22 +/- 3
5. Embedded Agency sequence: 20 +/- 3
Everything else 19 or below.
Average usefulness:
1. AI Safety Camp: 3.7 +/- 0.3
2. AI Alignment Newsletter: 3.2 +/- 0.4
3. Convos: 3.1 +/- 0.3
4. Rob Miles videos: 2.8 +/- 0.5 (honourable mention to AIRCS workshops, which
had one rating and scored 3 for usefulness)
5. AXRP: 2.8 +/- 0.3
Everything else 2.5 or below.
Recommendation ratings
Alignment professionals recommend to peers:
1. Convos with researchers: 3.7 +/- 0.2
2. AXRP: 3.3 +/- 0.2
3. Tie between ML safety newsletter at 3.0 +/- 0.4 and AI alignment newsletter at
3.0 +/- 0.5
4. Rob Miles videos: 2.6 +/- 0.5
5. Embedded Agency sequence: 2.5 +/- 0.5
Everything else 2.4 or lower
Alignment professionals recommend to newcomers (=
people trying to move into AI alignment career):
1. AGISF: 3.7 +/- 0.2
2. Rob Miles: 3.4 +/- 0.3
3. The Alignment Problem: 3.2 +/- 0.3
4. 80k podcast: 3.13 +/- 0.3
5. AI safety camp: 3.0 +/- 0.5
Everything else 2.8 or lower (AXRP is at 1.9 +/- 0.4)
Newcomers recommend to newcomers:
1. MLAB: 4.0 +/- 0.0 (2 ratings)
2. AGISF: 3.7 +/- 0.1
3. Rob Miles: 3.4 +/- 0.2
4. AI safety camp: 3.0 +/- 0.9
5. Human Compatible (the book): 2.8 +/- 0.3 (honourable mention to AIRCS
workshops which had one rating, and scored 3)
6. The Alignment Problem: 2.8 +/- 0.3

Everything else 2.6 or lower (AXRP is at 2.4 +/- 0.3)
One tidbit: newcomers seem to agree with the professionals about what newcomers
should engage with, in terms of ratings.
Details of the survey
The survey was run on GuidedTrack. Due to an error on my part, if anybody pressed
the 'back' button and changed a rating, this messed up their results unrecoverably
(hence the drop-oﬀ from the number of entries total and the number with data I could
use).
The list of resources:
AGI Safety Fundamentals Course
the AI Alignment Newsletter
AXRP - the AI X-risk Research Podcast
the ML Safety newsletter
Human Compatible (book)
The Alignment Problem (book)
Rob Miles videos
the Embedded Agency sequence on the Alignment Forum
the Value Learning sequence on the Alignment Forum
the Iterated Ampliﬁcation sequence on the Alignment Forum
the FLI podcast
the 80,000 Hours podcast
Life 3.0 (book)
Superintelligence (book)
AI Safety Camp
AIRCS workshops
the Machine Learning for Alignment Bootcamp
the ARCHES agenda by Andrew Critch and David Krueger
Unsolved Problems in ML Safety by Hendrycks et al
Concrete Problems in AI Safety by Amodei et al
Scalable agent alignment via reward modeling: a research direction by Leike et
al (aka "the recursive reward modelling agenda")
conversations with AI alignment researchers at conferences
talks by AI alignment researchers
the annual AI Alignment Literature Review and Charity Comparison
The rating scale for usefulness:
0: Not at all
1: A little
2: Moderately
3: Very
4: Extremely
The probability rating scale:
0: 0-20%
1: 20-40%
2: 40-60%

3: 60-80%
4: 80-100%
As well as the details published here, I also collected how many years people had
been interested in AI alignment and/or paid to work on technical AI alignment
research, as applicable. Also, people were able to write in comments about speciﬁc
resources, as well as the survey as a whole, and could write in the place they heard
about the survey.
For more details, you can see my GitHub repository for this survey. It contains the
GuidedTrack code to specify the survey, the results, and a script to analyze the
results. Note that I redacted some details of some comments to remove detail that
might identify a respondent.

Could a single alien message destroy
us?
This is a linkpost for https://youtu.be/st9EJg_t6yc
Merely listening to alien messages might pose an extinction risk, perhaps even more
so than sending messages into outer space. Our new video explores the threat posed
by passive SETI and potential mitigation strategies.
Below, you can ﬁnd the script of the video. Matthew Barnett, the author of this related
post, wrote the ﬁrst draft. Most of the original draft survives, but I've made signiﬁcant
restructuring, edits, deletions, and additions. 
One day, a few Earthly astronomers discover something truly remarkable. They've
pointed their radio telescopes at a previously unexplored patch in the night sky, and
recorded a binary message that is too inexplicable to have come from any natural
source. Curious about what the distant aliens have sent us, the scientists begin trying
to decipher the message. After an arduous process of code-breaking, the scientists
ﬁnd that the message encodes instructions on how to build a device. Unfortunately,
the aliens left no description about what the device actually does.
Excited to share their discovery with the world, the astronomers agree to publish the
alien instructions on the internet, and send a report to the United Nations.
Immediately, the news captivates the entire world. For once, there is indisputable
proof that we are not alone in the universe. And what's more: the aliens have sent us
a present, and no one knows what its purpose might be.
In a breathtaking frenzy that surpasses even the Space Race of the 1960s, engineers
around the world rush to follow these instructions, to uncover the secrets behind the
gift the aliens have left for us.
But soon after, a horrifying truth is revealed: the instructions do not describe a cure to
all diseases, or a method of solving world hunger. Rather, the aliens have sent us
explicit, and easy to follow instructions on how to build a very powerful bomb: an anti-
matter explosive device with the yield of over one thousand hydrogen bombs. The
most horrifying part is that the instructions require only common household materials,
combined in just the right way.

The horror of this development begins to sink in around the world. Many propose that
we should censor the information, in an attempt to prevent a catastrophe. But the
reality is that the information is already loose. Sooner or later, someone will build the
bombs, either from raw curiosity, or deliberate ill-intent. And then, right after that, the
world will end.
This story is unrealistic. In real life, there's probably no way to combine common
household materials in just the right way to produce an antimatter bomb. Rather, this
story illustrates the risk we take by listening to messages in the night sky, and being
careless about how these potential messages are disseminated.
With this video, we don't want to argue that humanity will necessarily go extinct if we
listen to alien messages, nor that this is necessarily among the biggest threats we're
facing. In fact, the probability that humanity will go extinct in this exact way is small,
but the risk we take from listening to alien messages is still an idea worth considering.
As with all potential existential threats, the entire future of humanity is at stake.
We'll model alien civilizations as being "grabby", in the sense described by Robin
Hanson's paper on Grabby Aliens, which we covered in two previous videos. Grabby
civilizations expand at a non-negligible fraction of the speed of light, and occupy all
available star systems in their wake. By doing so, every grabby civilization creates a
sphere of expanding inﬂuence. Together, all the grabby civilizations will one day
enclose the universe with technology and intelligently designed structures.
However, since grabby aliens cannot expand at the speed of light, there is a second
larger sphere centered around every grabby civilization's origin, which is deﬁned by
the earliest radio signals sent by the alien civilization as it ﬁrst gained the capacity for
deep-space communication. This larger sphere expands at the speed of light, faster
than the grabby civilization itself.
Let's call the space between the ﬁrst and second spheres the "outer shell" of the
grabby alien civilization. If grabby alien civilizations leave highly distinct marks on
galaxies and star systems they've occupied, then their civilization should be visible to
any observers within this outer shell. As we noted in the grabby aliens videos, if we
were in the outer shell of a grabby alien civilization, they would likely appear to be
large in the night sky. On the other hand, if grabby civilizations left more subtle traces
that we can't currently spot with our technology, that would explain why we aren't
seeing them.
In this video, let's assume that grabby aliens leave more subtle traces on the cosmos,
making it plausible that Earth could be in the outer shell of a grabby alien civilization
right now without us currently realizing that. This is a model variation, but it leaves
the basics of the Grabby Aliens theory intact.
Here's where things could turn out  dangerous for humanity. If, for example, a grabby
alien civilization felt threatened by competition that it might encounter in the future, it
could try to wipe out potential competitors inside this outer shell before they ever got
the chance to meet physically. This is because, if they wanted, the grabby alien
civilization could send out a manipulative deep-space message to any budding
civilization in the outer shell gullible enough to listen, forcing their self-destruction.
In our illustrative story we made the example of instructions for building antimatter
bombs with household material. A more realistic possibility could be instructions for
building advanced artiﬁcial intelligence, which then turns out to be malicious.

We could make a number of plausible hypotheses about the content of the message,
but it's diﬃcult to foresee what it would actually contain, as the alien civilization
would be a lot more advanced than us, and, potentially, millions of years old. They
would have much more advanced technology, and a lot of time to think carefully
about what messages to send to budding civilizations. They  could spend centuries to
craft the perfect message that would hijack or destroy infant civilizations that are
unfortunate enough to tune in.
But maybe you're still unconvinced. Potential ﬁrst contact with aliens could even be
the best thing to ever happen to humanity. Aliens might be very friendly to us, and
could send us information that would help our civilization and raise our well-being to
unprecedented levels. 
Perhaps this whole idea is rather silly. Our parochial, tribal brains are simply blind to
the reality that very advanced aliens would have abandoned warfare, domination, and
cold-hearted manipulation long ago, and would instead be devoted to the mission of
uplifting all sentient life.
On the other hand, life on other planets probably arose by survival of the ﬁttest, as
our species did, which generally favors organisms that are expansionist and greedy
for resources. Furthermore, we are more likely to get a message from an expansionist
civilization than a non-expansionist civilization, since the latter civilizations will
command far fewer resources and will presumably be more isolated from one another.
This provides us even more reason to expect that any alien civilization that we detect
might try to initiate a ﬁrst strike against us.
It's also important to keep in mind that the risk of a malicious alien message is still
signiﬁcant even if we think aliens are likely to be friendly. For instance, even if we
believe that 90% of alien civilizations in the universe will be friendly to us in the
future, the prospect of encountering the 10% that are unfriendly could be so horrifying
that we are better plugging our ears and tuning out for now, at least until we grow up
as a species, and ﬁgure out how to handle such information without triggering a
catastrophe.
But even if SETI is dangerous, banning the search for extraterrestrial intelligence is an
unrealistic goal at this moment in time. Even if it were the right thing to do to mitigate
risk of premature human extinction, there is practically no chance that enough people
will be convinced that this is the right course of action.
More realistically, we should instead think about what rules and norms humanity
should adopt to robustly give our civilization a better chance at surviving a malicious
SETI attack.
As a start, it seems wise to put in place a policy to review any conﬁrmed alien
messages for signs that they might be dangerous, before releasing any potentially
devastating information to the public.
Consider two possible policies we could implement concerning how we review alien
messages. 
In the ﬁrst policy, we treat every alien message with an abundance of caution. After a
signal from outer space is conﬁrmed to be a genuine message from extraterrestrials,
humanity forms a committee with the express purpose of debating whether this
information should be released to the public, or whether it should be sealed away for
at least another few decades, at which point another debate will take place.

In the second policy, after a signal is conﬁrmed to be a genuine message from aliens,
we immediately release all the data publicly, ﬂooding the internet with whatever
information aliens have sent us. In this second policy, there is no review process;
everything we receive from aliens, no matter the content, is instantly declassiﬁed and
handed over to the wider world without a moment's hesitation.
If you are even mildly sympathetic to our thesis here — that SETI is risky for humanity
— you probably agree that the second policy would be needlessly reckless, and might 
put our species in danger. Yet, the second policy is precisely what the inﬂuential SETI
Institute recommends humanity do in the event of successful alien contact. You can
ﬁnd more information in their document titled Protocols for an ETI Signal Detection,
which was adopted unanimously by the SETI Permanent Study Group of the
International Academy of Astronautics in 2010.
The idea that SETI might be dangerous is not new . It was perhaps ﬁrst showcased in
the 1961 British drama serial, A for Andromeda, in which aliens from Andromeda sent
humanity the instructions on how to build an artiﬁcial intelligence whose ﬁnal goal
was to subjugate humanity. In the show, humans ended up victorious over the alien
artiﬁcial intelligence, but  we would not be so lucky in the real world.
In intellectual communities and academia, the idea that SETI is dangerous has
received very little attention, either positive or negative. In its place, the risk from
METI has taken the spotlight, which is: sending messages to outer space rather than
listening to them. This might explain why, as a species, we do not appear to currently
be taking the risk from SETI very seriously.
Yet it's imperative that humanity safeguards its own survival. If we survive the next
few centuries, we have great potential as a species. In the long-run, we could reach
the stars and become a grabby civilization ourselves, potentially expanding into
thousands or millions of galaxies, creating trillions of worthwhile lives. But not
necessarily endangering lives already present on other star systems, of course! To
ensure we have a promising future, let's proceed carefully with SETI. It could end up
being the most important decision we ever make.
 

Clarifying wireheading terminology
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
See also: Towards deconfusing wireheading and reward maximization, Everett et al. (2019).
There are a few subtly diﬀerent things that people call "wireheading". This post is intended
to be a quick reference for explaining my views on the diﬀerence between these things. I
think these distinctions are sometimes worth drawing to reduce confusion.
1. Speciﬁcation gaming / reward hacking: The agent conﬁgures the world in such a way
that makes its reward(/utility) function achieve a high value in a way that is not what
the creators intended the reward function to incentivize.
1. Examples: the boat race example, our recent work on reward model
overoptimization
2. Reward function input tampering: The agent tampers with the inputs to its reward
function to make the reward function see the same inputs as it would when observing
an actual desirable worldstate.
1. Examples: sensor tampering, VR
3. Reward function tampering: The agent changes its reward function.[1]
1. Examples: meditation, this baba-is-you-like gridworld
4. Wireheading: The agent directly tampers with the reward signal going into the RL
algorithm.
1. Examples:  dopamine agonists, setting the reward register
Some crucial diﬀerences between these:
1 is a problem of both embedded[2] and non-embedded settings. 2 is a problem of
partially-observedness[3], which is unavoidable in embedded settings, but also comes
up in many non-embedded settings as well. 3 and 4 are problems of embeddedness;
they do not show up in non-embedded settings. 
2, 3, 4 are all about "not caring about the real world" in various ways, whereas 1 is
about "caring about the real world in the wrong way"
4 is about as close as you can get to a "reward-maximizing policy" in an embedded
setting.

 
1. ^
Note: this is not the same thing as changing a terminal goal! The reward function is not
necessarily the terminal goal of the policy, because of inner misalignment.
2. ^
Here, by "embeddedness" I mean in the sense of Demski and Garrabrandt (2019); in
particular, the fact that the agent is part of the environment, and not in a separate part
of the universe that only interacts through well deﬁned observation/action/reward
channels. RL is the prototypical example of a non-embedded agency algorithm.
3. ^
That is, the reward function is unable to perfectly observe the ground truth state of the
environment, which means that if there is another state the world cold be in that yields
the same observation, the reward cannot distinguish between the two.

What is epigenetics?
This is a linkpost for https://denovo.substack.com/p/what-is-epigenetics
Among all areas of biology related to my research, epigenetics is the one that is most
commonly misunderstood, not only by the general public but even by other scientists. After
being irritated one too many times,[1] I've decided to make a series of posts to explain what
epigenetics really is, why it's important, and how it's misunderstood. I will also explain how
epigenetics is important for my own research on making gametes from stem cells.
This ﬁrst post covers the deﬁnition of epigenetics, and the basic biology of epigenetic marks.
What is genetics?
Before deﬁning epigenetics, let's start with a deﬁnition of genetics. Genetics is the study of
genes, which are sequences of genetic material[2] that encode functional products.
Let's take the IGF2 gene as an example.
The human IGF2 gene, shown in the NCBI genome browser.
Depicted above is a region of human chromosome 11 containing the IGF2 gene, which
encodes the IGF2 protein, an important growth factor for fetal development.[3] The boxes
represent exons and lines represent introns. The darker green color is the protein-coding
sequence, and non-coding (i.e. untranslated) regions are shown in lighter green. Arrows
represent the direction of transcription.
The bottom of this image shows the location of common genetic variants (present at >1%
frequency). If you look closely, you might notice that none of them are in the protein-coding
sequence (the dark green boxes). This is not a coincidence, because nothing is ever a
coincidence most mutations to essential proteins (including IGF2) are harmful and thus
selected out of the population. However, there are several common mutations in non-coding
regions of this gene.
To recap, genetics is the study of genes (such as IGF2) and the eﬀects of genetic variation
on their functions.
What is epigenetics?
Epigenetics is the study of epigenetic marks, which are changes to genetic material that
alter gene expression, but do not change the genetic sequence. A decent analogy for

epigenetic marks is CAPITALIZATION, bolding, or strikethroughs in text.
DNA methylation and histone modiﬁcations are the two kinds of epigenetic marks.
Some people also consider long noncoding RNAs (such as those involved in X-chromosome
inactivation) to be epigenetic marks. Although these RNAs are undoubtedly important for
regulating gene expression, I would not classify them as epigenetic marks since they are not
direct modiﬁcations to genetic material.
In vertebrate animals, the cytosine in CG sequences often has a methyl group attached,
forming 5-methylcytosine. A CG sequence is also CG on the opposite strand, so the cytosines
on both strands can be methylated.
"DNA" signiﬁes that the base is attached to the rest of the DNA molecule.
To make things confusing, methylation at CG sequences is termed CpG methylation, the
lowercase p standing for phosphate. 5-methylcytosine will pair with guanine just like normal
cytosine, but it is not equivalent to cytosine in its interactions with DNA-binding proteins.
Generally, CpG methylation suppresses the expression of nearby genes. CpG sites often
cluster together to form "CpG islands" in important regulatory regions. Other organisms
(invertebrates, plants, fungi, bacteria) have diﬀerent ways of methylating DNA. I won't get
into them in this post series, but you should know that CpG methylation is not universal.
Modiﬁcations to histones are another important set of epigenetic marks. Histones are DNA
packaging proteins, which form complexes called nucleosomes. DNA winds around
nucleosomes sort of like thread around spools. The overall assembly of DNA and histones is
known as chromatin.

This crystal structure shows how DNA (gray) wrapped around histones (H2A =
yellow, H2B = red, H3 = blue, H4 = green) forms a nucleosome.
Chemical modiﬁcations to histones are important epigenetic marks that can have drastic
changes on gene expression. For example, trimethylation of lysine 4 on histone H3 (known
as H3K4me3) marks promoters of actively transcribed genes. However, methylation at other
histone sites (such as H3K9 and H3K27) is repressive. Besides methylation, there is also a
plethora of other histone modiﬁcations: acetylation, phosphorylation, ubiquitylation,
sumoylation, crotonylation . . . the list goes on and on, and more are being discovered every
year.
Most histone modiﬁcations are on the C-terminal tails, shown here as sequences
of amino acids. I hope you know your amino acid abbreviations!

Let's take another look at the IGF2 gene. Now I have added three additional display tracks
related to epigenetic marks:
The "CpG Islands" track shows areas containing many CG sequences that could be
methylated. Unfortunately, NCBI doesn't have any information on the actual methylation
status. The H3K4me3 tracks are more interesting. If you look closely, you may notice that
the distribution of H3K4me3 is diﬀerent in brain and skeletal muscle. This is not a
coincidence: diﬀerent types of cells have epigenetic marks in diﬀerent places, and
thus express diﬀerent genes.
Reading epigenetic marks
Epigenetic marks are "read" by proteins that interact with DNA and/or histones. Many of
these proteins have conserved domains that bind certain marks. A few of the many
examples are:
Methyl-CpG-binding domains bind methylated CpG sites
Bromodomains bind acetylated histones
Tudor domains bind methylated histones
These proteins are often transcription factors, which activate or repress gene expression.
Furthermore, epigenetic marks also alter the physical properties of histones, particularly the
electrostatic charge. Since DNA is negatively charged, marks that remove positive charges
(e.g. acetylation) or add negative charges (e.g. phosphorylation) will make the histones bind
to the DNA less strongly.
Scientists can also read epigenetic marks. For DNA methylation, the most common method is
bisulﬁte sequencing, which chemically converts unmethylated cytosines to uracils, followed
by sequencing of the DNA.[4]Any remaining cytosines observed in the sequence data must
have been methylated.
Histone modiﬁcations are typically measured by chromatin immunoprecipitation sequencing
(ChIP-seq), which uses an antibody to isolate histones bearing a particular epigenetic mark,
and then sequences the associated DNA. Cut&RUN is a newer method that is conceptually
similar but with higher sensitivity. These methods work well if and only if the antibody has
strong on-target binding and low oﬀ-target binding. Various companies all claim that their
proprietary antibodies are great, but it can often be challenging to ﬁnd an antibody that
actually works well, particularly for less commonly studied marks. Other methods such as

ATAC-seq can measure whether DNA is loosely packaged (known as euchromatin, allowing
for active transcription) or tightly packaged (known as heterochromatin, which represses
transcription). This is closely related to measuring histone modiﬁcations, but not exactly the
same.
Writing epigenetic marks
Epigenetic marks are written by specialized enzymes. Histone methylation and acetylation
are established by methyltransferases and acetyltransferases. Each of these enzymes will
typically be selective for only one particular target site. For example, H3K4 and H3K27 are
methylated by diﬀerent sets of enzymes. Histone phosphorylation is established by kinases.
One important example is phosphorylation of serine 139 on H2AX, forming a modiﬁcation
known as γH2AX. This modiﬁcation is added by the kinase ATM at sites of DNA damage, and
subsequently recruits DNA repair enzymes.
DNA is methylated by DNA methyltransferases. In mammals, these are DNMT1, DNMT3A,
and DNMT3B. DNMT3A and B are de novo methyltransferases, which means they can
methylate fully unmethylated CpG sites. These enzymes are the ones responsible for
determining where DNA methylation is added. Diﬀerent proteins can recruit DNMT3 to sites
that need to be methylated.
DNMT3 can add methyl groups at unmethylated CpG sites. After DNA replication,
the newly synthesized strand is unmethylated. DNMT1 recognizes
hemimethylated CpG sites and adds methylation on the new strand.
DNMT1 is a maintenance methyltransferase, which binds hemimethylated CpG sites and
adds a methyl group onto the unmethylated cytosine. This is necessary to maintain DNA
methylation after cell division, since newly synthesized DNA is always unmethylated. Most
cells express DNMT1 and maintain methylation at the same sites over multiple rounds of cell
division. However, the exceptions are extremely important, and I'll discuss them in a
later post.
Many epigenetic writer proteins recognize particular DNA sequence motifs. For example,
promoter sequences of highly expressed genes contain sequence motifs that are bound by
enzymes that deposit marks to activate gene expression. Also, certain DNA sequences
known as insulators prevent epigenetic marks from spreading past them (as with PRC2,
discussed below). These are just two of the many examples of genetics inﬂuencing
epigenetics.
Notably, histone marks and DNA methylation also interact. Methylated DNA can recruit
histone H3K9 methyltransferases which add additional repressive marks (H3K9me3).
Likewise, DNMT3 enzymes contain an ADD domain that binds to unmethylated H3K4,
meaning that the presence of H3K4me3 inhibits de novo methylation.
These marks all interact with themselves and each other by recruiting writer and eraser
enzymes, forming complicated feedback loops. For example, the PRC2 complex methylates
H3K27 to H3K27me3, and also binds to H3K27me3, which means that it spreads the

methylation to adjacent areas of chromatin. However, it is stopped by H3K27ac because
acetylated lysines cannot be methylated (and vice versa).
Researchers can write epigenetic marks at sites in the genome by attaching a writer enzyme
onto a CRISPR protein such as dCas9.[5] The dCas9 attaches to a target DNA sequence and
then the writer enzyme adds marks nearby. This can be useful in adding activating or
repressive marks to turn target genes on or oﬀ.
Erasing epigenetic marks
Epigenetic marks are absent from newly copied DNA and newly synthesized histones, so in
dividing cells they are lost by default unless actively re-written. This is very
important, and we'll come back to it in a later post.
There are also specialized proteins that can actively remove epigenetic marks. DNA
methylation can be removed by TET enzymes which oxidize the methyl group to 5-
carboxymethylcytosine. This is subsequently removed by thymine-DNA glycosylase[6] and
the missing base is repaired using normal cytosine. Histone methylation can be removed by
lysine-speciﬁc demethylases, and histone acetylation can be removed by histone
deacetylases.
As with writing marks, researchers can also erase them at targeted sites by using eraser
enzymes attached to dCas9.
TL;DR:
Epigenetics is the study of epigenetic marks: modiﬁcations to genetic material that
don't aﬀect the sequence, but control which genes get expressed.
In mammals, these are DNA methylation and histone modiﬁcations.
They can be read, written, and erased by specialized proteins.
Newly copied DNA lacks epigenetic marks.
Diﬀerent cells have diﬀerent patterns of marks and express diﬀerent genes.
Scientists can also read these marks through various sequencing-based technologies,
and write or erase them using modiﬁed CRISPR proteins.
Next time: epigenetics of the mammalian germline, and how it explains why an
egg can't fertilize another egg and generate viable oﬀspring. [7]
1. ^
The latest example was this press release which took a ﬁnding in C. elegans and said
"it may explain how a person's health and development could be inﬂuenced by the
experiences of his or her parents and grandparents." WHICH IT DEFINITELY DOESN'T, C.
elegans do things very diﬀerently from humans!
2. ^
Typically DNA, but also sometimes RNA, for viruses with RNA genomes.
3. ^
And also in several adult organs too. IGF2 is very interesting epigenetically and we'll
get into the details in the next post.
4. ^

A related method, enzymatic methyl-seq, uses enzymes instead of bisulﬁte treatment.
5. ^
d in dCas9 stands for "dead". The amino acids that cut the DNA are mutated but it can
still bind to the target.
6. ^
This enzyme also removes the thymine from T:G mismatches. This is important
because methylated cytosine can spontaneously deaminate and form thymine, and the
cell needs a way of repairing this.
7. ^
At least not without some hardcore bioengineering.

