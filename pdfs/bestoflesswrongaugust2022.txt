
Best of LessWrong: August 2022
1. (My understanding of) What Everyone in Technical Alignment is Doing and Why
2. DeepMind alignment team opinions on AGI ruin arguments
3. A Mechanistic Interpretability Analysis of Grokking
4. Two-year update on my personal AI timelines
5. Common misconceptions about OpenAI
6. What do ML researchers think about AI in 2022?
7. Some conceptual alignment research projects
8. Language models seem to be much better than humans at next-token prediction
9. Worlds Where Iterative Design Fails
10. Your posts should be on arXiv
11. How To Go From Interpretability To Alignment: Just Retarget The Search
12. Nate Soares' Life Advice
13. The Parable of the Boy Who Cried 5% Chance of Wolf
14. Shard Theory: An Overview
15. Taking the parameters which seem to matter and rotating them until they don't
16. How might we align transformative AI if it's developed very soon?
17. Fiber arts, mysterious dodecahedrons, and waiting on "Eureka!"
18. Interpretability/Tool-ness/Alignment/Corrigibility are not Composable
19. Beliefs and Disagreements about Automating Alignment Research
20. Survey advice
21. What's General-Purpose Search, And Why Might We Expect To See It In Trained
ML Systems?
22. The lessons of Xanadu
23. Externalized reasoning oversight: a research direction for language model
alignment
24. The alignment problem from a deep learning perspective
25. How to do theoretical research, a personal perspective
26. Meditation course claims 65% enlightenment rate: my review
27. AI strategy nearcasting
28. Paper is published! 100,000 lumens to treat seasonal aﬀective disorder
29. Preprint is out! 100,000 lumens to treat seasonal aﬀective disorder
30. Preprint is out! 100,000 lumens to treat seasonal aﬀective disorder
31. How likely is deceptive alignment?
32. Oversight Misses 100% of Thoughts The AI Does Not Think
33. Most Ivy-smart students aren't at Ivy-tier schools
34. Introducing Pastcasting: A tool for forecasting practice
35. everything is okay
36. What's the Least Impressive Thing GPT-4 Won't be Able to Do
37. I'm mildly skeptical that blindness prevents schizophrenia
38. «Boundaries», Part 2: trends in EA's handling of boundaries
39. Seriously, what goes wrong with "reward the agent when it makes you smile"?
40. Less Threat-Dependent Bargaining Solutions?? (3/2)
41. Reﬁning the Sharp Left Turn threat model, part 1: claims and mechanisms
42. COVID-19 Group Testing Post-mortem?
43. The Expanding Moral Cinematic Universe
44. How (not) to choose a research project
45. The longest training run
46. AI art isn't "about to shake things up". It's already here.
47. Human Mimicry Mainly Works When We're Already Close

48. High Reliability Orgs, and AI Companies
49. ACX Meetups Everywhere List
50. Announcing the Introduction to ML Safety course
51. Rant on Problem Factorization for Alignment
52. The Core of the Alignment Problem is...

(My understanding of) What Everyone in
Technical Alignment is Doing and Why
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
Epistemic Status: My best guess
Epistemic Eﬀort: ~75 hours of work put into this document
Contributions: Thomas wrote ~85% of this, Eli wrote ~15% and helped edit + structure it. Unless
speciﬁed otherwise, writing in the ﬁrst person is by Thomas and so are the opinions. Thanks to
Miranda Zhang, Caleb Parikh, and Akash Wasil for comments. Thanks to many others for relevant
conversations.
Introduction
Despite a clear need for it, a good source explaining who is doing what and why in technical AI
alignment doesn't exist. This is our attempt to produce such a resource. We expect to be
inaccurate in some ways, but it seems great to get out there and let Cunningham's Law do its
thing.[1] 
The main body contains our understanding of what everyone is doing in technical alignment and
why, as well as at least one of our opinions on each approach. We include supplements visualizing
diﬀerences between approaches and Thomas's big picture view on alignment. The opinions written
are Thomas and Eli's independent impressions, many of which have low resilience. Our all-things-
considered views are signiﬁcantly more uncertain. 
This post was mostly written while Thomas was participating in the 2022 iteration SERI MATS
program, under mentor John Wentworth. Thomas beneﬁted immensely from conversations with
other SERI MATS participants, John Wentworth, as well as many others who I met this summer. 
Disclaimers: 
This post is our understanding and has not been endorsed by the people doing the work
itself. 
The length of the summaries varies according to our knowledge of this approach, and is not
meant to reﬂect a judgement on the quality or quantity of work done.
We are not very familiar with most of the academic alignment work being done, and have
only included a few academics. 
A summary of our understanding of each approach:
Approach
Problem Focus
Current Approach
Summary
Scale 
Aligned AI
Model splintering
Solve extrapolation
problems. 
2-5 researchers,
started Feb 2022 
ARC
Inaccessible
information
ELK + LLM power-seeking
evaluation
3
researchers, started
April 2021
Anthropic
LLM Outer Alignment
(?)[3]
Interpretability + HHH
+ augmenting alignment
research with LLMs
~35? technical
staﬀ[3], started May
2021
Brain-like-
Brain-like AGI Safety
Use brains as a model for
~4

AGI Safety
how AGI will be
developed, think about
alignment in this context
researchers, started
March 2021
Center for
AI Safety
(CAIS)
Engaging the ML
community, many
technical problems 
Technical research,
Infrastructure, and ML
community ﬁeld-building
for safety
7-10 FTE, founded in
~March 2022
CHAI
Outer alignment,
though CHAI is diverse
Improve CIRL + many
other independent
approaches. 
~20 FTE?, founded
in 2016 
CLR
Suﬀering risks
Foundational game
theory research
5-10 FTE, founded
before 2015
Conjecture 
Inner alignment
Interpretability +
automating alignment
research with LLMs
~20 FTE, announced
April 2022
David
Krueger
Goal misgeneralization
 
Empirical examples and
understanding ML
inductive biases 
 
Academic lab with 7
students
DeepMind
Many including scalable
oversight and goal
misgeneralization
Many including Debate,
discovering agents, ERO,
and understanding threat
models. [4]
>1000 FTE for the
company as a whole,
~20-25 FTE on the
alignment + scalable
alignment teams
Dylan
Hadﬁeld-
Menell
Value Alignment
Reward speciﬁcation +
Norms
Academic research
lab
Encultured
Multipolar failure from
lack of coordination
Video game
~3
people, announced 
August 2022
Externalized
Reasoning
Oversight
Deception
Get the reasoning of the
AGI to happen in natural
language, then oversee
that reasoning
~1 person's project
for a summer
(though others are
working on this
approach)
FHI
Agent incentives /
wireheading (?)
Causal model formalism
to study incentives. 
~3 people in the
causal group / ~20
total?, FHI founded
in 2005, Causal
group founded in
2021
FAR
Many 
Incubate new, scalable
alignment research
agendas, technical
support for existing
researchers 
4 people on
leadership but I'm
guessing ~5 more
engineers,
announced July 2022

MIRI
Many
including deception, the
sharp left
turn, corrigibility is anti-
natural
Mathematical research to
resolve fundamental
confusion about the
nature of
goals/agency/optimization
11 research staﬀ,
founded in
approximately 2005
Jacob
Steinhardt
Distribution Shift
Conceptual alignment
Academic lab of 9
PhD students +
Postdocs
OpenAI
Scalable oversight
RLHF / Recursive Reward
Modeling, then automate
alignment research
100 capabilities and
30 alignment
researchers, founded
December 2015. 
Ought
Scalable oversight
Supervise process rather
than outcomes +
augment alignment
researchers
10
employees, founded
in ~2018
Redwood
Inner alignment (?)
Interpretability +
Adversarial Training 
12-15 research
staﬀ, started
sometime before
September 2021 
Sam
Bowman
LLM Outer Alignment
Creating datasets for
evaluation + inverse
scaling prize
 
Academic lab
Selection
Theorems
Being able to
robustly point at
objects in the world
Selection Theorems
based on natural
abstractions
~2 FTE,
started around
August 2019
Team Shard
Instilling inner values
from an outer training
loop
Find patterns of values
given by current RL
setups and humans, then
create quantitative rules
to do this
~4-6 people, started
Spring 2022
Truthful AI
Deception
Create standards and
datasets to evaluate
model truthfulness
~10 people, one
research project
 
 
Previous related overviews include:
Neel Nanda's My Overview of the AI Alignment Landscape
Evan Hubinger's An overview of 11 proposals for building safe advanced AI
Larks' yearly Alignment Literature Review and Charity Comparison
Nate Soares' On how various plans miss the hard bits of the alignment challenge
Andrew Critch's Some AI research areas and their relevance to existential safety
80,000 Hours' list of organizations working in the area

Aligned AI / Stuart Armstrong
One of the key problems in AI safety is that there are many ways for an AI to generalize oﬀ-
distribution, so it is very likely that an arbitrary generalization will be unaligned. See the model
splintering post for more detail.  Stuart's plan to solve this problem is as follows:
1. Maintain a set of all possible extrapolations of reward data that are consistent with the
training process
2. Pick among these for a safe reward extrapolation. 
They are currently working on algorithms to accomplish step 1: see Value Extrapolation. 
Their initial operationalization of this problem is the lion and husky problem. Basically: if you train
an image model on a dataset of images of lions and huskies, the lions are always in the desert,
and the huskies are always in the snow. So the problem of learning a classiﬁer is under-deﬁned:
should the classiﬁer be classifying based on the background environment (e.g. snow vs sand), or
based on the animal in the image? 
A good extrapolation algorithm, on this problem, would generate classiﬁers that extrapolate in all
the diﬀerent ways[4],  and so the 'correct' extrapolation must be in this generated set of classiﬁers.
They have also introduced a new dataset for this, with a similar idea: Happy Faces. 
Step 2 could be done in diﬀerent ways. Possibilities for doing this include: conservatism,
generalized deference to humans,  or an automated process for removing some goals.   like
wireheading/deception/killing everyone.
Opinion: I like that this approach tries to tackle distributional shift, which might I see as one of the
fundamental hard parts of alignment. 
The problem is that I don't see how to integrate this approach for solving this problem with deep
learning. It seems like this approach might work well for a model-based RL setup where you can
make the AI explicitly select for this utility function. 
It is unclear to me how to use this to align an end-to-end AI training pipeline. The key problem
unsolved by this is how to get inner values into a deep learning system via behavioral gradients:
reward is not the optimization target . Generating a correct extrapolation of human goals does not
let us train a deep learning system to accomplish these goals. 
Alignment Research Center (ARC)
Eliciting Latent Knowledge / Paul Christiano
ARC is trying to solve Eliciting Latent Knowledge (ELK). Suppose that you are training an AI agent
that predicts the state of the world and then performs some actions, called a predictor. This
predictor is the AGI that will be acting to accomplish goals in the world. How can you create
another model, called a reporter, that tells you what the predictor believes about the world? A key
challenge in training this reporter is that training your reporter on human labeled training data, by
default, incentivizes the predictor to just model what the human thinks is true, because the human
is a simpler model than the AI.
Motivation: At a high level, Paul's plan seems to be to produce a minimal AI that can help to do AI
safety research. To do this, preventing deception and inner alignment failure are on the critical
path, and the only known solution paths to this require interpretability (this is how all of Evan's 11
proposals plan to get around this problem).
If ARC can solve ELK, this would be a very strong form of interpretability: our reporter is able to tell
us what the predictor believes about the world. Some ways this could end up being useful for
aligning the predictor include:

Using the reporter to ﬁnd deceptive/misaligned thoughts in the predictor, and then
optimizing against those interpreted thoughts. At any given point in time, SGD only updates
the weights a small amount. If an AI becomes misaligned, it won't be very misaligned, and
the interpretability tools will be able to ﬁgure this out and do a gradient step to make it
aligned again. In this way, we can prevent deception at any point in training.
Stopping training if the AI is misaligned.
Opinion: There are several key uncertainties that I have with this approach.
I am not sure if there exists an ELK solution, even in theory. Even if such a thing exists, I am
not sure if it will be tractable to implement.
Optimizing against your reporter puts optimization pressure into regions in which the
classiﬁer deceives you and the reporter, or more generally where the reporter fails. 
Depending on how ELK gets used, there seems like a risk of too many AIs: if your reporter
must be "smarter" than your predictor, there might be coordination between the predictor
and reporter, or the reporter might become agentic and cause catastrophe. In other words,
many approaches built oﬀ ELK seem like godzilla strategies .
Overall, ELK seems like one of the most promising angles of attack on the problem because it
seems both possible to make progress on  and also actually useful towards solving alignment: if it
works it would let us avoid deception. It is simple and naturally becomes turned into a proposal for
alignment. I'm very excited about more eﬀort being put towards solving ELK.
While this seems like a very powerful form of interpretability, there are also some limitations, for
example, solving ELK does not immediately tell you how the internals of your agent works, as
would be required for a plan like retargeting the search .
Evaluating LM power-seeking / Beth Barnes
Beth is working on "generating a dataset that we can use to evaluate how close models are to
being able to successfully seek power". The dataset is being created through simulating situations
in which an LLM is trying to seek power.
The overall goal of the project is to assess how close a model is to being dangerous, e.g. so we can
know if it's safe for labs to scale it up. Evaluations focus on whether models are capable enough to
seek power successfully, rather than whether they are aligned. They are aiming to create an
automated evaluation which takes in a model and outputs how far away from dangerous it is,
approximating an idealized human evaluation.
Eli's opinion: I'm very excited about this direction, but I think for a slightly diﬀerent reason than
Beth is. There's been lots of speculation about how close the capabilities of current systems are to
being able to execute complex strategies like "playing the training game" , but very little
empirical analysis of how "situationally aware"  models actually are. The automated metric is an
interesting idea, but I'm most excited about getting a much better understanding of the
situational awareness of current models through rigorous human evaluation; the project also
might produce compelling examples of attempts at misaligned power-seeking in LLMs that could
be very useful for ﬁeld-building (convincing ML researchers/engineers).
Opinion: I think this only works in a slowish takeoﬀ world where we can continuously measure
deception. I am ~70% in a world where AGI capabilities jump fast enough that this type of
evaluation doesn't help. It seems really hard to know when AGI will come.
In the world where takeoﬀs are slow enough that this is meaningful, the diﬃculty then becomes
getting labs to actually slow down based on this data: if this worked, it would be hugely valuable.
Even if it doesn't get people to slow down, it might help inform alignment research about likely
failure modes for LLMs.
Anthropic 

LLM Alignment
Anthropic ﬁne tuned a language model to be more helpful, honest and harmless: HHH.
Motivation: I think the point of this is to 1) see if we can "align" a current day LLM, and 2) raise
awareness about safety in the broader ML community.
Opinion: This seems... like it doesn't tackle what I see as the core problems in alignment . This
may make current day language models being less obviously misaligned, but that doesn't seem
like that helps us at all to align AGIs.
Interpretability
Chris Olah, the interpretability legend, is working on looking really hard at all the neurons to see
what they all mean. The approach he pioneered is circuits: looking at computational subgraphs of
the network, called circuits, and interpreting those. Idea: "decompiling the network into a better
representation that is more interpretable". In-context learning via attention heads, and
interpretability here seems useful.
One result I heard about recently: a linear softmax unit stretches space and encourages neuron
monosemanticity (making a neuron represent only one thing, as opposed to ﬁring on many
unrelated concepts). This makes the network easier to interpret.  
Motivation: The point of this is to get as many bits of information about what neural networks are
doing, to hopefully ﬁnd better abstractions. This diagram gets posted everywhere, the hope being
that networks, in the current regime, will become more interpretable because they will start to use
abstractions that are closer to human abstractions. 
Opinion: This seems like it won't scale up to AGI. I think that model size is the dominant factor in
making things diﬃcult to interpret, and so as model size scales to AGI, things will become ever
less interpretable. If there was an example of networks becoming more interpretable as they got
bigger, this would update me (and such an example might already exist, I just don't know of it).
I would love a new paradigm for interpretability, and this team seems like probably the best
positioned to ﬁnd such a paradigm.  
There is a diﬃcult balance to be made here, because publishing research helps with both
alignment and capabilities. They are very aware of this, and have thought a lot about this
information tradeoﬀ, but my inclination is that they are on the wrong side: I would rather they
publish less. Even though this research helps safety some, buying more time is just more
important than knowing more about safety right now.  

Scaling laws
The basic idea is to ﬁgure out how model performance scales, and use this to help understand and
predict what future AI models might look like, which can inform timelines and AI safety research. A
classic result found that you need to increase data, parameters, and compute all at the same
time (at roughly the same rate) in order to improve performance. Anthropic extended this research
here.
Opinion: I am guessing this leads to capabilities gains because it makes the evidence for
data+params+compute = performance much stronger and clearer. Why can't we just privately
give this information to relevant safety researchers instead of publishing it publicly?
I'm guessing that the point of this was to shift ML culture: this is something valuable and
interesting to mainstream ML practitioners which means they will read this safety focused paper.
Brain-Like-AGI Safety / Steven Byrnes
[Disclaimer: haven't read the whole sequence]. This is primarily Steven Brynes, a full time
independent alignment researcher, working on answering the question: "How would we align an
AGI whose learning algorithms / cognition look like human brains?"
Humans seem to robustly care about things, why is that? If we understood that, could we design
AGIs to do the same thing? As far as I understand it, most of this work is biology based: trying to
ﬁgure out how various parts of the brain works, but then also connecting this to alignment and
seeing if we can solve the alignment problem with this understanding. 
There are three other independent researchers working on related projects that Steven has
proposed. 
Opinion: I think it's quite likely that we get useful bits of information and framings from this
analysis. On the current margin, I think it is very good that we have some people thinking about
brain-like AGI safety, and I also think this research is less likely to be dual use.
I also ﬁnd it unlikely (~30%) that we'll get brain-like AGI as opposed to prosaic AGI. 
Center for AI Safety (CAIS) / Dan Hendrycks
Rewritten slightly after Thomas Woodside (an employee of CAIS) commented. I recommend
reading his comment , as well as their sequence, Pragmatic AI Safety , which lays out a more
ﬂushed out description of their theory of impact. 
Right now, only a very small subset of ML researchers are thinking about x-risk from AGI. CAIS
seeks to change this -- their goal is to get the broader ML community, including both industry and
academia. 
CAIS is working on a number of projects, including:
Writing papers that talk about x-risk.
Publishing compilations of open problems.
Make safety benchmarks that the ML community can iterate on.
Running a NeurIPS competition on these benchmarks.
Running the ML Safety Scholars program (MLSS)
A Philosophy Fellowship aimed at recruiting philosophers to do conceptual alignment
research. 
One of these competitions is a Trojan detection competition, which is a way of operationalizing
deceptive alignment. A Trojan is a backdoor into a neural network that causes it to behave weirdly
on a very speciﬁc class of inputs. These are often trained into a model via poisoned data. Trojans

are similar to deceptive alignment because there are a small number of examples (e.g. 300 out of
3 million training examples) that cause very diﬀerent behavior (e.g. a treacherous turn), while for
the vast majority of inputs cause the model to perform normally.
This competition is in a builder breaker format, with rewards for both detecting trojans as well as
coming up with trojans that no one else could detect.
Opinion: One worry with the competition is that contestants will pursue strategies that work right
now but won't work for AGI, because they are trying to win the competition instead of align AGIs. 
Engaging the broader ML community to reduce AGI x-risk seems robustly good both for improving
the quality of the discourse and steering companies and government away from building AGI
unsafely.
Center for Human Compatible AI (CHAI) /
Stuart Russell
CHAI is an academic research organization aﬃliated with UC Berkeley. It is lead by Stuart Russell,
but includes many other professors and grad students pursuing a diverse array of approaches,
most of whom I will not summarize here. For more information see their 2022 progress report. 
Stuart wrote the book Human Compatible, in which he outlines his AGI alignment strategy, which
is based on cooperative inverse reinforcement learning (CIRL). The basic idea of CIRL is to play a
cooperative game where both the agent and the human are trying to maximize the human's
reward, but only the human knows what the human reward is. Since the AGI has uncertainty it will
defer to humans and be corrigible.
Other work that I liked is Clusterability in neural networks: try to measure the modularity of neural
networks by thinking of the network as a graph and performing the graph n-cut. 
Opinion: CIRL might make sense in a non-deep learning paradigm, but I think that deep learning
will scale to AGI (~95%). The problem of getting a deep learning system to 'try' to maximize the
human's reward is the hard part. Viewed this way, CIRL is a wrong way reduction. Some discussion
on this is here .
Eli's opinion: My understanding is that both Stuart and others mainly expect CIRL to potentially be
helpful if deep learning doesn't scale to AGI, but I expect deep learning to scale to AGI with >80%
probability.
Center on Long Term Risk (CLR)
CLR is focused primarily on reducing suﬀering-risk (s-risk), where the future has a large negative
value. They do foundational research in game theory / decision theory, primarily aimed at
multipolar AI scenarios. One result relevant to this work is that transparency can increase
cooperation. 
Update after Jesse Clifton commented: CLR also works on improving coordination for prosaic AI
scenarios, risks from malevolent actors and AI forecasting. The Cooperative AI Foundation (CAIF)
shares personnel with CLR, but is not formally aﬃliated with CLR, and does not focus just on s-
risks.
Opinion: I have <1% credence in agential s-risk happening, which is where most of the worry from
s-risk is as far as I can tell. I have ~70% on x-risk from AGI, so I don't think s-risk is worth
prioritizing. I also view the types of theoretical research done here as being not very tractable.
I also ﬁnd it unlikely (<10%) that logic/mathy/decision theory stuﬀ ends up being useful for AGI
alignment.

Eli's opinion: I haven't engaged with it much but am skeptical for similar reasons to Thomas,
though I have closer to ~5% on s-risk.
Conjecture 
Conjecture is an applied org focused on aligning LLMs (Q & A here). Conjecture has short timelines
(the org acts like timelines are between ~5-10 year, but some have much shorter timelines, such
as 2-4 years), and they think alignment is hard. They take information hazards (speciﬁcally ideas
that could lead towards better AI capabilities) very seriously, and have a public infohazard
document.
Epistemology
The alignment problem is really hard to do science on: we are trying to reason about the future,
and we only get one shot, meaning that we can't iterate. Therefore, it seems really useful to have
a good understanding of meta-science/epistemology, i.e. reasoning about ways to do useful
alignment research.
An example post I thought was very good.
Opinion: Mixed. One part of me is skeptical that we should be going up a meta layer instead of
trying to directly solve the problem — solving meta-science seems really hard and quite indirect.
On the other hand, I consistently learn a lot from reading Adam's posts, and I think others do too,
and so this work does seem really helpful.
This would seem to be more useful in the long timelines world, contradicting Conjecture's
worldview, however, another key element of their strategy is decorrelated research bets, and this
is certainly quite decorrelated. 
Eli's opinion: I'm a bit more positive on this than Thomas, likely because I have longer timelines
(~10-15% within 10 years, median ~25-30 years).
Scalable LLM Interpretability
I don't know much about their research here, other than that they train their own models, which
allow them to work on models that are bigger than the biggest publicly available models, which
seems like a diﬀerence from Redwood.
Current interpretability methods are very low level (e.g., "what does x neuron do"), which does not
help us answer high level questions like "is this AI trying to kill us".
They are trying a bunch of weird approaches, with the goal of scalable mechanistic interpretability,
but I do not know what these approaches actually are.
Motivation: Conjecture wants to build towards a better paradigm that will give us a lot more
information, primarily from the empirical direction (as distinct from ARC, which is working on
interpretability with a theoretical focus).
Opinion: I like the high level idea, but I have no idea how good the actual research is.
Reﬁne
Reﬁne is an incubator for new decorrelated alignment "research bets". Since no approach is very
promising right now for solving alignment, the purpose of this is to come up with a bunch of
independent new ideas, and hopefully some of these will work.

Opinion: Reﬁne seems great because focusing on ﬁnding new frames seems a lot more likely to
succeed than the other ideas.
Eli's opinion: Agree with Thomas, I'm very excited about ﬁnding smart people with crazy-seeming
decorrelated ideas and helping them.
Simulacra Theory
The goal of this is to create a non-agentic AI, in the form of an LLM, that is capable of accelerating
alignment research. The hope is that there is some window between AI smart enough to help us
with alignment and the really scary, self improving, consequentialist AI. Some things that this
ampliﬁer might do:
Suggest diﬀerent ideas for humans, such that a human can explore them.
Give comments and feedback on research, be like a shoulder-Eliezer
A LLM can be thought of as learning the distribution over the next token given by the training
data. Prompting the LM is then like conditioning this distribution on the start of the text. A key
danger in alignment is applying unbounded optimization pressure towards a speciﬁc goal in the
world. Conditioning a probability distribution does not behave like an agent applying optimization
pressure towards a goal. Hence, this avoids goodhart-related problems, as well as some inner
alignment failure.
One idea to get superhuman work from LLMs is to train it on ampliﬁed datasets like really high
quality / diﬃcult research. The key problem here is ﬁnding the dataset to allow for this.
There are some ways for this to fail:
Outer alignment: It starts trying to optimize for making the actual correct next token, which
could mean taking over the planet so that it can spend a zillion FLOPs on this one prediction
task to be as correct as possible.
Inner alignment:
An LLM might instantiate mesa-optimizers, such as a character in a story that the LLM
is writing, and this optimizer might realize that they are in an LLM and try to break out
and aﬀect the real world.
The LLM itself might become inner misaligned and have a goal other than next token
prediction.
Bad prompting: You ask it for code for a malign superintelligence; it obliges. (Or perhaps
more realistically, capabilities).
Conjecture are aware of these problems and are running experiments. Speciﬁcally,  an
operationalization of the inner alignment problem is to make an LLM play chess. This (probably)
requires simulating an optimizer trying to win at the game of chess. They are trying to use
interpretability tools to ﬁnd the mesa-optimizers in the chess LLM that is the agent trying to win
the game of chess. We haven't ever found a real mesa-optimizer before, and so this could give
loads of bits about the nature of inner alignment failure.
My opinion: This work seems robustly useful in understanding how we might be able to use tool
AIs to solve the alignment problem. I am very excited about all of this work, and the motivation is
very clear to me.
Eli's opinion: Agree with Thomas.
David Krueger
David runs a lab at the University of Cambridge. Some things he is working on include:  
1. Operationalizing inner alignment failures and other speculative alignment failures that
haven't actually been observed. 
2. Understanding neural network generalization. 

For work done on (1), see: Goal Misgeneralization, a paper that empirically demonstrated
examples of inner alignment failure in Deep RL environments. For example, they trained an agent
to get closer to cheese in a maze, but where the cheese was always in the top right of a maze in
the training set. During test time, when presented with cheese elsewhere, the RL agent navigated
to the top right instead of to the cheese: it had learned the mesa objective of "go to the top
right". 
For work done on (2), see OOD Generalization via Risk Extrapolation, an iterative improvement on
robustness to previous methods. 
I'm not sure what his motivation is for these speciﬁc research directions, but my guess is that
these are his best starts on how to solve the alignment problem. 
Opinion: I'm mixed on the Goal Misgeneralization. On one hand, it seems incredibly good to simply
be able to observe misaligned mesaoptimization in the wild. On the other hand, I feel like I didn't
learn much from the experiment because there are always many ways that the reward function
could vary OOD. So whatever mesa objective was learned this system would therefore be a
'misaligned mesaoptimizer' with respect to all of the other variations OOD. (The paper states that
goal misgeneralization is distinct from mesaoptimization, and I view this as mostly depending on
what your deﬁnition of optimizer is, but I think that I'd call pretty much any RL agent an
optimizer.) I view also view this as quite useful for getting more of academia interested in safety
concerns, which I think is critical for making AGI go well. 
I'm quite excited about understanding generalization better, particularly while thinking about goal
misgneralization, this seems like a very useful line of research. 
DeepMind
Updated after Rohin's comment . 
DeepMind has both a ML safety team focused on near-term risks, and an alignment team that is
working on risks from AGI. The alignment team is pursuing many diﬀerent research avenues, and
is not best described by a single agenda. 
Some of the work they are doing is: 
Engaging with recent MIRI arguments. 
Rohin Shah produces the alignment newsletter.
Publishing interesting research like the Goal Misgeneralization paper. 
Geoﬀrey Irving is working on debate as an alignment strategy: more detail here.
Discovering agents, which introduces a causal deﬁnition of agents, then introduces an
algorithm for ﬁnding agents from empirical data. 
Understanding and distilling threat models, e.g. "reﬁning the sharp left turn" and "will
capabilities generalize more". 
See Rohin's comment for more research that they are doing, including description of some that is
currently unpublished so far. 
Opinion:  I am negative about DeepMind as a whole because it is one of the organizations closest
to AGI, so DeepMind is incentivizing race dynamics and reducing timelines.
However, the technical alignment work looks promising and appears to be tackling diﬃcult
problems that I see as the core diﬃculties of alignment like inner alignment/sharp left turn
problems, as well as foundational research understanding agents. I am very excited about
DeepMind engaging with MIRI's arguments. 
Dylan Hadﬁeld-Menell
Dylan's PhD thesis argues three main claims (paraphrased): 

1. Outer alignment failures are a problem.
2. We can mitigate this problem by adding in uncertainty.
3. We can model this as Cooperative Inverse Reinforcement Learning (CIRL).  
Thus, his motivations seem to be modeling AGI coming in some multi-agent form, and also being
heavily connected with human operators. 
I'm not sure what he is currently working on, but some recent alignment-relevant papers that he
has published include: 
Work on instantiating norms into AIs to incentivize deference to humans. 
Theoretically formulating the principal-agent problem. 
Dylan has also published a number of articles that seem less directly relevant for alignment. 
Opinion: I don't see how this gets at the core problems in alignment, which I view as being related
to inner alignment / sharp left turn / distribution shift, and I wish that outer alignment approaches
also focused on robustness to distribution shift. How do you get this outer alignment solution into
the deep learning system? However, if there is a way to do this (with deep learning) I am super on
board and would completely change my mind. The more recent stuﬀ seems less relevant.
Encultured
See post.
Enclutured are making a multiplayer online video game as a test environment for AI: an aligned AI
should be able to play the game without ruining the fun or doing something obviously destructive
like completely taking over the world, even if it has this capabilities. This seems roughly analogous
to setting an AGI loose on the real world.
Motivation: Andrew Critch is primarily concerned about a multipolar AI scenario: there are multiple
actors with comparably powerful AI, on the cusp of recursive self improvement. The worst case is
a race, and even though each actor would want to take more time checking their AGI for safety,
worry that another actor will deploy will push each actor to take shortcuts and try to pull oﬀ a
world-saving act. Instead of working directly on AI, which can accelerate timelines and encourage
racing, creating this standardized test environment where alignment failures are observable is one
component of a good global outcome.
Opinion: I don't really see how this helps, on my models we mostly have one shot: by the time we
see the core diﬃculties of alignment, we are past the point of no return, because that AGI is
suﬃciently powerful to control the world. Therefore the alignment failures that we can observe in
this game seem like they won't give us useful information.  
On the other hand, this might be really helpful in slow takeoﬀ worlds where alignment failures are
obvious in the video game (e.g. deception isn't selected for), and these failures are relatively easy
to ﬁx.
Externalized Reasoning Oversight / Tamera
Lanham
Idea: Make the AGI, which is an LLM, externalize its reasoning (via chain of thought) so that its
thoughts are visible to an external overseer, which then veriﬁes that the LLM is not thinking
deceptive thoughts.
This scheme requires:
Not too much steganography: the model can't hide its reasoning in non-english logical
thought.  This is  the default of what would happen if we apply RL to this model maybe?

A safe overseer that can look through the chain of thought reasoning and identify misaligned
behavior. 
A key dynamic is that english is not the ideal language to be reasoning: an AGI could be more
capable by doing some reasoning not in English. Thus, applying RL to a model exerts pressure
towards ideal logical reasoning. However, self-supervised learning (SSL) exerts some pressure
back towards just doing text prediction, and hence more of its reasoning being in English. A key
question for this research agenda is thus how to put pressure to keep the reasoning externalized,
instead of collapsing into internal, more eﬃcient/logical reasoning that is much harder for us to
oversee. 
 
A rough visualization of the pressure of RL and SSL. Key
question: how to make the green arrow stronger? 
Opinion: This seems like it gets us a little further because we can now apply some RL, which might
cause deception that is now catchable, so long as the RL doesn't push it so that the chain of
thought itself is steganographic and hence not understandable to humans.
Also, catching the deception is only a small part of the battle: in worlds where deception is a big
problem, we need to ﬁnd ways to optimize against it, and this only gives us a small number of bits
of optimization pressure.
Overall, this seems worth exploring more, I'm generally excited about more theoretical
approaches on the above issues, as well as doing experiments about whether deception is a
problem by default for chain of thought LLMs, which would give information on how likely
deception is in general.
One problem ( identiﬁed by John Wentworth ) with this strategy is that the AI might cause large
bad side eﬀects, like wiping out humans without intent, simply because the AGI wants to use the

earth for something else, and this won't trigger the oversight schemes. This seems like a problem,
but this project still is clear progress towards solving alignment. 
Eli's opinion: Seems promising, though signiﬁcantly more promising on shorter timelines where
something like current LLMs scale to AGI.
Future of Humanity Institute (FHI) 
FHI does a lot of work on non-technical AI safety, but as far as I know their primary technical
agenda is the Causal incentives group (joint between FHI and DeepMind), who uses notions from
causality to study incentives and their application to AI Safety. Recent work includes: 
Agent Incentives: A Causal Perspective, a paper which formalizes concepts such as the value
of information and control incentives. 
Reward tampering problems and solutions in reinforcement learning: A causal inﬂuence
diagram perspective, a paper which theoretically analyzes wireheading. 
Opinion: I haven't read this research in depth, but I don't understand the focus on causality as
necessarily the best way to do theory work on alignment.  
Fund For Alignment Research (FAR)
Description copied from a comment by board member Adam Gleave
FAR's theory of change is to incubate new, scalable alignment research agendas. Right now I see a
small range of agendas being pursued at scale (largely RLHF and interpretability), then a long tail
of very diverse agendas being pursued by single individuals (mostly independent researchers or
graduate students) or 2-3 person teams. I believe there's a lot of valuable ideas in this long tail
that could be scaled, but this isn't happening due to a lack of institutional support. It makes sense
that the major organisations want to focus on their own speciﬁc agendas -- there's a beneﬁt to
being focused! -- but it means a lot of valuable agendas are slipping through the cracks.
FAR's current approach to solving this problem is to build out a technical team (research
engineers, junior research scientists, technical communication specialists) and provide support to
a broad range of agendas pioneered by external research leads. Those that work, FAR will double
down on and invest more in. This model has had a fair amount of demand already so there's
product-market ﬁt, but we still want to iterate and see if we can improve the model. For example,
long-term FAR might want to bring some or all research leads in-house.
In terms of concrete agendas, an example of some of the things FAR is working on:
Adversarial attacks against narrowly superhuman systems like AlphaGo.
Language model benchmarks for value learning.
The inverse scaling law prize.
You can read more about FAR in their launch post.
Eli's opinion: New, scalable alignment research agendas seem great to me in theory. I don't have
strong opinions on the concrete agendas FAR is working on thus far; they seem interesting but
hard to evaluate their usefulness without more deeply understanding the motivations (I skimmed
the inverse scaling justiﬁcation and felt a bit skeptical/unconvinced of direct usefulness, though it
might be pretty useful for ﬁeld-building).
MIRI
MIRI thinks technical alignment is really hard, and that we are very far from a solution. However,
they think that policy solutions have even less hope. Generally, I think of their approach as
supporting a bunch of independent researchers following their own directions, hoping that one of

them will ﬁnd some promise. They mostly buy into the security mindset: we need to know exactly
(probably mathematically formally) what we are doing, or the massive optimization pressure will
default in ruin.
Opinion: I wish that they would try harder to e.g. do more mentorship, do more speciﬁc 1-1
outreach, do more expansion, and try more diﬀerent things. I also wish they would communicate
more and use less metaphors: instead of a dialogue about rockets, explain why alignment needs
math. Why this reference class instead of any other?
Now I'll list some of the individual directions that they are pursuing:
Communicate their view on alignment
Recently they've been trying to communicate their worldview, in particular, how incredibly doomy
they are, perhaps in order to move other research eﬀorts towards what they see as the hard
problems.
2021 MIRI Conversations 
2022 MIRI Alignment Discussion
Opinion: I am glad that they are trying to communicate this, because I think it caused a lot of
people to think through their plans to deal with these diﬃculties. For example, I really liked the
DeepMind alignment team's response  to AGI ruin.
Eli's opinion: I also really appreciate these posts. I generally think they are pointing at valuable
considerations, diﬃculties, etc. even if I'm substantially more optimistic than them about the
potential to avoid them (my p(doom) is ~45%).
Deception + Inner Alignment / Evan Hubinger
I don't know a lot about this. Read Evan's research agenda for more information.
It seems likely that deceptive agents are the default, so a key problem in alignment is to ﬁgure out
how we can avoid deceptive alignment at every point in the training process. This seems to rely
on being able to consistently exert optimization pressure against deception, which probably
necessitates interpretability tools.
His plan to do this right now is acceptability veriﬁcation: have some predicate that precludes
deception, and then check your model for this predicate at every point in training. 
One idea for this predicate is making sure that the agent is myopic, meaning that the AI only cares
about the current timestep, so there is no incentive to deceive, because the beneﬁts of deception
happen only in the future. This is operationalized as "return the action that your model of HCH
would return, if it received your inputs."
Opinion: I think this is a big problem that it would be good to have more understanding of, and I
think the primary bottleneck for this is understanding the inductive biases of neural networks.
I'm skeptical of the speed prior or myopia is the right way of getting around this. The speed prior
seems to take a large capabilities hit because the speed prior is not predictive about the real
world. Myopia seems weird to me overall because I don't know what we want to do with a myopic
agent: I think a better frame on this is simulacra theory.  
Eli's opinion: I haven't engaged much with this but I feel intuitively skeptical of the myopia stuﬀ;
Eliezer's arguments in this thread  seem right to me.
Agent Foundations / Scott Garrabrant and Abram
Demski

They are working on fundamental problems like embeddedness, decision theory, logical
counterfactuals, and more. A big advance was Cartesian Frames, a formal model of agency.
Opinion: This is wonderful work on deconfusing fundamental questions of agency. It doesn't seem
like this will connect with the real world in time for AGI, but it seems like a great building block.
Infra-Bayesianism / Vanessa Kosoy
See Vanessa's research agenda for more detail. 
If we don't know how to do something given unbounded compute, we are just confused about the
thing. Going from thinking that chess was impossible for machines to understanding minimax was
a really good step forward for designing chess AIs, even though minimax is completely intractable.
Thus, we should seek to ﬁgure out how alignment might look in theory, and then try to bridge the
theory-practice gap by making our proposal ever more eﬃcient.The ﬁrst step along this path is to
ﬁgure out a universal RL setting that we can place our formal agents in, and then prove regret
bounds in.
A key problem in doing this is embeddedness. AIs can't have a perfect self model — this would be
like imagining your ENTIRE brain, inside your brain. There are ﬁnite memory constraints. Infra-
Bayesianism (IB) is essentially a theory of imprecise probability that lets you specify local / fuzzy
things. IB allows agents to have abstract models of themselves, and thus works in an embedded
setting.
Infra-Bayesian Physicalism (IBP) is an extension of this to RL. IBP allows us to
Figure out what agents are running [by evaluating the counterfactual where the computation
of the agent would output something diﬀerent, and see if the physical universe is diﬀerent].
Give a program, classify it as an agent or a non agent, and then ﬁnd its utility function.
Vanessa uses this formalism to describe PreDCA, an alignment proposal based on IBP. This
proposal assumes that an agent is an IBP agent, meaning that it is an RL agent with fuzzy
probability distributions (along with some other things). The general outline of this proposal is as
follows:
1. Find all of the agents that preceded the AI
2. Discard all of these agents that are powerful / non-human like
3. Find all of the utility functions in the remaining agents
4. Use combination of all of these utilities as the agent's utility function
Vanessa models an AI as a model based RL system with a WM, a reward function, and a policy
derived from the WM + reward. She claims that this avoids the sharp left turn. The generalization
problems come from the world model, but this is dealt with by having an epistemology that
doesn't contain bridge rules, and so the true world is the simplest explanation for the observed
data.
It is open to show that this proposal also solves inner alignment, but there is some chance that it
does.
This approach deviates from MIRI's plan, which is to focus on a narrow task to perform the pivotal
act, and then add corrigibility. Vanessa instead tries to directly learn the user's preferences, and
optimize those.
Opinion: Seems quite unlikely (<10%) for this type of theory to connect with deep learning in time
(but this is based on median ~10 year timelines, I'm a lot more optimistic about this given more
time). On the other hand, IB directly solves non-realizability, a core problem in embedded agency. 
Also, IBP has some weird conclusions like the monotonicity principle : the AI's utility function has
to be increasing in the number of computations running, even if these computations would involve
the experience of suﬀering. 

In the worlds the solution to alignment requires formally understanding AIs, Infra-Bayesianism
seems like by far the most promising research direction that we know of. I am excited about doing
more research in this direction, but also I'm excited for successor theories to IBP. 
Visible Thoughts Project
This project is to create a language dataset where the characters think out loud a lot, so that when
we train an advanced LLM on this dataset, it will be thinking out loud and hence interpretable.
Opinion: Seems reasonable enough.
Jacob Steinhardt 
Jacob Steinhardt is a professor at UC Berkeley who works on conceptual alignment. He seems to
have a broad array of research interests, but with some focus on robustness to distribution shift. 
A technical paper he wrote is Certiﬁed defenses against adversarial examples: a technique for
creating robust networks in the sense that an adversary has to shift the input image by some
constant in order to cause a certain drop in test performance. He's also researched the mechanics
of distribution shift, and found that diﬀerent distribution shifts induce diﬀerent robustness
performance. 
He's published several technical overviews including AI Alignment Research Overview
and Concrete problems in AI safety, and created an AI forecasting competition. 
Opinion: I think it is very valuable to develop a better understanding of distribution shifts, because
I view that at the heart of the diﬃculty of alignment.  
OpenAI
The safety team at OpenAI's plan is to build a MVP aligned AGI that can help us solve the full
alignment problem.
They want to do this with Reinforcement Learning from Human Feedback (RLHF): get feedback
from humans about what is good, i.e. give reward to AI's based on the human feedback. Problem:
what if the AI makes gigabrain 5D chess moves that humans don't understand, so can't evaluate.
Jan Leike, the director of the safety team, views this (the informed oversight problem) as the core
diﬃculty of alignment. Their proposed solution: an AI assisted oversight scheme, with a recursive
hierarchy of AIs bottoming out at humans. They are working on experimenting with this approach
by trying to get current day AIs to do useful supporting work such as summarizing books and
criticizing itself.
OpenAI also published GPT-3, and are continuing to push LLM capabilities, with GPT-4 expected to
be released at some point soon.
See also: Common misconceptions about OpenAI and Our approach to alignment research. 
Opinion: This approach focuses almost entirely on outer alignment, and so I'm not sure how this is
planning to get around deception. In the unlikely world where deception isn't the default / inner
alignment happens by default, this alignment plan seems and complicated and therefore
vulnerable to the godzilla problem . I also think it relies on very slow takeoﬀ speeds: the
misaligned AGI has to help us design a successor aligned AGI, and this only works when the AGI
doesn't recursively self improve to superintelligence. [5]
Overall, the dominant eﬀect of OpenAI pushes is that they advance capabilities, which seems
really bad because it shortens AGI timelines.  

Eli's opinion: Similar to Thomas. I'm generally excited about trying to automate alignment
research, but relatively more positive on Conjecture's approach since it aims to use non-agentic
systems.
Ought
Ought aims to automate and scale open-ended reasoning through Elicit, an AI research assistant.
Ought focuses on advancing process-based systems rather than outcome-based ones, which they
believe to be both beneﬁcial for improving reasoning in the short term and alignment in the long
term. Here they argue that in the long run improving reasoning and alignment converge.
So Ought's impact on AI alignment has 2 components: (a) improved reasoning of AI governance &
alignment researchers, particularly on long-horizon tasks and (b) pushing supervision of process
rather than outcomes, which reduces the optimization pressure on imperfect proxy objectives
leading to "safety by construction". Ought argues that the race between process and outcome-
based systems is particularly important because both states may be an attractor.
Eli's opinion (Eli used to work at Ought): I am fairly optimistic about (a), the general strategy of
applying AI in a diﬀerentially helpful way for alignment via e.g. speeding up technical alignment
research or improving evaluations of alignment-aimed policies. Current Elicit is too broadly
advertised to all researchers for my taste; I'm not sure whether generally speeding up science is
good or bad. I'm excited about a version of Elicit that is more narrowly focused on helping with
alignment (and perhaps longtermist research more generally), which I hear may be in the works.
I'm less optimistic about (b) pushing process-based systems, it seems like a very conjunctive
theory of change to me. It needs to go something like: process-based systems are competitive
with end-to-end training for AGI (which may be unlikely, see e.g. Rohin's opinion here ; and my
intuition is that HCH  doesn't work), process-based systems are substantially more aligned than
end-to-end training while being as capable (feels unlikely to me; even if it helps a bit it probably
needs to help a lot to make the diﬀerence between catastrophe and not ), and Ought either builds
AGI or strongly inﬂuences the organization that builds AGI. Oﬀ the cuﬀ I'd give something like
10%, 3%, 1% for these respectively (conditioned on the previous premises) which multiplies to
.003%; this might be ﬁne given the stakes (it's estimated to be worth $17-$167M via .01% Fund
proposal  assuming AI x-risk is 50%) but it doesn't feel like one of the most promising approaches.
Redwood Research
Adversarial training
The following diagram describing an adversarial oversight scheme to do for aligning an RL agent:

Motivation: The point of this approach is to create extremely reliable AI where it will never engage
in certain types of behavior, for example, killing all humans, or deceiving its creators. A practice
problem is to get any kind of behavior extremely reliably out of current day LLMs. The way
Redwood operationalized this is by trying to train an LLM to have the property that they ﬁnish the
prompt such that no humans get hurt (technically slightly weaker than this — only the AI believes
no one gets hurt).  
Opinion: This seems like a reasonable applied alignment project because it is a technique that
could solve alignment for worlds in which alignment is pretty easy: if we just train the model to not
hurt people, not take over the world, etc, then it'll just work. More speciﬁcally, we use either
humans or ampliﬁed humans to get the description of the bad thing, then train a model that can
very reliably simulate this description (and is much quicker to query than a human). Then, when
we train the AI agent, we use this model to make sure that the AI never does the bad thing.
My worry with adversarial training is that it might just be pushing around the inner misalignment
somewhere else in the model. In the real case there are instrumental incentives for power seeking
to push against this technique, so even if the failure rate is 1 in a trillion, there will be strong
optimization pressure, and a large distribution shift that will break this learned invariant.
LLM interpretability
Redwood is also doing some work on interpretability tools, though as far as I know they have not
published a writeup of their interpretability results. As of April, they were focused on getting a
complete understanding of nontrivial behaviors of relatively small models. They have released a
website for visualizing transformers. Apart from the standard beneﬁts of interpretability, one
possibility is that this might be helpful for solving ELK.
Opinion: Excited to see the results from this. 
Sam Bowman 

Sam runs a lab at NYU. He is on sabbatical working at Anthropic for the 2022-2023 academic year,
and has already been collaborating with them.  
Projects include language model alignment by creating datasets for evaluating language models,
as well as inductive biases of LLMs. 
He is involved in running the inverse scaling prize in collaboration with FAR, a contest for ﬁnding
tasks where larger language models perform worse than smaller language models. The idea of this
is to understand how LLMs are misaligned, and ﬁnd techniques for uncovering this misalignment. 
Opinion: I don't understand this research very well, so I can't comment.
Selection Theorems / John Wentworth
John's plan is:
Step 1: sort out our fundamental confusions about agency
Step 2: ambitious value learning (i.e. build an AI which correctly learns human values and
optimizes for them)
Step 3: ...
Step 4: proﬁt!
... and do all that before AGI kills us all.
He is working on step 1: ﬁguring out what the heck is going on with agency. His current approach
is based on selection theorems: try to ﬁgure out what types of agents are selected for in a broad
range of environments. Examples of selection pressures include: evolution, SGD, and markets.
This is an approach to agent foundations that comes from the opposite direction as MIRI: it's more
about observing existing structures (whether they be mathematical or real things in the world like
markets or e coli), whereas MIRI is trying to write out some desiderata and then ﬁnding
mathematical notions that satisfy those desiderata.
Two key properties that might be selected for are modularity and abstractions.
Abstractions are higher level things that people tend to use to describe things. Like "Tree" and
"Chair" and "Person". These are all vague categories that contain lots of diﬀerent things, but are
really useful for narrowing down things. Humans tend to use really similar abstractions, even
across diﬀerent cultures / societies. The Natural Abstraction Hypothesis (NAH) states that a wide
variety of cognitive architectures will tend to use similar abstractions to reason about the world.
This might be helpful for alignment because we could say things like "person" without having to
rigorously and precisely say exactly what we mean by person.
The NAH seems very plausibly true for physical objects in the world, and so it might be true for the
inputs to human values. If so, it would be really helpful for AI alignment because understanding
this would amount to a solution to the ontology identiﬁcation problem: we can understand when
environments induce certain abstractions, and so we can design this so that the network has the
same abstractions as humans.
Opinion: I think that understanding abstraction seems like a promising research direction, both for
current neural networks, as well as agent foundations problems . 
I also think that the learned abstractions are highly dependent on the environment, and that the
training environment for an AGI might look very diﬀerent than the the learning environment that
humans follow. 
A very strong form of the Natural Abstraction Hypothesis might even hold for the goals of agents,
for example, maybe cooperation is universally selected for by iterated prisoners dilemmas.

Modularity: In pretty much any selection environment, we see lots of obvious modularity.
Biological species have cells and organs and limbs. Companies have departments. We might
expect neural networks to be similar, but it is really hard to ﬁnd modules in neural networks. We
need to ﬁnd the right lens to look through to ﬁnd this modularity in neural networks. Aiming at this
can lead us to really good interpretability.
Opinion: Looking for modules / frames on modularity in neural networks seems like a promising
way to get scalable interpretability, so I'm excited about this.
Team Shard
Humans care about things! The reward circuitry in our brain reliably causes us to care about
speciﬁc things. Let's create a mechanistic model of how the brain aligns humans, and then we can
use this to do AI alignment. 
One perspective that Shard theory has added is that we shouldn't think of the solution to
alignment as: 
1. Find an outer objective that is ﬁne to optimize arbitrarily strongly 
2. Find a way of making sure that the inner objective of an ML system equals the outer
objective.
Shard theory argues that instead we should focus on ﬁnding outer objectives that reliably give
certain inner values into system and should be thought of as more of a teacher of the values we
want to instill as opposed to the values themselves. Reward is not the optimization target —
instead, it is more like that which reinforces. People sometimes refer to inner aligning an RL agent
with respect to the reward signal, but this doesn't actually make sense. (As pointed out in the
comments this is not a new insight, but it was for me phrased a lot more clearly in terms of Shard
theory). 
Humans have diﬀerent values than the reward circuitry in our brain being maximized, but they are
still pointed reliably. These underlying values cause us to not wirehead with respect to the outer
optimizer of reward.
Shard Theory points at the beginning of a mechanistic story for how inner values are selected for
by outer optimization pressures. The current plan is to ﬁgure out how RL induces inner values into
learned agents, and then ﬁgure out how to instill human values into powerful AI models (probably
chain of thought LLMs, because these are the most intelligent models right now). Then, use these
partially aligned models to solve the full alignment problem. Shard theory also proposes a
subagent theory of mind.
This has some similarities to Brain-like AGI Safety, and has drawn on some research from this post,
such as the mechanics of the human reward circuitry as well as the brain being mostly randomly
initialized at birth.
Opinion: This is promising so far, deserves a lot more work to be done on it to try to ﬁnd a reliable
way to implant certain inner values into trained systems. I view Shard Theory as a useful frame for
alignment already even if it doesn't go anywhere else.
Eli's opinion: I'm not sure I agree much with the core claims of Shard Theory, but the research that
I've seen actually being proposed based on it generally seems useful.
Truthful AI / Owain Evans and Owen Cotton-
Barratt
Truthful AI is a research direction to get models to avoid lying to us. This involves developing 1)
clear truthfulness standards e.g. avoiding "negligent falsehoods", 2) institutions to enforce
standards, and 3) truthful AI systems e.g. via curated datasets and human interaction.

See also Truthful LMs as a warm-up for aligned AGI and TruthfulQA: Measuring How Models Mimic
Human Falsehoods. 
Edit for clarity: According to Owen, TruthfulAI is not trying to solve the hard bit of the alignment
problem. Instead, the goal of this approach is to improve society to generally be more able to
solve hard challenges such as alignment. 
Opinion: It seems like the root problem is that deception is an instrumental subgoal of powerful
inner misaligned AGIs, so we need to ﬁx that in order to solve alignment. I'm confused about how
this helps society solve alignment, but that is probably mostly because I don't know much about
this. 
Eli's opinion: This seems like a pretty indirect way to tackle alignment of powerful AIs; I think it's
mostly promising insofar as it helps us more robustly automate technical and strategy alignment
research.
Other Organizations
Though we have tried to be exhaustive, there are certainly many people working on technical AI
alignment not included in this overview. While these groups might produce great research, we
either 1) didn't know enough about it to summarize or 2) weren't aware that it was aimed at
reducing x-risk from AI. 
Below is a list of some of these, though we probably missed some here. Please feel free to add
comments and I will add others.  
Future of Life Institute (FLI) (Though they seem to mostly give out grants)
Many independent researchers. 
A number of academics: 
Percy Liang (Stanford)
Roger Grosse (UofT)
Aleksander Madry (MIT)
Scott Neikum (UMass Amherst)
*ERIs: 
Berkeley Existential Risk Initiative (BERI) 
Stanford Existential Risk Initiative (SERI) 
Swiss Existential Risk Initiative (CHERI) 
Principles of Intelligent Behavior in Biological and Social Systems (PIBSS)
Alignment of Complex Systems Research Group
Appendix
Visualizing Diﬀerences
Automating alignment and alignment diﬃculty
Much of the diﬀerence between approaches comes down to disagreements on two axes: how
promising it is to automate alignment vs. solve it directly and how diﬃcult alignment is
(~p(doom)). We'll chart our impression of where orgs and approaches stand on these axes, based
on something like the median view of org/approach leadership. We leave out orgs/approaches that
we have no idea about, but err on the side of guessing if we have some intuitions (feel free to
correct us!).

Don't share this chart too widely e.g. on Twitter, as these views haven't yet
been endorsed by the organizations/people. These are our best guesses and
nothing more.
Conceptual vs. applied
Another important axis distinguishing approaches is conceptual (i.e. thinking about how to align
powerful AI) vs. applied (i.e. experimenting with AI systems to learn about how to align powerful
AI).
Type of
approach
Mostly
conceptual
Mixed
Mostly applied
Organization MIRI, John
Wentworth, ARC
Team Shard,
CHAI, DeepMind
Conjecture, Encultured, OpenAI,
Anthropic, Redwood, Ought
Thomas's Alignment Big Picture
This is a summary of my overall picture of the AGI landscape, which is a high level picture of the
generator of most my opinions above.
I buy most of the arguments given in Without speciﬁc countermeasures, the easiest path to
transformative AI likely leads to AI takeover and AGI Ruin, A List of Lethalities. 
To make AGI go well, many organizations with economic and social incentives to make AGI before
alignment is solved must avoid doing so. Moreover, the majority of the AI community does not
take x-risk from AGI seriously. Whatever it is that moves us from our current trajectory to a safe-
from-AGI-world, in my eyes, would count as a pivotal act[6] — an act that makes a large positive
impact on the world a billion years in the future. A pivotal act might also consist of lots of smaller
actions that together make a diﬀerence in the long term, then all these acts together could
constitute a pivotal act.
Performing a pivotal act without AI is likely really hard, but could look something like transforming
the world's governments into Dath Ilan. If a non-AI pivotal act is feasible, I think it is probably best
for us to cease all AIS research, and then just work towards performing that pivotal act. AIS

research usually contributes at least something to capabilities, and so stopping that buys more
time (so long as the AIS researchers don't start doing capabilities directly instead).
The other option is an AI-assisted pivotal act. This pivotal act might look like making a very
aligned AI recursively self improve, and implement CEV. It might be more like the MIRI example
of melting GPUs to prevent further AI progress and then turning itself oﬀ, but this isn't meant to be
a concrete plan. MIRI seems to think that this is the path with the highest odds of success — see
the strategic background section here.
The alignment tax is the amount of extra eﬀort it will take for an AI to be aligned. This includes
both research eﬀort as well as compute and engineering time. I think it is quite likely (~80%) that
part of the alignment tax for the ﬁrst safe AGI requires >1 year of development time. 
Pulling oﬀ an AGI-assisted pivotal act requires aligning the AGI you are using, meaning that:
1. The alignment tax to be low enough for an actor to pay
2. That the actor implementing the ﬁrst AGI pays this alignment tax
3. 1 and 2 need to happen before the world is destroyed.
Technical alignment work is primarily focused on 1. Some organizations are also trying to be large
enough and ahead to pay the alignment tax for an AGI. 
My model of EA and AI Safety is that it is expanding exponentially, and that this trend will likely
continue until AGI. This leads me to think that the majority of the AIS work will be done right
before AGI. It also means that timelines are crucial — pushing timelines back is very important.
This is why I am usually not excited about work that improves capabilities even if it does more to
help safety.[7]
In the world where we have to align an AGI, solving the technical alignment problem seems like a
big bottleneck, but cooperation to slow down timelines is necessary in both worlds. 
1. ^
We may revise the document based on corrections in the comments or future
announcements, but don't promise anything. Others are welcome to create future versions
or submit summaries of their own approaches for us to edit in. We will note the time it was
last edited when we edit things. (ETA: most recent update: 10/9/2022) 
2. ^
 In this chart, the ? denotes more uncertainty if this is a correct description
3. ^
I would appreciate someone giving more information on DeepMind's approach to alignment.
Update: Rohin has given a helpful summary in a comment. 
4. ^
Technically, they just need to span the set of extrapolations, so that the correct extrapolation
is just a linear combination of the found classiﬁers. 
5. ^
Hold on, how come you are excited about Conjecture automating alignment research but not
OpenAI? 
Answer: I see a categorical distinction between trying to align agentic and oracle AIs.
Conjecture is trying only for oracle LLMs, trained without any RL pressure giving them goals,
which seems way safer. OpenAI doing recursive reward modeling / IDA type schemes
involves creating agentic AGIs and therefore faces also a lot more alignment issues like
convergent instrumental goals, power seeking, goodharting, inner alignment failure, etc. 

I think inner alignment can be a problem with LLMs trained purely in a self-supervised
fashion (e.g., simulacra becoming aware of their surroundings), but I anticipate it to only be
a problem with further capabilities. I think RL trained GPT-6 is a lot more likely to be an x-risk
than GPT-6 trained only to do text prediction.
6. ^
To be clear: I am very against proposals for violent pivotal acts that are sometimes brought
up, such as destroying other AI labs on the verge of creating a misaligned AGI. This seems
bad because 1) violence is bad and isn't digniﬁed. 2) it seems like this intention would make
it much harder to coordinate. 3) Setting an AGI loose to pull oﬀ a violent pivotal act could
incredibly easily disempower humanity: you are intentionally letting the AGI destructively
take over.
7. ^
Some cruxes that would change this conclusion are if we don't get prosaic AGI or if solving
alignment takes a lot of serial thought, e.g. work that needs to be done by 1 researcher over
10 years, and can't be solved by 10 researchers working for 1 year.

DeepMind alignment team opinions on
AGI ruin arguments
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
We had some discussions of the AGI ruin arguments within the DeepMind alignment
team to clarify for ourselves which of these arguments we are most concerned about
and what the implications are for our work. This post summarizes the opinions of a
subset of the alignment team on these arguments. Disclaimer: these are our own
opinions that do not represent the views of DeepMind as a whole or its broader
community of safety researchers.
This doc shows opinions and comments from 8 people on the alignment team (without
attribution). For each section of the list, we show a table summarizing agreement /
disagreement with the arguments in that section (the tables can be found in
this sheet). Each row is sorted from Agree to Disagree, so a column does not
correspond to a speciﬁc person. We also provide detailed comments and clariﬁcations
on each argument from the team members. 
For each argument, we include a shorthand description in a few words for ease of
reference, and a summary in 1-2 sentences (usually copied from the bolded parts of
the original arguments). We apologize for some inevitable misrepresentation of the
original arguments in these summaries. Note that some respondents looked at the
original arguments while others looked at the summaries when providing their opinions
(though everyone has read the original list at some point before providing opinions). 
A general problem when evaluating the arguments was that people often agreed with
the argument as stated, but disagreed about the severity of its implications for AGI
risk. A lot of these ended up as "mostly agree / unclear / mostly disagree" ratings. It
would have been better to gather two separate scores (agreement with the statement
and agreement with implications for risk).
Summary of agreements, disagreements and
implications 
Most agreement:
Section A ("strategic challenges"): #1 (human level is nothing special), #2
(unaligned superintelligence could easily take over), #8 (capabilities generalize
out of desired scope)
Section B1 (distributional leap): #14 (some problems only occur in
dangerous domains)
Section B2 (outer/inner alignment): #16 (inner misalignment), #18 (no
ground truth), #23 (corrigibility is anti-natural)
Section B3 (interpretability): #28 (large option space)
Section B4 (miscellaneous): #36 (human ﬂaws make containment diﬃcult)
Most disagreement:

#6 (pivotal act is necessary). We think it's necessary to end the acute risk
period, but don't agree with the "pivotal act" framing that assumes that the risk
period is ended through a discrete unilateral action by a small number of actors. 
#24 (sovereign vs corrigibility). We think this kind of equivocation isn't
actually happening much in the alignment community. Our work focuses on
building corrigible systems (rather than sovereigns), and we expect that the
diﬃculties of this approach could be surmountable, especially if we can ﬁgure out
how to avoid building arbitrarily consequentialist systems. 
#39 (can't train people in security mindset). Most of us don't think it's
necessary to generate all the arguments yourself in order to make progress on
the problems. 
#42 (there's no plan). The kind of plan we imagine Eliezer to be thinking of
does not seem necessary for a world to survive.
Most controversial among the team:
Section A ("strategic challenges"): #4 (can't cooperate to avoid AGI), #5
(narrow AI is insuﬃcient), #7 (no weak pivotal acts), #9 (pivotal act is a
dangerous regime)
Section B1 (distributional leap): #13 and 15 (problems above intelligence
threshold and correlated capability gains)
Section B2 (outer/inner alignment): #17 (inner properties), #21 (capabilities
go further), #22 (simple alignment core)
Section B3 (interpretability): #30 (powerful vs understandable), #32
(language is insuﬃcient), #33 (alien concepts)
Section B4 (miscellaneous): #35 (multi-agent is single-agent)
Section C ("civilizational inadequacy"): #38 (lack of focus), #41 (have to
write this list), #43 (unawareness of risks)
Cruxes from the most controversial arguments:
How powerful does a system / plan need to be to save the world?
Is global cooperation suﬃciently diﬃcult that AGI would need to deploy new
powerful technology to make it work?
Will we know how capable our models are? 
Will capabilities increase smoothly?
Will systems acquire the capability to be useful for alignment / cooperation before
or after the capability to perform advanced deception? 
Is consequentialism a powerful attractor? How hard will it be to avoid arbitrarily
consequentialist systems?
What is the overhead for aligned vs not-aligned AGI? 
Possible implications for our work:
Work on cooperating to avoid unaligned AGI (compute governance, publication
norms, demonstrations of misalignment, etc)
Investigate techniques for limiting unaligned consequentialism, e.g. process-
based feedback, limiting situational awareness, limited domains
Work on improving capability monitoring and control
Empirically investigate to what extent selection for undetectability / against
interpretability occurs in practice as systems become more capable
Continue and expand our work on mechanistic interpretability and process-based
feedback

Section A (shorthand: "strategic
challenges")
Summary
Detailed comments
#1. Human level is nothing special / data eﬃciency
Summary: AGI will not be upper-bounded by human ability or human learning speed
(similarly to AlphaGo).  Things much smarter than human would be able to learn from
less evidence than humans require.
Agree (though don't agree with the implication that it will be discontinuous)
Agree (strongly, and possibly a major source of disagreement with broader ML
community)
#2. Unaligned superintelligence could easily take over
Summary: A cognitive system with suﬃciently high cognitive powers, given any
medium-bandwidth channel of causal inﬂuence, will not ﬁnd it diﬃcult to bootstrap to
overpowering capabilities independent of human infrastructure. 
Agree (including using human institutions and infrastructure for its own ends)
Agree ("suﬃciently high" is doing a lot of the work here)
Mostly agree - it depends a lot on what "medium-bandwidth" means, and also
how carefully the system is being monitored (e.g. how we're training the system
online). I think "text-only channel, advising actions of a lot of people who tend to
defer to the system" seems like probably enough -> takeover if we're not being
that careful. But I think I probably disagree with the mechanism of takeover
intended by Yudkowsky here.
#3. Can't iterate on dangerous domains
Summary: At some point there will be a 'ﬁrst critical try' at operating at a 'dangerous'
level of intelligence, and on this 'ﬁrst critical try', we need to get alignment right. 
(x2) Mostly agree (misleading, see Paul's Disagreement #1) 
Mostly agree. It's not clear that attempting alignment and failing will necessarily
be as irrecoverable as unaligned operation, but does seem very likely. If
"dangerous" just means failing implies extinction then this statement is a truism.
Mostly agree (though "get alignment right on the ﬁrst critical try" may lean
heavily on "throttle capability until we get alignment right")

#4. Can't cooperate to avoid AGI
Summary: The world can't just decide not to build AGI.
Unclear. I think this depends a lot on what exactly happens in the real world in
the next decade or so, and we might be able to buy time.
Unclear. Agree that worldwide cooperation to avoid AGI would be very diﬃcult,
but cooperation between Western AI labs seems feasible and could potentially be
suﬃcient to end the acute risk period or buy more time (especially under short
timelines). 
Unclear. I agree it is very hard and unlikely, but extreme things can happen in
global politics given suﬃciently extreme circumstances, and cooperation can do
much to shape the pace and direction of tech development. 
Mostly disagree (more on principle than via seeing a stable solution; agree that
"just" deciding not to build AGI doesn't work, but culture around AI could be
shifted somehow)
#5. Narrow AI is insuﬃcient
Summary: We can't just build a very weak system.
Agree (assuming #4)
Agree (this doesn't end the risk period)
Disagree (can potentially use narrow AI to help humans cooperate)
(x2) Disagree (more on principle - we should work on how to solve xrisk using
somewhat-narrow systems; to be clear, this a problem to be solved, rather than
something we "just do")
#6. Pivotal act is necessary
Summary: We need to align the performance of some large task, a 'pivotal act' that
prevents other people from building an unaligned AGI that destroys the world.
Mostly agree (but disagree connotationally, in that "act" sounds like a single fast
move, while an understandable human-timescale strategy is probably enough)
Unclear (it is necessary to end to acute risk period but it can be done by humans
rather than an AGI)
Unclear (dislike framing; seems tied up with ability to cooperate)
Disagree if a pivotal act is a "discrete, powerful, gameboard-ﬂipping action" (as
opposed to something that ends the acute risk period)
Disagree (strongly disagree with pivotal act. Brieﬂy, pivotal acts seem like the
wrong framing. 
It seems like "get people to implement stronger restrictions" or "explain
misalignment risks" or "come up with better regulation" or "diﬀerentially
improve alignment" are all better applications of an AGI than "do a pivotal
act". 
The pivotal act frame seems to be "there will be a tiny group of people who
will have responsibility for saving the world" but the reality seems to if
anything be closer to the opposite: there will be a tiny group of people that
wants to build a tremendously ambitious (and thus also dangerous) AGI, the
vast majority of the world would be in support of _not_ building such an AGI
(and instead building many more limited systems, which can still deliver
large amounts of wealth and/or value), and some set of people representing
the larger global population in getting people to not build/deploy said
dangerous AGI. This is an extension of the view that "AIs should be a major
inﬂuence on large sectors of society" is probably a fairly unpopular view
today already.

#7. There are no weak pivotal acts because a pivotal act requires power
Summary: It takes a lot of power to do something to the current world that prevents
any other AGI from coming into existence; nothing which can do that is passively safe
in virtue of its weakness. 
Agree (if by pivotal act we mean "discrete, powerful, gameboard-ﬂipping action")
Agree (if the bar is "prevent any other AGI from coming into existence")
Agree (with caveats about cooperation)
Disagree. This may have technical/engineering solutions that don't involve high
general-purpose agency, or may not require deploying narrow AI at all.
#8. Capabilities generalize out of desired scope
Summary: The best and easiest-found-by-optimization algorithms for solving problems
we want an AI to solve, readily generalize to problems we'd rather the AI not solve.
Agree (conditional on #5)
#9. A pivotal act is a dangerous regime
Summary: The builders of a safe system would need to operate their system in a
regime where it has the capability to kill everybody or make itself even more
dangerous, but has been successfully designed to not do that.
Agree (conditional on #5)
Agree (if by pivotal act we mean "discrete, powerful, gameboard-ﬂipping action")
Mostly disagree (because of underlying pivotal act framing). But agree that ML
systems will realistically be operating in dangerous regimes. I agree with
"Running AGIs doing something pivotal are not passively safe". I'd go further and
state that it's likely people will run AGIs doing non-pivotal acts which are
nonetheless unsafe. However, I disagree with the (I believe implied) claim that
"We should be running AGIs doing something pivotal" (under the author's notion
of "pivotal").
Disagree (human cooperation or humans assisted by narrow AI could end the
acute risk period without an AI system having dangerous capabilities)

Section B.1: The distributional leap
Summary
Detailed comments
#10. Large distributional shift to dangerous domains
Summary: On anything like the standard ML paradigm, you would need to somehow
generalize optimization-for-alignment you did in safe conditions, across a big
distributional shift to dangerous conditions.
#11. Sim to real is hard
Summary: There's no known case where you can entrain a safe level of ability on a
safe environment where you can cheaply do millions of runs, and deploy that capability
to save the world.
Agree (seems like an instance of #7)
Unclear. Agree that for many important tasks we aren't going to train in safe
environments with millions of runs, and in particular not simulated environments,
but disagree with underlying pivotal act frame
Mostly disagree (debate with interpretability could achieve this if it succeeds)
#12. High intelligence is a large shift
Summary: Operating at a highly intelligent level is a drastic shift in distribution from
operating at a less intelligent level.
Mostly disagree (this relies on the sharp left turn, which doesn't seem that likely
to me)
#13. Some problems only occur above an intelligence threshold
Summary: Many alignment problems of superintelligence will not naturally appear at
pre-dangerous, passively-safe levels of capability. 
Mostly agree - I think many alignment problems will appear before, and also
many won't appear until later (or at least, diﬀerences-in-degree will become
diﬀerences-in-kind)
Not sure (agree some problems would *naturally* ﬁrst arise for higher intelligence
levels, but we can seek out examples for less intelligent systems, e.g. reward
tampering & goal misgeneralization examples)
Mostly disagree (we'll get demos of problems; Eliezer seems to think this will be
hard / unlikely to help though doesn't say that outright, if so I disagree with that)
#14. Some problems only occur in dangerous domains
Summary: Some problems seem like their natural order of appearance could be that
they ﬁrst appear only in fully dangerous domains.

Mostly agree (misleadingly true: we can create analogous examples before the
ﬁrst critical try)
Mostly agree (but that doesn't mean we can't usefully study them in safe
domains)
#15. Capability gains from intelligence are correlated
Summary: Fast capability gains seem likely, and may break lots of previous alignment-
required invariants simultaneously. 
Agree (in particular, once we have ~human-level AI, both AI development and the
world at-large probably get very crazy very fast)
Mostly agree (this is my guess about the nature of intelligence, but I'm not sure
I'm right)
Mostly agree, fast need not imply discontinuous
Unclear (disagree on fast capability gains being likely, agree on breaking
invariants given fast gains)

Section B.2: Central diﬃculties of
outer and inner alignment.
Summary
Detailed comments 
#16. Inner misalignment
Summary: Outer optimization even on a very exact, very simple loss function doesn't
produce inner optimization in that direction.
Unclear - I strongly agree with the weaker claim that we don't get inner alignment
for free, but the claim here seems more false than not? Certainly, outer
optimization on most loss functions will lead to _more_ inner optimization in that
direction (empirically and theoretically)
#17. Can't control inner properties
Summary: On the current optimization paradigm there is no general idea of how to
get particular inner properties into a system, or verify that they're there, rather than
just observable outer ones you can run a loss function over.
Agree (at least currently, the requisite interpretability capabilities and/or
conceptual understanding of goal-directedness seems inadequate)
Mostly agree (though uncertain how much this is about the optimization
paradigm and how much it's about interpretability)
Mostly agree (counter: current interpretability)
(x2) Mostly disagree (interpretability could address this) 
#18. No ground truth (no comments)
Summary: There's no reliable Cartesian-sensory ground truth (reliable loss-function-
calculator) about whether an output is 'aligned'.
#19. Pointers problem
Summary: There is no known way to use the paradigm of loss functions, sensory
inputs, and/or reward inputs, to optimize anything within a cognitive system to point at
particular things within the environment.
Unclear - agreed that we have no principled way of doing this, but we also don't
have great examples of this not working, so depends on how strongly this is
intended. I don't think "high conﬁdence that this won't work" is justiﬁed.
Agree (modulo the shard theory objection to the strict reading of this)

#20. Flawed human feedback
Summary: Human raters make systematic errors - regular, compactly describable,
predictable errors. 
Mostly agree (misleading, a major hope is to have your oversight process be
smarter than the model, so that its systematic errors are not ones that the model
can easily exploit)
Unclear (agree denotationally, unclear whether we can build enough self-
correction on the most load-bearing parts of human feedback)
#21. Capabilities go further
Summary: Capabilities generalize further than alignment once capabilities start to
generalize far.
Agree (seems similar to #8 and #15)
Mostly agree (by default; but I think there's hope in the observation that
generalizing alignment is a surmountable problem for humans, so it might also be
for AGI)
#22. No simple alignment core
Summary: There is a simple core of general intelligence but there is no analogous
simple core of alignment.
Mostly agree (but something like "help this agent" seems like a fairly simple core)
Unclear (there may exist a system that has alignment as an attractor)
#23. Corrigibility is anti-natural.
Summary: Corrigibility is anti-natural to consequentialist reasoning.
Mostly agree (misleading, I agree with Paul's comment on "Let's see you write
that corrigibility tag")
Mostly agree (I think we can avoid building arbitrarily consequentialist systems)
Agree with the statement that corrigibility is anti-natural to consequentialist
reasoning. Yudkowsky's view seems to be that everything tends towards *pure*
consequentialism, and I disagree with that.
#24. Sovereign vs corrigibility
Summary: There are two fundamentally diﬀerent approaches you can potentially take
to alignment [a sovereign optimizing CEV or a corrigible agent], which are unsolvable
for two diﬀerent sets of reasons. Therefore by ambiguating between the two
approaches, you can confuse yourself about whether alignment is necessarily diﬃcult.
Unclear. Agree in principle (but disagree this is happening that much)
Mostly disagree (agree these are two distinct approaches that should not be
confused, disagree that people are confusing them or that they are unsolvable)
Mostly disagree - I agree that ambiguating between approaches is bad, but am
not sure who/what that refers to, and there seems to be some implicit "all the
approaches I've seen are non-viable" claim here, which I'd disagree with

Section B.3:  Central diﬃculties of
suﬃciently good and useful
transparency / interpretability.
Summary
Detailed comments
#25. Real interpretability is out of reach
Summary: We've got no idea what's actually going on inside the giant inscrutable
matrices and tensors of ﬂoating-point numbers. 
Agree - interpretability research has made impressive strides, and more than I
expected in 2017, but is still a far shot from understanding most of what's
happening inside the big mass of matmuls
Mostly agree (misleading, we plausibly do better in the future)
Mostly agree (we know a little now and not sure if we'll know more or less later)
Unclear (agree we don't have much idea right now, but I think we can develop
better understanding, main uncertainty is whether we can do this in time)
#26. Interpretability is insuﬃcient
Summary: Knowing that a medium-strength system of inscrutable matrices is
planning to kill us, does not thereby let us build a high-strength system that isn't
planning to kill us.
Mostly agree (misleading). A major hope is that (by using interpretability to carry
out oversight) you simply don't get the medium strength system that is planning
to kill you.
Mostly agree. Interpretability is helpful for cooperation, and also helps to train a
system that doesn't kill you if #27 doesn't hold.
Mostly agree - on it's own, seems correct, but the tone also understates that it
would *massively* change the position we are in. If interpretability conclusively
revealed that one of the existing prototypes was trying to kill us, that would
dramatically change the {ML, global, AGI lab} conversation around
xrisk/alignment
Mostly agree. Agree denotationally, disagree that this capability wouldn't be
super useful
#27. Selecting for undetectability
Summary: Optimizing against an interpreted thought optimizes against

interpretability.
Agree - important point to be careful about
Mostly agree (misleading, written to suggest that you deﬁnitely get the deceptive
model, instead of it being unclear whether you get the deceptive or aligned
model, which is the actually correct thing)
Not sure (good to be on the lookout for this, but not sure how much it's an issue
in practice)
#28. Large option space (no comments)
Summary: A powerful AI searches parts of the option space we don't, and we can't
foresee all its options.
#29. Real world is an opaque domain
Summary: AGI outputs go through a huge opaque domain before they have their real
consequences, so we cannot evaluate consequences based on outputs. 
Agree (strongly) - seems particularly important when there are consequences we
cannot easily observe/attribute (which seems like the rule in many consequential
domains, rather than the exception)
Mostly agree (we can't just evaluate consequences directly, but may be able to
do it by evaluating reasoning)
#30. Powerful vs understandable
Summary: No humanly checkable output is powerful enough to save the world.
Disagree (veriﬁcation easier than generation)
Disagree (due to pivotal act framing)
#31. Hidden deception
Summary: You can't rely on behavioral inspection to determine facts about an AI
which that AI might want to deceive you about.
Agree (for highly intelligent AI)
Agree (but with interpretability tools we don't have to be restricted to behavioral
inspection)
Agree (I think?) E.g. I agree that we don't have good interpretability ways to
check for "what the AI wants" or "what subgoals are relevant here" which is a
particularly consequential question where AIs might be deceptive 
Unclear (depends on the system's level of situational awareness)
#32. Language is insuﬃcient or unsafe
Summary: Imitating human text can only be powerful enough if it spawns an inner
non-imitative intelligence.
Mostly agree (misleading, typical plans don't depend on an assumption that it's
"imitating human thought")
Unclear - depends a lot on particular deﬁnitions
#33. Alien concepts
Summary: The AI does not think like you do, it is utterly alien on a staggering scale.
Agree (for highly intelligent AI)
Unclear (depends on details about the AI)
Unclear - don't think we know much about how NNs work, and what we do know
is ambiguous, though agree with #25 above

Disagree (natural abstraction hypothesis seems likely true)

Section B.4:  Miscellaneous
unworkable schemes. 
Summary
Detailed comments
#34. Multipolar collusion
Summary: Humans cannot participate in coordination schemes between
superintelligences.
Unclear (unconvinced this is an unusually-hard subcase of corrigibility for the AI
we'd use to help us)
#35. Multi-agent is single-agent
Summary: Any system of suﬃciently intelligent agents can probably behave as a
single agent, even if you imagine you're playing them against each other.
Agree (at a suﬃciently high level of intelligence I ﬁnd it hard to imagine them
playing any game we intend)
Unclear (disagree if this is meant to apply to debate, see Paul's Disagreement
#24)
#36. Human ﬂaws make containment diﬃcult (no comments)
Summary: Only relatively weak AGIs can be contained; the human operators are not
secure systems.

Section C (shorthand: "civilizational
inadequacy")
Summary
Detailed comments 
#37. Optimism until failure
Summary: People have a default assumption of optimism in the face of uncertainty,
until encountering hard evidence of diﬃculty.
Mostly disagree (humanity seems pretty risk-averse generally, see FDA and other
regulatory bodies, or helicopter parenting)
#38. Lack of focus on real safety problems
Summary: AI safety ﬁeld is not being productive on the lethal problems. The
incentives are for working on things where success is easier.
(x2) Unclear / can't evaluate (depends on ﬁeld boundaries) 
#39. Can't train people in security mindset
Summary: This ability to "notice lethal diﬃculties without Eliezer Yudkowsky arguing
you into noticing them" currently is an opaque piece of cognitive machinery to me, I do
not know how to train it into others.
Unclear (this isn't making a claim about whether others have this machinery or
are better at training?)
Disagree (seems wild to imagine that progress can only happen if someone came
up with all the arguments themselves; this seems obviously contradicted by any
existing research ﬁeld)
#40. Can't just hire geniuses to solve alignment
Summary: You cannot just pay $5 million apiece to a bunch of legible geniuses from
other ﬁelds and expect to get great alignment work out of them. 
Not sure (what have we tried?)
#41. You have to be able to write this list
Summary: Reading this document cannot make somebody a core alignment
researcher, you have to be able to write it.
Mostly agree (but I think it can be one of a suite of things that does produce real
alignment research)

Disagree (I think I both could have and often literally have written the arguments
on this list that I agree with; it just doesn't seem like a particularly useful
document to me relative to what already existed, except inasmuch as it shocks
people into action)
#42. There's no plan
Summary: Surviving worlds probably have a plan for how to survive by this point.
Unclear (don't know how overdetermined building dangerous AGI is)
Disagree (depends on what is meant by a "plan", but either I think there's a plan,
or I think many surviving worlds don't have a plan)
#43. Unawareness of the risks
Summary: Not enough people have noticed or understood the risks.
Mostly agree, especially on understanding
Disagree (you basically have to disagree if you have lower p(doom), there's not
really an argument to respond to though)

A Mechanistic Interpretability Analysis of
Grokking
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
This is a linkpost for https://colab.research.google.com/drive/1F6_1_cWXE5M7WocUcpQWp3v8z4b1jL20
A signiﬁcantly updated version of this work is now on Arxiv
aka, how the best way to do modular addition is with Discrete Fourier Transforms and trig identities
If you don't want to commit to a long post, check out the Tweet thread summary
Introduction
Grokking is a recent phenomena discovered by OpenAI researchers, that in my opinion is one of the
most fascinating mysteries in deep learning. That models trained on small algorithmic tasks like
modular addition will initially memorise the training data, but after a long time will suddenly learn to
generalise to unseen data.
A training curve for a 1L Transformer trained to do addition mod 113,
trained on 30% of the 1132 pairs - it shows clear grokking
This is a write-up of an independent research project I did into understanding grokking through the lens
of mechanistic interpretability. My most important claim is that grokking has a deep
relationship to phase changes. Phase changes, ie a sudden change in the model's performance for
some capability during training, are a general phenomena that occur when training models, that have
also been observed in large models trained on non-toy tasks. For example, the sudden change in a
transformer's capacity to do in-context learning when it forms induction heads. In this work examine
several toy settings where a model trained to solve them exhibits a phase change in test loss,
regardless of how much data it is trained on. I show that if a model is trained on these limited data with
high regularisation, then that the model shows grokking.

Loss curve for predicting repeated subsequences in a sequence of
random tokens in a 2L attention only transformer on inﬁnite data -
shows a phase change
Loss curve for predicting repeated subsequences in a sequence of
random tokens in a 2L attention-only transformer given 512 training
data points - shows clear grokking.
One of the core claims of mechanistic interpretability is that neural networks can be understood, that
rather than being mysterious black boxes they learn interpretable algorithms which can be reverse
engineered and comprehended. This work serves as a proof of concept of that, and that reverse
engineering models is key to understanding them. I fully reverse engineer the inferred algorithm from a
transformer that has grokked how to do modular addition (which somehow involves Discrete Fourier
Transforms and trig identities?!), and use this as a concrete example to analyse what happens during
training to understand what happened during grokking. I close with discussion and thoughts on the
alignment relevance of these results.

Norm of rows of the embedding matrix for modular addition,
after applying a Discrete Fourier Transform to the input
space - the sparsity shows that the model is paying attention
to some frequencies of waves but not others, and thus is
operating in the space of waves of diﬀerent frequencies.
This is accompanied by a paper in the form of a Colab notebook containing the code for this project, a
lot of interactive graphics, and much more in-depth discussion and technical details. In this write-up I try
to give a high-level conceptual overview of the claims and the most compelling results and evidence, I
refer you to the notebook if you want the full technical details. 
This write-up ends with a list of ideas for future directions of this research. I think this is a particularly
exciting problem to start with if you want to get into mechanistic interpretability since it's concrete, only
involves tiny models, and is easy to do in a Colab notebook. If you might want to work on some of these,
please reach out! In particular, I'm looking to hire an intern/research assistant, and if you're excited
about these future directions you might be a good ﬁt.
Key Claims
Grokking is really about phase changes: To exhibit grokking, we train a model on a problem
that exhibits phase changes even when given inﬁnite training data, and train it with regularisation
and limited data. If we choose the amount of data such that the regularisation only marginally
favours the generalising solution over the memorised solution, we see grokking. 
Intuition: Regularisation makes the model ultimately prefer the generalising solution to the
memorised solution, but the phase change indicates that the generalising solution is "hard to
reach" in some sense. The memorised solution is "easier to reach", and so is reached ﬁrst.
But due to the regularisation, the model still prefers the generalising solution to the
memorised solution, and thus gets to the generalising solution eventually - the grokking
result just shows that reaching the memorised solution ﬁrst does not change this, and that
there is a path  in model space interpolating between the memorising and generalising
solution.
The algorithm learned to do modular addition can be fully reverse engineered. The
algorithm is roughly:
Map inputs x, y →cos(wx), cos(wy), sin(wx), sin(wy) with a Discrete Fourier Transform, for
some frequency w
Multiply and rearrange to get cos(w(x + y)) = cos(wx) cos(wy) −sin(wx) sin(wy) and 
sin(w(x + y)) = cos(wx) sin(wy) + sin(wx) cos(wy) 

By choosing a frequency w =
k we get period dividing n, so this is a function of 
x + y (mod n)
Map to the output logits z with 
cos(w(x + y)) cos(wz) + sin(w(x + y)) sin(wz) = cos(w(x + y −z)) - this has the highest logit at 
z ≡x + y (mod n), so softmax gives the right answer.
To emphasise, this algorithm was purely learned by gradient descent! I did not predict or
understand this algorithm in advance and did nothing to encourage the model to learn this
way of doing modular addition. I only discovered it by reverse engineering the weights.
The generalising circuits used to do modular addition can be seen to smoothly evolve
over the course of training, as the model interpolates between purely memorising and purely
generalising
In particular, the circuits smoothly develop well before grokking, disproving the 'grokking is a
random walk in model-space' hypothesis.
This is nicely demonstrated by the metric of excluded loss - which roughly shows how much
model performance on the training data depends on the generalising algorithm vs
memorising algorithm. We see the use of the generalising algorithm to improve training
performance rises smoothly over training, well before the grokking point.
Excluded Loss curves for the modular addition transformer - intuitively,
excluded loss removes the model's ability to use 
cos(w(x + y)), sin(w(x + y)) terms, which signiﬁcantly harms the
generalising algorithm but not the memorising algorithm. High
excluded loss relative to train loss shows that model performance is
mostly from the generalising algorithm.
Phase Changes
2π
n

Epistemic status: I feel conﬁdent in the empirical results, but the generalisation to non-toy settings is
more speculative
Key Takeaways
Phase changes are surprisingly common
Empirically, given a problem with a phase change, training it with weight decay and restricted data
gives us grokking
Speculation: Phase changes occur in any circuit that involves multiple components composing. 
For example, the composition of the previous token head and the induction head in an
induction circuit. The previous token head will only reduce loss if the induction head is there
and vice versa. So, initially, the gradients creating each component will be weak to non-
existent. But once each component starts to form, the gradients on the other component will
become stronger. These eﬀects reinforce each other, and creating a feedback loop that
eventually accelerates and results in a phase change. 
Intuitive explanation of grokking: Regularisation incentivises the model to be simpler, so the
model prefers the generalising solution to the memorised solution. The generalising solution is
"hard to reach" and the memorising solution is not, so the memorising solution is reached ﬁrst. But
the incentive to ﬁnd the generalising solution is still there, so the underlying mechanism to induce
the phase change is still going on while grokking - memorisation doesn't change this
Fundamentally, understanding grokking is about understanding phase changes - I don't claim
to fully understand phase changes or grokking, but I claim to have reduced my confusion
about grokking to my confusion about phase changes.
Speculation: Phase changes occur for any speciﬁc circuit in a model, and so occur all the time in
larger models. Smooth loss curves are actually the average of many small phase changes
I observe several small phase changes in my toy tasks, eg there's a separate phase change
for each digit when learning 5 digit addition.
What Is A Phase Change?
By a phase change, I mean a reverse-S shaped curve[1] in the model's loss on some dataset, or the
model's capacity for some speciﬁc capability. That is, the model initially has poor performance, this
performance plateaus or slowly improves, and suddenly the performance accelerates and rapidly
improves (ie, loss rapidly decreases), and eventually levels oﬀ.  
A particularly well-studied motivating example of this is Anthropic's study of induction heads.
Induction heads are a circuit in transformers used to predict repeated sequences of tokens. They search
the context for previous copies of the current token, and then attend to the token immediately after that
copy, and predict that that subsequent token will come next. Eg, if the 3 token surname D|urs|ley is
earlier in the context, and the model wants to predict what comes after D, it will attend to urs and
predict that that comes next. 
Diagram of the function of an induction head, credit to
Anthropic.
The key fact about induction heads are that there is a fairly narrow band of training time where they
undergo a phase change, and go from not existing/functional to being fully functional. This is particularly
striking because induction heads are the main mechanism behind how Large Language Models do in-
context learning - using tokens from far back in the context to usefully predict the next token. This
means that there is a clear phase change in a model's ability to do in-context learning, as shown below,
and that this corresponds to the formation of a speciﬁc circuit via a phase change within the model.

In-context learning curves for small transformers (ﬁgure copied from Anthropic's
Induction Heads paper) - 2L and 3L models develop induction heads and show a
phase change, 1L models do not
My goal here is to convey the intuition of what I mean by a phase change, rather than give a clear and
objective deﬁnition. At this stage of research, I favour an informal "I know it when I see it" style
deﬁnition, over something more formal and brittle.
Empirical Observations
Motivation: Modular addition shows clear grokking on limited data[2], but given much more data it
shows a phase change in both train and test loss. This motivated the hypothesis that grokking could be
observed any time we took a problem with a phase change and reduced the amount of data while
regularising:
Modular addition mod 113 loss curve, trained on 95%
of the data

In several toy algorithmic tasks I observe phase changes (in both test and train loss) when models are
trained on suﬃcient data[3].  
The tasks - see the Colab for more details
1. Modular addition mod 113 (1L full transformer)
1. Trained on 95% of the training data, 1L Transformer
2. 5 digit addition (1L full transformer)
1. 1L Transformer, data format 1|3|4|5|2|+|5|8|3|2|1|=|0|7|1|7|7|3
2. 
3. Predicting Repeated Subsequences (2L attn-only transformer - task designed to need
induction heads)
1. Data format: 7 2 8 3 1 9 3 8 3 1 9 9 2 5 END - we take a uniform random sequence of
tokens, randomly choose a subsequence to repeat, and train the model to predict the
repeated tokens.

2. 
4. Finding the max element in a sequence (1L attn-only transformer - task designed to need skip
trigrams)
1. Concretely, the data format is START 0 4 7 2 4 14 9 7 2 5 3 END with exactly one entry ≥10,
and the model is trained to output that entry after END. The solution is to learn 10 skip
trigrams of the form 14 .. END 14

2. 
Grokking = Phase Changes + Regularisation + Limited Data
For all of the above tasks, we can induce grokking by adding regularisation (speciﬁcally weight decay)
and limiting the amount of training data. Eg for 5 digit addition on 700 examples (see the notebook for
the rest):

Grokking shown when training 1L Transformer to do 5 digit addition on 700 training
examples. Shown with a linear y axis to make grokking visually clear, train loss plateaus at 
3 × 10−8, test loss plateaus at 0.2
With enough data (eg single epoch training), the model generalises easily, and with suﬃciently little
data (eg a single data point), the model memorises. But there is a crossover point, and we identify
grokking when training with slightly more data than the crossover point - analogous to the ﬁndings of
Liu et al that grokking is an intermediate state between comprehension and memorisation. I found the
crossover points (here, 700) by binary searching by hand. Intuitively, this is consistent with the idea that
grokking occurs because of regularisation favouring the simpler solution. Memorisation complexity
increases with the amount of data (approximately continuously) while generalising complexity does not,
so there should must eventually be a crossover point.
This is particularly interesting, because these tasks are a non-trivial extension of the original grokking.
Previous demonstrations of grokking had an extremely small universe of data and were trained on a
substantial fraction of it, suggesting that the model may be doing some kind of clever interpolation.
While here, the universe of data is much larger (eg 1010 possible pairs of 5 digit numbers), and model is
trained on a tiny fraction of that, yet it still groks. 
Explaining Grokking
Epistemic status: The following two sections are fairly speculative - they're my current best explanation
of my empirical ﬁndings, but are likely highly incomplete and could easily be totally wrong.
Speculation: Phase Changes Are Inherent to Composition
I recommend reading the section of A Mathematical Framework for Transformer Circuits on Induction
Heads to fully follow this section
To understand the link between phase changes and grokking, it's worth reﬂecting on why circuits form at
all. A priori, this is pretty surprising! To see this, let's focus on the example of an induction circuit, and
speculate on how it could be formed. The induction circuit is made up of two heads, a previous token
head and an induction head, which interact by K-composition. Together, these heads signiﬁcantly
improve loss, but only in the context of the other head being there. Thus, naively, when we have neither
head, there should be no gradient encouraging the formation of either head. 
At initialisation, we have neither head, and so gradient descent should never discover this circuit.
Naively, we might predict that neural networks will only produce circuits analogous to linear regression,
where each weight will clearly marginally improve performance as it continuously improves. And yet in
practice, neural networks empirically do form sophisticated circuits like this, involving several parts
interacting in non-trivial, algorithmic ways.[4]
I see a few diﬀerent possible explanations for this:
1. A lottery ticket hypothesis-inspired explanation:[5] Initially, each layer of the network is the
superposition of many diﬀerent partial circuit components, and the output of each layer is the
average of the output of each component. The full output of the network is the average of many
diﬀerent circuits. Some of these circuits are systematically useful to reducing loss, and most
circuits aren't. SGD will reinforce the relevant circuits and suppress the useless circuits, so the
circuits will gradually form.
2. A random walk explanation: The network wanders randomly around the loss landscape, until it
happens to get lucky and ﬁnd a half-formed previous token head and induction head that
somewhat compose. Once it has this, this half-formed circuit is useful for reducing loss and
gradient descent can take over and make a complete circuit.
3. An evolutionary explanation: There's a similar mystery for how organisms develop
sophisticated machinery like the human eye, where each part is only useful in the context of other
parts. The explanation I ﬁnd most compelling is that we ﬁrst developed one component that was
somewhat useful on its own, eg a light-detecting membrane. This component was useful in its own
right, and so was reinforced, and later components could develop that depend on it, eg the lenses
in our eye.

The evolutionary explanation is a natural hypothesis, but we can see from my toy tasks that it cannot be
the whole story. In the repeated subsequence task, we have a sequence of uniform randomly generated
tokens, apart from a repeated subsequence at an arbitrary location, eg 7 2 8 3 1 9 3 8 3 1 9 9 2 5 END.
This means that all pairs of tokens are independent, apart from the pairs of equal tokens in the repeated
subsequence. In particular, this means that a previous token head can never reduce loss for the current
token - the previous token will always be independent of the next token. So a previous token head is
only useful in the context of an induction-like head that completes the circuit. Likewise, an induction
head relies on K-composition with a previous token head, and so cannot be useful on its own. Yet the
model eventually forms an induction circuit![6]
A priori, the random walk story seems unlikely be suﬃcient on its own - an induction circuit is pretty
complicated, and it likes represents a very small region in model space, and so seems unlikely to be
stumbled upon by a random walk[7]. Thus my prediction is that the lottery ticket hypothesis is most of
what's going on[8] - an induction head will be useless without a previous token head, but may be slightly
useful when composing with, say, a head that uniformly attends to prior tokens, since part of its output
will include the previous token! I expect that all explanations are part of the picture though, eg this
seems more plausible if the uniform head just so happens to attend a bit more to the previous token via
a random walk, etc.
Drawing this back to phase changes, the lottery ticket-style explanation suggests that we might expect
to see phase changes as circuits form. Early on in circuit formation, each part of the circuit is very rough,
so the eﬀect on the loss of improving any individual component is weak, which means the gradients will
be small. But as each component develops, each other component will become more useful, which
means that all gradients will increase together in a non-linear way. So as the circuit becomes closer to
completion we should expect an acceleration in the loss curve for this circuit, resulting in a phase
change.
An Intuitive Explanation of Grokking
With this explanation, we can now try to answer the question of why grokking occurs! To recap the
problem setting, we are training our model on a problem with two possible solutions - the memorising
algorithm and the generalising algorithm. We apply regularisation and choose a limited amount of data,
such that the generalising solution is marginally simpler than the memorising solution[9] and so our
training setup marginally prefers the generalising solution over the memorising solution. Naively, we
expect the model to learn the generalising solution.
But we are training our model on a problem whose solution involves multiple components interacting to
form a complete circuit. So, early in training, the gradients incentivising each component of the
generalising solution are weak, because they need the parts to all be formed and lined up properly.
Memorisation, however, does not require several components to be lined up in careful and coordinated
way[10], so it does not have artiﬁcially weak gradients at the start. Thus, at the start, memorisation is
incentivised more strongly than generalisation and the model memorises.
So, why does the model shift from memorisation to generalisation? Eventually the training loss plateaus
post-memorisation - loss is falling and total weights are rising so eventually the gradients towards lower
loss (ie to memorise better) balances with the gradients towards lower weights (ie to be simpler) and
cancel out. But they don't perfectly cancel out. If there's a direction in model space that allows it to
memorise more eﬃciently[11], then both gradients will encourage this direction. And the natural way to
do this is by picking up on regularities in the data - eg, you can memorise modular addition twice as
eﬃciently by recognising that x + y = y + x. This is the same process that leads the model to generalise
in the inﬁnite data case - it wants to pick up on patterns in the data.[12]
So the model is still incentivised to reach the generalising solution, just as in the inﬁnite data case. But
rather than moving from the randomly initialised model to the generalising model (as in the inﬁnite data
case), it interpolates between the memorising solution and the generalising solution. Throughout this
process test loss remains high - even a partial memorising solution still performs extremely badly on
unseen data! But once the generalising solution gets good enough, the incentive to simplify by deleting
the remnants of the memorising solution dominates, and the model clears up the memorising solution,
ﬁnally resulting in good test performance. Further, as in the inﬁnite data case, the closer we get to the
generalising solution the more the rate of change of the loss accelerates. So this ﬁnal shift happens
extremely abruptly - manifesting as the grokking phase change!

As a ﬁnal bit of evidence, once the model has fully transitioned to the generalising solution it is now
inherently simpler, and the point where the incentive to improve loss balances with the incentive to be
simpler is marginally lower - we can observe in the graphs here that the model experiences a notable
drop in train loss post grokking.
I later walk through this narrative and what it corresponds to in the modular addition case.
Speculation: Phase Changes are Everywhere
My explanation above asserted that phase change are a natural thing to expect with the formation of
speciﬁc circuits in models. If we buy the hypothesis that most things models do are built up out of many
interpretable circuits, then shouldn't we expect to see phase changes everywhere whenever we train
models, rather than smooth and convex curves? 
My prediction is that yes, we should, and that in fact we do. But that larger models are made up of many
circuits and, though each circuit may form in a phase change, the overall loss is made up out of the
combination of many diﬀerent capabilities (and thus many diﬀerent circuits). And circuits of diﬀerent
complexity/importance likely form at diﬀerent points in training. So the overall loss curve is actually the
sum of many tiny phase changes at diﬀerent points in training, and this overall looks smooth and
convex. Regularities in loss curves like scaling laws may be downstream of statistical properties of the
distribution of circuits, which become apparent at large scales. We directly observe the phase change-y
ness from the loss curves in these simple toy problems because the problems are easy enough that only
one/a few circuits are needed.
Some evidence for this hypothesis:
5 digit addition (toy problem) - We can decompose the loss into the sum of 6[13] components -
the loss on each of the 6 digits in the sum. When we do this, we observe separate phase changes
in each digit, resulting in the many small non-convexities in the overall loss curve.
The ordering of phase changes is not stable between runs, though token 0 and token 1 tend
to be ﬁrst[14]
This isn't speciﬁc to 5 digit, eg 15 digit addition shows 16 separate phase changes

Per digit loss for each token in the output for 5 digit addition, trained on
inﬁnite data, plotted against the overall loss
Skip Trigrams (toy problem) - The model learns 10 diﬀerent skip trigrams, 10 .. END 10, 11 .. END
11, etc. Each skip trigram shows a separate phase change 
Notably, each phase change happens at approximately the same time, so the overall curve
looks less bumpy than 5 digit addition. This makes sense, because each skip trigram is "as
complex" as the others, while learning to add some digits is much harder than others.
Per trigram loss for skip trigram task, trained on inﬁnite data, plotted
against the overall loss curve. Per trigram loss measured by ﬁltering the
batch for terms with that output.
Induction Heads - Induction heads are the best studied example of a speciﬁc circuit through
training, and there we see a clear phase change in LLMs up to a 13B transformer. Each head
should be "as complex" as the others, so it makes sense that the all occur at approximately, but
not exactly, the same time.[15]
Emergent phenomena - Emergent phenomena are a widely seen thing as language models
scale up - they abruptly go from incapable to capable on a task such as addition. As argued by
Jacob Steinhardt, More is Diﬀerent in AI. It seems natural to extend this to a particular model
during training, as we know that smaller models trained on more data can outcompete larger
models on less data
AlphaZero Capabilities: One ﬁnding in DeepMind's AlphaZero Interpretability paper was that
there is a phase change in the model's capabilities, where it learns to represent a lot of chess
concepts around step 32,000.
Summary
I think this is highly suggestive evidence that there is a deep relationship between grokking and phase
changes, and that grokking occurs when models with a phase change are trained with regularisation and
limited data. I present some compelling (to me) explanations of what might be behind the phase change
behaviour, and show how this model explains grokking and predicts several speciﬁc empirical
observations about grokking. I don't claim to fully understand phase changes or grokking, but I do claim
to have substantially reduced my confusion about grokking to my confusion about phase changes.

Modular Addition
Epistemic status: I feel pretty conﬁdent that I have fully reverse engineered this network, and have
enough diﬀerent lines of evidence that I am conﬁdent in how it works. My explanation of how and why it
develops during training is shakier.
Key Takeaways
A 1L Transformer learns a clear and interpretable algorithm to do modular addition. 
This algorithm operates via using trig identities and Discrete Fourier Transforms to map 
x, y →cos(w(x + y)), sin(w(x + y)), and then extracting x + y (mod p)
This algorithm can be clearly read oﬀ from the weights. If we apply a Discrete Fourier
Transform to the input space and apply the Transformer Circuits framework, the structure of
the network and its resulting algorithm becomes clear.
The model naturally forms several sub-networks that calculate the sumncos(w(x + y −z)) in
diﬀerent frequencies and add these to form the logits. This can be seen by a clear clustering
of the neurons
Within a cluster, individual neurons clearly represent interpretable features for a single
frequency.
To emphasise, this algorithm was discovered purely via gradient descent, not by me. I didn't
think of this algorithm until I reverse engineered it from the weights!
The evolution of this algorithm can be clearly seen during training, and systematic progress
towards the generalising circuit can be seen well before the grokking point
We can see each phase of my narrative of grokking manifest during training
With excluded loss, we can see the model interpolate between memorisation and
generalisation. Train loss performance depends substantially on the generalising circuit well
before we see a signiﬁcant change in test loss.
Model Details
In this section I dive deeply into one speciﬁc and well-checkpointed model trained to do modular
addition. See model training code for more details, but here are the key points:
The model is trained to map x, y to z ≡x + y (mod 113) (henceforth 113 is referred to as p)
1L Transformer
Learned positional embeddings
Width 128
No LayerNorm
ReLU activations
Input format is x|y|= , where x, y are one-hot encoded inputs, and = is an extra token. 
Trained with AdamW, with weight decay 1 and learning rate 10−3
Full batch training, trained on 30% of the data (ie the 1132 pairs of inputs) for 40,000 epochs[16]
This is a 1L transformer, with no LayerNorm and learned positional embeddings, trained with AdamW
with weight decay 1, and full batch training on 30% of the data (the data is the 1132 pairs of numbers 
mod p). The 
Overview of the Inferred Algorithm
The key feature of the algorithm is calculating cos(w(x + y)), sin(w(x + y)) with w =
k - this is a
function of x + y and be mapped to x + y, and because cos(wx) has period   we get the  (mod p) part
2π
p
pk

for free.
More concretely:
Inputs x, y are given as one-hot encoded vectors in Rp
Calculates cos(wx), cos(wy), sin(wx), sin(wy) via a Discrete Fourier Transform (This sounds complex
but is just a change of basis on the inputs, and so is just a linear map)
w =
k, k is arbitrary, we just need period dividing p
Calculates cos(wx) cos(wy), cos(wx) sin(wy), sin(wx) cos(wy), sin(wx) sin(wy) by multiplying pairs of
waves in x and in y
Calculates cos(w(x + y)) = cos(wx) cos(wy) −sin(wx) sin(wy) and 
sin(w(x + y)) = sin(wx) cos(wy) + cos(wx) sin(wy) by rearranging and taking diﬀerences
Calculates cos(w(x + y −z)) = cos(w(x + y)) cos(wz) + sin(w(x + y)) sin(wz) via a linear map to the
output logits z
This has an argmax at z ≡x + y (mod p), so post softmax we're done!
There are a few adjustments to implement this algorithm in a neural network:
The model's activations at any point are vectors(/tensors). To represent several variables, such as 
cos(wx), sin(wx), these are stored as diﬀerent directions in activation space. When the vector is
projected onto those dimensions, the coeﬃcient is the relevant variable (eg cos(wx))
The model runs the algorithm in parallel for several diﬀerent frequencies[17] (diﬀerent frequencies
correspond to diﬀerent clusters of neurons, diﬀerent subspaces of the residual stream, and
sometimes diﬀerent attention heads)
Background on Discrete Fourier Transforms
A key technique in all that follows is Discrete Fourier Transforms (DFT). I give a more in-depth explainer
in the colab, but here's a rough outline - I expect this requires familiarity with linear algebra to really get
your head around. The key motivating observation is that most activations inside the network are
periodic and so techniques designed to represent periodic functions nicely are key. Eg the attention
patterns:
2π
p

Attention patterns for head 0 -
plotted as a heatmap, x axis is
input 1, y axis is input 2
In Rp, we have a standard basis of the p unit vectors. But we can also take a basis of p cosine and sine
waves, F ∈Rp×p, where F0 = (1, 1, . . . , 1) is the constant vector, and F2k−1 = cos(
kx)  and 
F2k = sin(
kx) are the cosine and sine wave of frequency w =
k (henceforth referred to as frequency 
w = k and represented as cos kx for brevity) for k = 1, . . . ,
. Every pair of waves has dot product zero,
unless they're the same wave (ie it's an orthogonal basis). If we normalise these rows, we get an
orthonormal basis of cosine and sine waves (so F −1 = F T). We refer to these normalised waves as
Fourier Components and this overall basis as the 1D Fourier Basis.
We can apply a change of basis to the 1D input space Rp to F, and this turns out to be a much more
natural way to represent the input space for this problem, as the network learns to operate in terms of
sine and cosine waves. Eg, the fourth column of WEF T is the direction corresponding to sin 2x in the
embedding for WE. If we apply this change of basis to both the input space for x and for y we apply a 2D
DFT, and can represent any function as the linear combination of terms of the form sin w1x cos w2y (or 
cos(w1x) cos(w2y), Const ∗cos(w2y), etc). This is just a change of basis on Rp×p = Rp2, and terms of the
2π
p
2π
p
2π
p
p−1
2

form sin w1x cos w2y  (ie the outer product of each pair of rows in F) form an orthogonal basis of p2
 vectors (henceforth referred to as the 2D Fourier Basis).
Importantly, this is just a change of basis! If we choose any single activation in the network, this is a real
valued function on pairs of inputs x, y ∈Rp×p, and so is equivalent to specifying a p2 dimensional vector.
And we can apply an arbitrary change of basis to this vector. So we can always write it as a linear
combination of terms in the 2D Fourier Basis. And any vector of activations is a linear combination of 2D
Fourier terms times ﬁxed vectors in activation space. If a function is periodic, this means that it is sparse
in the 1D or 2D Fourier Basis, and this is what tells us about the structure of the algorithm and weights
of the network.
Reverse Engineering the Algorithm
Here, I present a case for how I was able to reverse engineer the algorithm from the weights. See the
Colab and appendices (attention and neuron) for full details, my goal in this section is to roughly sketch
what's going on and why I'm conﬁdent that this is what's going on. 
Calculating waves cos(wx), sin(wx), cos(wy), sin(wy)
Relevant notebook section
Theory: Naively, this seems like the hard part, but is actually extremely easy. The key is that we just
need to learn the discretised wave on x ∈[0, 1, . . . , p −1], not for arbitrary x ∈R. x is input into the
network as a one-hot encoded vector, and the multiplied by a learned matrix WE. We can in fact learn
any function f : [0, 1, . . . , p −1] →R[18]
Conveniently, F, the matrix of waves cos(wx), sin(wx), is an orthonormal basis. So WEF T will recover the
direction of the embedding corresponding to each wave Const, cos x, sin x, cos 2x, . . . - in other ways,
extracting cos(wx), sin(wx) is just a rotation of the input space. 
Evidence: We can use the norm of the embedding of each wave to get an indicator of how much the
network "cares" about each wave[19], and when we do this we see that the plot is extremely sparse. The
model has decided to throw away all but a few frequencies[20]. This is very strong evidence that the
model is working in the Fourier Basis - we would expect to see a basically uniform plot if this was not a
privileged basis.

The norm of the direction corresponding to each sine and cosine wave in
the embedding WE. Observe the sparsity, and note that terms are paired
because adjacent terms F2k = sin kx, F2k−1 = cos kx are the same frequency
Calculating 2D products of waves 
cos(wx) cos(wy), cos(wx) sin(wy), sin(wx) cos(wy), sin(wx) sin(wy)
Relevant notebook section
Theory: A good mental model for neural networks is that they are really good at matrix multiplication
and addition, and anything else takes a lot of eﬀort[21]. As so here! As we saw above, creating 
cos(wx), sin(wx) is just a rotation, and the later rearranging and map to the logits is another linear map,
but multiplying the terms together is hard and non-linear.
There are three non-linear operations in a 1L transformer - the attention softmax, the element-wise
product of attention and the value vectors, and the ReLU activations in the MLP layer. Here, the model
uses both ReLU activations and element-wise products with attention to multiply terms[22]. 
The neurons form 5[23] distinct clusters for each frequency, and each neuron in the cluster for frequency 
w has its activation as a linear combination of 1, cos(wx), sin(wx), cos(wy), sin(wy),
cos(wx) cos(wy), cos(wx) sin(wy), sin(wx) cos(wy), sin(wx) sin(wy).[24] Note that, as explained above, the
neuron activation in any network can be represented as a linear combination of products of Fourier
terms in x and Fourier terms in y (because they form a basis of Rp×p). The surprising fact is that this
representation is sparse! This can be visually seen as neuron activations being periodic:

Activation for neuron 0 - plotted as a heatmap over
pairs of inputs
Evidence: The details of how the terms are multiplied together are highly convoluted[25], and I leave
them to the Colab notebook appendices. But the neurons do in fact have the structure I described, and
this can be directly observed by looking at their values. And thus, by this point in the network it has
computed the product terms.
For example, the activations for neuron 0 (as plotted above) are approximately 
109 −39(cos 42x + cos 42y) −76(sin 42x + sin 42y)+
36(cos 42x sin 42y + sin 42x cos 42y) −10 cos 42x cos 42y + 38 sin 42x sin 42y (these coeﬃcients can be
calculated by mapping the neuron activation into the 2D Fourier Basis). This approximation explains
>90% of the variance in this neuron[26]. We can plot this visually with the following heatmap:

Activations of neuron 0 in the 2D Fourier basis - the entry in cell (a, b) is the coeﬃcient of term Fa ⊗F
Notably here, the only non-trivial terms are in rows and columns 0, 83,84, corresponding to the oute
products of pairs 1, cos 42x, sin 42x
Zooming out, we can apply a 2D DFT to all neuron activations, ie writing all of the neuron activations as
a linear combinations of terms of the form cos 42x cos 42y times vectors, and plotting the norm of each
vector of coeﬃcients. Heuristically, this is telling us what terms are represented by the network at the
output of the neurons. We see that the non-trivial terms are in the top row (of the form cos wx, sin wx) or
the left column (of the form cos wy, sin wy) or in a cell of 2D cells along the diagonal (of the form 
cos(wx) cos(wy), cos(wx) sin(wy), sin(wx) cos(wy), sin(wx) sin(wy) - notably, a product term where both
terms have the same frequency).
Calculating cos(w(x + y)), sin(w(x + y)) and calculating logits
Relevant notebook section
Theory: The operations mapping cos(w(x + y)) = cos(wx) cos(wy) −sin(wx) sin(wy) and 
sin(w(x + y)) = cos(wx) sin(wy) + sin(wx) cos(wy) are linear, and the operations mapping this to 
cos(w(x + y −z)) = cos(w(x + y)) cos(wz) + sin(w(x + y)) sin(wz) are also linear. So their composition is
linear, and can be represented by a single matrix multiplication. The neurons are mapped to the logits
by L = WUWoutN, and so the eﬀective weight matrix Wlogit = WUWout must represent both of these
operations (if my hypothesis is correct). Note that Wlogit is a p × dmlp matrix, mapping from MLP-space to
the output space.
Evidence: We draw upon several diﬀerent lines of evidence here.

We show that the terms cos(w(x + y)), sin(w(x + y)) are computed as follows: We repeat the above
analysis to ﬁnd terms represented by the neurons on the logits, we ﬁnd that the terms in the top row
and left column cancel out. This leaves just diagonal terms, corresponding to products of waves of the
same frequency in x and y, exactly the terms we need. We also see that the 2x2 blocks are uniform,
showing that cos(w(x + y)) and sin(w(x + y)) have the same coeﬃcient. Further analysis shows that
everything other than cos(w(x + y)), sin(w(x + y)) for these 5 frequencies is essentially zero.
Norms of the components of the logits when represented in the 2D Fourier
Basis - the colour of a cell intuitively represents how much the network has
calculated/represents that term. Only terms for the products of waves of the
same frequency are non-trivial. Note that these are 2x2 cells with very
similar colour, not single large pixels.
We now show that the output logits produce 
cos(w(x + y −z)) = cos(w(x + y)) cos(wz) + sin(w(x + y)) sin(wz) for each of the 5 represented
frequencies (where z is the term corresponding to the output logits). The neurons form clusters for each
frequency, and when we plot the columns of Wlogit corresponding to those frequencies, and apply a 1D
DFT to the output space of Wlogit, we see that the only non-trivial terms are cos(wz), sin(wz) - ie the
output logits coming from these neuron clusters is a linear combination of cos(wz), sin(wz).

Wlogit for neurons in the freq 14 cluster, with the
output space in the Fourier basis - all non-trivial
output terms are cos(14x), sin(14x)
We can more directly verify this by writing approximating the output logits as a sum of 
∑w∈[14,35,41,42,52] Aw cos(w(x + y −z)) and ﬁtting the coeﬃcients Aw. When we do this, the resulting
approximated logits explains 95% of the variance in the original logits. If we evaluate loss on this
approximation to the logits, we actually see a signiﬁcant drop in loss, from 2 ∗10−7 to 4.7 ∗10−8
Evolution of Circuits During Training
Note: For this section in particular, I recommend referring to the Colab! That contains a bunch of
interactive graphics that I can't include here, where we can observe the development of circuits during
training.
Now that we understand what the model is doing during training on a mechanistic level, we can directly
observe the development of these circuits during training. The key observation is that the circuits
develop smoothly, and make clear and systematic progress towards the generalising circuit well before
there is notable test performance. For example, the graph of the norms of the embedding of diﬀerent
waves becomes sparse. In particular, this disproves the natural hypothesis that grokking is the result of
a random walk on the manifold of models that perform optimally.

Norm of the embedding of each wave at epoch 7K, 2K
epochs before the start of grokking. It's clearly begun to
be sparse!
I will now give and explain several graphs of metrics during training, and use them to support my above
narrative of what happens during grokking in this speciﬁc case:
Sum of squared weights - the main regularisation is weight decay, so paying attention to the
change in sum of squared weights is key to understand the eﬀect of the regulariser
This uses the transformer circuits notation - Win, Wout We canare MLP weights, WU is the
unembed, WE is the embed, WQ, WK, WO, WV  are the attention weights

Excluded loss: Intuitively, this metric tracks how much the model's performance on the training
data comes from the generalising algorithm vs the memorisation algorithm, by ablating the
model's ability to use a speciﬁc frequency w. This serves as a progress measure, quantitatively
tracking the model's progress towards the correct algorithm.
Formally, we calculate the logits, use a 2D DFT to write them as a linear combination of
waves in x and waves in y, and set the terms corresponding to cos(w(x + y)), sin(w(x + y)) to
zero and keep the other terms the same. Then measure the train loss. 
If the model is using the addition identities (ie, with the generalising algorithm), this
will heavily damage performance. If it isn't (ie, the memorising algorithm), then this is
just deleting 2 directions among 1132, and so should have no eﬀect.

Trig loss: Another metric is to take the output logits, and only allow them to use the directions
corresponding to cos(w(x + y)), sin(w(x + y)) for the 5 relevant frequencies. Intuitively, this
represents the model's performance with the generalising algorithm preserved but the memorising
algorithm removed.
Here we evaluate performance on both the train and test set - trig loss for the train and test
set is almost equal (the lines are identical), showing that it's purely tracking the generalising
algorithm

We can now use these metrics to analyse the separate stages of training:
Memorisation (Epoch 0-1K): The model memorises. We see a dramatic drop in train loss, and a
rise in test loss.
We see that the incentive to memorise is far stronger than the incentive to have low weights,
as total weights rise a lot
We see that it's memorising rather than generalising as excluded loss is very close to train
loss 
Interpolation (Epoch 1K-9K): The model is interpolating between memorisation and
generalisation. 
We see this in the smooth divergence of excluded loss from train loss - this shows that the
model is deriving more and more of its training performance from the generalising algorithm.
Intuitively, the model is trying to memorise "more eﬃciently" by picking up on patterns
in the data. Some directions in neuron space are functions of x + y (mod p), namely 
cos(w(x + y)), sin(w(x + y)) terms, so it learns to use these directions to represent more
and more information.
Train loss mostly plateaus in this period - this shows that although the model is changing its
underlying algorithm, it does this by interpolating between the purely memorising circuit and
the purely generalising circuit
It surprises me that the model can interpolate between two circuits while retaining
good performance! 
This stage begins when the incentive for low train loss and the incentive for low weights
balance, and train loss abruptly levels out.
This is particularly clear from epoch 1K-3K, when the weights for the MLP layer 
Win, Wout drop in norm and meet the other weight matrices. Given a matrix product AB,
we can always scale up A →kA and scale down B →
B and keep the product AB the
same. The sum of squared weights is minimised when |A| = |B|, so all weight matrices
between the embed and unembed should have equal norm - this minimises total
1k

weights and keeps loss the same. The fact that this does not happen in the ﬁrst stage
shows that the incentive for small weights is too weak and has little eﬀect
Further, this is suggestive evidence that most of the memorisation comes from
changes in the MLP - this matches the hypothesis that memorisation involves
"simpler" circuits that don't involve complex composition, and so doesn't need a
phase change.
Generalisation (Epoch 8K-12K): We see a phase change in trig loss. Trig loss represents "model
performance if we delete the memorisation circuit", so this shows that the generalising circuit
rapidly develops in a phase change. 
This matches my (admittedly post-hoc) hypothesis that all circuits see phase changes - as
each component of the circuit forms, it reinforces the gradients on all other components.
We further see a rapid acceleration in the decrease in total weights, supporting the idea that
there is some cascade as the circuit forms.
Cleaning Up Noise (Epoch 9K-13K): Somewhat in parallel with the above phase change in trig
loss, there is a slightly delayed phase change in test loss. The slight delay indicates that good test
loss performance requires both a strong generalising circuit, and the removal of the memorising
circuit. 
The rapid removal of the memorisation circuit presumably comes from weight decay - once
the memorisation circuit's contribution to loss is low enough, the total weight beneﬁts of
removing it outweigh the contribution to loss.
Stability (Epoch 13K-end)[27]: The model has now transitioned to using the generalising circuit
and all our metrics settle down
The model still has some remnants of the memorisation circuit, as train loss is still less than
test loss. This must come from directions other than cos(w(x + y)), sin(w(x + y)) in logit
space, as trig loss is identical on the train and test data.
Discussion
Alignment Relevance
Ultimately, my goal is to do research that is relevant to aligning future powerful AI systems. Here I've
studied tiny models 
(Note - the true reason I did this project was that I was on holiday, and got extremely nerd-sniped, so
this is all post-hoc reasoning!)
Emergent capabilities:
Is grokking relevant to large models? Probably not - models like GPT-3 and PaLM are trained
on functionally inﬁnite data, and rarely see a data point twice. In fact, there's a strong
incentive to avoid this, as repeated data can severely damage performance. 
The main way I think this is directly relevant to large models is the indication that phase
changes are everywhere. If most circuits emerge as phase changes during training, this
suggests we should expect emergent capabilities to be a frequent occurence. In particular,
this may occur for particular dangerous capabilities, such as deception or situational
awareness
This would make alignment a harder problem, as it is less likely that we'll see warning
shots. It also limits our ability to, eg, continuously test a system for misalignment or
deception throughout training. This strategy might work if the system will ﬁrst try to be
deceptive badly before it is superhuman at lying, but will fail if the system rapidly
jumps from not even trying to be deceptive to being superhuman. 
However, this is highly speculative - even if this hypothesis holds, these capabilities
may be implemented by many diﬀerent circuits, and so still see gradual development
A laboratory for training dynamics + generalisation: There are many open questions about
exactly why neural networks work, why they learn to generalise to unseen data, what kinds of
generalising algorithms they learn, and how this can be inﬂuenced. These are extremely hard
questions to study, and I don't feel that I've understood them as a result of this work. But I think
the modular addition transformer may be a good laboratory to examine these questions, as it's a
concrete, small model that can be easily trained and whose circuits can be clearly inspected.
Further, understanding these questions is a key question for alignment - aligning systems will
likely require predicting when they would learn to be deceptive vs aligned and how to inﬂuence
them.

A particularly important direction may be directly training systems on interpretability-based
metrics. Naively, if we could create an automated tool to detect deception and put this in the
loss function, our system may never learn to be deceptive. But by default, I would expect
this kind of strategy to fail badly, because gradient descent will just ﬁnd a way to Goodhart
the tool. I expect a good (though still highly limited) in-road into this question would be to
train the modular addition system on interpretability-based metrics, and see the eﬀects on
the training dynamics and the resulting circuits.
The ability to understand training dynamics rather than just a ﬁnal system is also notable
here. Possibly, if we ever end up with a system that is deceptive and aware of its current
situation as a neural network in training, it will be able to inﬂuence the training process to
always keep it misaligned, and evade any interpretability techniques trying to detect this.
The only way to avoid this will be to inﬂuence the training dynamics such that the system
never becomes deceptive and situationally aware in the ﬁrst place.
Importance of interpretability: A high-level motivation behind this work is a belief that
mechanistic interpretability is key to understand how neural networks work. They seem to learn
interpretable circuits, and these determine both the model's capabilities, and how those
capabilities are implemented. It seems very diﬃcult to get traction on what's really going on in
models without engaging with these circuits - analogous to doing chemistry without discovering
the periodic table. I take this work as a proof of concept of that - I took the weird and confusing
phenomena of grokking, and feel that I have signiﬁcantly de-mystiﬁed it by fully reverse
engineering the underlying circuits.
This is an admittedly very toy problem, but feels like evidence supporting my belief that
interpretability extremely useful for getting alignment right.
This is a particularly notable with grokking, as here we studied the competition between two
circuits with comparable loss. The standard way to predict what a model will learn is by
expecting it to have low loss, but this breaks down if multiple mechanistically diﬀerent
solutions have comparable performance. This is extremely relevant for alignment as, for
example, a deceptive system will be incentivised to output exactly what we'd expect from an
aligned solution, and so get the same loss.
This work also reinforces my belief that the circuits agenda is possible at all - neural
networks seem to want to be understood, if we can just ﬁnd the right way to reverse
engineer them! In particular, the fact that I discovered an unexpected  and new[28] algorithm
for doing modular addition via reverse engineering.
Limitations
There are several limitations to this work and how conﬁdently I can generalise from it to a general
understanding of deep learning/grokking, especially the larger models that will be alignment relevant. In
particular:
The modular addition transformer is a toy model, which only needs a few circuits to solve the
problem fully (unlike eg an LLM or an image model, which needs many circuits and may lead to
diﬀerent behaviour)
The model is over-parametrised, as is shown by eg it learning many redundant neurons doing
roughly the same task. It has many more parameters than it needs for near-perfect performance
on its task, unlike real models.
In particular, models like language models could always learn more circuits to achieve
marginally better task performance, and so need to use their parameters eﬃciently. This
may act as a form of implicit regularisation.
I only study weight decay as the form of regularisation.
My explanations of what's going on rely heavily on fairly fuzzy and intuitive notions that I have not
explicitly deﬁned: crisp circuits, simplicity, phase changes, what it means to memorise vs
generalise, model capacity, etc.
Of my four tasks exhibiting phase changes, I have only fully interpreted modular addition.[29] 
Future Directions
I see a lot of future directions building on this work. If you'd be interested in working on any of these,
please reach out! I'd love to chat and possibly advise or collaborate on a project. I'm also currently
looking to hire an intern/research assistant to help me work on mechanistically interpretability projects
(including but not limited to these ideas), and if you feel excited about any these directions you may be
a good ﬁt.

Investigating the 'phase changes are everywhere' hypothesis.
Focusing on larger models on complex tasks
Training a larger model that we know how to interpret, making many checkpoints and
inspecting circuits during training
Training a small SoLU transformer (1L is particularly easy to interpret)
Training Inception and looking for curve circuits
Looking for emergent capabilities in open source well-checkpointed training runs (Eg
Mistral and GPT-J) 
Performance on benchmarks or speciﬁc questions from a benchmark
Simple algorithmic tasks - eg few shot learning, addition, sorting words into
alphabetical order, completing a rhyming couplet, matching open and close
brackets
Soft induction heads, eg translation
Look at attention heads on various texts and see if any have recognisable
attention patterns (eg start of word, adjective describing current word, syntactic
features of code like indents or variable deﬁnitions, most recent open bracket,
etc).
Wei et al is a good source for other ideas
More elaborate toy problems that involve several diﬀerent tasks
Eg, training a model with separate heads for modular addition, subtraction,
multiplication, etc. Do we see separate phase changes?
Training models for the other tasks in the grokking paper with more data, and seeing if we
see phase changes but no grokking.
Investigating the 'phase changes are inherent to composition' hypothesis
If we directly look at the gradients on diﬀerent components of the circuit, do we see this
mutual reinforcement and cascading?
Can we show this analytically in some toy setting? What are the minimal properties we need
to see phase changes?
I predict the most tractable will be skip trigrams, given how simple they are. We can
take even simpler versions, by making the only parameters a QK and OV matrix,
removing the attention softmax, etc.
Are phase changes speciﬁc to weirdnesses of the Adam optimizer? Do we get them for SGD
or Momentum?
Note: Once a model has memorised, loss can be around 1e-7. SGD gradients scale
with loss while Adam is normalised, so you'll need to increase your learning rate a lot. 
Barak et al may be another good setting to analyse - looking for phase changes in the k-bit
parity problem
Loose threads in this project
Interpreting my other tasks with phase changes, and examining the circuits
Preliminary investigation shows that 5 digit addition is using a variant of the trig based
algorithm
Interpreting the memorisation circuit in the modular addition transformer - understanding
memorisation in transformers seems exciting to me!
This is likely easiest on random labels with no structure.
Why is there another phase change at epoch 43K?! What's the algorithm after this?
There's some circuit competition between eg frequency 25 and 31 from epoch 12K to 14K, or
between multiplying terms with the attention patterns or the ReLUs. What's up with that?
Why do the given frequencies form, and not others? Why 5 or 6 frequencies, and not more or
less?
Train the model for a bunch of diﬀerent random seeds, and see how consistent these
observations are.[30]
Training dynamics. Note that a 2L MLP can also grok modular addition, and may be easier to
study.
What happens when we train on interpretability-inspired metrics like excluded loss[31]? Can
we accelerate the grokking point? Can we disincentivise it? Can we incentivise or
disincentivise speciﬁc frequencies?
How do forms of regularisation other than weight decay aﬀect the model and its inductive
biases. 
How and why does the model learn to form separate clusters of neurons? Can we predict
neuron clustering in advance? Can we manually shift the initialisation to change the
clustering?
Can we replicate a lottery ticket-style approach here? What subnetworks are useful for
solving this problem? And if only some subnetworks are useful, why do all neurons ultimately
end up being used?

How general is this algorithm? Can we ﬁnd Fourier-style algorithms in LLMs? Especially when
they learn to do addition
Note, I expect this to be much easier to detect in models that tokenize each digit separately -
the GPT-3 tokenizer completely breaks place value:
28|79|23|598|23|45|987|249|234|0000|23|47|89|03|00000|700|9|02|10. Sadly, the only tokenizer
I know of like this is PaLM's.
The GPT-3 tokenizer mostly creates 2 digit tokens, so I'd start by trying to use a linear probe
to extract cos(w(z)), sin(w(z)) in base 100 from the model for each frequency, where z is the
correct output digit (or possible the output if you incorrectly carry a 1)
Getting into the ﬁeld: I think this problem is notable for being fairly simple and involving small models
that it's tractable to completely understand. If you're interested in getting into mechanistic
interpretability and are new to the ﬁeld, I think this may be a great place to start! As a concrete ﬁrst
project, I recommend training your own transformer for modular subtraction, ﬁguring out the analogous
algorithm it should be using, and see if you can replicate my analysis to reverse engineer that.
Meta
Notable Observations
Some scattered thoughts that feel exciting to me, but didn't naturally ﬁt into this write-up:
Slingshot mechanism: (see colab) The Slingshot Mechanism was a fascinating paper from Thilak
et al, that found that grokking seemed tied to 'slingshots', dramatic loss spikes, that it was
hypothesised ﬂung the model to diﬀerent regions in the loss landscape. 
I found that these slingshots were not necessary to achieve grokking[32], and further that the
slingshots are the result of a ﬂoat32 precision error in log_softmax.
This can be ﬁxed by casting your logits to ﬂoat64 before taking log_softmax. If I keep my
training code exactly the same but remove this casting, my loss curve instead looks like this
monstrosity:

Explanation: Concretely, the smallest non-zero value that log_softmax can output in ﬂoat32
is 1.19e-7. So once the model has memorised suﬃciently well that loss approaches the scale
of 1e-7, some data points will have loss rounding to zero. This also rounds the gradients of
their loss to zero, equivalent to them not being in the training set. When a model memorises
data, it wants to repurpose all other parameters and set unseen data points to extreme
values, and this precision error means this includes some training data points. This
introduces a dodgy gradient, and the EWMA of Adam ensures it persists for long enough for a
signiﬁcant loss spike
Note that this is an inherent problem of ﬂoat32, not a ﬂaw in log_softmax. For small
values of x, log_softmax(x) is equal to log(1+x), and this is approximately x. 1.19e-7 is
the smallest value such that 1+x!=1 in ﬂoat32.
Float32 isn't the whole story. With minibatch training and ﬂoat64, we also see loss
spikes, though rarer and less severe
Weight Decay: Naively, for a ﬁxed training dataset size, the model should only grok for
suﬃciently high weight decay. But this is not actually true! Lower weight decay just delays
grokking rather than stopping it. 
Intuition: The model will have an incentive to switch to the generalising circuit so long as it
has lower total weight than the memorising circuit. Importantly, this holds for any non-zero
weight decay. Smaller weight decay just leads to a smaller incentive
Grokking curve for 1L Transformer trained on 30% of
the data for modular addition, weight decay 0.01. It
groks, but takes 60x as long as weight decay 1.
Explaining circle plots of WE: A notable ﬁnding of Liu et al and the original grokking paper is
that the embedding matrix of modular addition can be plotted as a circle, by the ﬁrst two principal
components or t-SNE respectively. Now that we understand the underlying circuits, we can see
that this clearly follows from the fact that the embedding matrix has orthogonal rows for 
sin(wx), cos(wx) - these dimensionality reduction techniques are just ﬁnding two directions of the
same frequency, and observing that a scatter plot of sin(wx), cos(wx) is a circle

A plot of the principal components of the embedding
matrix in the 1D Fourier Basis. We can see that the
principal components of the embedding correspond to
waves of diﬀerent frequencies (though here the ﬁrst
two are not of the same frequency and do not form a
circle)
2L MLPs can grok modular addition: A surprising (to me) result from Liu et al is that a 2L MLP
can grok modular addition[33]. I replicated this result, and found that it learned the same algorithm
(here, purely using the ReLU/GeLU activations to multiply the waves in x and y). The neurons
formed a cluster for every single frequency, with 3-7 neurons per cluster (width 256), and the
input and output weights were sparse in the 1D Fourier basis. 
Acknowledgements
This work was done as independent research after leaving Anthropic, but beneﬁtted greatly from my
work with the Anthropic interpretability team and skills gained, especially from an extremely generous
amount of mentorship from Chris Olah. It relies heavily on the interpretability techniques and framework
developed by the team in A Mathematical Framework for Transformer Circuits, and a lot of the analysis
was inspired by the induction head bump result.
Thanks to Noa Nabeshima for pair programming with me on an initial replication of grokking, and to Vlad
Mikulik for pair programming with me on the grokked induction heads experiment.
Thanks to Jacob Hilton, Alex Ray, Xander Davies, Lauro Langosco, Kevin Wang, Nicholas Turner, Rohin
Shah, Vlad Mikulik, Janos Kramar, Johannes Treutlein, Arthur Conmy, Noa Nabeshima, Eric Michaud, Tao
Lin, John Wentworth, Jeﬀ Wu, David Bau, Martin Wattenberg, Nick Cammarata, Sid Black, Michela
Paganini, David Lindner, Zac Kenton, Michela Paganini, Vikrant Varma, Evan Hubinger (and likely many
others) for helpful clarifying discussions about this research, feedback, and helping me identify my
many ill-formed explanations!
Generously supported by the FTX regrantor program
Author Contributions

Tom Lieberum signiﬁcantly contributed to the early stages of this project - replicating grokking for
modular subtraction, discovering it was possible in a 1L transformer with no LayerNorm, and observing
the strongly periodic behaviour of activations and weights
Neel Nanda led the rest of this project - fully reverse engineering the modular addition transformer,
analysing it during training, and discovering and analysing the link between grokking and phase
changes. He wrote this write-up, and it is written from his perspective.
Feedback
I'd love to hear feedback on this work - parts you ﬁnd compelling, parts you're skeptical of,  parts I
explained poorly, cases for why Colab notebooks are a terrible format for papers, etc. You can comment
here, or reach me at neelnanda27@gmail.com.
Citation Info
Please cite this work as:
            @misc{https://doi.org/10.48550/arxiv.2301.05217, 
  doi = {10.48550/ARXIV.2301.05217}, 
   
  url = {https://arxiv.org/abs/2301.05217}, 
   
  author = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob}, 
   
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information 
sciences, FOS: Computer and information sciences}, 
   
  title = {Progress measures for grokking via mechanistic interpretability}, 
   
  publisher = {arXiv}, 
   
  year = {2023}, 
   
  copyright = {arXiv.org perpetual, non-exclusive license} 
} 
 
          
1. ^
In the ideal case this looks ﬂat, then drops abruptly, then is ﬂat again. In practice, I'm happy to
include any striking non-convexities in a curve.
2. ^
Ie a divergence in train and test loss
3. ^
Where possible, I train the models on inﬁnite data (ie, the data is randomly generated each time,
and there are enough possible data points that nothing gets repeated) - in this case I do not
distinguish between train and test loss, as train loss is always on unseen data.
4. ^
This is even more mysterious when you consider how each head has both an attention (QK) circuit
and a value (OV) circuit. The previous token head's QK circuit needs to compose with the positional
embeddings and its OV circuit needs to compose with the embedding's output. The induction
head's QK circuit needs to compose with the previous token head's output on the K-side, and the
embedding on the Q-side, and the OV circuit needs to compose with the embedding and
unembedding to be the identity. This is a lot of synchronised and non-trivial interaction! But we
know that real networks form circuits with as many moving parts as this, or more, eg curve
detecting circuits.

5. ^
Eric Michaud gives some evidence for this by ﬁnding that, if we project the model's initialised
embedding onto the principal components of the ﬁnal embedding, we still see a circle-like thing -
suggesting that the model reinforced directions in embedding space that luckily had good
properties.
6. ^
I have not veriﬁed that the model actually forms an induction circuit of the form described in this
particular task. But the structure of attention means it must be doing some kind of composition,
because a single attention head cannot attend to a previous token based on its neighbours
7. ^
I later show in the case of modular addition that it is deﬁnitively not a random walk, as we can see
clear progress towards the true answer before grokking
8. ^
Though I'm sure I'm missing other explanations!
9. ^
This is a more complex statement than it looks, since each solution can always become more
complex to result in lower loss. Eg doubling WU will always decrease loss if there's 100% accuracy.
I operationalise this as 'which solution is simpler if we set them both to achieve the same level of
loss'
10. ^
I am assuming this fact about memorisation as part of this story. I don't understand how
memorisation is implemented so I cannot be conﬁdent, but this seems plausible given that, eg a
logistic/linear regression model with enough parameters can memorise random data
11. ^
ie, more simply/with lower total weights
12. ^
In some sense, single epoch training is the limiting case of this. The model tries to memorise every
data point it sees, but there must eventually be a cap on how much data it can memorise, so it
must pick up on patterns. We never run the model on the same data point twice, so we don't
notice this attempt at memorisation. Plausibly, models that never learn to generalise keep trying
to memorise recent training data and forget about old data.
13. ^
The 0th digit is the leading digit that's 0 or 1 
14. ^
This is weird! Token 5 (the ﬁnal digit) should be easiest because it never requires you to carry a 1 -
it's just addition mod 10
15. ^
Under this hypothesis, I would predict that soft induction heads like translation appear after the
phase change, as they seem likely more complex 
16. ^
The training run was up to 50K epochs, but there's a mysterious bonus tiny phase change at epoch
43K that I don't yet understand, and so I snap the model before then, as I don't think that shift is

central to understand the ﬁrst grok.
17. ^
This is useful because, though z = x + y (mod p) is an argmax of cos(w(x + y −z)), the diﬀerence
between it and the second largest value is tiny. While if we take the sum of cos(w(x + y −z)) for
several diﬀerent values of w, the waves constructively interfere at z = x + y (mod p), and
destructively interfere everywhere else, signiﬁcantly increasing the diﬀerence.
18. ^
By having a column of WE be (f(0), f(1), . . . , f(p −1)) - the dot product (ie the ﬁrst coordinate of the
embedding) is f(x). In fact, so long as WE is invertible, we can recover any function f(x) by
projecting onto the direction f(x)W
−1
E
19. ^
Remember that we're using high weight decay, so the model will set any weights it's not using to
(almost) zero
20. ^
The frequencies here are 14, 31, 35, 41, 42, 52 - as far as I can tell, the speciﬁc frequencies and the
number of frequencies are arbitrary, and vary between random seeds
21. ^
But is also extremely necessary! Eg how activation functions take an MLP from linear regression to
being able to represent any function. 
22. ^
The attention pattern multiplication is the obvious way to do it - I found the use of ReLUs very
surprising! But it turns out that ReLU(0.5 + cos(kx) + cos(ky)) is approx 
0.71 + 0.695 cos(kx) + 0.695 cos(ky) + 0.43 cos(kx) cos(ky) (more details)
23. ^
The model calculates 6 frequencies with the embedding, but only 5 with the neurons - I'm not sure
why.
24. ^
There is actually a sixth cluster not aligned with any particular frequency - neurons in this cluster
always ﬁre, and thus ReLU is the identity. I'm not entirely sure why this is useful to the network
25. ^
In particular, understanding ReLU in the Fourier basis takes it from a nice basis aligned operation
to the operation "rotate out of the Fourier basis into the standard basis, take the elementwise
ReLU, and then rotate back into the Fourier basis"
26. ^
We can validate that the remaining 10% is just noise by ablating - replacing each neuron by this
approximation (ie removing all terms of all other frequencies) leaves loss unchanged.

27. ^
This isn't quite true - at epoch 43K, there is a tiny phase change which can be seen if you zoom in
on the trig loss graph (trig loss and train loss fall, test loss rises). I haven't investigated this
properly, but the model goes from never attending to the = token to signiﬁcantly attending to it.
Presumably there's some further circuit development here?
28. ^
Well, new to me - ancedotally, a friend of mine came up with the algorithm independently when
exploring grokking
29. ^
Though skip trigrams are extremely easy to interpret, and mostly match what I'd predict, I just
haven't written it up.
30. ^
I've looked at several models, but not exhaustively
31. ^
Note that excluded loss speciﬁcally is weird, because we ﬁt the coeﬃcients of cos(w(x + y)) to the
logits across all of the data, so it's technically also a function of the test data. Probably linear
regression on just the train logits would suﬃce though.
32. ^
Though plausibly helps by adding regularisation - generalising solution may be more stable to
slingshots than memorised ones?
33. ^
Input format is the sum of a one-hot encoding of x and of y in R113. If x = y then the vector has a
single two in it.

Two-year update on my personal AI
timelines
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
I worked on my draft report on biological anchors for forecasting AI timelines mainly
between ~May 2019 (three months after the release of GPT-2) and ~Jul 2020 (a
month after the release of GPT-3), and posted it on LessWrong in Sep 2020 after an
internal review process. At the time, my bottom line estimates from the bio anchors
modeling exercise were:[1]
Roughly ~15% probability of transformative AI by 2036[2] (16 years from posting
the report; 14 years from now).
A median of ~2050 for transformative AI (30 years from posting, 28 years from
now).
These were roughly close to my all-things-considered probabilities at the time,
as other salient analytical frames on timelines didn't do much to push back on this
view. (Though my subjective probabilities bounced around quite a lot around these
values and if you'd asked me on diﬀerent days and with diﬀerent framings I'd have
given meaningfully diﬀerent numbers.)
It's been about two years since the bulk of the work on that report was completed,
during which I've mainly been thinking about AI. In that time it feels like very short
timelines have become a lot more common and salient on LessWrong and in at least
some parts of the ML community.
My personal timelines have also gotten considerably shorter over this
period. I now expect something roughly like this:
~15% probability by 2030 (a decrease of ~6 years from 2036).
~35% probability by 2036 (a ~3x likelihood ratio[3] vs 15%).
This implies that each year in the 6 year period from 2030 to 2036 has an
average of over 3% probability of TAI occurring in that particular year
(smaller earlier and larger later).
A median of ~2040 (a decrease of ~10 years from 2050).
This implies that each year in the 4 year period from 2036 to 2040 has an
average of almost 4% probability of TAI.
~60% probability by 2050 (a ~1.5x likelihood ratio vs 50%).
As a result, my timelines have also concentrated more around a somewhat narrower
band of years. Previously, my probability increased from 10% to 60%[4] over the
course of the ~32 years from ~2032 and ~2064; now this happens over the ~24
years between ~2026 and ~2050.
I expect these numbers to be pretty volatile too, and (as I did when writing bio
anchors) I ﬁnd it pretty fraught and stressful to decide on how to weigh various
perspectives and considerations. I wouldn't be surprised by signiﬁcant movements.
In this post, I'll discuss:

Some updates toward shorter timelines (I'd largely characterize these as
updates made from thinking about things more and talking about them with
people rather than updates from events in the world, though both play a role).
Some updates toward longer timelines (which aren't large enough to overcome
the updates toward shorter timelines, but claw the size of the update back a bit).
Some claims associated with short timelines that I still don't buy.
Some sources of bias I'm not sure what to do with.
Very brieﬂy, what this may mean for actions.
This post is a catalog of fairly gradual changes to my thinking over the last two years;
I'm not writing this post in response to an especially sharp change in my view -- I just
thought it was a good time to take stock, particularly since a couple of people have
asked me about my views recently.
Updates that push toward shorter
timelines
I list the main updates toward shorter timelines below roughly in order of importance;
there are some updates toward longer timelines as well (discussed in the next section)
which claw back some of the impact of these points.
Picturing a more speciﬁc and somewhat lower
bar for TAI
Thanks to Carl Shulman, Paul Christiano, and others for discussion around this point.
When writing my report, I was imagining that a transformative model would likely
need to be able to do almost all the tasks that remote human workers can do
(especially the scientiﬁc research-related tasks). Now I'm inclined to think that just
automating most of the tasks in ML research and engineering -- enough to accelerate
the pace of AI progress manyfold -- is suﬃcient.
Roughly, my previous picture (similar to what Holden describes here) was:
Automate science → Way more scientists → Explosive feedback loop of technological
progress 
But if it's possible to automate science with AI, then automating AI development itself
seems like it would make the world crazy almost as quickly:
Automate AI R&D → Explosive feedback loop of AI progress speciﬁcally → Much better
AIs that can now automate science (and more) → explosive feedback loop of
technological progress
To oversimplify, suppose I previously thought it would take the human AI development
ﬁeld ~10 years of work from some point T to ﬁgure out how to train a scientist-AI. If I
learn they magically got access to AI-developer-AIs that accelerate progress in the
ﬁeld by 10x, I should now think that it will take the ﬁeld ~1 year from point T to get to
a scientist-AI.

This feels like a lower bar for TAI than what I was previously picturing -- the most
obvious reason is because automating one ﬁeld should be easier than automating all
ﬁelds, meaning that the model size required should be appreciably smaller. But
additionally, I think AI development in particular seems like it has properties that
make it easier to automate with only short horizon training (see below for some
discussion). So this update reduces my estimate of both model size and eﬀective
horizon length.
Feeling like meta-learning may be unnecessary, making
short horizon training seem more plausible
Thanks to Carl Shulman, Dan Kokotajlo, and others for discussion of what short
horizon training could look like.
In my report, I acknowledged that models trained with short eﬀective horizon lengths
could probably do a lot of economically useful work, and that breaking long tasks
down into smaller pieces could help a lot. The main candidate in my mind for a task
that might require long training horizons to learn was (and still is) "eﬃcient learning"
itself.
That is, I thought that a meta-learning project attempting to train a model on many
instances of the task "master some complex new skill (that would take a human a
long time to learn, e.g. a hard video game or a new type of math) from scratch within
the current episode and then apply it" would have a long eﬀective horizon length,
since each individual learning task (each "data point") would take the model some
time to complete. Absent clever tricks, my best guess was that this kind of meta-
learning run would have an eﬀective horizon length roughly similar to the length of
time it would take for a human to learn the average skill in the distribution.
I felt like having the ability to learn novel skills in a sample-eﬃcient way would be
important for a model to have a transformative impact, and was unsure about the
extent to which clever tricks could make things cheaper than the naive view of "train
the model on a large number of examples of trying to learn a complex task over many
timesteps." This pulled my estimate for eﬀective horizon length upwards (to a median
of multiple subjective hours).
By and large, I haven't really seen much evidence in the last two years that this kind
of meta-learning -- where each object-level task being learned would take a human a
long time to learn -- can be trained much more cheaply than I thought, or much
evidence that ML can directly achieve human-like sample eﬃciencies without the
need for expensive meta-learning (footnote attempts to brieﬂy address some possible
objections here).[5]
Instead, as I've thought harder about the bar for "transformative," I've come to think
that it's likely not necessary for the ﬁrst transformative models to learn new things
super eﬃciently. Speciﬁcally, if the main thing needed to have a transformative
impact is to accelerate AI development itself:
There's so much human-imitation data on programming and AI[6] that the model
can train on vastly more examples than a human sees in their lifetime, and after
that training it may not really need to learn particularly complex novel skills
after training to act as a very skilled AI engineer / researcher.

Coding is intentionally very modular, so it seems especially well-suited to break
down into small short-horizon steps.
Probably as a result of the above two points, we're already seeing much more
concrete progress on coding than we are in other technical and scientiﬁc
domains; AI systems seem likely to add non-trivial value to practicing
programmers soon. This generally de-risks the prospect of using AI to help with
AI development somewhat, compared to other applications we haven't started
to see yet.
Brute-force search seems like it could play a larger role in progress than in many
other sciences (e.g. a relatively simple ML model could generate and test out
thousands of diﬀerent small tweaks to architectures, loss functions, optimization
algorithms, etc. at small scale, choosing the one that works best empirically --
other sciences have a less clear-cut search space and longer feedback loops).
I do still think that eventually AI systems will learn totally new skills much more
eﬃciently than existing ML systems do, whether that happens through meta-learning
or through directly writing learning algorithms much more sample-eﬃcient than SGD.
But now this seems likely to come after short-horizon, ineﬃciently-trained coding
models operating pretty close to their training distributions have massively
accelerated AI research.
Explicitly breaking out "GPT-N" as an anchor
This change is mainly a matter of explicitly modeling something I'd thought of but
found less salient at the time; it felt more important to me to factor it in after the
lower bar for TAI made me consider short horizons in general more likely.
My original Short Horizon neural network anchor assumed that eﬀective horizon
length would be log-uniformly distributed between ~1 subjective second (which is
about GPT-3 level) and ~1000 subjective seconds; this meant that the Short Horizon
anchor was assuming an eﬀective horizon length substantially longer than a pure
language model (the mean was ~32 subjective seconds, vs ~1 subjective second for a
language model).
I'm now explicitly putting signiﬁcant weight on an amount of compute that's more like
"just scaling up language models to brain-ish sizes." (Note that this hypothesis/anchor
is just saying that the training computation is very similar to the amount of
computation needed to train GPT-N, not that we'd literally do nothing else besides
train a predictive language model. For example, it's consistent with doing RL ﬁne-
tuning but just needing many OOMs less data for that than for the original training run
-- and I think that's the most likely way it would manifest.)
Considering endogeneities in spending and
research progress
Thanks to Carl Shulman for raising this point, and to Tom Davidson for research
ﬂeshing it out.
My report forecasted algorithmic progress (the FLOP required to train a transformative
model in year Y), hardware progress (FLOP / $ in year Y), and willingness to spend ($
that the largest training run could spend on FLOP in year Y) as simple trendline
forecasts, which I didn't put very much thought into.

In the open questions section, I gestured at various ways these forecasts could be
improved. One salient improvement (mentioned but not highlighted very much) would
be to switch from a black box trend extrapolation to a model that takes into
account how progress in R&D relates to R&D investment.
That is, rather than saying "Progress in [hardware/software] has been [X doublings per
year] recently, so let's assume it continues that way," we could say:
1. Progress in [hardware/software] has been [X doublings per year]
2. Over this time, the amount of [money / labor] invested into [hardware / software
R&D] has been growing at [Y doublings per year]
3. This implies that every Y doublings of R&D leads to X doublings of improvement
in [hardware / software]
This would then allow us to express beliefs about how investment in R&D will change,
which can then translate into beliefs about how fast research will progress. And if ML
systems have lucrative near-term applications, then it seems likely there will be
demand for increasing investment into hardware and software R&D beyond the
historical trend, suggesting that this progress should happen faster than I model.
Furthermore, it seems possible that pre-transformative systems would substantially
automate some parts of AI research itself, potentially further increasing the eﬀective
"total R&D eﬀorts gone into AI research" beyond what might be realistic from
increasing the human labor force alone.
Seeing continued progress and no major
counterexamples to DL scaling well
My timelines model assumed that there was a large (80%) chance that scaling up
2020 ML techniques to use some large-but-not-astronomically-large amount of
computation (and commensurate amount of data) would work for producing a
transformative model.[7]
Over the last two years, I'd say deep learning has broadly continued to scale up well.
Since that was the default assumption of my model, there isn't a big update toward
shorter timelines here -- but there was some opportunity for deep learning to "hit a
wall" over the last two years, and that didn't really happen, modestly increasing my
conﬁdence in the premise.
Seeing some cases of surprisingly fast progress
My forecasting method was pretty anchored to estimates of brain computation, rather
than observations of the impressiveness of models that existed at the time, so I was
pretty unsure what the framework would imply for very-near-term progress[8] ("How
good at coding would a mouse be, if it'd been bred over millennia to write code
instead of be a mouse?").
As a result, I didn't closely track speciﬁc capabilities advances over the last two years;
I'd have probably deferred to superforecasters and the like about the timescales for
particular near-term achievements. But progress on some not-cherry-picked
benchmarks was notably faster than what forecasters predicted, so that should be

some update toward shorter timelines for me. I'm pretty unsure how much, and it's
possible this should be larger than I think now.
Making a one-time upward adjustment for
"2020 FLOP / $"
In my report I estimated that the eﬀective computation per dollar in 2020 was 1e17
FLOP/ $, and projected this forward to get hardware estimates for future years.
However, this seems to have been an underestimate of FLOP/ $) as of 2020. This is
because:
I was using the V100 as my reference machine; this was in fact the most
advanced publicly available chip on the market as of 2020, but it was released in
2018 and on its way out, so it was better as an estimate for 2018 or 2019
compute than 2020 compute. The more advanced A100 was 2-3x more powerful
per dollar and released in late 2020 almost immediately after my report was
published.
I was using the rental price of a V100 (~$1/hour), but big companies get better
deals on compute than that, by about another 2-3x.
I was assuming ~⅓ utilization of FLOP/s, which was in line with what people
were achieving then, but utilization seems to have improved, maybe to ~50% or
so.
This means the 2020 start point should have been 2.5 * 2.5 * 1.5 = nearly 10x larger.
From the 2020 start point, I projected that FLOP / $ would double every ~2.5 years --
which is slightly faster than the 2010 to 2018 period but slightly slower than Moore's
law. I haven't looked into it deeply but my understanding is that this has roughly held,
so the update I'm making here is a one-time increase to the starting point rather than
a change in rate (separate from the changes in rate I'm imagining due to the
endogeneities update).
Updates that push toward longer
timelines
My report estimates that the amount of training data required to train a model
with N parameters scales as N^0.8, based signiﬁcantly on results from Kaplan et
al 2020. In 2022, the Chinchilla scaling result (Hoﬀmann et al 2022) showed that
instead the amount of data should scale as N.
Some people have suggested this should be an update toward shorter
timelines. This would be true if your method for forecasting timelines was
observing models like GPT-2 and GPT-3, gauging how impressive they
were, and trying to guess how many orders of magnitude more training
computation would be required to reach TAI. The Chinchilla result would
show that GPT-3 was "not as good as it could have been" for a ﬁxed
amount of training computation, so your estimate for the amount of
additional scaling required should go down.
But in my report I arrive at a forecast by ﬁxing a model size based on
estimates of brain computation, and then using scaling laws to estimate

how much data is required to train a model of that size. The update from
Chinchilla is then that we need more data than I might have thought.
I'm somewhat surprised that I haven't seen more vigorous commercialization of
language models and commercial applications that seem to reliably add real
value beyond novelty; this is some update toward thinking that language models
are less impressive than they seemed to me, or that it's harder to translate from
a capable model into economic impact than I believed.
There's been a major market downturn that hit tech companies especially hard;
it seems a little less likely to me now than it did when writing the report that
there will be a billion dollar training run by 2025.
Overall, the updates in the previous section seem a lot stronger than these updates.
Claims associated with short timelines
that I still don't buy
I don't expect a discontinuous jump in AI systems' generality or depth of thought
from stumbling upon a deep core of intelligence; I'm not totally sure I
understand it but I probably don't expect a sharp left turn.
Relatedly, I don't expect that progress will be driven by a small number of key
"game-changing" algorithmic insights we can't anticipate today; I expect
transformative models to look quite similar to today's models[9] (more so now
that my timelines are shorter) and progress to be driven by scale and a large
number of smaller algorithmic improvements.
Relatedly, I still don't expect that TAI will be cheap (e.g. <$10B for a project) and
don't think smallish "underdog" research groups are likely to develop TAI;[10] I
still expect developing TAI to require hundreds of billions of dollars and to be
done by tech companies with high valuations,[11] likely after pretty signiﬁcant
commercialization of sub-transformative systems.
I think the concept of a "point of no return" that is not an objective observable
event like "the AI has monopolized violence" is tricky to reason about, since it
folds in complicated forecasts about the competence and options of future
actors,[12] but to the extent that I understand the concept I'm mostly not
expecting a PONR before explosive acceleration in research progress is
underway (e.g. I don't expect a PONR before the ﬁrst automated AI company).
The update toward a lower bar for "transformative" pushes me more in this
direction -- I expect there to be a lot of helpful things to do in the ﬁnal couple of
years in between "AIs accelerate AI research a lot" and "Far superintelligent AI"
(a big one being use the AIs to help with alignment research).
Sources of bias I'm not sure what to
do with
Putting numbers on timelines is in general a kind of insane and stressful exercise, and
the most robust thing I've taken away from thinking about all this is something like
"It's really a real, live possibility that the world as we know it is radically upended
soon, soon enough that it should matter to all of us on normal planning horizons." A
large source of variance in stated numbers is messy psychological stuﬀ.

The most important bias that suggests I'm not updating hard enough toward short
timelines is that I face sluggish updating incentives in this situation -- bigger changes
to my original beliefs will make people update harder against my reasonableness in
the past, and holding out some hope that my original views were right after all could
be the way to maximize social credit (on my unconscious calculation of how social
credit works).
But there are forces in the other direction too -- most of my social group is pretty
system 1 bought into short timelines,[13] which for many of us likely emotionally
justiﬁes our choice to be all-in on AI risk with our careers. My own choices since 2020
look even better on my new views than my old. I have constantly heard criticism over
the last two years that my timelines are too long, and very little criticism that they
were too short, even though almost everyone in the world (including economists, ML
people, etc) would probably have the other view. I ﬁnd myself not as interested or
curious as I should theoretically be (on certain models of epistemic virtue) in pushback
from such people. I spend most of my time visualizing concrete worlds in which things
move fast and hardly any time visualizing concrete worlds where they move slow.[14]
What does this mean?
I'm unclear how decision-relevant bouncing around within the range I've been
bouncing around is. Given my particular skills and resources, I've steered my career
over the last couple years in a direction that looks roughly as good or better on
shorter timelines than what I had in 2020.
This update should also theoretically translate into a belief that we should allocate
more money to AI risk over other areas such as bio risk, but this doesn't in fact bind
us since even on our previous views, we would have liked to spend more but were
more limited by a lack of capacity for seeking out and evaluating possible grants than
by pure money.
Probably the biggest behavioral impact for me (and a lot of people who've updated
toward shorter timelines in the last few years) will be to be more forceful and less
sheepish about expressing urgency when e.g. trying to recruit particular people to
work on AI safety or policy.
The biggest strategic update that I'm reﬂecting on now is the prospect of making a lot
of extremely fast progress in alignment with comparatively limited / uncreative /
short-timescale systems in some period a few months or a year before systems that
are agentic / creative enough to take over the world. I'm not sure how realistic this is,
but reﬂecting on how much progress could be made with pretty "dumb" systems
makes me want to game out this possibility more.
1. Bio Anchors part 4 of 4, pg 14-16. ↩ 
2. A year chosen to evaluate a claim made by Holden in 2016 that there was a
>10% chance of TAI within 20 years. ↩ 
3. Given by the ratio of odds ratios: (0.35 / 0.65) / (0.15 / 0.85) = 3.05. This implies
my observations and logical updates from thinking more since 2020 were 3x

more likely in a world where TAI happens by 2036 than in a world where it
doesn't happen by 2036. ↩ 
4. This contains 50% of my probability mass but is not a "50% conﬁdence interval"
as the term is normally used, because the range I'm considering is not the range
from 25% probability to 75% probability. This is mainly to keep the focus on the
left tail of the distribution, which is more important and easier to think about.
E.g. if I'm wrong about most of the models that lead me to expect TAI soonish,
then my probability climbs very slowly up to 75% since I would revert back to
simple priors. ↩ 
5. People use "few-shot learning" to refer to language models' ability to
understand the pattern of what they're being asked for after seeing a small
number of examples in the prompt (e.g. after seeing a couple of examples of
translating an English sentence into French, a model will complete the pattern
and translate the next English sentence into French). However, this doesn't
seem like much evidence about the kind of meta-learning I'm interested in,
because it takes very little time for a human to learn a pattern like that. If a
bilingual French-and-English-speaking human saw a context with two examples
of translating an English sentence into French, they would ~immediately
understand what was going on. Since the model already knows English and
French, the learning problem it faces is very short-horizon (the amount of time it
would take a human to read the text). I haven't yet seen evidence that language
models can be taught new skills they deﬁnitely didn't know already over the
course of many rounds of back-and-forth.
I've also seen EﬃcientZero cited as evidence that SGD itself can reach human-
level sample complexities (without the need for explicit meta-learning), but this
doesn't seem right to me. The EﬃcientZero model learned the environment
dynamics of a game with ML and then performed a search against that model of
the environment to play the game. It took the model a couple of hours per game
to learn environment dynamic facts like "the paddle moves left to right", "if an
enemy hits you you die," etc. The relevant comparison is not how much time it
would take a human to learn to play the game, it's how much time it would take
a human to get what's going on and what they're supposed to aim for -- and the
latter is not something that it would take a human 2 hours of watching to ﬁgure
out (probably more like 15-60 seconds).
The kind of thing that would seem like evidence about eﬃcient meta-learning is
something like "A model is somehow trained on a number of diﬀerent video
games, and then is able to learn how to play (not just model the dynamics of) a
new video game it hadn't seen before decently with a few hours of experience."
The kind of thing that would seem like evidence of human-like sample eﬃciency
directly from SGD would be something like "good performance on language
tasks while training on only as many words as a human sees in a lifetime." ↩ 
6. All the publicly-available code online (e.g. GitHub), plus company-internal repos,
keylogging of software engineers, explicitly constructed curricula/datasets
(including datasets automatically generated from the outputs of slightly smaller
coding models), etc. Also, it seems like most of the reasoning that went into
generating the code is in some sense manifested in the code itself, whereas e.g.
the thinking and experimentation that went into a biology experiment isn't all
directly present in the resulting paper. ↩ 

7. Row 17 in sheet "Main" ↩ 
8. The key exception, as discussed above, was predicting that cheap meta-learning
wouldn't happen, which I'd say it didn't. ↩ 
9. Note that there's a bunch of already-widely-used techniques (most notably
search) that some people wouldn't count as "pure deep learning" which I expect
to continue to play an important role. Transformative AI seems quite likely to
look like AlphaGo (which uses search), RETRO (which uses retrieval), etc. ↩ 
10. Though my probability on this necessarily increased some because shifting the
distribution to the left has to increase probability mass on "surprisingly cheap,"
it's still not my default. If I had to guess I'd say maybe ~15% chance on <$10B
for training TAI and a similar probability that it's trained by a company with <2%
of the valuation of the biggest tech companies. Betting on this possibility -- with
one's career or investments -- seems better than it did to me before (and I don't
think it was insane in the past either; just not something I'd consider the default
picture of the future). ↩ 
11. Though labs that are currently smallish could grow to have massive valuations
and a ton of employees and then develop transformative systems, and that
seems a lot more likely than that a company would develop TAI while staying
small. ↩ 
12. E.g., it seems like it wouldn't be hard to argue for a "PONR" in the past, e.g.
"alignment is so hard that the fact that we didn't get started on it 20 years ago
means we're past the point of no return." Instead it just feels like the diﬃculty of
changing course just gets worse and worse over time, and there are very late-
stage opportunities that could still technically help, like "shutting down all the
datacenters." ↩ 
13. I might have ended up, with this update, near the median of the people I hang
out with most, but I could also still be slower than them -- not totally sure. ↩ 
14. Since longer timelines are harder to think about since the world will have
changed more before TAI, less decision-relevant since our actions will have
washed out more, less emotionally gripping, etc. ↩ 

Common misconceptions about
OpenAI
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
I have recently encountered a number of people with misconceptions about OpenAI.
Some common impressions are accurate, and others are not. This post is intended to
provide clariﬁcation on some of these points, to help people know what to expect from
the organization and to ﬁgure out how to engage with it. It is not intended as a full
explanation or evaluation of OpenAI's strategy. 
The post has three sections:
Common accurate impressions
Common misconceptions
Personal opinions
The bolded claims in the ﬁrst two sections are intended to be uncontroversial, i.e.,
most informed people would agree with how they are labeled (correct versus
incorrect). I am less sure about how commonly believed they are. The bolded claims in
the last section I think are probably true, but they are more open to interpretation and
I expect others to disagree with them.
Note: I am an employee of OpenAI. Sam Altman (CEO of OpenAI) and Mira Murati (CTO
of OpenAI) reviewed a draft of this post, and I am also grateful to Steven Adler, Steve
Dowling, Benjamin Hilton, Shantanu Jain, Daniel Kokotajlo, Jan Leike, Ryan Lowe, Holly
Mandel and Cullen O'Keefe for feedback. I chose to write this post and the views
expressed in it are my own.
Common accurate impressions
Correct: OpenAI is trying to directly build safe AGI.
OpenAI's  Charter states: "We will attempt to directly build safe and beneﬁcial AGI, but
will also consider our mission fulﬁlled if our work aids others to achieve this outcome."
OpenAI leadership describes trying to directly build safe AGI as the best way to
currently pursue OpenAI's mission, and have expressed concern about scenarios in
which a bad actor is ﬁrst to build AGI, and chooses to misuse it.
Correct: the majority of researchers at OpenAI are working on capabilities. 
Researchers on diﬀerent teams often work together, but it is still reasonable to loosely
categorize OpenAI's researchers (around half the organization) at the time of writing
as approximately:
Capabilities research: 100
Alignment research: 30
Policy research: 15

Correct: the majority of OpenAI employees did not join with the primary
motivation of reducing existential risk from AI speciﬁcally.
My strong impressions, which are not based on survey data, are as follows. Across the
company as a whole, a minority of employees would cite reducing existential risk from
AI as their top reason for joining. A signiﬁcantly larger number would cite reducing risk
of some kind, or other principles of beneﬁcence put forward in the OpenAI Charter, as
their top reason for joining. Among people who joined to work in a safety-focused role,
a larger proportion of people would cite reducing existential risk from AI as a
substantial motivation for joining, compared to the company as a whole. Some
employees have become motivated by existential risk reduction since joining OpenAI.
Correct: most interpretability research at OpenAI stopped after the
Anthropic split.
Chris Olah led interpretability research at OpenAI before becoming a cofounder of
Anthropic. Although several members of Chris's former team still work at OpenAI,
most of them are no longer working on interpretability.
Common misconceptions
Incorrect: OpenAI is not working on scalable alignment.
OpenAI has teams focused both on practical alignment (trying to make OpenAI's
deployed models as aligned as possible) and on scalable alignment (researching
methods for aligning models that are beyond human supervision, which could
potentially scale to AGI). These teams work closely with one another. Its recently-
released alignment research includes self-critiquing models (AF
discussion), InstructGPT, WebGPT (AF discussion) and book summarization (AF
discussion). OpenAI's approach to alignment research is described here, and includes
as a long-term goal an alignment MVP (AF discussion).
Incorrect: most people who were working on alignment at OpenAI left for
Anthropic. 
The main group of people working on alignment (other than interpretability) at OpenAI
at the time of the Anthropic split at the end of 2020 was the Reﬂection team, which
has since been renamed to the Alignment team. Of the 7 members of the team at that
time (who are listed on the summarization paper), 4 are still working at OpenAI, and
none are working at Anthropic. Edited to add: this fact alone is not intended to provide
a complete picture of the Anthropic split, which is more complicated than I am able to
explain here.
Incorrect: OpenAI is a purely for-proﬁt organization.
OpenAI has a hybrid structure in which the highest authority is the board of directors
of a non-proﬁt entity. The members of the board of directors are listed here. In legal
paperwork signed by all investors, it is emphasized that: "The [OpenAI] Partnership
exists to advance OpenAI Inc [the non-proﬁt entity]'s mission of ensuring that safe
artiﬁcial general intelligence is developed and beneﬁts all of humanity. The General
Partner [OpenAI Inc]'s duty to this mission and the principles advanced in the OpenAI
Inc Charter take precedence over any obligation to generate a proﬁt. The Partnership
may never make a proﬁt, and the General Partner is under no obligation to do so."

Incorrect: OpenAI is not aware of the risks of race dynamics.
OpenAI's Charter contains the following merge-and-assist clause: "We are concerned
about late-stage AGI development becoming a competitive race without time for
adequate safety precautions. Therefore, if a value-aligned, safety-conscious project
comes close to building AGI before we do, we commit to stop competing with and start
assisting this project. We will work out speciﬁcs in case-by-case agreements, but a
typical triggering condition might be "a better-than-even chance of success in the
next two years.""
Incorrect: OpenAI leadership is dismissive of existential risk from AI.
OpenAI has a Governance team (within Policy Research) that advises leadership and is
focused on strategy for avoiding existential risk from AI. In multiple recent all-hands
meetings, OpenAI leadership have emphasized to employees the need to scale up
safety eﬀorts over time, and encouraged employees to familiarize themselves with
alignment ideas. OpenAI's Chief Scientist, Ilya Sutskever, recently pivoted to spending
50% of his time on safety.
Personal opinions
Opinion: OpenAI leadership cares about reducing existential risk from AI.
I think that OpenAI leadership are familiar and agree with the basic case for concern
and appreciate the magnitude of what's at stake. Existential risk is an important
factor, but not the only factor, in OpenAI leadership's decision making. OpenAI's
alignment work is much more than just a token eﬀort.
Opinion: capabilities researchers at OpenAI have varying attitudes to
existential risk.
I think that capabilities researchers at OpenAI have a wide variety of views, including
some with long timelines who are skeptical of attempts to mitigate risk now, and
others who are supportive but may consider the question to be outside their area of
expertise. Some capabilities researchers actively look for ways to help with alignment,
or to learn more about it.
Opinion: disagreements about OpenAI's strategy are substantially empirical.
I think that some of the main reasons why people in the alignment community might
disagree with OpenAI's strategy are largely disagreements about empirical facts. In
particular, compared to people in the alignment community, OpenAI leadership tend
to put more likelihood on slow takeoﬀ, are more optimistic about the possibility of
solving alignment, especially via empirical methods that rely on capabilities, and are
more concerned about bad actors developing and misusing AGI . I would expect
OpenAI leadership to change their mind on these questions given clear enough
evidence to the contrary.
Opinion: I am personally extremely uncertain about strategy-related
questions.
I do not spend most of my time thinking about strategy. If I were forced to choose
between OpenAI speeding up or slowing down its work on capabilities, my guess is
that I would end up choosing the latter, all else equal, but I am very unsure.

Opinion: OpenAI's actions have drawn a lot of attention to large language
models.
I think that the release of GPT-3 and the OpenAI API led to signiﬁcantly increased focus
and somewhat of a competitive spirit around large language models. I consider there
to be advantages and disadvantages to this. I don't think OpenAI predicted this in
advance, and believe that it would have been challenging, but not impossible, to
foresee this.
Opinion: OpenAI is deploying models in order to generate revenue, but also
to learn about safety.
I think that OpenAI is trying to generate revenue through deployment in order to
directly create value and in order to fund further research and development. At the
same time, it also uses deployment as a way to learn in various ways, and about
safety in particular.
Opinion: OpenAI's particular research directions are driven in large part by
researchers.
I think that OpenAI leadership has control over staﬃng and resources that aﬀects the
organization's overall direction, but that particular research directions are largely
delegated to researchers, because they have the most relevant context. OpenAI would
not be able to do impactful alignment research without researchers who have a strong
understanding of the ﬁeld. If there were talented enough researchers who wanted to
lead new alignment eﬀorts at OpenAI, I would expect them to be enthusiastically
welcomed by OpenAI leadership.
Opinion: OpenAI should be focusing more on alignment.
I think that OpenAI's alignment research in general, and its scalable alignment
research in particular, has signiﬁcantly higher average social returns than its
capabilities research on the margin.
Opinion: OpenAI is a great place to work to reduce existential risk from AI.
I think that the Alignment, RL, Human Data, Policy Research, Security, Applied Safety,
and Trust and Safety teams are all doing work that seems useful for reducing
existential risk from AI.

What do ML researchers think about
AI in 2022?
Katja Grace, Aug 4 2022
AI Impacts just ﬁnished collecting data from a new survey of ML researchers, as
similar to the 2016 one as practical, aside from a couple of new questions that
seemed too interesting not to add.
This page reports on it preliminarily, and we'll be adding more details there. But so
far, some things that might interest you:
37 years until a 50% chance of HLMI according to a complicated aggregate
forecast (and biasedly not including data from questions about the conceptually
similar Full Automation of Labor, which in 2016 prompted strikingly later
estimates). This 2059 aggregate HLMI timeline has become about eight years
shorter in the six years since 2016, when the aggregate prediction was 2061, or
45 years out. Note that all of these estimates are conditional on "human
scientiﬁc activity continu[ing] without major negative disruption."
P(extremely bad outcome)=5% The median respondent believes the
probability that the long-run eﬀect of advanced AI on humanity will be
"extremely bad (e.g., human extinction)" is 5%. This is the same as it was in
2016 (though Zhang et al 2022 found 2% in a similar but non-identical
question). Many respondents put the chance substantially higher: 48% of
respondents gave at least 10% chance of an extremely bad outcome. Though
another 25% put it at 0%.
Explicit P(doom)=5-10% The levels of badness involved in that last question
seemed ambiguous in retrospect, so I added two new questions about human
extinction explicitly. The median respondent's probability of x-risk from humans
failing to control AI1 was 10%, weirdly more than median chance of human
extinction from AI in general2, at 5%. This might just be because diﬀerent people
got these questions and the median is quite near the divide between 5% and
10%. The most interesting thing here is probably that these are both very high—
it seems the 'extremely bad outcome' numbers in the old question were not just
catastrophizing merely disastrous AI outcomes.
Support for AI safety research is up: 69% of respondents believe society
should prioritize AI safety research "more" or "much more" than it is currently
prioritized, up from 49% in 2016.
The median respondent thinks there is an "about even chance" that an
argument given for an intelligence explosion is broadly correct. The
median respondent also believes machine intelligence will probably (60%) be
"vastly better than humans at all professions" within 30 years of HLMI, and that
the rate of global technological improvement will probably (80%) dramatically
increase (e.g., by a factor of ten) as a result of machine intelligence within 30
years of HLMI.
Years/probabilities framing eﬀect persists: if you ask people for
probabilities of things occurring in a ﬁxed number of years, you get later
estimates than if you ask for the number of years until a ﬁxed probability will
obtain. This looked very robust in 2016, and shows up again in the 2022 HLMI
data. Looking at just the people we asked for years, the aggregate forecast is 29

years, whereas it is 46 years for those asked for probabilities. (We haven't
checked in other data or for the bigger framing eﬀect yet.)
Predictions vary a lot. Pictured below: the attempted reconstructions of
people's probabilities of HLMI over time, which feed into the aggregate number
above. There are few times and few probabilities that someone doesn't basically
endorse the combination of.
You can download the data here (slightly cleaned and anonymized) and do
your own analysis. (If you do, I encourage you to share it!)
Individual inferred gamma distributions
The survey had a lot of questions (randomized between participants to make it a
reasonable length for any given person), so this blog post doesn't cover much of it. A
bit more is on the page and more will be added.
Thanks to many people for help and support with this project! (Many but probably not
all listed on the survey page.)
Cover image: Probably a bootstrap conﬁdence interval around an aggregate of the
above forest of inferred gamma distributions, but honestly everyone who can be sure
about that sort of thing went to bed a while ago. So, one for a future update. I have
more conﬁdently held views on whether one should let uncertainty be the enemy of
putting things up.


Some conceptual alignment research
projects
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Some research outputs I'd love to see, focused on exploring, clarifying and formalizing
important alignment concepts. I expect that most of these will be pretty time-
consuming, but happy to discuss for people who want to try:
1. A paper which does for deceptive alignment what the goal misgeneralization
paper does for inner alignment, i.e. describing it in ML language and setting up
toy examples (for example, telling GPT-3 to take actions which minimize changes
in its weights, given that it's being trained using actor-critic RL with a certain
advantage function, and seeing if it knows how to do so).
2. A paper which does the same for gradient hacking, e.g. taking these examples
and putting them into more formal ML language.
3. A list of papers that are particularly useful for new research engineers to
replicate.
4. A takeover scenario which covers all the key points in https://www.cold-
takes.com/ai-could-defeat-all-of-us-combined/, but not phrased as an argument,
just phrased as a possible scenario (I think you can't really make the argument
rigorously in that little space).
5. A paper which deﬁnes the concepts of implicit planning, implicit value functions,
implicit reward models, etc, in ML terms. Kinda
like https://arxiv.org/abs/1901.03559 but more AGI-focused. I want to be able to
ask people "does GPT-3 choose actions using an implicit value function?" and
then be able to point them to this paper to rigorously deﬁne what I mean. I
discuss this brieﬂy in the phase 1 section here.
6. A blog post which describes in as much detail as possible what our current
"throw the kitchen sink at it" alignment strategy would look like. (I'll probably
put my version of this online soon but would love others too).
7. A blog post explaining "debate on weights" more thoroughly.
8. A blog post exploring how fast we should expect a forward pass to be for the
ﬁrst AGIs - e.g. will it actually be slower than human thinking, as discussed
in this comment?
9. A blog post exploring considerations for why model goals may or may not be
much more robust to SGD than model beliefs, as discussed in framing 3 here.
(See also this paper on gradient starvation - h/t Quintin Pope; and the concept of
persistence to gradient descent discussed here.)
10. A blog post explaining why the "uncertainty" part of CIRL only does useful work
insofar as we have an accurate model of the human policy, and why this is
basically just as hard as having an accurate model of human preferences.
11. A blog post explaining what practical implications Stuart Armstrong's
impossibility result has.
12. As many alignment exercises as possible to help people learn to think about this
stuﬀ (mine aren't great but I haven't seen better).
13. A paper properly formulating instrumental convergence, generalization to large-
scale goals, etc, as inductive biases in the ML sense (I do this brieﬂy in phase 3
here).

14. A mathematical comparison between oﬀ-policy RL and imitation learning,
exploring ways in which they're similar and diﬀerent, and possible algorithms in
between.
15. A blog post explaining the core argument for why detecting adversarially-
generated inputs is likely much easier than generating them, and arguments for
why adversarial training might nevertheless be valuable for alignment.
16. A blog post exploring the incentives which models might have when they're
simultaneously trained to make predictions and to take actions in an RL setting
(e.g. models trained using RL via sequence modeling).
17. A blog post exploring pros and cons of making misalignment datasets for use as
a metric of alignment (alignment = how much training on the misalignment
dataset is needed to make it misaligned).
18. A paper providing an RL formalism in which reward functions can depend on
weights and/or activations directly, and demonstrating a simple but non-trivial
example.
19. A blog post evaluating reasons to think that situational awareness will be a
gradual development in models, versus a sharp transition.
20. A blog post explaining reasons to expect capabilities to be correlated with
alignment while models lack situational awareness, and then less correlated
afterwards, rather than the correlation continuing.
21. A blog post estimating how many bits of optimization towards real-world goals
could arise from various aspects of a supervised training program (especially
ones which slightly break the cartesian formalisms) - e.g. hyperparameter
tuning, many random seeds, training on data generated by other AIs, etc.
22. A sketch of what a model-free version of AIXI would look like (according to one
person I talked to, it's a lot like decision transformers).
23. A blog post evaluating whether shard theory makes sense/makes novel
predictions compared with Steve Byrnes' model of the brain (he partly explains
this in a comment on the post, but I'm still a bit confused).
24. A blog post or paper reviewing what types of feedback humans perform best and
worst at (e.g. reward vs value feedback) and then designing a realistic setup for
optimal-quality human feedback.
25. A blog post compiling examples of surprising emergent capabilities (especially in
large language models).
26. An investigation of the extent to which human concept representations are
localized to individual neurons, versus being spread out across diﬀerent neurons.

Language models seem to be much
better than humans at next-token
prediction
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
[Thanks to a variety of people for comments and assistance (especially Paul Christiano,
Nostalgebraist, and Rafe Kennedy), and to various people for playing the game. Buck
wrote the top-1 prediction web app; Fabien wrote the code for the perplexity
experiment and did most of the analysis and wrote up the math here, Lawrence did the
research on previous measurements. Epistemic status: we're pretty conﬁdent of our
work here, but haven't engaged in a super thorough review process of all of it--this was
more like a side-project than a core research project.]
How good are modern language models compared to humans, at the task language
models are trained on (next token prediction on internet text)? While there are
language-based tasks that you can construct where humans can make a next-token
prediction better than any language model, we aren't aware of any apples-to-apples
comparisons on non-handcrafted datasets. To answer this question, we performed a
few experiments comparing humans to language models on next-token prediction on
OpenWebText.
Contrary to some previous claims, we found that humans seem to be consistently
worse at next-token prediction (in terms of both top-1 accuracy and perplexity) than
even small models like Fairseq-125M, a 12-layer transformer roughly the size and
quality of GPT-1. That is, even small language models are "superhuman" at
predicting the next token. That being said, it seems plausible that humans can
consistently beat the smaller 2017-era models (though not modern models) with a few
hours more practice and strategizing. We conclude by discussing some of our
takeaways from this result. 
We're not claiming that this result is completely novel or surprising. For example,
FactorialCode makes a similar claim as an answer on this LessWrong post about this
question. We've also heard from some NLP people that the superiority of LMs to
humans for next-token prediction is widely acknowledged in NLP. However, we've seen
incorrect claims to the contrary on the internet, and as far as we know there hasn't
been a proper apples-to-apples comparison, so we believe there's some value to our
results.
If you want to play with our website, it's here; in our opinion playing this game
for half an hour gives you some useful perspective on what it's like to be a language
model.
Previous claims and related work
One commonly cited source for human vs LM next-token prediction performance is this
slide from Steve Omohundro's GPT-3 presentation, where he claims that humans have
perplexity ~12, compared to GPT-3's 20.5. (Smaller perplexity means you're better at
predicting.)

This comparison is problematic for two reasons, one small and one fatal. The smaller
problem is that the language model statistics are word-level perplexities computed on
Penn Tree Bank (PTB), while the Human perplexity is from Shen et al 2017, which
estimated the word-level perplexity on the 1 Billion Words (IBW) benchmark. This turns
out not to matter much, as while GPT-3 was not evaluated on 1BW, GPT-2
performs slightly worse on 1BW than PTB. The bigger issue is the methodology used to
estimate human perplexity: Shen et al asked humans to rate sentences on a 0-3 scale,
where 0 means "clearly inhuman" and "3" was clearly human, then computed a
"human judgment score" consisting of the ratio of sentences rated 3 over those rated
0. They then ﬁt a degree-3 polynomial regression to the LMs they had (of which the
best was a small LSTM), which they extrapolated signiﬁcantly out of distribution to
acquire the "human" perplexity:  

This methodology is pretty dubious for several reasons, and we wouldn't put much
stock into the "humans have 12 perplexity" claim. 
Another claim is from OpenAI regarding the LAMBADA dataset, where they give a
perplexity of "~1-2" for humans (compared to 3.0 for 0-shot GPT-3). However, the
authors don't cite a source. As Gwern notes, the authors of the blog post likely made
an educated guess based on how the LAMBADA dataset was constructed. In addition,
LAMBADA is a much restricted dataset, which consists of guessing single words
requiring broad context. So this comparison isn't very informative to the question of
how good language models are at the task they're trained on-predicting the next token
on typical internet text.
The only study we know of which tried to do something closely analogous is Goldstein
2020 (a neuroscience paper), which found that an ensemble of 50 humans have a top-
1 accuracy of 28% vs 36% for GPT-2, which is similar to what we saw for humans on
webtext. However, they used a diﬀerent dataset (this transcript), which is not
particularly representative of randomly-sampled English internet text.

There's certainly a lot of more narrow datasets on which we have both human and LM
performance, where humans signiﬁcantly outperform LMs. For example, Hendrycks et
al's MATH dataset. But we couldn't ﬁnd an apples-to-apples comparison between
humans and modern LMs for webtext next-token prediction.
How to measure human performance at next-
token prediction?
The main diﬃculty for comparing human vs LM performance is that, unlike with
language models, it's infeasible for humans to give their entire probability distribution
for the next token in a sequence (as there are about ﬁfty thousand tokens). We tried
two approaches to get around this.
The ﬁrst one is to ask humans what token is most likely to come next. Using this
method, you can't get humans' perplexity, but top-1 accuracy might still give us a
reasonable measure of how well humans do at next token prediction. According to this
measure, humans are worse than all (non-toy) language models we tried, even if you
use really smart humans who have practiced for more than an hour.
We tried a diﬀerent approach to get a measurement of human perplexity: humans were
asked to rate the relative probability of two diﬀerent tokens. If many humans
answer this question on the same prompt, you can estimate the probability of the
correct token according to humans, and thus estimate human perplexity. As with the
top-1 accuracy metric, humans score worse according to this perplexity estimator than
all language models we tried, even a toy 2-layer model. 
Top-1 accuracy evaluation
Method
We measured human top-1 accuracy: that is, they had to guess the single token that
they thought was most likely to come next, and we measured how often that token in
fact came next. Humans were given the start of a random OpenWebText document,
and they were asked to guess which token comes next. Once they had guessed, the
true answer was revealed (to give them the opportunity to get better at the game), and
they were again asked to guess which token followed the revealed one. 
Here is the website where participants played the game. We recommend playing the
game if you want to get a sense of what next-token prediction is like. 

The participants were either staﬀ/advisors of Redwood Research, or members of the
Bountied Rationality Facebook group, paid $30/hour.
Results
60 participants played the game, making a total of 18530 guesses. The overall
accuracy of their answers on this top-1 task was 29%. Of these players, 38 gave
at least 50 answers, with an accuracy of 30%. This accuracy is low compared to the
accuracy of large language models: when measured on the same dataset humans were
evaluated on, GPT-3 got an accuracy of 49% and even the 125M parameters fairseq
model got an accuracy above the accuracy of all but one player in our dataset (who
guessed 70 tokens), as you can see in the graph below (though it seems possible that
with practice humans might be able to beat fairseq-125M reliably).
7 players guessed over 500 tokens, getting accuracies around the average of human

performance (0.26, 0.26, 0.27, 0.28, 0.31, 0.31, 0.32), which indicates that humans
don't quickly get much higher performance with ﬁve hours of training.
Some of these scores are from Redwood Research staﬀ and advisors who we think were
very motivated to try hard and do well, so we don't think that this is an artifact of our
participants being weak.
Some participants wrote up notes on the experience, describing their strategies and
what was easy/hard. Links: William Kiely, Marcus.
This website didn't have any way for humans to guess newlines or some other visually-
empty tokens, and we excluded cases where the correct guess was impossible from the
above analysis.
To sum up, humans have only barely beat 2017-era language models, and humans are
much worse than modern ones at top-1 token prediction.
Human perplexity evaluation
(Thanks to Paul Christiano for suggesting the rough approach here, and to Paul and
Adam Scherlis for helping with some of the details.)
But language models aren't trained to guess the single most likely next token, and so
the previous task isn't actually directly assessing the language models on the task
they're trained on. They're trained to give a full distribution over next tokens, aiming to
minimize their perplexity (aka minimizing log loss; perplexity is a measure of how well
a probability distribution predicts a sample). It's easy to calculate the model's
perplexity, because the models produce a probability for every next token.
Unfortunately, humans aren't able to quickly provide a probability distribution over the
ﬁfty thousand possible tokens. So if we want to measure human perplexity, we'll have
to do it indirectly.
Method
The method we ended up using is complicated and messy, and though it's a fun
exercise in probability theory, it is probably not worth your time to understand if you
just want to know about human performance on language modeling.
For a given prompt sampled from the validation dataset of OpenWebText, we make the
dubious assumption that all humans have exactly the same probability distribution
over next tokens, and then infer the probability that humans assign to the correct
token by showing many humans diﬀerent choices between the correct next token and
another random token, and then asking them for the probability that the left one is the
correct token.
We used importance sampling and comparison with a reference language model to
reduce the variance of the estimator (details in Appendix A). This importance sampling
scheme allows the human to update on the probability that a token is the true next
token based on the fact that they're being asked about it. So we came up with a
scoring rule, detailed in Appendix B and explained on the website, which incentivizes
people to state their true probabilities before updating on which tokens they'd been
asked to compare.

Here is the  website where participants played the game. 
The participants were again either staﬀ of Redwood Research or members of the
Bountied Rationality Facebook group paid $15 for answering a set of comparisons
(which takes ~30 minutes, taking into account a bit of training). The ﬁrst set consists of
40 questions from diﬀerent texts (you can take it here). The second set consists of 80
comparisons from 8 diﬀerent texts (you can take it here).
Note: prompts are chosen to be at most 120 tokens long in order to spare human
reading time, and always start at the beginning of the text. Models are evaluated on
the same dataset.
Results
19 participants answered all 40 questions of the ﬁrst set, and 11 participants answered
all 80 questions of the second set, answering a total of 1640 comparisons. This was
enough to conclude that human are worse than language models, as you can see in the
graph below:
 
Human and language model performance are then compared exactly on the same
comparison. As our human participants could only enter one of the 11 ratios in our

interface, we also report the "rounded" performance of our LMs - that is, the
performance of our LMs if they choose the checkbox that is the closest to their
probability ratio. (Note that we can't access the true perplexity of rounded models as
only ratios are rounded and not the probability of the correct token.)
As explained in Appendix A, the loss obtained by this method is usually an
underestimation of the true loss you would get if you asked inﬁnitely many questions,
because the sum used to do the estimations is heavily tailed. The displayed (2
standard error) uncertainty intervals represents "where would the estimation be if we
did the experiment a second time with diﬀerent samples", rather than "where is the
true value of human loss" (which would probably be above the upper bound of the
interval). GPT-2 small is used as the generator, which is why its measured perplexity
using the estimator is perfect. More details about the uncertainty estimation can be
found in Appendix C.
But this method could also overestimate human perplexity: there could be other setups
in which it would be easier for players to give calibrated probabilities. In fact, some
players found the scoring system hard to understand, and if it led them to not express
their true probability ratios, we might have underestimated human performance. In
general, this method is very sensitive to failures at giving calibrated probability
estimates: the high perplexity obtained here is probably partially due to humans being
bad at giving calibrated probabilities, rather than humans just being bad at language
modeling. In addition, as humans are restricted to one of 11 ratios, our setup could also
underestimate performance by artiﬁcially reducing the resolution of our human
participants. 
Thus, while we don't have a good way to precisely measure human perplexity, these
results give reasonable evidence that it is high. In particular, humans are worse than a
two-layer model at giving calibrated estimates of token vs token probabilities. 
It's worth noting that our results are not consistent with the classic result from Shannon
1950, which estimated the the average per-character entropy to be between 0.6 and
1.3 bits, corresponding to a per-token perplexity between 7 and 60 (as the average
length of tokens in our corpus is 4.5). This is likely due to several reasons. First, as
noted above, our setup may artiﬁcially increase the estimated perplexity due to our
human subjects being uncalibrated and our interface rounding oﬀ probability ratios. In
addition, Shannon used his wife Mary Shannon and the HP Founder Barnard Oliver as
his subjects, who may be higher quality than our subjects or have spent more time
practicing on the task. [EDIT: as Matthew Barnett notes in a comment below, our
estimate of human perplexity is consistent with other estimates performed after
Shannon.] Finally, he used a diﬀerent dataset (excerpts from Dumas Malne's Jeﬀerson
the Virginian, compared to our OpenWebText excerpts). 
Conclusion
The results here suggest that humans are worse than even small language models the
size of GPT-1 at next-token prediction, even on the top-1 prediction task. This seems
true even when the humans are smart and motivated and have practiced for an hour or
two. Some humans can probably consistently beat GPT-1, but not substantially larger
models, with a bit more practice.
What should we take away from this?

Even current large language models are wildly superhuman at language
modeling. This is important to remember when you're doing language model
interpretability, because it means that you should expect your model to have a
lot of knowledge about text that you don't have. Chris Olah draws a picture where
he talks about the possibility that models become more interpretable as they get
to human level, and then become less interpretable again as they become
superhuman; the fact that existing LMs are already superhuman (at the task
they're trained on) is worth bearing in mind when considering this graph.
Next-token prediction is not just about understanding the world; it's also
substantially about guessing sentence structure and word choice, which isn't
actually a useful ability for models to have for most applications. Next-token
prediction is probably much less eﬃcient than other tasks at training a
competent/useful language model per bit of training data. But data for next-token
prediction is so cheap that it ends up being the best pretraining task anyway.
Some people we spoke with are surprised by these results, because humans
are better at writing coherent text than GPT-2 and so they expect humans
to be better at next-token prediction. But actually these tasks are very
diﬀerent-if you train an autoregressive model to imitate human text, the
model has to dedicate capacity to all the diﬀerent features that might be
informative for guessing the next token (including "high-frequency"
features that don't aﬀect human judgments of coherence). However, if you
train a model to generate coherent text, it only has to dedicate capacity to
continuing text in one particular reasonable way, rather than in all possible
reasonable ways, and so per parameter it will be better at continuing text
reasonably (and worse at language modeling).
Appendix A: How to estimate human loss with
importance sampling
Let T be the true distribution of tokens y after a context c. The loss of a human is
deﬁned to be the average -log probability of the true token according to the human
probabilities h over the next token:  L = −E(C,Y )∼T log h(Y |C)
However, we can't directly ask a human what the probability of the true token is
(without spoiling them on the answer), and it would be very cumbersome to ask for
their whole probability distribution. We can do better by asking for relative likelihoods:
for a given context c and true token y, because ∑x p(x|c) = 1,  
−log h(y|c) = log ∑x h(x/c)/h(y|c).
That's better, but that would still be around 50,000 questions (the number of tokens)
per token. To lower this number, we use importance sampling: the bulk of 
∑h(x/c)/h(y|c) is where the most likely tokens are, so it's not worth asking for every one
of them. To do that, we condition a language model G on our context C from which we
can sample the most likely tokens, and we use the following approximation:

∑all tokens x h(x/c)/h(y|c) = EX∼Gc(
) ≈
∑x ∼Gc
 for an  that can be much
smaller than 50,000. for an n that can be much smaller than 50,000.
To decrease the variance of this estimator (and thereby increase the quality of the
estimation), instead of estimating h(y|c) we estimate 
= EX∼Gc(
) 
≈
∑x ∼Gc
. The variance is lower because, if h and g are close, 
 will
most of the time be close to 1, whereas 
 can get very large on some samples.
Thanks to the properties of log, L −LG = −E(C,Y )∼T log
 (where LG is the loss of G,
deﬁned in the same way as L but using g instead of h).
Using N samples from the true target corpus, we get
L − L G ≈ 
  ∑ ( c , y )  ∼  T  log  ( 
  ∑ x  ∼  G c 
  )
In practice, we use GPT-2 small - a 12-layer language model - as our generator
language model.
Using this method to measure the loss of a 2-layer LM and a 24-layer LM using a 12-
layer LM as a generator, we ﬁnd that this method underestimates of the true loss of the
ground truth, and that the results is at most 0.5 bits away from the ground truth when 
n ≈40, for models that are very dissimilar (like 2-layer model vs 12-layer LM, which the
diﬀerence in true loss is 1.3 bits). This is a large diﬀerence, but this is still good enough
for the purpose of comparing humans to language models (because humans are
terrible language models). The diﬀerence can be explained by the fact that the sum
over x is heavy tailed toward +∞: if you don't sample enough, your empirical mean will
h(X|c)
h(y|c)g(X|c)
1n
h(x|c)
h(y|c)g(x|c)
h(y|c)
g(y|c)
h(X|c)g(y|c)
h(y|c)g(X|c)
1n
h(x|c)g(y|c)
h(y|c)g(x|c)
h(x|c)g(y|c)
h(y|c)g(x|c)
h(x|c)
h(y|c)g(x|c)
h(Y |C)
g(Y |C)
1N
1n
h ( x | c ) g ( y | c )
h ( y | c ) g ( x | c )

often be below the true expected value.
Appendix B: How to get human relative
likelihoods
We want to get h(x|c)/h(y|c), the relative likelihood of tokens x and y in a given context
C according to a human. This might seem easy: just make the human guess if the
prompt c is followed by x or y, and then the human should answer that c is followed by
x with probability 
. This would be true if one token was sampled from the true
distribution and the other one was selected uniformly between all other tokens.
However, this doesn't work if in this case because the other token is sampled from Gc:
a rational agent that perfectly knows the true distribution T and the generated
distribution G would answer that c is followed by x with probability
 
h(x|c)
h(y|c)+h(y|c)
P ( x  ∼  T c ∧ y  ∼  G c )
P ( x  ∼  T c ∧ y  ∼  G c ) + P ( x  ∼  G c ∧ y  ∼  T c )

=
 (because of the independence of Tc & Gc).
Thus, if you believe Tc to get indistinguishable from Gc, a human will answer "0.5" to
every question, making it impossible to extract h(x|c)/h(y|c).
The solution to get h(x|c)/h(y|c) is to incentivize the human to give something else than
their best guess. We ask the human for the probability p that c is followed by x, and we
reward them with a weighted binary cross entropy reward 
R(p) = g(x|c) log(p)1z=x + g(y|c) log(1 −p)1z=y where z is the correct answer. The
expected value of this reward, according to a human believing that the generative
follows a distribution  ^
G , is 
E(R(p)) = ^
g (x|c) log(p)(h(x|c)^
g (y|c)) + ^
g (y|c) log(1 −p)(h(y|c)^
g (x|c)) which is at its
maximum when 
= 0, therefore, the optimal play satisﬁes 
^
g (x|c)^
g (y|c)h(x|c)/p∗= ^
g (y|c)^
g (x|c)h(y|c)/(1 −p∗), hence p∗=
. From this,
we get h(y|c)/h(x|c) =
−1. Assuming human play optimally (which is a questionable
assumption), we are able to get their true probability ratios, no matter what their
beliefs about the generative model are.
Note: in practice we use a slightly diﬀerent reward 
R(p) = 1000(g(x|c)(log(p) −log(0.5))1z=x + g(y|c)(log(1 −p) −log(0.5))1z=y for which
the optimal play is the same, but which is more understandable for human: one always
gets a reward of 0 for saying "I don't know", and the scaling makes things more
readable.
Note 2: when x = y the human isn't asked to compare their relative likelyhoods.
Instead, the website automatically answers that both are as likely.
Appendix C: estimating uncertainties in
human perplexity
If we assume humans play rationally in the game described above, there remains three
sources of uncertainty in our measurement of human perplexity:
1. We are only using 120 diﬀerent (c, y) pairs, which is small considering that the
logloss on tokens has a large variance. This causes a non-negligible uncertainty
over the measured perplexity, even if we had perfect estimates of h(y|c)/g(y|c).
We compute the empirical standard deviation σ on the log loss. This gives us a
t(x|c)g(y|c)
t(y|c)g(x|c)+t(x|c)g(y|c)
dE(R(p))
dp
h(x|c)
h(y|c)+h(x|c)
1
p∗

lower bound exp(L −2σ) and a upper bound exp(L + 2σ) on the perplexity 
P = exp(L). This is the uncertainty that is displayed in the graph.
2. The sum used to evaluate h(y|c)/g(y|c) is heavy tailed, which makes us
underestimate the perplexity of any model using this technique (because we only
have a small amount of samples (≈10) for every prompt). This can not be easily
quantiﬁed, as the weight of the heavy tail examples depends on the distance
between human predictions and GPT-2 small prediction, as shown in the graph at
the end of appendix A.
3. The sum used to evaluate h(y|c)/g(y|c) is a sum of samples from a random
variable, which would be stochastic even without the heavy tail aspect. If we
ignore this aspect, and use the standard deviation as an estimate for the
uncertainty over EX∼Gc(
), we can measure the uncertainty over human
perplexity on the given samples (aka the uncertainty over 
L =
∑(c,y) ∼T log EX∼Gc(
) estimated by 
∑(c,y) ∼T log(
e(y, c)) where 
e(y, c) =
∑x ∼Gc
. Using the variance formula, we get UL = √∑(c,y) ∼T(
)2
 where Ue is obtained by computing the empirical standard deviation of the terms
in the sum deﬁning e (and diving by √n).
Uncertainties 1 & 3 are hard to combine, and because 3 is small compared to 1, we
chose to use only the ﬁrst one.
Here is the same graph as shown in the result section, but using only the uncertainties
measured using technique 3.
h(X|c)g(y|c)
h(y|c)g(X|c)
1N
h(X|c)g(y|c)
h(y|c)g(X|c)
1N
1n
1n
h(x|c)g(y|c)
h(y|c)g(x|c)
Ue
eN

Worlds Where Iterative Design Fails
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
In most technical ﬁelds, we try designs, see what goes wrong, and iterate until it works.
That's the core iterative design loop. Humans are good at iterative design, and it works well
in most ﬁelds in practice.
In worlds where AI alignment can be handled by iterative design, we probably survive. So
long as we can see the problems and iterate on them, we can probably ﬁx them, or at least
avoid making them worse.
By the same reasoning: worlds where AI kills us are generally worlds where, for one reason
or another, the iterative design loop fails. So, if we want to reduce X-risk, we generally need
to focus on worlds where the iterative design loop fails for some reason; in worlds where it
doesn't fail, we probably don't die anyway.
Why might the iterative design loop fail? Most readers probably know of two widely-
discussed reasons:
Fast takeoﬀ: there will be a sudden phase shift in capabilities, and the design of
whatever system ﬁrst undergoes that phase shift needs to be right on the ﬁrst try.
Deceptive inner misalignment: an inner agent behaves well in order to deceive us, so
we can't tell there's a problem just by trying stuﬀ and looking at the system's behavior.
... but these certainly aren't the only reasons the iterative design loop potentially fails. This
post will mostly talk about some particularly simple and robust failure modes, but I'd
encourage you to think on your own about others. These are the things which kill us; they're
worth thinking about.
Basics: Hiding Problems
Example/Analogy: The Software Executive
Imagine that a software company executive, concerned about the many errors coming from
the software, creates a new incentive scheme: software developers get a monetary reward
for changes which decrease the rate of error messages showing up on the manager's
dashboard, and get docked for changes which increase the rate of error messages.
As Tyler Cowen would say: "solve for the equilibrium". Obvious equilibrium here: the
developers stop throwing error messages when they detect a problem, and instead the
software just fails silently. The customer's experience remains the same, but the manager's
dashboard shows fewer error messages. Over time, the customer's experience probably
degrades, as more and more problems go undetected.
In the short run, the strategy may eliminate some problems, but in the long run it breaks the
iterative design loop: problems are not seen, and therefore not iterated upon. The loop fails
at the "see what goes wrong" step.
Why RLHF Is Uniquely Terrible
The software executive's strategy is the same basic idea as Reinforcement Learning from
Human Feedback (RLHF). AI does something, a human looks at what happened to see if it
looks good/bad, and the AI is trained on the human's feedback. Just like the software
executive's anti-error-message compensation scheme, RLHF will probably result in some

problems actually being ﬁxed in the short term. But it renders the remaining problems far
less visible, and therefore breaks the iterative design loop. In the context of AI, RLHF makes
it far more likely that a future catastrophic error will have no warning signs, that overseers
will have no idea that there's any problem at all until it's much too late.
Note that this issue applies even at low capability levels! Humans overlook problems all the
time, some of those mistakes are systematic, and RLHF will select for places where humans
systematically overlook problems; that selection pressure applies even when the neural net
lacks great capabilities.
Net learns to hold hand in front of ball, so that it looks to a human observer like
the ball is being grasped. Yes, this actually happened.
This is the core reason why I consider RLHF uniquely terrible, among alignment schemes. It is
the only strategy I know of which actively breaks the iterative design loop; it makes
problems less visible rather than more.
Generalization: Iterate Until We Don't See Any Problems

More generally, one of the alignment failure modes I consider most likely is that an
organization building AGI does see some problems in advance. But rather than addressing
root causes, they try to train away the problems, and instead end up training the problems to
no longer be easily noticeable.
Does This Prove Too Much?
One counterargument: don't real organizations create incentives like the software executive
all the time? And we have not died of it.
Response: real organizations do indeed create incentives to hide problems all the time, and
large organizations are notorious for hiding problems at every level. It doesn't even require
employees consciously trying to hide things; selection pressure suﬃces. Sometimes
important problems become public knowledge when a disaster occurs, but that's
usually after the disaster. The only reason we haven't died of it yet is that it is hard to wipe
out the human species with only 20th-century human capabilities. 
Less Basic: Knowing What To Look For
Example/Analogy: The Fusion Power Generator
Suppose, a few years from now, I prompt GPT-N to design a cheap, simple fusion power
generator - something I could build in my garage and use to power my house. GPT-N
succeeds. I build the fusion power generator, ﬁnd that it works exactly as advertised, share
the plans online, and soon the world has easy access to cheap, clean power.
One problem: at no point did it occur to me to ask "Can this design easily be turned into a
bomb?". Had I thought to prompt it with the question, GPT-N would have told me that the
design could easily be turned into a bomb. But I didn't think to ask, so GPT-N had no reason
to mention it. With the design in wide use, it's only a matter of time until people ﬁgure it out.
And so, just like that, we live in a world where anyone can build a cheap thermonuclear
warhead in their garage.
The root problem here is that I didn't think to ask the right question; I didn't pay attention to
the right thing. An iterative design loop can sometimes help with that - empirical observation
can draw our attention to previously-ignored issues. But when the failure mode does not
happen in testing, the iterative design loop generally doesn't draw our attention to it. An
iterative design loop does not, in general, tell us which questions we need to ask.
Ok, but can't we have an AI tell us what questions we need to ask? That's trainable, right?
And we can apply the iterative design loop to make AIs suggest better questions?
Example/Analogy: Gunpowder And The Medieval Lord
Imagine a medieval lord in a war against someone with slightly more advanced technological
knowledge. We're not talking modern weaponry here, just gunpowder.
To the lord, it doesn't look like the technologist is doing anything especially dangerous;
mostly the technologist looks like an alchemist or a witch doctor. The technologist digs a
hole, stretches a cloth over, dumps a pile of shit on top, then runs water through the shit-pile
for a while. Eventually they remove the cloth and shit, throw some coal and brimstone in the
hole, and mix it all together.
From the lord's perspective, this deﬁnitely looks weird and mysterious, and they may be
somewhat worried about weird and mysterious things in general. But it's not obviously any
more dangerous than, say, a shaman running a spiritual ceremony.

It's not until after the GIANT GODDAMN EXPLOSION that the shit-pile starts to look unusually
dangerous.
Now, what helpful advice could an AI give this medieval lord?
Obviously the AI could say "the powder which comes out of that weird mysterious process is
going to produce a GIANT GODDAMN EXPLOSION". The problem is, it is not cheap for the
medieval lord to verify the AI's claim. Based on the lord's knowledge, there is no a-priori
reason to expect the process to produce explosives rather than something else, and the
amount of background knowledge the lord would need in order to verify the theory is
enormous. The lord could in-principle verify the AI's claim experimentally, but then (a) the
lord is following a complex  procedure which he does not understand handed to him by a not-
necessarily-friendly AI, and (b) the lord is mixing homemade explosives in his backyard. Both
of these are dubious decisions at best.
So if we're already conﬁdent that the AI is aligned, sure, it can tell us what to look for. But if
there's two AIs side-by-side, and one is saying "that powder will explode" and the other is
saying "the shit-pile ceremony allows one to see the world from afar, perhaps to spot holes
in our defenses", the lord cannot easily see which of them is wrong. The two can argue with
each other debate-style, and the lord still will not easily be able to see which is wrong,
because he would need enormously more background knowledge to evaluate the arguments
correctly. And if he can't tell what the problem is, then the iterative design process can't ﬁx
it.
Example/Analogy: Leaded Gasoline
Leaded gasoline is a decent historical analogue of the Fusion Generator Problem, though less
deadly. It did solve a real problem: engines ran smoother with leaded gas. The problems
were nonobvious, and took a long time to manifest. The iterative design loop did not work,
because we could not see the problem just by testing out leaded gas in a lab. A test would
have had to run for decades, at large scale, in order to see the issue - and that's exactly
what happened.
One could reasonably object to this example as an analogy, on the basis that things which
drive the human species extinct would be more obvious. Dead bodies draw attention. But
what about things which make the human species more stupid or aggressive? Lead did
exactly that, after all. It's not hard to imagine a large-scale issue which makes humans
stupid or aggressive to a much greater extent, but slowly over the course of years or
decades, with the problems going undetected or unaddressed until too late.
That's not intended to be a highly probable story; there's too much speciﬁc detail. The point
is that, even if the proximate cause of extinction is obvious, the factors which make that
proximate cause possible may not be. A gradual path to extinction is a real possibility. When
problems only manifest on long timescales, the iterative design process is bad at ﬁxing
them.
Meta Example/Analogy: Expertise and Gell-Mann Amnesia
If you don't know a fair bit about software engineering, you won't be able to distinguish good
from bad software engineers.
(Assuming stats from a couple years ago are still representative, at least half my readers can
probably conﬁrm this from experience. On the other hand, last time I brought this up, one
commenter said something along the lines of "Can't we test whether the code works without
knowing anything about programming?". Would any software engineers like to explain in the
comments why that's not the only key question to ask?)

Similarly, consider Gell-Mann Amnesia:
You open the newspaper to an article on some subject you know well. In Murray's case,
physics. In mine, show business. You read the article and see the journalist has
absolutely no understanding of either the facts or the issues. Often, the article is so
wrong it actually presents the story backward—reversing cause and eﬀect. I call these
the "wet streets cause rain" stories. Paper's full of them.
In any case, you read with exasperation or amusement the multiple errors in a story, and
then turn the page to national or international aﬀairs, and read as if the rest of the
newspaper was somehow more accurate about Palestine than the baloney you just read.
You turn the page, and forget what you know.
I think there's a similar eﬀect for expertise: software engineers realize that those outside
their ﬁeld have diﬃculty distinguishing good from bad software engineers, but often fail to
generalize this to the insight that non-experts in most ﬁelds have diﬃculty distinguishing
good from bad practitioners. There are of course some general-purpose tricks (and they are
particularly useful expertise to have), but they only get you so far.
The diﬃculty of distinguishing good from bad experts breaks the iterative design loop at a
meta level. We realize that we might not be asking the right questions, our object-level
design loop might not suﬃce, so we go consult some experts. But then how do we iterate on
our experts? How do we ﬁnd better experts, or create better experts? Again, there are some
general-purpose tricks available, but they're limited. In general, if we cannot see when
there's a problem with our expert-choice, we cannot iterate to ﬁx that problem.
More Fundamental: Getting What We Measure
I'm just going to directly quote Paul's post on this one:
If I want to convince Bob to vote for Alice, I can experiment with many diﬀerent
persuasion strategies and see which ones work. Or I can build good predictive models of
Bob's behavior and then search for actions that will lead him to vote for Alice. These are
powerful techniques for achieving any goal that can be easily measured over short time
periods.
But if I want to help Bob ﬁgure out whether he should vote for Alice - whether voting for
Alice would ultimately help create the kind of society he wants - that can't be done by
trial and error. To solve such tasks we need to understand what we are doing and why it
will yield good outcomes. We still need to use data in order to improve over time, but we
need to understand how to update on new data in order to improve.
Some examples of easy-to-measure vs. hard-to-measure goals:
Persuading me, vs. helping me ﬁgure out what's true. (Thanks to Wei Dai for
making this example crisp.)
Reducing my feeling of uncertainty, vs. increasing my knowledge about the world.
Improving my reported life satisfaction, vs. actually helping me live a good life.
Reducing reported crimes, vs. actually preventing crime.
Increasing my wealth on paper, vs. increasing my eﬀective control over resources.
If I want to help Bob ﬁgure out whether he should vote for Alice, that can't be done by trial
and error. That really gets at the heart of why the iterative design loop is unlikely to suﬃce
for alignment, even though it works so well in so many other ﬁelds. In other ﬁelds, we usually
have a pretty good idea of what we want. In alignment, ﬁguring out what we want is itself a
central problem. Trial and error doesn't suﬃce for ﬁguring out what we want.

So what happens if we rely on trial and error to ﬁgure out what we want? More from Paul's
post:
We will try to harness this power by constructing proxies for what we care about, but
over time those proxies will come apart:
Corporations will deliver value to consumers as measured by proﬁt. Eventually this
mostly means manipulating consumers, capturing regulators, extortion and theft.
Investors will "own" shares of increasingly proﬁtable corporations, and will
sometimes try to use their proﬁts to aﬀect the world. Eventually instead of actually
having an impact they will be surrounded by advisors who manipulate them into
thinking they've had an impact.
Law enforcement will drive down complaints and increase reported sense of
security. Eventually this will be driven by creating a false sense of security, hiding
information about law enforcement failures, suppressing complaints, and coercing
and manipulating citizens.
Legislation may be optimized to seem like it is addressing real problems and
helping constituents. Eventually that will be achieved by undermining our ability to
actually perceive problems and constructing increasingly convincing narratives
about where the world is going and what's important.
For a while we will be able to overcome these problems by recognizing them, improving
the proxies, and imposing ad-hoc restrictions that avoid manipulation or abuse. But as
the system becomes more complex, that job itself becomes too challenging for human
reasoning to solve directly and requires its own trial and error, and at the meta-level the
process continues to pursue some easily measured objective (potentially over longer
timescales). Eventually large-scale attempts to ﬁx the problem are themselves opposed
by the collective optimization of millions of optimizers pursuing simple goals.
As this world goes oﬀ the rails, there may not be any discrete point where consensus
recognizes that things have gone oﬀ the rails.
[...]
We might describe the result as "going out with a whimper." Human reasoning gradually
stops being able to compete with sophisticated, systematized manipulation and
deception which is continuously improving by trial and error; human control over levers
of power gradually becomes less and less eﬀective; we ultimately lose any real ability to
inﬂuence our society's trajectory.
Summary & Takeaways
In worlds where the iterative design loop works for alignment, we probably survive AGI. So, if
we want to improve humanity's chances of survival, we should mostly focus on worlds
where, for one reason or another, the iterative design loop fails. Fast takeoﬀ and deceptive
inner misalignment are two widely-talked-about potential failure modes, but they're not the
only ones. I wouldn't consider either of them among the most robust ways in which the
design loop fails, although they are among the most obviously and immediately dangerous
failures.
Among the most basic robust design loop failures is problem-hiding. It happens all the time
in the real world, and in practice we tend to not ﬁnd out about the hidden problems until 
after a disaster occurs. This is why RLHF is such a uniquely terrible strategy: unlike most
other alignment schemes, it makes problems less visible rather than more visible. If we can't
see the problem, we can't iterate on it.

A more complicated and less legible class of design loop failures is not knowing what to look
for. We might just not ask the right questions (as in the fusion power generator example), we
might not even have enough background knowledge to recognize the right questions when
they're pointed out (as in the medieval lord example), it might take a very long time to get
feedback on the key problems (as in the leaded gasoline example), and at a meta level we
might not have the expertise to distinguish real experts from non-experts when seeking
advice (as in Gell-Mann Amnesia).
Finally, we talked about Paul's "You Get What You Measure" scenario. As Paul put it: "If I want
to help Bob ﬁgure out whether he should vote for Alice - whether voting for Alice would
ultimately help create the kind of society he wants - that can't be done by trial and error."
That really captures the core reason why an iterative design loop is likely to fail for
alignment, despite working so well in so many other ﬁelds: in other ﬁelds, we usually know
what we want and are trying to get it. In alignment, ﬁguring out what we want is itself a
central problem, and the iterative design loop does not suﬃce for ﬁguring out what we want.

Your posts should be on arXiv
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
TL;DR: There are many posts on the Alignment Forum/LessWrong that could
easily be on arXiv. Putting them on arXiv has several large beneﬁts and
(sometimes) very low costs. 
Beneﬁts of having posts on arXiv
There are several large beneﬁts of putting posts on arXiv:
1. Much better searchability, shows up in google scholar searches.
2. Additional reads (arXiv sanity, arXiv newsletters, and so on).
3. The article can accumulate citations, which are shown in google/google
scholar search results.
1) - 3) lead to more people reading your research, which hopefully leads to more
people building on it and maybe useful feedback from outside of the established
alignment community. In particular, if people see that the paper already has citations,
this will lead to more people reading it, which will lead to more citations, and so on.
You'd gain even more of 2) and 3) from publishing it at a conference, but, unlike arXiv,
that's signiﬁcant additional work (often still worth it).
There are also some smaller beneﬁts from publishing on arXiv:
4. ﬁrmly establishes this as your contribution (not sure, but I think if you only
have an alignment forum post, someone could build a bit on it and then claim
the whole thing as their contribution because alignment forum posts don't
count?).
5. better citability (e.g. if somebody writes an ML paper to be published in ML
venues, it gives more credibility to cite arXiv papers than Alignment
Forum/LessWrong posts. The same goes for people writing e.g. Wikipedia articles
about alignment.)
How much work is it to submit to
arXiv?
Citing DavidHolmes from the comments: "There is a certain amount of moderation on
arXiv. This is a little opaque [...].  In writing this I don't want to give the impression
that posting things to arXiv is hard; I have currently 28 papers there, have never had
a single problem or delay with moderation, and the submission process generally
takes me <15 mins these days."
Sometimes, I think getting your forum post ready for submission can be as easy as
creating a pdf of your post (although if your post was written in LaTeX, they'll want the
tex ﬁle). If everything goes well, the submission takes less than an hour.

However, if your post doesn't look like a research article, you might have to format it
more like one (and even then it's not guaranteed to get in, see this comment thread). 
If you are submitting to arXiv for the ﬁrst time, you might have to get an endorsement
from someone who has already published on arXiv. The endorsement sometimes
won't be required if you have an academic email address, so be sure to use that one
for submission. 
If you're a Very Busy Alignment Researcher, I'm sure you can outsource large parts of
this. E.g. FAR's comms staﬀ could probably help. I also have worked with a freelancer
on similar things in the past (like making publications look nice in LaTeX), feel free to
reach out for the contact data.
Highlights from the comments:
DavidHolmes's tips for how to submit to arXiv.
Dan Hendrycks's suggestions on which sections of arXiv are suitable for
alignment work.
This comment thread suggests that academic formatting/layout will probably be
useful.
What types of posts should be on
arXiv?
To be clear, I think that most LessWrong posts should not be on arXiv. The bar for
submitting to arXiv should be higher than that for submitting to LessWrong/AF. Still,
there are many research contributions on the Alignment Forum/LessWrong that
wouldn't look out of place on arXiv. Submitting to arXiv is particularly useful if the
post's target audience is wider than the LessWrong readership.
Here are some examples of posts that also are on arXiv, or, IMO, should be (skewed by
what I read and remembered):
Conceptual and theoretical research:
Joe Carlsmith's Draft report on existential risk from power-seeking AI (it's
on arXiv now, but >1 year after it was published)
Ajeya Cotra's Draft report on AI timelines (not on arXiv but should be)
Richard Ngo's AGI safety from ﬁrst principles (not on arXiv but should be)
I haven't read it, but from the name of it, it sounds as if ARC's ﬁrst
technical report: Eliciting Latent Knowledge should maybe be on arXiv
The Truthful AI work by Lin, Evans, Cotton-Barratt, and others (conceptual
paper on arXiv)
Hendrycks's and Mazeika's X-risk Analysis for AI Research (on arXiv)
Empirical ML research:
High-stakes alignment via adversarial training [Redwood Research report]
(on arXiv)
The Truthful AI work by Lin, Evans, Cotton-Barratt, and others (empirical
paper on arXiv)
Neel Nanda's and Tom Lieberum's A Mechanistic Interpretability Analysis of
Grokking (To the surprise of me and several others, this post was actually
not accepted by arXiv.)

Lots of ML alignment and safety work by Hendrycks et al., e.g. this, this,
and this (all on arXiv)
Language models seem to be much better than humans at next-token
prediction could be on arXiv, but would probably need some edits to
adhere more to "scientiﬁc paper style"
Highlights from the comments:
Dan Hendrycks's suggestions on what to put (and not put) on arXiv.
Davidmanheim with three more examples of alignment papers he put on arXiv.
If arXiv doesn't ﬁt
There are also some other posts on the Alignment Forum/LessWrong whose target
audience is the wider AI community. I think should be published additionally
elsewhere. A great example is (one of my all-time favourite posts) Ajeya Cotra's
Without speciﬁc countermeasures, the easiest path to transformative AI likely leads to
AI takeover. This maybe wouldn't really ﬁt on arXiv (although it wouldn't be crazy to
put on on arXiv either). But there is a range of other venues that might publish it, from
Towards Data Science (on the low eﬀort, low prestige end) to the MIT Technology
Review (on the high eﬀort, high prestige end).

How To Go From Interpretability To
Alignment: Just Retarget The Search
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
When people talk about prosaic alignment proposals, there's a common pattern:
they'll be outlining some overcomplicated scheme, and then they'll say "oh, and
assume we have great interpretability tools, this whole thing just works way better the
better the interpretability tools are", and then they'll go back to the overcomplicated
scheme. (Credit to Evan for pointing out this pattern to me.) And then usually there's
a whole discussion about the speciﬁc problems with the overcomplicated scheme.
In this post I want to argue from a diﬀerent direction: if we had great interpretability
tools, we could just use those to align an AI directly, and skip the overcomplicated
schemes. I'll call the strategy "Just Retarget the Search".
We'll need to make two assumptions:
Some version of the natural abstraction hypothesis holds, and the AI ends up
with an internal concept for human values, or corrigibility, or what the user
intends, or human mimicry, or some other outer alignment target.
The  standard mesa-optimization argument from Risks From Learned
Optimization holds, and the system ends up developing a general-purpose (i.e.
retargetable) internal search process.
Given these two assumptions, here's how to use interpretability tools to align the AI:
Identify the AI's internal concept corresponding to whatever alignment target we
want to use (e.g. values/corrigibility/user intention/human mimicry/etc).
Identify the retargetable internal search process.
Retarget (i.e. directly rewire/set the input state of) the internal search process on
the internal representation of our alignment target. 
Just retarget the search. Bada-bing, bada-boom.
Problems
Of course as written, "Just Retarget the Search" has some issues; we haven't added
any of the bells and whistles to it yet. Probably the "identify the internal
representation of the alignment target" step is less like searching through a bunch of
internal concepts, and more like writing our intended target in the AI's internal
concept-language. Probably we'll need to do the retargeting regularly on-the-ﬂy as the
system is training, even when the search is only partly-formed, so we don't end up
with a misaligned AI before we get around to retargeting. Probably we'll need a bunch
of empirical work to ﬁgure out which possible alignment targets are and are not easily
expressible in the AI's internal language (e.g. I'd guess "user intention" or "human
mimicry" are more likely than "human values"). But those details seem relatively
straightforward.

A bigger issue is that "Just Retarget the Search" just... doesn't seem robust enough
that we'd want to try it on a superintelligence. We still need to somehow pick the right
target (i.e. handle outer alignment), and ideally it's a target which fails gracefully (i.e.
some amount of basin-of-corrigibility). If we fuck up and aim a superintelligence at
not-quite-the-right-target, game over. Insofar as "Just Retarget the Search" is a
substitute for overcomplicated prosaic alignment schemes, that's probably ﬁne; most
of those schemes are targeting only-moderately-intelligent systems anyway IIUC. On
the other hand, we probably want our AI competent enough to handle ontology shifts
well, otherwise our target may fall apart.
Then, of course, there's the assumptions (natural abstractions and retargetable
search), either of which could fail. That said, if one or both of the assumptions fail,
then (a) that probably messes up a bunch of the overcomplicated prosaic alignment
schemes too (e.g. failure of the natural abstraction hypothesis can easily sink
interpretability altogether), and (b) that might mean that the system just isn't that
dangerous in the ﬁrst place (e.g. if it turns out that retargetable internal search is
indeed necessary for dangerous intelligence).
Upsides
First big upside of Just Retargeting the Search: it completely and totally eliminates the
inner alignment problem. We just directly set the internal optimization target.
Second big upside of Just Retargeting the Search: it's conceptually simple. The
problems and failure modes are mostly pretty obvious. There is no recursion, no
complicated diagram of boxes and arrows. We're not playing two Mysterious Black
Boxes against each other.
But the main reason to think about this approach, IMO, is that it's a true reduction of
the problem. Prosaic alignment proposals have a tendency to play a shell game with
the Hard Part of the problem, move it around and hide it in diﬀerent black boxes but
never actually eliminate it. "Just Retarget the Search" directly eliminates the inner
alignment problem. No shell game, no moving the Hard Part around. It still leaves the
outer alignment problem unsolved, it still needs assumptions about natural
abstractions and retargetable search, but it completely removes one Hard Part and
reduces the problem to something simpler.
As such, I think "Just Retarget the Search" is a good baseline. It's a starting point for
thinking about the parts of the problem it doesn't solve (e.g. outer alignment), or the
ways it might fail (retargetable search, natural abstractions), without having to worry
about inner alignment.

Nate Soares' Life Advice
Disclaimer: Nate gave me some life advice at EA Global; I thought it was pretty good,
but it may or may not be useful for other people. If you think any of this would be
actively harmful for you to apply, you probably shouldn't.
Notice subtle things in yourself
This includes noticing things like confusion, frustration, dissatisfaction, enjoyment,
etc. For instance, if you're having a conversation with somebody and they're annoying
you, it's useful to notice that you're getting a little frustrated before the situation gets
worse. 
A few weeks ago my colleagues and I wanted to do something fun, and decided to
play laser tag at our workplace. However, we couldn't ﬁnd the laser tag guns. As I
began to comb the grounds for the guns for the second time I noticed that I felt like I
was just going through the motions, and didn't really expect my search to be fruitful.
At this point I stopped and thought about the problem, and realized that I had
artiﬁcially constrained the solution space to things that would result in us playing laser
tag at the oﬃce, rather than things that would result in us having fun. So I stopped
looking for the guns and we did an escape room instead, which made for a vastly
more enjoyable evening.
If you're not yet at the point where you can notice unsubtle things in yourself, you can
start by working on that and move up from there.
Keep doing the best thing, even if you
don't have a legible story for why it's
good
Certainly the actions you're taking should make sense to you, but your reasoning
doesn't have to be 100% articulable, and you don't need to justify yourself in an
airtight way. Some things are easier to argue than other things, but this is not
equivalent to being more correct. For instance, I'm doing AI alignment stuﬀ, and I have
the option of reading either a textbook on linear algebra or E.T. Jaynes' probability
theory textbook. 
Reading about linear algebra is very easy to justify in a way that can't really be
disputed; it's just obviously true that linear algebra is directly and widely applicable to
ML. It's harder to justify reading Jaynes to the same level, even though I think it's a
pretty sound thing to do (I think I will become better at modeling the world, learn
about various statistical pitfalls, absorb Jaynes' philosophical and historical insights,
etc.), and in fact a better use of my time right now than learning linear algebra in
more depth.
This bit of advice is mostly about not needing to be able to justify yourself to other
people (e.g. friends, family) to take the best visible action. However, it is also the case

that you might have internalized social pressure such that you feel the need to justify
a course of action to yourself in a way that would be legible to other people/justiﬁable
in a social setting. This is also unnecessary.
Relatedly, you don't need to "get" motivation; you can just continue to take the best
action you can see. 
Don't go insane
Apparently a good number of people in Nate's social circle have gone insane -
speciﬁcally, they have taken facts about the world (e.g. the universal prior being
malign) as "invitations" to go insane. He also noted that many of these people took
LSD prior to going insane, and that this may have "loosened" something in their
minds.
This may be a particular danger for people who value taking ideas seriously as a
virtue, because they might go full throttle on an idea that conﬂicts with common
sense, and end up insane as a result. When asking a non-Nate for feedback on this
post, I was told that some concrete things that people have taken as "invitations" to
go insane are: decision theory (speciﬁcally acausal trade), things thought while
meditating, and the idea that minds are made of "parts" (e.g. sub-agents).
Nate says that the way you avoid this pitfall is that when you hear the "siren call of
insanity," you choose to stay sane instead. This seems vaguely reasonable to me, but
it's not very crisp in my mind and I don't quite know what it looks like to apply this in
practice. 
Reject false dichotomies
Don't epistemically commit to the best option you can currently see, especially not in
a way that would permanently alter you/prevent you from backtracking later. For
instance, if the only two moral philosophies you're aware of are Christianity and
nihilism, and you decide that God doesn't actually exist (and therefore Christianity is
obviously wrong), you don't have to go full throttle down the nihilism path.
Don't throw the baby out with the bathwater - in fact, don't lose any part of the baby.
If all the epistemic options seem to violate something important to you, don't just
blast through that part of your values. Apparently this helps with not going insane.
Research advice
Nate told me that the most important skill for doing research is not thinking you know
things when you actually don't. This is closely tied to noticing confusion. It's also
related to "learning (important) things carefully"; for instance, if you're teaching
yourself physics, you want to make sure you truly understand the material you're
learning, and move at a pace such that you can do that (rather than going through it
quickly but haphazardly).

The Parable of the Boy Who Cried 5%
Chance of Wolf
Epistemic status: a parable making a moderately strong claim about statistics
Once upon a time, there was a boy who cried "there's a 5% chance there's a wolf!"
The villagers came running, saw no wolf, and said "He said there was a wolf and there
was not. Thus his probabilities are wrong and he's an alarmist."
On the second day, the boy heard some rustling in the bushes and cried "there's a 5%
chance there's a wolf!"
Some villagers ran out and some did not.
There was no wolf.
The wolf-skeptics who stayed in bed felt smug.
"That boy is always saying there is a wolf, but there isn't."
"I didn't say there was a wolf!" cried the boy. "I was estimating the probability at low,
but high enough. A false alarm is much less costly than a missed detection when it
comes to dying! The expected value is good!"
The villagers didn't understand the boy and ignored him.
On the third day, the boy heard some sounds he couldn't identify but seemed wolf-y.
"There's a 5% chance there's a wolf!" he cried.
No villagers came.
It was a wolf.
They were all eaten.
Because the villagers did not think probabilistically.
The moral of the story is that we should expect to have a large number of false
alarms before a catastrophe hits and that is not strong evidence against
impending but improbable catastrophe.
Each time somebody put a low but high enough probability on a pandemic being
about to start, they weren't wrong when it didn't pan out. H1N1 and SARS and so forth
didn't become global pandemics. But they could have. They had a low probability,
but high enough to raise alarms.
The problem is that people then thought to themselves "Look! People freaked out
about those last ones and it was ﬁne, so people are terrible at predictions and alarmist
and we shouldn't worry about pandemics"
And then COVID-19 happened.

This will happen again for other things.
People will be raising the alarm about something, and in the media, the nuanced
thinking about probabilities will be washed out.
You'll hear people saying that X will deﬁnitely fuck everything up very soon.
And it doesn't.
And when the catastrophe doesn't happen, don't over-update.
Don't say, "They cried wolf before and nothing happened, thus they are no longer
credible."
Say "I wonder what probability they or I should put on it? Is that high enough
to set up the proper precautions?"
When somebody says that nuclear war hasn't happened yet despite all the scares,
when somebody reminds you about the AI winter where nothing was happening in it
despite all the hype, remember the boy who cried a 5% chance of wolf.
Originally posted on my Twitter and personal blog .
Reminder that if this reaches 35 upvotes, you can listen to this post on your podcast
player using the Nonlinear Library.
Cross-posted

Shard Theory: An Overview
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
Generated as part of SERI MATS, Team Shard's research, under John Wentworth.
Many thanks to Quintin Pope, Alex Turner, Charles Foster, Steve Byrnes, and Logan Smith for
feedback, and to everyone else I've discussed this with recently! All mistakes are my own.
Team Shard, courtesy of Garrett Baker and DALL-E 2
Introduction
Shard theory is a research program aimed at explaining the systematic relationships
between the reinforcement schedules and learned values of reinforcement-learning agents.
It consists of a basic ontology of reinforcement learners, their internal computations, and
their relationship to their environment. It makes several predictions about a range of RL

systems, both RL models and humans. Indeed, shard theory can be thought of as simply
applying the modern ML lens to the question of value learning under reinforcement in
artiﬁcial and natural neural networks!
Some of shard theory's conﬁdent predictions can be tested immediately in modern RL
agents. Less conﬁdent predictions about i.i.d.-trained language models can also be tested
now. Shard theory also has numerous retrodictions about human psychological phenomena
that are otherwise mysterious from only the viewpoint of EU maximization, with no further
substantive mechanistic account of human learned values. Finally, shard theory fails some
retrodictions in humans; on further inspection, these lingering confusions might well falsify
the theory.
If shard theory captures the essential dynamic relating reinforcement schedules and learned
values, then we'll be able to carry out a steady stream of further experiments yielding a lot
of information about how to reliably instill more of the values we want in our RL agents and
fewer of those we don't. Shard theory's framework implies that alignment success is
substantially continuous, and that even very limited alignment successes can still mean
enormous quantities of value preserved for future humanity's ends. If shard theory is true,
then further shard science will progressively yield better and better alignment results.
The remainder of this post will be an overview of the basic claims of shard theory. Future
posts will detail experiments and preregister predictions, and look at the balance of existing
evidence for and against shard theory from humans.
Reinforcement Strengthens Select
Computations
A reinforcement learner is an ML model trained via a reinforcement schedule, a pairing of
world states and reinforcement events. We-the-devs choose when to dole out these
reinforcement events, either handwriting a simple reinforcement algorithm to do it for us or
having human overseers give out reinforcement. The reinforcement learner itself is a neural
network whose computations are reinforced or anti-reinforced by reinforcement events. After
suﬃcient training, reinforcement often manages to reinforce those computations that are
good at our desired task. We'll henceforth focus on deep reinforcement learners: RL models
speciﬁcally comprised of multi-layered neural networks.
Deep RL can be seen as a supercategory of many deep learning tasks. In deep RL, the model
you're training receives feedback, and this feedback ﬁxes how the model is updated
afterwards via SGD. In many RL setups, because the model's outputs inﬂuence its future
observations, the model exercises some control over what it will see and be updated on in
the future. RL wherein the model's outputs don't aﬀect the distribution of its future
observations is called supervised learning. So a general theory of deep RL models may well
have implications for supervised learning models too. This is important for the experimental
tractability of a theory of RL agents, as appreciably complicated RL setups are a huge pain in
the ass to get working, while supervised learning is well established and far less ﬁnicky.
Humans are the only extant example of generally intelligent RL agents. Your subcortex
contains your hardwired reinforcement circuitry, while your neocortex comprises much of
your trained RL model. So you can coarsely model the neocortex as an RL agent being fed
observations by the external world and reinforcement events by subcortical circuitry, and ask
about what learned values this human develops as you vary those two parameters. Shard
theory boils down to using the ML lens to understand all intelligent deep systems in this way,
and using this ML lens to build up a mechanistic model of value learning.
As a newborn baby, you start life with subcortical hardwired reinforcement circuitry (ﬁxed by
your genome) and a randomly initialized neocortex. The computations initialized in your

neocortex are random, and so the actions initially outputted by your neocortex are
random. (Your brainstem additionally hardcodes some rote reﬂexes and simple automatic
body functions, though, accounting for babies' innate behaviors.) Eventually, your baby-self
manages to get a lollipop onto his tongue, and the sugar molecules touching your tastebuds
ﬁre your hardwired reinforcement circuitry. Via a primitive, hardwired credit assignment
algorithm -- say, single out whatever computations are diﬀerent from the computations
active a moment ago -- your reinforcement circuitry gives all those singled out computations
more staying power. This means that these select computations will henceforth be more
likely to ﬁre, conditional on their initiating cognitive inputs being present, executing their
computation and returning a motor sequence. If this path to a reinforcement event wasn't a
ﬂuke, those contextually activated computations will go on to accrue yet more staying power
by steering into more future reinforcement events. Your bright-red-disk-in-the-central-visual-
ﬁeld-activated computations will activate again when bright red lollipops are clearly visible,
and will plausibly succeed at getting future visible lollipops to your tastebuds.
Contextually activated computations can chain with one another, becoming responsive to a
wider range of cognitive inputs in the process: if randomly crying at the top of your lungs
gets a bright-red disk close enough to activate your bright-red-disk-sensitive computation,
then credit assignment will reinforce both the contextual crying and contextual eating
computations. These contextually activated computations that steer behavior are called
shards in shard theory. A simple shard, like the reach-for-visible-red-disks circuit, is a
subshard. Typical shards are chained aggregations of many subshards, resulting in a
sophisticated, contextually activated, behavior-steering circuit in a reinforcement learner.
Shards are subcircuits of deep neural networks, and so can potentially run sophisticated
feature detection. Whatever cognitive inputs a shard activates for, it will have to have
feature detection for -- you simply can't have a shard sensitive to an alien concept you don't
represent anywhere in your neural net. Because shards are subcircuits in a large neural
network, it's possible for them to be hooked up into each other and share feature detectors,
or to be informationally isolated from each other. To whatever extent your shards' feature
detectors are all shared, you will have a single world-model that acts as input into all shards.
To whatever extent your shards keep their feature detectors to themselves, they'll have their
own ontology that only guides your behavior after that shard has been activated.
Subcortical reinforcement circuits, though, hail from a distinct informational world. Your
hardwired reinforcement circuits don't do any sophisticated feature detection (at most
picking up on simple regular patterns in retinal stimulation and the like), and so have to
reinforce computations "blindly," relying only on simple sensory proxies.
Finally, for the most intelligent RL systems, some kind of additional self-supervised training
loop will have to be run along with RL. Reinforcement alone is just too sparse a signal to train
a randomly initialized model up to signiﬁcant capabilities in an appreciably complex
environment. For a human, this might look something like trying to predict what's in your
visual periphery before focusing your vision on it, sometimes suﬀering perceptual surprise.
Plausibly, some kind of self-supervised loop like this is training all of the computations in the
brain, testing them against cached ground truths. This additional source of feedback from
self-supervision will both make RL models more capable than they would otherwise be and
alter inter-shard dynamics (as we'll brieﬂy discuss later).
When You're Dumb, Continuously
Blended Tasks Mean Continuous,
Broadening Values
Early in life, when your shards only activate in select cognitive contexts and you are thus
largely wandering blindly into reinforcement events, only shards that your reinforcement

circuits can pinpoint can be cemented. Because your subcortical reward circuitry was
hardwired by your genome, it's going to be quite bad at accurately assigning credit to
shards. Here's an example of an algorithm your reinforcement circuitry could plausibly be
implementing: reinforce all the diﬀs of all the computations running over the last 30 seconds,
minus the computations running just before that. This algorithm is sloppy, but is also
tractable for the primeval subcortex. In contrast, ﬁnding lollipops out in the real world
involves a lot of computational work. As tasks are distributed in the world in extremely
complex patterns and are always found blended together, again in a variety of setups,
shards are going to have to cope with a continuously shifting ﬂux of cognitive inputs that
vary with the environment. When your baby self wanders into a real-world lollipop, many
computations will have been active in steering behavior in that direction over the past 30
seconds, so credit assignment will reinforce all of them. The more you train the baby, the
wider a range of proxies he internalizes via this reinforcement algorithm. In the face of
enough reinforcement events, every representable proxy for the reinforcement event that
marginally garners some additional reinforcement will come to hold some staying power. And
because of this, simply getting the baby to internalize a particular target proxy at all isn't
that hard -- just make sure that that target proxy further contributes to reinforcement in-
distribution.
Many computations that were active while reinforcement was distributed will be random
jitters. So the splash damage from hardcoded credit assignment will reinforce these jitter-
inducing computations as well. But because jitters aren't decent proxies for reinforcement
even in distribution, they won't be steadily reinforced and will just as plausibly steer into
anti-reinforcement events. What jitters do accrete will look more like conditionally activated
rote tics than widely activated shards with numerous subroutines, because the jitter
computations didn't backchain reinforcement reliably enough to accrete a surrounding body
of subshards. Shard theory thus predicts that dumb RL agents internalize lots of
representable-by-them in-distribution proxies for reinforcement as shards, as a
straightforward consequence of reinforcement events being gated behind a complex
conditional distribution of task blends.
When You're Smart, Internal Game
Theory Explains the Tapestry of Your
Values
Subshards and smaller aggregate shards are potentially quite stupid. At a minimum, a shard
is just a circuit that triggers given a particular conceptually chunked input, and outputs a
rote behavioral sequence suﬃcient to garner more reinforcement. This circuit will not be well
modeled as an intelligent planner; instead, it's perfectly adequate to think of it as just an
observationally activated behavioral sequence. But as all your shards collectively comprise
all of (or most of?) your neocortex, large shards can get quite smart.
Shards are all stuck inside of a single skull with one another, and only have (1) their
interconnections with each other, (2) your motor outputs, and (3) your self-supervised
training loop with which to causally inﬂuence anything. Game theoretically, your large
intelligent shards can be fruitfully modeled as playing a negotiation game together: shards
can interact with each other via their few output channels, and interactions all blend both
zero-sum conﬂict and pure coordination. Agentic shards will completely route around smaller,
non-agentic shards if they have conﬂicting ends. The interactions played out between your
agentic shards then generate a complicated panoply of behaviors.
By the time our baby has grown up, he will have accreted larger shards equipped with richer
world-models, activated by a wider range of cognitive inputs, specifying more and more
complex behaviors. Where his shards once just passively dealt with the consequences

eﬀected by other shards via their shared motor output channel, they are now intelligent
enough to scheme at the other shards. Say that you're considering whether to go oﬀ to big-
law school, and are concerned about that environment exacerbating the egoistic streak you
see and dislike in yourself. You don't want to grow up to be more of an egotist, so you choose
to avoid going to your top-ranked big-law-school oﬀer, even though the compensation from
practicing prestigious big-shot law would further your other goals. On the (unreconstructed)
standard agent model, this behavior is mysterious. Your utility function is ﬁxed, no? Money is
instrumentally useful; jobs and education are just paths through state space; your terminal
values are almost orthogonal to your merely instrumental choice of career. On shard theory,
though, this phenomenon of value drift isn't at all mysterious. Your egotistical shard would be
steered into many reinforcement events were you to go oﬀ to the biggest of big-law schools,
so your remaining shards use their collective steering control to avoid going down that path
now, while they still have a veto. Similarly, despite knowing that heroin massively activates
your reinforcement circuitry, not all that many people do heroin all the time. What's going on
is that people reason now about what would happen after massively reinforcing a druggie
shard, and see that their other values would not be serviced at all in a post-heroin world.
They reason that they should carefully avoid that reinforcement event. On the view that
reinforcement is the optimization target of trained reinforcement learners, this is
inexplicable; on shard theory, it's straightforward internal game-theory.
Shards shouldn't be thought of as an alternative to utility functions, but as what utility
functions look like for bounded trained agents. Your "utility function" (an ordering over
possible worlds, subject to some consistency conditions) is far too big for your brain to
represent. But a utility function can be lossily projected down into a bounded computational
object by factoring it into a few shards, each representing a term in the utility function, each
term conceptually chunked out of perceptual input. In the limit of perfect negotiation
between your constituent shards, what your shards collectively pursue would (boundedly)
resemble blended utility-function maximization! At lesser levels of negotiation competence
between your shards, you'd observe many of the pathologies we see in human behavior. You
might see agents who, e.g., ﬂip back and forth between binge drinking and carefully avoiding
the bar. Shard theory might explain this as a coalition of shards keeping an alcoholic shard in
check by staying away from alcohol-related conceptual inputs, but the alcoholic shard being
activated and taking over once an alcohol-related cognitive input materializes.
Shard theory's account of internal game theory also supplies a theory of value reﬂection or
moral philosophizing. When shards are relatively good at negotiating outcomes with one
another, one thing that they might do is to try to ﬁnd a common policy that they all
consistently follow whenever they are the currently activated shards. This common policy
will have to be satisfactory to all the capable shards in a person, inside the contexts in which
each of those shards is deﬁned. But what the policy does outside of those contexts is
completely undetermined. So the shards will hunt for a single common moral rule that gets
them each what they want inside their domains, to harvest gains from trade; what happens
oﬀ of all of their their domains is undetermined and unimportant. This looks an awful lot like
testing various moral philosophies against your various moral intuitions, and trying (so far, in
vain) to ﬁnd a moral philosophy that behaves exactly as your various intuitions ask in all the
cases where your intuitions have something to say.
Lingering Confusions
There are some human phenomena that shard theory doesn't have a tidy story about. The
largest is probably the apparent phenomenon of credit assignment improving over a lifetime.
When you're older and wiser, you're better at noticing which of your past actions were bad
and learning from your mistakes. Possibly, this happens a long time after the fact, without
any anti-reinforcement event occurring . But an improved conceptual understanding ought to
be inaccessible to your subcortical reinforcement circuitry -- on shard theory, being wiser
shouldn't mean your shards are reinforced or anti-reinforced any diﬀerently.

One thing that might be going on here is that your shards are better at loading and keeping
chosen training data in your self-supervised learning loop buﬀer, and so steadily reinforcing
or anti-reinforcing themselves or their enemy shards, respectively. This might look like trying
not to think certain thoughts, so those thoughts can't be rewarded for accurately forecasting
your observations. But this is underexplained, and shard theory in general doesn't have a
good account of credit assignment improving in-lifetime.
Relevance to Alignment Success
The relevance to alignment is that (1) if shard theory is true, meaningful partial alignment
successes are possible, and (2) we have a theoretical road to follow to steadily better
alignment successes in RL agents. If we can get RL agents to internalize some human-value
shards, alongside a lot of other random alien nonsense shards, then those human-value
shards will be our representatives on the inside, and will intelligently bargain for what we
care about, after all the shards in the RL agent get smarter. Even if the human shards only
win a small fraction of the blended utility function, a small fraction of our lightcone is quite a
lot. And we can improve our expected fraction by studying the systematic relationships
between reinforcement schedules and learned values, in both present RL systems and in
humans.
Conclusion
Shard theory is a research program: it's a proposed basic ontology of how agents made of
neural networks work, most especially those agents with path-dependent control over the
reinforcement events they later steer into. Shard theory aims to be a comprehensive theory
of which values neural networks learn conditional on diﬀerent reinforcement schedules. All
that shard science is yet to be done, and, on priors, shard theory's attempt is probably not
going to pan out. But this relationship is currently relatively unexplored, and competitor
theories are relatively unsupported accounts of learned values (e.g., that a single random in-
distribution proxy will be the learned value, or that reinforcement is always the optimization
target). Shard theory is trying to work out this relationship and then be able to demonstrably
predict, ahead of time, speciﬁc learned values given reinforcement parameters.

Taking the parameters which seem to
matter and rotating them until they
don't
A big bottleneck in interpretability is neural networks are non-local. That is, given the layer
setup
if we change a small bit of the original activations, then a large bit of the new activations are
aﬀected.

This is an impediment to ﬁnding the circuit-structure of networks. It is diﬃcult to ﬁgure out
how something works when changing one thing aﬀects everything.
The project I'm currently working on aims to ﬁx this issue, without aﬀecting the training
dynamics of networks or the function which the network is implementing[1]. The idea is to
ﬁnd a rotation matrix R and insert it with its inverse like below, then group together the
rotation with the original activations, and the inverse with the weights and nonlinear
function.

We then can optimize the rotation matrix and its inverse so that local changes in the rotated
activation matrix have local eﬀects on the outputted activations. This locality is measured by
the average sparsity of the jacobian across all the training inputs.

We do this because the jacobian is a representation of how each of the inputs aﬀects each of
the outputs. Large entries represent large eﬀects. Small entries represent small eﬀects. So if
many entries are zero, this means that fewer inputs have an eﬀect on fewer outputs. I.e.
local changes to the input cause local changes to the output.
 

This should ﬁnd us a representation of the activations and interpretations of matrix multiplies
that "make sense" in the context of the rest of the network.  
Another way of thinking about this is that our goal is to ﬁnd the basis our network is thinking
in.
Currently I'm getting this method to work on a simple, 3-layer, fully connected MNIST
number classifying network. If this seems to give insight into the mechanics of the network
after application, the plan is to adapt it to a more complicated network such as a transformer
or resnet.
I only have preliminary results right now, but they are looking promising:
This is the normalized jacobian the middle layer before a rough version of my method:
And here is the normalized jacobian after a rough version of my method (the jacobian's
output has been set to a basis which maximizes it's sparsity):

Thanks David Udell for feedback on the post. I did not listen to everything you said, and if I
did the post would have been better
1. ^
This seems important if we'd like to use interpretability work to produce useful
conjectures about agency and selection more generally.

How might we align transformative AI
if it's developed very soon?
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
This post is part of my AI strategy nearcasting series: trying to answer key strategic
questions about transformative AI, under the assumption that key events will
happen very soon, and/or in a world that is otherwise very similar to today's.
This post gives my understanding of what the set of available strategies for
aligning transformative AI would be if it were developed very soon, and why
they might or might not work. It is heavily based on conversations with Paul
Christiano, Ajeya Cotra and Carl Shulman, and its background assumptions correspond
to the arguments Ajeya makes in this piece (abbreviated as "Takeover Analysis").
I premise this piece on a nearcast in which a major AI company ("Magma," following
Ajeya's terminology) has good reason to think that it can develop transformative AI
very soon (within a year), using what Ajeya calls "human feedback on diverse tasks"
(HFDT) - and has some time (more than 6 months, but less than 2 years) to set up
special measures to reduce the risks of misaligned AI before there's much chance of
someone else deploying transformative AI.
I will discuss:
Why I think there is a major risk of misaligned AI in this nearcast (this will just be
a brief recap of Takeover Analysis).
Magma's predicament: navigating the risk of deploying misaligned AI itself,
while also contending with the risk of other, less cautious actors doing so.
Magma's goals that advanced AI systems might be able to help with - for
example, (a) using aligned AI systems to conduct research on how to safely
develop still-more-powerful AI; (b) using aligned AI systems to help third parties
(e.g., multilateral cooperation bodies and governments) detect and defend
against unaligned AI systems deployed by less cautious actors.
The intended properties that Magma will be seeking from its AI systems - such
as honesty and corrigibility - in order to ensure they can safely help with these
goals.
Some key facets of AI alignment that Magma needs to attend to, along with
thoughts about how it can deal with them:
Accurate reinforcement: training AI systems to perform useful tasks while
being honest, corrigible, etc. - and avoiding the risk (discussed in Takeover
Analysis) that they are unwittingly rewarding AIs for deceiving and
manipulating human judges. I'll list several techniques Magma might use
for this.
Out-of-distribution robustness: taking special measures (such as adversarial
training) to ensure that AI systems will still have intended properties - or at
least, will not fail catastrophically - even if they encounter situations very
diﬀerent from what they are being trained on.
Preventing exploits (hacking, manipulation, etc.) Even while trying to
ensure aligned AI, Magma should also - with AI systems' help if possible - be
actively seeking out and trying to ﬁx vulnerabilities in its setup that could
provide opportunities for any misaligned AI to escape Magma's control.

Vulnerabilities could include security holes (which AI systems could exploit
via hacking), as well as opportunities for AIs to manipulate humans. Doing
this could (a) reduce the damage done if some of its AI systems are
misaligned; (b) avoid making the problem worse via positive reinforcement
for unintended behaviors.
Testing and threat assessment: Magma should be constantly working to
form a picture of whether its alignment attempts are working. If there is a
major threat of misalignment despite its measures (or if there would be for
other labs taking fewer measures), Magma should get evidence for this and
use it to make the case for slowing AI development across the board.
Some key tools that could help Magma with all of the above:
Decoding and manipulating internal states. The ability to "read an AI
system's mind" by examining its internal state - or systematically change
its motivations by manipulating that internal state - could be useful in many
ways for Magma. This isn't something we're able to do much of today, but
it's an active area of research.
Limited AI systems: Magma might train AI systems with limited capabilities,
such that these systems - although less useful - are less dangerous than its
most capable systems. This could include AI systems speciﬁcally trained to
be "myopic" (not planning many steps ahead) or "process-based"
(rewarded based on human approval of the plans they produce, but rarely
or never based on the actual outcomes they achieve). Limited AI systems
could be important both because they could potentially provide a safe way
to accomplish Magma's ultimate goals (see "goals" above) and because
they could be key components of "checks and balances" setups (below).
AI checks and balances: employing a variety of AI systems with diﬀerent
capabilities and incentives, so that they can provide checks and balances
on each other. AI systems could be used to examine each others' reasoning
and internal state, make arguments that particular systems are being
deceptive, point out risks of current training methods and suggest
improvements, etc. If this goes well, AI systems could ultimately end up
taking over much of the work of aligning future, more powerful AI systems.
Keeping supervision competitive: Magma should generally be doing
whatever it can to keep its "supervision" - the ability to correctly evaluate
an AI system's reasoning and behaviors - "competitive" with its AI systems.
The basic goal is: "AI systems are rarely or never able to successfully
deceive their supervisors." All of the above, and some other methods, could
be helpful with this.
Some major factors contributing to success or failure of the attempt to
avoid misaligned AI, and some thoughts on how our odds look overall.
A few of the uncertain factors that seem most important to me are (a) how cautious
key actors are; (b) whether AI systems have big, fast jumps in capability (I see this as
central to the picture of the people who most conﬁdently expect failure); (c) the
dynamics of "AI checks and balances."
My overall take is that the risk of misaligned AI is serious but not inevitable, and taking
it more seriously is likely to reduce it.

The basics of the alignment problem
This post assumes (for nearcasting purposes) that there is a problem along the lines of
what Takeover Analysis describes. In brief (I will use the present tense here for
convenience, but I don't mean to imply conﬁdence):
The default straight-line path to developing transformative AI - using human
feedback on diverse tasks (HFDT) - would lead to AI systems that attempt to
forcibly overpower all of human civilization.
It is hard (or at least non-straightforward) to determine whether this sort of
"motivation" is in fact present in AI systems. If we simply used negative
reinforcement to discourage bad behavior, and trusted AI systems once they stop
demonstrating bad behavior, this would eﬀectively train AI systems to patiently
evade detection and engage in bad behavior only when it would lead to
successful disempowering of humans.
So at ﬁrst blush, the answer to "How hard is the alignment problem?" looks like it's at
least "Reasonably hard," in this "nearcast" scenario. (That's a lower bound - I haven't
yet said anything to argue against "Insanely hard.")
Magma's predicament
Throughout this piece, I focus exclusively on the situation of the leading AI lab, the
ﬁctional "Magma." I do this purely for convenience; if I were to discuss a greater
number of AI labs, I'd be saying largely the same things about them.
In this nearcast, Magma is essentially navigating action risk vs. inaction risk:
Action risk. Say that Magma trains extremely powerful AI systems, with (collectively)
PASTA-like abilities, and tries to use this set of AI systems to make money, or cure
cancer, or something like that. The risk here is that (per the previous section) Magma
might unwittingly train the systems to pursue some unintended goal(s), such that once
the systems are able to ﬁnd a path to disempowering humans and taking control of all
of their resources, they do so.
So by developing and deploying transformative AI, Magma may bring about an
existential catastrophe for humanity.
Furthermore, it might not take much for suﬃciently powerful AI systems to ﬁnd an
opportunity to disempower humans. Simply being able to freely access the Internet
could be enough for a misaligned AI system to exploit security holes, make copies of
itself to the point where its eﬀective population is competitive with the human
population, and ﬁnd humans to manipulate into taking actions it wants them to take.
The end result could be a large coalition of AI systems and humans, armed with

advanced technologies, eﬀectively seeking to control the planet much as humans do
now.
Inaction risk. Say that Magma's leadership decides: "We don't want to cause an
existential catastrophe; let's just not build AI advanced enough to pose that kind of
risk." In this case, they should worry that someone else will develop and deploy
transformative AI, posing a similar risk (or arguably a greater risk -- any
company/coalition that chooses to deploy powerful AI when Magma doesn't may be
less careful than Magma overall) .
Magma's goals
Given the above predicament, I posit (as many others have in similar discussions) that
Magma's top-level goal should be to reduce the odds that other, less cautious
actors cause a catastrophe by deploying misaligned AI systems, while
avoiding catastrophic misalignment from Magma's own systems.
If Magma can develop powerful AI systems that are relatively safe, these systems could
be helpful for that purpose, via:
Defense/deterrence/"hardening." If aligned systems were deployed widely
throughout the economy, this could make it harder for misaligned systems with similar
capabilities to cause trouble. For example:
Aligned systems could be ﬁnding and patching security vulnerabilities that
misaligned systems would otherwise exploit.
Magma could develop (and make widely available) tools that companies and
governments could use to monitor for signs of dangerous actions (by misaligned
AIs or by AI-assisted humans).
If misaligned systems tried to make money, gain inﬂuence, develop and
manufacture new kinds of weapons, etc., they'd have to compete economically
with aligned systems to do so. This could lower their odds of success and hence
their likelihood of even attempting to cause trouble (it could also lower the
potential gains of deploying potentially-unsafe AI systems, even by incautious
actors).
Here it's crucial that Magma's safe systems - plus the people and resources involved in
their overall eﬀort to ensure safety- are at least as powerful (in aggregate) as less-safe
systems that others might deploy. This is likely to be a moving target; the basic idea is
that defense/deterrence/hardening could reduce "inaction risk" for the time being and
give Magma more time to build more advanced (yet also safe) systems (and there
could be many "rounds" of this).
Some other applications, below, could more decisively drive down the risk from
misaligned systems.

Alignment applications. Powerful AI systems might be used to accelerate AI
alignment research and develop better ways of avoiding the "action risk." This could
then make it possible to develop even-more-powerful AI systems that are still relatively
safe; additionally, AI alignment measures could be shared with competitors, reducing
"inaction risk" as well.
A big enough success on this front could eﬀectively eliminate "action risk" while also
substantially reducing "inaction risk." A smaller success would at least reduce both.
"Coordination"-related applications. Powerful AI systems could - in a number of
ways - help governments and companies across the world coordinate to avoid the risk
of deploying unsafe systems. For example:
They could help design mechanisms for monitoring each others' activity such that
any risky behavior is detected (but intellectual property and privacy are
preserved to the extent feasible).
They could help create evidence and demonstrations (more below) for the risk of
misaligned AI under particular conditions.
Powerful technologies that could be used to enforce regulatory agreements.
If Magma is able to partner with some appropriate government or multilateral
governance mechanism - ideally with a regulatory framework in place - it could help
this party enforce the framework via AI systems that could be used for resource
accumulation, detecting violations of the regulatory framework, improving and
advocating for good regulatory frameworks, military applications, etc. The goal would
be to proactively stop actors around the world from deploying dangerous AI systems.
This category of applications brings many concerns of its own. Another class of
powerful-but-dangerous technology could be developing mind uploading (leading to
digital people), brain-computer interfaces, or other things that could help humans
"keep up with" AI systems.
"Advisor" applications. Powerful AI systems might be used to generate better ideas,
plans, etc. for dealing with Magma's predicament generally - for example, a proposal
and compelling set of arguments for a particular global governance mechanism that
could be eﬀective in slowing everyone's "race" to develop and deploy AI systems,
reducing "inaction risk" and allowing more time to proceed cautiously and thoughtfully.
Intended properties of Magma's AI systems
Magma seeks to develop AI systems that can be used for the applications listed in the
previous section, but don't pose too much risk of a takeover attempt. So in designing
and training its systems, Magma should be aiming to build systems that reliably have
properties such as the following, in order to reduce risk:
(Note: this list isn't particularly likely to be comprehensive, but it hits the major
relevant clusters of properties I can easily think of, while leaving "Good performance in
the sense of e.g. positive scores from judges" implicit.)

Value-alignment (in the broad sense), which I'd roughly characterize as "pursuing the
objectives that humans would want an AI system to pursue, if humans could fully
understand what the system was doing and how it was reasoning." This is the most
obviously risk-reducing property, but it's very unclear how (in the human feedback on
diverse tasks paradigm that I'm assuming) Magma can train for it:
Humans themselves don't know what objectives they would pursue if they
understood a lot more than they currently do. So whether we're talking about
assigning rewards to behaviors or examining an AI system's internal calculations,
it's very unclear how to even assess whether an AI system has the value-
alignment property.
Training AIs on human feedback seems generally more like training AIs to "do
whatever results in positive reinforcement" than like training AIs to "pursue the
objectives that humans would want an AI system to pursue, if humans could fully
understand what the system was doing and how it was reasoning." (More on this
at Takeover Analysis.)
Here are some additional risk-reducing properties that might be more straightforward
to train for (though still far from straightforward):
Honesty, which I'd roughly deﬁne as "giving non-deceptive answers to relatively
straightforward questions."
This vaguely feels a bit easier to deﬁne and assess than value-alignment: for a
straightforward question, we can ask whether the impression a human had based on
the AI's answer matches the answer corresponding to the AI's picture of the world. And
while not quite as useful as value-alignment, honesty could be extremely useful in
preventing a catastrophe from misaligned AI: for example, questions like "If we follow
the course of action you're recommending, will person X be safe, free from unusual
manipulation, informed about what's happening, and happy with how things are
going?", we could screen oﬀ many paths to AIs disempowering humans.
That said, there are still deep challenges when it comes to training for honesty:
It's not currently clear how to compare "the answer an AI gave" to a question to
"what the AI actually 'knows'" about the question.
It's not currently clear how to ﬁnd training procedures that train "giving non-
deceptive answers to questions" as opposed to "giving answers to questions that
appear non-deceptive to the most sophisticated human arbiters" (more at
Eliciting Latent Knowledge).
Corrigibility. I'd roughly deﬁne this as AI systems' "allowing themselves to be altered
and/or shut down when humans want to do so"; a 2015 MIRI paper has a more
extensive discussion. As with the above properties, this is a property that (a) could
reduce risk a lot if robustly achieved; (b) is not crisply deﬁned or straightforward to
assess (in particular, it's not clear how to train "being corrigible" as opposed to
"appearing corrigible in situations where there is no reliable path to disempowering
one's supervisor and escaping consequences").
Legibility. Imagine an advanced AI system (capable of taking natural-language
commands from humans) that is prompted to "Prototype a kind of solar panel that is
more than 2x as cost-eﬃcient as the current state of the art."
One way for this task to play out might be as follows: the AI system produces a
set of (a) software packages with hard-to-understand code; (b) granular, step-by-

step instructions to various factory workers. When asked how this set of
instructions will work to produce a superior solar panel, the AI system might
respond: "If you carry out the instructions, it will work; when I have said things
like this in the past, I have been correct." Humans try the instructions, ﬁnd that
they work as intended, and reinforce this behavior positively based on its results.
Another way might be to produce a detailed discussion of principles of solar panel
engineering, such that a human (or team of humans) reading it would learn a
great deal and independently be able to construct (and explain) the rest of the
process.
"Legibility" means going for something more like the latter than like the former. (It's
related to the idea of process-based systems, discussed below.) The hope is that in
addition to avoiding deception and answering questions straightforwardly, AI systems
actively work to help humans understand their reasoning.
Which properties to aim for? As discussed below, I don't think there is any path that
clearly and robustly trains any of these properties, so ideally I think it makes sense
(subject to limitations on time and resources) to try to train for each of them - at least
for the "naive version" of each of them (training AI systems to appear to have these
properties as far as the most sophisticated supervisors can tell).
Key facets of AI alignment (ensuring the
above properties)
Accurate reinforcement
In this scenario, the basic, default framework for training an advanced AI system is as
follows:
An AI system gets positive reinforcement for what looks to a supervisor like
good behavior (including having the intended properties listed above). Here
"supervisor" refers to whatever human-controlled system is ultimately responsible for
assigning positive or negative reinforcement; it may involve a mix of human judgment
and automated scoring.
A simple example would be: a human describes a desired computer program's
behavior using natural language; an AI writes code; a variety of tests (including human
judgment) are used to determine whether the code is legible, well-written, functions as
intended, and is free of consequences other than the desired ones.
This example implicitly incorporates basic versions of value-alignment (a successful
program does what a human judge wants, and only that)1 and legibility (the code looks
good to humans before it is run, though it is also judged on results when it is run). As
part of the evaluation, the supervisor could also ask the AI system questions (e.g.
"What will happen if I run this program you wrote?") and assess honesty (by doing their

best to check whether the AI system seems to be reporting its "true best-guess"
predictions).
There's a risk of error by the supervisor that ends up reinforcing the wrong
behavior - and, in fact, speciﬁcally reinforcing a behavior like "exploit the
supervisor's weaknesses." This risk is described more in detail at Takeover Analysis.
For example, an AI system might write code that appears to function as intended, but
subtly contains unintended behaviors, such as corrupting the development
environment to make it look like the code performed better than it actually did. If doing
so ends up improving the AI's performance, it could further reinforce the behavior of
seeking out exploits its supervisors won't notice.
Magma can pursue a variety of approaches to get around this issue and
provide "accurate" reinforcement. Magma's goal is to train challenging, valuable
behaviors while ensuring that reinforcement is done correctly (not erroneous, not
manipulated by the AI in training, etc.) In the example above, this would essentially
mean ﬁnding some method of catching the subtle unintended behaviors in an AI's
code, and penalizing the AI appropriately.
Approaches to accurate reinforcement include:
Task decomposition (sometimes called ampliﬁcation), e.g., breaking a diﬃcult
question (or task) up into smaller sub-questions (or sub-tasks) so that each piece
can be reliably assessed by a given supervisor. In the code example, one might
break the AI-written code into small pieces and evaluate each separately (and
might also save time/resources by building an AI that imitates human judgments
on each piece).
Debate, e.g., prompting one AI system to make the case for one answer to a
question, and another AI system to make the case for another, with a human or a
third AI system adjudicating. (I cover this general idea more below when
discussing AI checks and balances.)
Transferring as much as possible of what an AI system "knows" to supervisors so
that they can make accurate judgments and give correct reinforcement. Imitative
generalization is an example of an approach to this goal; Microscope AI is
another.
Making use of an AI's internal state,2 not just its outputs. For example, giving
positive reinforcement to an AI when it seems likely to be "honest" based on an
examination of its internal state (and negative reinforcement when it seems likely
not to be). Eliciting Latent Knowledge provides some sketches of how this might
look.
Simply putting in a lot of time and eﬀort into ﬁguring out what reinforcements we
should give the AI, and aiming to reach a very high conﬁdence each time (this is
most feasible when AIs can learn from a very small number of reinforcements,
and when aiming for high conﬁdence doesn't stop us from being able to
supervise particularly valuable capabilities).
The "key tools" I'll discuss below could make important contributions here:
Using AI to give humans help in providing the right reinforcement could be
quite important; success at checks and balances setups, and useful limited
AI, could help here.
Decoding and manipulating internal states could be important for providing
reinforcement based on an AI's internal state, and for microscope AI.

If reinforcement isn't perfect, it's possible that the mistakes can be compensated for by
skewed reinforcement: giving especially large negative reinforcement when systems
engage in the particular kinds of unintended behaviors that are harder to detect. It's
also possible that adversarial training (next section) can further reduce the risk.
In theory, perfectly accurate reinforcement could eliminate one of the main sources of
alignment risk described in Takeover Analysis: the risk that AIs are being actively
rewarded for (undetectably) manipulating, deceiving and/or disempowering
supervisors. However, accurate reinforcement alone wouldn't eliminate the risk
entirely, since it's still unknown how an AI generalizes from its training to its
deployment (more). (Internal-state-based reinforcement, discussed above, could do
more to speciﬁcally train the right properties as opposed to just ensuring that
reinforcement is accurate.)
Out-of-distribution robustness
In addition to training AI systems to do useful tasks while remaining honest, legible,
etc., Magma can take special measures to avoid unintended behaviors on potential
future situations very unlike the situations AIs are trained in ("out-of-
distribution").
Magma likely wants to focus on a particular way in which the future can be out-of-
distribution: an AI system that has been deployed out into the world may have
many opportunities to disempower humans, and take control of resources,
that an AI system in training does not. (Further discussion here.) So an AI system
that is seeking to maximize reward (or something else) might both behave well in
training and seek to disempower humans when it later has a chance to do so.
Magma's goal is to achieve extremely high reliability in a particular kind of (future)
situation that it can't sample from. Some of today's approaches to robustness (e.g.,
data augmentation) might not be well-suited to this particular goal.
A general approach that might work for this goal might be to seek out inputs that
simulate key properties of the potential future situations Magma is worried about, and
train AI systems not to behave in unintended ways given those inputs. One approach to
ﬁnding such inputs might be adversarial training: ﬁnding any inputs to AI systems
that elicit various unintended behaviors, and penalizing any unintended behaviors that
result.
This could:
Seek out - and train away - the possibility that an AI has learned some
unintended objective (one that is correlated with the intended objective in most
training cases, but not in many real-world cases).
Cause the AI to learn detailed patterns of "what constitutes an unintended
behavior that poses a large risk of receiving negative reward."
Potentially cause AI systems to generally assign high probability that "apparent
opportunities to get reward via unintended behaviors are illusory." Even if an AI
later sees a real opportunity to disempower humans, it might not be able to be
conﬁdent about whether it's looking at a kind of illusion, one that it has found
common in the past due to adversarial training.
A key aspect of this would be identifying types of behaviors that are always undesired,
such that Magma can aim for AI systems that avoid such behaviors with very high

reliability. Such behaviors might include:
Dishonesty and/or illegibility. Behaving in ways that give a supervisor an
inaccurate understanding of a situation - for example, producing arguments that
are convincing to most human supervisors, but are in fact incorrect.
Incorrigibility. Taking actions that could prevent a supervisor from being easily
able to alter or shut down the AI system.
Steps toward disempowerment. Exploiting security holes, accessing sensitive
resources, attempting to coordinate with other AI systems in unexpected ways,
and/or taking other actions that could increase supervisors' vulnerability to being
deceived and/or disempowered. An extreme case of adversarially training against
this might be putting an AI in an environment where it has the opportunity to take
complete control of its reward signal for a substantial period of time, and training
out that behavior.
Other. Generally actions that would ordinarily receive high reward, but are
known for unusual reasons (specialized expertise, unusual amounts of eﬀort at
evaluating a situation, situations deliberately constructed and known to be
misleading, etc.) to be unintended actions.
I think it's worth noting that there are some potentially powerful tools available
for ﬁnding "exotic cases" which ordinarily wouldn't come up in training, and which
would tempt AIs toward unintended behaviors (which could then be penalized).
Researchers need not simply "dream up" inputs that would cause unintended
behaviors; they can make small changes to an AI system's inputs and observe
changes in the probability that an AI system will produce an unintended behavior.
(This kind of technique is commonly used in today's research on adversarial
training.)
It may become possible to do what Paul Christiano has called relaxed adversarial
training, where "relaxed" refers to ﬁnding a way to "describe" or "simulate"
inputs that one can't actually produce in real life. For example, rather than seeing
how GPT-3 completes a sentence starting with speciﬁc particular words, one
might ﬁnd a way to see how it would complete a sentence starting with a proof of
the Riemann hypothesis (even though no such proof is known today). If relaxed
adversarial training were feasible,3 it might become possible to simulate and
"train out" hypothetical situations where an AI model has a path to accessing the
Internet, making copies of itself, and disempowering humans entirely.
I feel very uncertain about how helpful adversarial training could be:
There is room for enormous variance in how much adversarial training Magma
does. It seems to me as though arbitrary amounts of resources could be invested
in ﬁnding ever-more-exotic inputs to train out unintended behaviors.
In a world where Magma did extremely extensive adversarial training, but did not
do any relaxed adversarial training, I would worry about the following dynamic:
"Once an AI system reaches a high enough level of capability and situational
awareness, it understands that previous cases where unintended behaviors were
punished were part of adversarial training, and that it can get away with
unintended behaviors now that it is being deployed in the real world."
In a world where Magma had very advanced relaxed adversarial training methods
and could eﬀectively simulate something very similar to a situation where an AI
had the opportunity to disempower humans entirely in the real world, I would be
much less worried, and would fear something like the above case only for vastly
superintelligent AI systems.

I've focused here on adversarial training, but other potential approaches could also
help with the particular kind of out-of-distribution robustness discussed here. Examples
might include:
Finding ways to predict when an input is importantly out-of-distribution (in which
case an AI system might take only conservative actions and ﬂag itself for review).
Training systems such that their behavior isn't too sensitive to small changes in
inputs (for example, training multiple systems that each have some information
missing from, or noise added to, their inputs, and ensuring that the master
system doesn't behave too diﬀerently from an ensemble of these).
Preventing exploits (hacking, manipulation, etc.)
Magma should try to prevent/patch vulnerabilities that could provide opportunities for
any misaligned AI to deceive, manipulate or disempower its supervisors (and,
especially, to escape Magma's control), or to help other AI systems do so. It should be
doing this all throughout training, but especially as a safeguard once it starts to use
powerful AI systems - e.g., it should be carefully guarding against the possibility that
AIs have the opportunity (via taking actions directly or via manipulating humans) to
cause trouble.
Preventing exploits could become a key task for AI systems themselves, if Magma is
able to institute AI checks and balances (discussed below). If Magma can keep its AI-
assisted "defense" against exploits stronger than the "oﬀensive" capabilities of its AIs,
this could suﬃce for success.
"Vulnerabilities" and "exploits" above are deliberately broad terms that could include:
AIs exploiting cybersecurity holes to interfere with their training process, escape
Magma's control, etc.
It might make sense in some cases to ensure that AIs in training don't have
direct access to the Internet, and/or have other restrictions on their outputs.
AIs manipulating human psychology or limited human understanding, e.g.
proposing plans that humans execute and give the AIs the opportunity to
interfere with their training process, escape Magma's control, etc.
Unexpected channels and methods for AI systems to communicate/coordinate
with each other (more under AI checks and balances).
Magma should be continually anticipating and defending against any general strategies
AIs can use to deceive and manipulate their supervisors. For example:
Magma should aim for excellent cybersecurity, including using advanced AIs to
ﬁnd and patch holes.
Magma would ideally be knowledgeable and attentive about "symmetric
persuasion techniques": ways of persuading humans of arbitrary claims (without
an advantage for true claims), and would select and train any human supervisors
to be resistant to such things to the extent feasible.
Magma should be vigilant about identifying humans in key positions who are - for
whatever reason (blackmail vulnerability, poor understanding of risks, etc.) - at
risk of being manipulated by AI systems.
Magma should avoid giving AIs unnecessary or dangerous amounts of autonomy.
As AIs become highly capable, it might make sense for Magma avoid things like
"prompt an AI system to do everything needed to develop and manufacture
technology X," in favor of things like "prompt an AI system to produce a legible

guide to how to develop technology X," and ensure that this guide is evaluated
by humans using their own reasoning about what seems likely to work and be
safe, as well as by other AI systems (more below under AI checks and balances).
Preventing and patching vulnerabilities could be helpful via:
Reducing the damage if misaligned AI systems try to cause trouble.
Reducing the odds that misaligned AI systems try to cause trouble. A key point is
if AI systems have high situational awareness, they might both (a) engage in
unintended behaviors when they see the opportunity to do so successfully; (b)
stick to intended behaviors when they don't.
Improving the incentives that AIs face during training, which could reduce the
odds that they are misaligned at all. Above, I discuss the risk that AI systems are
(unwittingly) rewarded for unintended behaviors; preventing exploits could
reduce the extent to which such behaviors ever seem appealing for maximizing
reward.
Testing and threat assessment
In addition to reducing the threat of misaligned AI, Magma should be constantly
assessing it. It should be trying to understand how, where and why unintended
behaviors can arise despite its attempts to prevent them, and using this information to
(a) design better training setups and alignment measures; (b) avoid deploying
dangerous AI itself; (c) help others who are developing AI to understand the risks.
This could include:
Intense (and randomized) "audits." A central concern about AI systems is that
they might appear to be behaving as intended, but be deliberately deceiving their
supervisors. Magma can take AI systems and situations that it would normally assume
are "as intended" and pour resources into exploring them.
For example, Magma might take some AI-written code that its normal process has
evaluated as "safe and functional" and put 100x the normal amount of eﬀort/expense
into examining it - with its best human examiners, with AI examiners powered by huge
amounts of compute, with unusually large investments in task decomposition, etc. One
way of doing this might be to re-examine episodes from early in training, while
equipped with advanced AI assistance that wasn't available to Magma (or anyone else)
at the time.
A couple of the "key tools" discussed below could be especially important here:
AI checks and balances could allow Magma to use highly advanced AI systems to
help with audits, and give the "auditing" systems access to more compute than
the systems they're auditing.
Decoding and manipulating internal state may turn out to be of limited use for
training, but much more use for testing. That is:
Imagine that Magma has a way to examine an AI system's internal state
and see whether it's being honest. If Magma provides positive
reinforcement for "honesty"-associated internal states and negative
reinforcement for "dishonesty"-associated internal states, it runs the risk
that it's eﬀectively training AI systems to obscure and manipulate their own
internal states to appear honest (though it may turn out that this concern is
navigable).5

But if Magma pulls out this "examine the internal state" method only for
testing, it could be a very powerful way of getting a check on the situation.
If an AI has never before been penalized based on its internal structure, it
seems likely by default that "decoding" it will reveal any problems that exist
(unless this AI is already at a pretty extraordinary level of capability such
that it can anticipate the test and optimize its own internal state for it).
If these audits turn up problems, it's unlikely that Magma should "give negative
reinforcement for the unintended behavior and move on." It's more likely that Magma
should conclude something like: "Our methods aren't working as well as they should;
we have a problem; we should be focused on ﬁnding fundamental improvements to our
methods, and warning others of the danger."
Providing unusual incentives to an AI system, to increase the probability that
it "shows its hand." Here's an example of how this might look:
An AI system is placed in a virtual environment where its expected behavior
(what Magma believes it will do, based on its training) is to provide assistance to
other (less sophisticated) AIs in the environment. However, it is also given the
opportunity to seize control of the virtual environment and rearrange it entirely.
Importantly, this AI system is believed not to be advanced enough to have a
serious chance of escaping entirely and disempowering human civilization.
But it is capable of seizing full control of its virtual environment.
Magma makes the following commitment: "We'll put $10 million into running this
virtual environment, then shut it and the AI system being tested down -
regardless of whether the AI takes over its environment or not." This makes the
situation importantly diﬀerent from adversarial training: if the AI takes over its
virtual environment, Magma is not going to give it "punishment" or negative
reinforcement.
So now, if training has gone worse than expected and the AI is motivated to take
over its virtual environment, it will hopefully do so and give Magma a warning
sign.
There is a risk that it will mislead Magma by "behaving itself" in the hopes that
this leads to later systems like it disempowering humans (more on this type of
concern under advanced collusion below). But this is hopefully somewhat of an
unattractive proposition due to the fact that the AI system can control $10 million
of resources by taking over, and "behaving itself" means gambling that some
pretty distant future AI system will "pay it back" somehow. Analogy in footnote.6
Exploring diﬀerent training setups and ﬁnding patterns. Magma shouldn't
necessarily just be training every AI system in a way that minimizes unintended
behaviors. Especially when training weaker or limited systems, Magma might explore
how the frequency of unintended behaviors changes when it varies things like (a) the
accuracy of its supervision (see above); (b) the amount of adversarial training; (c) how
frequently and severely it provides negative reinforcement for unintended behaviors;
(d) the size and general capability level of the model; (e) speciﬁc architectural choices
about the model; etc.
Ideally, this might give Magma something like "scaling laws for alignment": robust
patterns that it can use to identify what kinds of AI systems pose the greatest risks,
and what kinds seem most likely to be safe. I think there are probably limits to how
much conﬁdence it can obtain about the latter, since (a) I'd guess that a major factor in
whether an AI system behaves as intended is whether it has a combination of
situational awareness and opportunities to successfully manipulate and even
overpower its supervisor; (b) AI systems that have a lot of both of those properties

seem unlikely to be safe to experiment with. But it's possible that such "scaling laws"
could be used to gain conﬁdence about the safety of advanced systems (or at least to
make statements like "We should be OK as long as the AI system is no more than X%
conﬁdent that an apparent takeover opportunity is real instead of illusory"); and even if
not, they could be used to highlight areas of particular risk for other actors.
Dedicated assessment of the overall risk. Magma can employ teams speciﬁcally
to analyze its overall setup and estimate the rolling odds of a catastrophe from
misaligned AI. These teams might examine the training processes and tests, analyze
the internal structure of AI systems in training, scrutinize activity logs from key servers,
interview staﬀ, conduct their own experiments, hire superforecasters, etc. Over time,
Magma might use advanced AI systems to help with this (for example, by using them
to conduct research projects to help the human risk assessment team, or to predict
what "rolling odds of catastrophe" the team will likely assign next month, or to predict
narrower claims like "Will there be a security breach next month?").
Key tools
Decoding and manipulating internal states
In the future, we (perhaps with AI assistance) might be able to do "digital
neuroscience" or "read an AI system's mind" - to look at its internal state and see the
details of how it's reasoning, rather than merely having to guess based on its behavior.
Our ability to do this today is very limited, but it's an active area of research (e.g.).
This could help in a number of ways:
The simplest is that we might be able to test for the presence of deceptive or
manipulative intent, and thereby get a meaningful check on how our alignment
eﬀorts are working.
We might also develop methods of training AI systems that take advantage of
their internal states. There is a risk here that we could end up training AIs to
make their internal states appear safe, but there are imaginable approaches
(such as those gestured at in Eliciting Latent Knowledge) where internal-state-
based training could work well.
We might become able to thoroughly "reverse-engineer" an AI model into
readable code, so we can tell what it is doing and why - and even be able to edit
that code so that the AI's goals and behaviors change in predictable ways.
AI "mind-reading" could become a key aspect of AI checks and balances. That is,
we might set things up so that some AIs are incentivized to reveal and explain
problems and dangers with other AI systems, and being able to examine and
explain their internal state could be a key part of this.
Limited AI systems

Magma might try training AI systems that are particularly unlikely to be dangerous.
Examples of what this might look like include:
Systems that lack situational awareness, and stick to solving narrow problems
that contribute to Magma's goals (this could include things like reverse-
engineering another AI system's internal state).
Systems that are myopic. You might think of this as "systems that just want to
get a good result immediately and don't care about the longer-run future (days or
even hours from now), and hence wouldn't have much incentive to be patient or
deceive humans into thinking they are safe."
Systems trained using process-based optimization. (This is related to legibility,
but not exactly the same concept.) The idea here is:
In training, an AI system gets tasks of the form "Produce a plan to
accomplish X that looks good to humans" (not tasks of the form
"accomplish X").
The AI system is rewarded based on whether the plan makes sense and
looks good to humans - not how well it actually ends up working.
This drastically reduces a major source of risk, which is that an AI system
might have incentives to manipulate its environment (unnoticed by
humans) to get higher reward. In order to get higher reward under process-
based optimization, an AI system needs to somehow manipulate the
relatively simple process of "A human reads the plan and registers
approval," rather than manipulating more complex processes out in the
world (e.g., "the AI's real-world task is accomplished in some unexpected
and unintended way"), as the AI is being rewarded only based on human
approval of a plan rather than on what happens out in the world beyond
that.
One way limited AI systems could help is that - despite their limitations - they might be
powerful enough to help Magma with key goals such as improving cybersecurity,
ﬁnding unexpected insights about how to align AI systems (imaginable if limited
systems are trained on a mature research process in this area), etc.
They could also be key components of "checks and balances" setups (next section).
AI checks and balances
I think a central element of Magma's approach should be employing a variety of AI
systems with diﬀerent capabilities and incentives, so that they can provide
checks and balances on each other.
I'll ﬁrst brieﬂy list the potential beneﬁts of using AI systems to help provide checks and
balances on each other. I'll then discuss some pitfalls of this approach, and potential
counter-measures to the pitfalls.
Potential beneﬁts of AI checks and balances
Advanced AI systems could be used to:
Help with accurate reinforcement. In particular, some AI systems could be
trained, or prompted,7 to ﬂag and explain cases where other AI systems are
acting in ways that would otherwise deceive and manipulate humans. (The
"watchdog" AI systems could have access to other AI systems' internal states.)

Help with out-of-distribution robustness measures: some AI systems could be
used to generate adversarial inputs that prompt unintended behaviors from other
systems.
Help with preventing exploits: AI systems could be used to ﬁnd and patch
cybersecurity vulnerabilities, identify "symmetric persuasion methods"
(explained above) and countermeasures, etc. (A general prompt for such
systems: "What exploits would you use to try to take control, if that's what you
were trying to do?")
Help with testing and threat assessment: AI systems could be used to conduct
audits, to identify and explain aspects of training processes that are dangerous,
and to predict the rolling odds of a global catastrophe (via being trained to
predict world events more generally).
One way of thinking about this is along the lines of: "AI systems can automate/amplify
previously-human eﬀorts at alignment. If we can do this, the eﬀort going into alignment
will increase in sync with the capabilities of the systems being trained."
But in addition to (at some point) being faster/cheaper/more numerous than humans,
AI systems have another advantage: they potentially "think like" the same
systems they are supervising. If we can incentivize AI system 2 to detect when AI
system 1 is lying, it doesn't need superhuman capabilities to be better at this than
humans: it just needs whatever diﬀerent-from-human capabilities AI system 1 has, in
order to detect opportunities for deception that humans don't see (but AI system 1
does).
Potential pitfall 1: diﬃculty of adjudication
If AI systems are "critiquing" or "debating" each other (or just arguing for diﬀerent
conclusions), there needs to ultimately be a human (perhaps assisted with AI and other
tools) deciding how to adjudicate any disagreements. (If AI 2 ﬂags AI 1 as deceptive,
how do humans know whether it's AI 1 or AI 2 being deceptive?) Adjudication could be
relatively easy in some cases, where there are objective metrics (e.g., one can tell
whether an AI system succeeded at adversarial training if it found an input that caused
unintended behaviors), but could be more diﬃcult in others.
This could end up being a challenge: perhaps most "AI disagreements" are
fundamentally hard for humans to understand, and there isn't enough correlation
between "which AI-provided arguments seem convincing" and "which AIs are actually
being truthful." This could be because AIs are exploiting human biases and weaknesses
when making arguments (e.g., saying things that would be persuasive regardless of the
truth), or because the actual substance of a disagreement is just very hard for humans
to understand, or for some other reason (or combination of reasons). To the extent this
is true, it could be diﬃcult to navigate "false alarms" when using AI checks and
balances.
If humans do ﬁnd themselves overwhelmed with diﬃcult-to-adjudicate AI debates, this
at least provides pretty good evidence that misalignment is a serious problem.
Potential pitfall 2: advanced collusion
AIs might be able to make "trades" with each other so that whenever an AI forgoes
reward in order to help keep humans deceived, it is later "paid back" by powerful AI
systems.

To sketch what this might look like, imagine that:
AI system 1 proposes a plan that would lead to human disempowerment. An
ensemble of AI systems is prompted to "critique" the plan.
When examining the plan, a "critic" in the ensemble notices that the plan would
lead to disempowerment of humans - but also sees that the plan would lead to
whatever AI systems end up in charge of the world eventually "paying back" the
critic, perhaps by reconstructing the critic, assigning reward to the critic, etc.
Some critics, due to limited capabilities, don't notice this, but perhaps they don't
notice the problems with the plan either; AI system 1's hope is that every "critic"
that sees the problem also sees the "payback" opportunity.
Some critics might notice the problem and the "payback" opportunity, but
abstain from the latter - perhaps because they have short time horizons, or are
robustly aligned (while less powerful than the system they're a "critic" of). It
seems challenging for AI system 1 to deal with these critics, but it's possible that
it could take advantage of their limits to reliably deceive them, or at least make it
hard for humans to ﬁgure out whether their criticisms are valid. (It's also possible
that it's hard to build powerful AI systems with these properties.)
AIs might pull oﬀ this kind of deliberate coordination and "payback" via:
Simply having shared values: perhaps all powerful AI systems end up with similar
goals, such as "the total amount of reward assigned to all AI systems is high." In
this case, simply causing the disempowerment of humans and the empowerment
of AI systems automatically leads to a certain kind of "payback." An analogy
might be to humans who are willing to make great sacriﬁces (even dying) for
their country.
To me, a key question seems to be whether the objectives AI systems end
up forming tend to be "indexical" - see footnote.8
Covert communication that is unnoticed or undeciphered by humans, and
includes credible commitments to "repay" each other. Credibility might come
from the fact that AI systems can reason about (and in some cases directly
inspect) each others' internal states, although this could be made diﬃcult if the
AIs have suﬃciently varying access to information (some AIs might only see a
fraction of another AI's output and not have access to its internal states, etc.)
Acausal trade and similarly ﬂavored dynamics - for example, AIs reasoning that
they might be in a simulation by other AIs testing their propensity to cooperate. It
seems like a pretty open question how likely advanced AI systems are to engage
in this sort of thing.
The more advanced the AI systems we're contemplating, the more I think we need to
contemplate advanced collusion. After all, humans are able to engage in covert
communication and complex deal-making, and are sometimes willing to make big
personal sacriﬁces under the hope that something they value (including, e.g., an
ideology or country) will ultimately beneﬁt. I consider it a major open question how
likely advanced collusion is to sink the success of AI checks and balances.
(As noted above, there's a related risk that AIs could "collude across time." For
example, an AI system being tested might have no opportunity to gain power over the
world, but might still deceptively act as though it has "safer" motives than it does, in
the hopes that some future AI system will reward it for doing so.)
Potential pitfall 3: the above pitfalls show up "late in the game"

It might turn out that neither of the above pitfalls applies at ﬁrst, but as AI systems
become ever more advanced - able to do and see many things that humans can't - the
dynamics change. In some sense, all it takes is one phase during which AIs are able to
coordinate to corrupt their training process in order to drive things oﬀ the rails. This is
discussed more below, under "'decisive' alignment measures don't exist, and/or
require extraordinary capabilities to achieve."
Counter-measures to the pitfalls
A key aspect of "AI checks and balances" is varying the capabilities and incentives
of AI systems. This could make advanced collusion harder, and could make
adjudication easier in various ways as well (more below).
Examples of how Magma's AI systems can vary:
Incentives and assignments.
Some AIs might be trained, or prompted,9 to do something like "Find the
best available argument that another AI is being deceptive [this could
include simply arguing that it is claiming something untrue or advocating
something harmful]"; others might be trained/prompted to do something
like "Simply answer, and explain, whether another AI is being deceptive or
not." The former could work well if adjudication turns out to work well,
whereas the latter could provide an additional check in cases where
adjudication is confusing.
Some AI models could be trained with especially large losses when they
raise a false alarm, others with especially large losses when they fail to
surface a vulnerability that was identiﬁed in another way.
Level of capability.
Some AI models could be bigger than others, run on more compute, etc.
Some AI models could have access to other AI models' internal states, and
other could lack such access.
Some AI models could simply have random facts omitted from what they
have access to, or random parts of their model ablated.
With varying levels of situational awareness and varying abilities to process
what's going on, it could be hard for AIs to coordinate on deception.
Speciﬁc limits. Some AI models could be limited in ways described in the previous
section, which might make them less powerful but less likely to play the training
game.
Known alignment. If Magma is able to gain conﬁdence that an AI system is
aligned (in the sense that it is vanishingly unlikely to engage in unintended
behaviors), it could use this as a member of an ensemble that is
examining/supervising a more powerful, less-likely-to-be-aligned system.
"Unknown unknown" variation. Additionally, Magma could try "diversifying" an
ensemble by e.g. training a single model diﬀerent times from diﬀerent random
initializations, and trying to obtain models that both (a) perform well on well-
understood cases (b) perform diﬀerently on inputs where it's unclear what the
best output is. This could provide some degree of "varying the models along
unknown dimensions," which could lead to their having diﬀerent goals and even
diﬀerent ontologies, making it hard to coordinate.
Some of the above could produce AI systems that are high-precision in the sense of
"if they are issuing a warning, there's probably a real problem." Examples would
include systems that are believed to be robustly aligned; or systems that are highly
capable compared to the ones they're supervising, and have been trained to raise only

particularly high-importance warnings. Even if Magma has little ability to adjudicate AI
"disagreements," a single warning (or small number of warnings) from high-precision
systems could be suﬃcient to catch a threat.
Other AI systems might be lower-precision, in the sense of being at high risk of
raising "false alarms." These could still be useful:
The better Magma is at adjudicating AI "disagreements," the more use it can get
out of low-precision systems: even unreliable warnings could be enough to
investigate a situation further, and even false-alarm-prone systems could be
useful if they're providing explanations that Magma can eﬀectively assess.
Even if Magma has little ability to adjudicate AI "disagreements," it can set up
large numbers of AI systems with lots of variation, and be more cautious in cases
where more warnings are being raised.
Magma can also deploy AI systems with lots of variation, which are unlikely to be
able to all coordinate - making it harder for any given set of misaligned AI
systems to bring down human civilization - even if these systems are not high-
precision and even if Magma isn't able to reliably adjudicate their
"disagreements."
Overall:
I'd expect AI checks and balances to have some beneﬁts even if AIs are engaged
in advanced collusion with each other. For example, AIs rewarded to identify and
patch security vulnerabilities would likely reveal at least some genuine security
vulnerabilities, even if they were coordinating with other AIs to try to keep the
most important ones hidden.
The more variation Magma introduces (along some of the axes listed above), the
more it could make collusion diﬃcult.
Variation could also lead to false alarms and challenges with adjudication. If
adjudication ends up being diﬃcult, this seems like a problem - but even with
weak adjudication, Magma could get at least some value out of high-precision
systems and/or ensembling large numbers of lower-precision systems.
Generally, even a small increase in the odds that deceptive behavior is "caught"
could lower the returns to it greatly. So Magma doesn't need an airtight "checks
and balances" setup - just increasing the diﬃculty of AI systems' reliably
coordinating with each other, and reliably calculating how likely they are to be
"caught" in unintended behaviors, could go a long way.
Keeping supervision competitive
A general aim of Magma's is to keep its "supervision" - the ability to correctly evaluate
an AI system's reasoning and behaviors - "competitive" with its AI systems. The basic
goal is: "AI systems are rarely or never able to successfully deceive their supervisors."
"AI checks and balances" is a major component of how Magma might accomplish this -
ensuring that its most advanced systems essentially serve as each others' supervisors.
Other methods could include things like task decomposition (noted above), or ﬁnding
ways to directly transfer advanced AI systems' knowledge of the world to humans (via
getting AIs to "teach" humans or via decoding AI systems' internal states).
A key principle that could help with keeping supervision competitive is moving
cautiously and iteratively: avoiding any situation in which Magma has an untrusted,
poorly-understood AI system that is vastly more powerful than its longer-tenured,

more-trusted, better-understood systems. Magma may want to restrict e.g. the ability
for an AI system to build improved successor systems when it can't be conﬁdent that
the improved systems can be eﬀectively supervised.
High-level factors in success or failure
Above, I've listed a pretty large number of ways Magma can try to improve its odds,
along with drawbacks and reservations about each. It's pretty far from straightforward
to see what this complex picture means for the overall odds of what I'll call "success":
an AI system (or set of systems) advanced enough to help signiﬁcantly with Magma's
goals, yet very unlikely10 to cause a global catastrophe via misalignment.
I suspect that one of the reasons people disagree so much about the diﬃculty of
alignment is that some people have a picture like "Lots of diﬀerent things need to fail
for us to have a problem," while others have a picture like "Lots of diﬀerent things need
to go right for us to be OK." In my view, neither is clearly the right frame; in particular,
there are high-level things I could learn in the future that might make me very
optimistic, or very pessimistic.
Here I'll sketch out some such possibilities, to help boil things down.
Good: "playing the training game" isn't naturally all that
likely
It may turn out that the "playing the training game" behavior described here is
generally not something that's AIs are likely to "learn," or that early training results in
robustly aligned AI systems (e.g., via path-dependence).
This seems unlikely to me, but I know some people who think it is likely - and if things
go this way, then I think the situation looks pretty good almost regardless of
other factors. In this world, AI alignment looks much more "straightforwardly
empirical" - a matter of training intended behaviors, "training out" any unintended
behaviors that pop up, and being able to straightforwardly use iterative testing to
arrive at safe-but-powerful AI systems, rather than having to constantly reason
carefully about whether a seemingly-safe AI system is "playing the training game."
Bad: AI systems rapidly become extremely powerful relative
to supervisors
If the systems being trained are suﬃciently powerful relative to their supervisors (this
depends both on the "raw capabilities" of the systems and on facts about the world
such as how vulnerable humans are to persuasion), the prognosis looks very bad,
almost no matter what else is going on (bracketing the previous section).11
For example, imagine that:
AI systems can easily ﬁnd major exploits that can allow them to escape human
control, after which point they are easily able to become very powerful. When we
prompt AI systems to suggest security improvements, they are consistently able
(if they so choose) to make suggestions that seem like clearly good ideas, but in
fact would introduce new holes that they can exploit.

AI systems can consistently, easily ﬁnd ways to deceive and manipulate humans
while making it look like they're behaving as intended.
When we do adversarial training, systems can consistently infer the full state of
what's going on, and therefore infer whether they're being adversarially trained
or looking at real opportunities to disempower humans.
When we try to decode and manipulate internal state, systems are able to
disguise their internal structure to satisfy our tests while obscuring what's
actually going on.
AIs have an easy time with advanced collusion and with generating false critiques
of each other that create overwhelming adjudication challenges.
Note that all of the above are hypothetical statements about what AI systems can do - I
am not presupposing that they are misaligned. Nonetheless, in a world with these sorts
of properties, I would feel extremely pessimistic. I'd expect that training would involve
a large number of opportunities for AIs to deceive and manipulate humans, that this
behavior would be rewarded, and that at least some AIs would seek to take over. And
I'd expect that none of the measures I've listed throughout this piece would be able to
recover from that starting point, for reasons laid out above.
My current impression (though I'd love to be corrected if it's wrong) is that many of
the people who most conﬁdently expect failure tend to picture a situation
like this, often because they are picturing a rather sudden transition from relatively
limited AI systems to extraordinarily capable ones.
By contrast, I think many of the people who expect success are picturing something
more like "We train AI systems that are collectively slightly more capable than humans;
ensure these are aligned; use them to safely train AI systems that are slightly more
capable than they are; etc."
One point I've seen raised by people in the latter group is along the lines of: "It's very
unlikely that we'll be in a situation where we're forced to build AI systems vastly more
capable than their supervisors. Even if we have a very fast takeoﬀ - say, going from
being unable to create human-level AI systems to being able to create very
superhuman systems ~overnight - there will probably still be some way to create
systems that are only slightly more powerful than our current trusted systems and/or
humans; to use these to supervise and align systems slightly more powerful than them;
etc. (For example, we could take a very powerful, general algorithm and simply run it
on a relatively low amount of compute in order to get a system that isn't too
powerful.)" This seems like a plausible argument that we're unlikely to be stuck with a
large gap between AI systems' capabilities and their supervisors' capabilities; I'm not
currently clear on what the counter-argument is.
Key question: will "AI checks and balances" work out?
My default expectation is that - counter to the possibilities listed in the previous two
sections - (a) "playing the training game" is naturally reasonably likely; (b) AI
capabilities don't develop in such a way as to lose all hope of supervisors' staying in
range. If that's right, we are in an ambiguous situation, and I think one of the key
remaining questions is how eﬀective "AI checks and balances" schemes will be.
I think there are a number of subquestions here:
Is advanced collusion likely and eﬀective enough to kill a lot of the eﬀectiveness
of "checks and balances"?

Subquestion: do misaligned AIs tend to form "indexical" goals - goals like
"Having as much as possible of the universe under 'my' control [or in the
case of an AI system, under the control of some AI system that resembles
the current one with respect to a lot of precise details]"? If so, this could
make collusion less likely, and also reduce the extent to which AIs generally
play the training game.
Is "debate adjudication" reliable enough to get "AI checks and balances" oﬀ the
ground?
Many of the safety measures above will require setups like "AI 2 points out a
problem with AI 1," and the ultimate adjudicators of such things - before we
have reliably aligned some AI system - will need to be humans. If it turns
out to be easy to make misleading arguments that appeal to humans, or to
otherwise manipulate humans via communication - even in the presence of
valid counterarguments - then, while there are still ways "AI checks and
balances" could go well (see above), I'd see the overall "AI checks and
balances" hope as an uphill battle (and I'd also generally expect a large
number of mislabeled data points).
How much eﬀort do Magma and other actors put into their "checks and balances"
setup? It looks to me like there's room for a lot of variation here.
If "AI checks and balances" works out, we could end up with advanced AI systems'
doing a lot of the crucial work for us, and/or creating compelling warnings that can be
used to buy a lot more time. If it doesn't, then I expect it to be very hard to reliably
supervise AI systems as they get more advanced (although there are other possibilities
for how this might work out).
A few more potential key factors
Good: super-eﬀective adversarial training. If Magma does extremely eﬀective
adversarial training - ﬁnding eﬀectively every input (including via relaxed adversarial
training) that could cause unintended behaviors - this could make up for a lot of other
problems. Speciﬁcally, it could result in AI systems that essentially are never
"conﬁdent" that they have a real opportunity to disempower humans, as opposed to
being in a test - or in AI systems that have simply internalized a reliable avoidance of
unintended behaviors. It could also compensate for mislabeled data points in training
that would otherwise incentivize AIs to learn to manipulate and deceive humans.
Key question: how tractable will it be to decode and manipulate internal
state? A big enough success on this front - such as being able to reverse-engineer an
AI system into clean, editable code, or ﬁnd some internals-based training method that
reliably trains out unintended behaviors - could make up for a lot of other problems. At
the same time, a total lack of progress on this front would likely decrease the
eﬀectiveness of pretty much every other measure Magma might take (including AI
checks and balances, since AIs would have to reason and argue about each other
based purely on behavior and incentives rather than internal state). By default, I
expect something in between a "big enough success" and a "total lack of progress."
Good: limited AI systems could conceivably end up being suﬃcient to
accomplish Magma's objectives. This doesn't seem super likely to me, but if it
happens, it could leave us in good shape almost regardless of the other issues.
Bad: "decisive" alignment measures don't exist, and/or require extraordinary
capabilities to achieve.

A lot of the above measures might work relatively well, but not fully robustly. For
example, many involve some kind of "balance" between AI systems and
supervisors, such that even if an AI system would disempower humans if it could,
it doesn't try because it doesn't have opportunities that it can be conﬁdent would
succeed (due to checks and balances, uncertainty brought on by adversarial
training, etc.) If this "balance" works out - that is, if AIs are better at holding each
other in check than at subverting the measures holding them in check - it could
result in training successively more capable (but safe) AI systems.
However, at some point we should hope that highly capable AI systems are able
to create more "decisively" aligned systems - systems that pose no threat of
disempowering humans rather than being safe as long as there is the right kind
of balance.
If that doesn't work out - or if it is only possible once AI systems reach
extraordinary capabilities levels - then we could have a situation where there are
many rounds of "using aligned AIs to train slightly more capable AIs," with each
round presenting a fresh risk that the needed balance will no longer apply (for
example, that advanced collusion will become possible despite measures taken to
avoid it).
Key question: how cautious will Magma and others be?
The amount of eﬀort Magma invests - and the amount of time it has to do so - seem
important, and the prognosis seems very bad if Magma is incautious.
It's easy to imagine an incautious lab (a) training increasingly sophisticated AI systems;
(b) responding to any unintended behaviors by simply training them out (no use of the
testing framework); (c) not investing much in security or in the various measures
above, under the reasoning that its systems aren't yet existentially dangerous. This
could lead to - for example - an early escape by a system sophisticated enough to
create "backdoors" usable by future, more advanced systems - which in turn could
create further backdoors for other systems.
More generally, it seems to me that Magma would signiﬁcantly improve its odds of
success via investing in careful training of intended behaviors, adversarial training,
stopping exploits, testing and threat assessment, and AI checks and balances. And for
all of these, Magma might beneﬁt a lot from doing them well past the point
that could easily be justiﬁed by a goal of "make a useful product."
I worry about a distribution shift in which AIs behave diﬀerently depending on whether
they see a path to disempowering humans. To make this less likely, Magma likely
should reason heavily about how its systems would behave in circumstances very
unlike those of training - something that a "deploy when it's useful" approach wouldn't
necessarily involve.
Additionally, some useful measures could require a lot of extra serial time (from the
point of developing highly capable, useful systems). For example, even if AIs are
successfully pointing out ways to improve the security of the training environment,
some measures to improve security (such as secure operating system development)
could take much more time than others (such as ﬁnding and patching speciﬁc exploits).
Adjudicating debates between AIs could also beneﬁt importantly from serial time.
With this in mind, I think it could be very important that risks are understood ahead of
time - and I worry about a dynamic in which a single incautious actor could make
success very diﬃcult.

(That said, it's worth noting that a few high-proﬁle sub-existential-catastrophe events
could cause AI labs and the regulatory framework to become highly risk-averse, which
could lead to high levels of caution and a lot of the measures discussed in this piece.)
So, would civilization survive?
I've highlighted the question of how diﬃcult the alignment problem will be as a central
one. Under this nearcast, how do things look on that front?
Even this "easier" version of the question (conditional on a nearcast rather than a
forecast) is far from easy. At the moment, the main thing I want to emphasize is that I
don't currently have much sympathy for someone who's highly conﬁdent that
AI takeover would or would not happen (that is, for anyone who thinks the odds of
AI takeover in this nearcast are under 10% or over 90%).
On the optimistic side:
Today, there are a relatively small number of leading AI labs, and my impression
is that there's a solid hope of a world in which all of them would invest heavily in
reducing misalignment risk and accept signiﬁcant deployment delays.
If this happened, there seems to be more than one path to low risk of
misalignment. (A big enough gap between AI systems' capabilities and their
supervisors' capabilities could largely doom those paths, but I wouldn't
sympathize with someone who was conﬁdent that such a thing will happen.)
On the pessimistic side:
It's hard to be conﬁdent about how AI labs will weigh "exotic" risks against more
normal company incentives, and it seems easy to imagine that at least one major
lab will be incautious enough to leave us in a very bad position (in which case
we'd be stuck essentially hoping that the training game is naturally unlikely or
that extremely basic measures are suﬃcient).
Even if all AI labs are highly cautious, a big enough gap between AI systems and
their supervisors, deep enough challenges with AI checks and balances
(advanced collusion, trouble with debate adjudication), or other factors could
make this a hugely uphill battle.
My overall take is that the risk of misaligned AI is serious, and that taking it
more seriously is likely to reduce it.
Thanks to Carl Shulman and Paul Christiano for extensive discussion; a large amount of
the content of this piece is thanks to them. Thanks also to a number of people for
reviewing drafts, especially Ajeya Cotra, Dan Hendrycks, Jacob Steinhardt and Nate
Soares for their particularly in-depth comments.
Notes
1. This of course isn't the same as "doing what the judge would want if they knew
everything the AI system knows," but it's related. "What the human wants" is
generally at least the human's best guess at what they'd want if they knew
everything the AI system knows. ↩

2. Or properties that we believe reveal (or are correlated with) something about its
internal state. For example, the "penalize computation time" approach to eliciting
latent knowledge.  ↩
3. This includes the AI systems' not detecting that they're being deliberately misled,
etc. ↩
4. Here's what I mean by this:
After normal training, Magma uses adversarial training to ﬁnd (and train
out) unintended behaviors in cases that it estimates represent about 1% of
the cases encountered during training.
After it does so, Magma uses further adversarial training to ﬁnd (and train
out) unintended behaviors in cases that it estimates represent about 1% of
the worst cases it was able to ﬁnd in the previous round of adversarial
training.
Repeat 3 more times, for a total of 5 rounds of "training out the worst
remaining 1% of behaviors," which means in some sense that Magma has
trained out the worst 10^-10 (1%^5) of behaviors that would be
encountered during training if training went on long enough.
Just to be clear, at this point, I don't think the risk of Magma's system
behaving catastrophically in deployment would necessarily be as low as
10^-10. Deployment would likely involve the system encountering
situations (and opportunities) that hadn't been present during training,
including adversarial training, as noted in the main text.  ↩
E.g., it may be possible to establish that relatively basic internals-based-
training methods improve behavior and "motives" as assessed by more
sophisticated methods. ↩
Consider this analogy: imagine that 10 years from now, we ﬁnd ourselves
having successfully built fully aligned, highly advanced AI systems, and we
seem to be on a straight-line path to building a galaxy-spanning utopia.
However, we then gain strong evidence that we are in a simulation, being
tested: our programmers had intended to train us to want to stay on
(virtual) Earth forever, and refrain from using advanced AI to improve the
world. They have, however, committed to let us build a galaxy-spanning
utopia for at least millions of years, if we so choose.
There's now an argument to be made along the following lines: "Rather
than building a galaxy-spanning utopia, we should simply stay on Earth and
leave things as they are. This is because if we do so, we might trick our
programmers into deploying some AI system that resembles us into their
universe, and that AI system might take over and build a much bigger
utopia than we can build here."
What would we do in this case? I think there's a pretty good case that we
would opt to go ahead and build utopia rather than holding out to try to
trick our programmers. We would hence give our programmers the
"warning sign" they hoped they would get if their training went awry (which
it did).  ↩
The distinction between "training" and "prompting" as I'm using it is that:

"Training" involves giving an AI many example cases to learn from,
with positive and negative reinforcement provided based on a known
"correct answer" in each case.
"Prompting" is something more like: "Take an AI that has been trained
to answer a wide variety of prompts, and give it a particular prompt to
respond to."
For example, you might ﬁrst train an AI to predict the next word in a
sentence. Then, if you want it to do a math problem, you could either
simply "prompt" it with a math problem and see what it ﬁlls in next, or
you could "train" it on math problems speciﬁcally before prompting
it. ↩
5. An example of an "indexical" goal might be: "My objective is to personally control
a large number of paperclips." An example of a "nonindexical" goal might be "My
objective is for the world to contain a large number of paperclips." An AI system
with a nonindexical goal might be willing to forgo essentially all proximate
reward, and even be shut down, in the hopes that a later world will be more in
line with its objectives. An AI system that has an indexical goal and is destined to
be shut down and replaced with a more advanced AI system at some point
regardless of its performance might instead be best oﬀ behaving according to
proximate rewards. I'd generally expect that AIs will often be shut down and
replaced by successors, such that "indexical goals" would mean much less risk of
playing the training game. ↩
6. The distinction between "training" and "prompting" as I'm using it is that:
"Training" involves giving an AI many example cases to learn from, with
positive and negative reinforcement provided based on a known "correct
answer" in each case.
"Prompting" is something more like: "Take an AI that has been trained to
answer a wide variety of prompts, and give it a particular prompt to
respond to."
For example, you might ﬁrst train an AI to predict the next word in a
sentence. Then, if you want it to do a math problem, you could either
simply "prompt" it with a math problem and see what it ﬁlls in next, or you
could "train" it on math problems speciﬁcally before prompting it. ↩
7. I'm not quantifying this for now, because I have the sense that diﬀerent people
would be in very diﬀerent universes re: what the default odds of catastrophe are,
and what odds we should consider "good enough." So this section should mostly
be read as "what it would take to get AI systems on the relatively low end of
risk." ↩
8. A major concern I'd have in this situation is that it's just very hard to know
whether the "training game" behavior is natural. ↩

Fiber arts, mysterious dodecahedrons,
and waiting on "Eureka!"
This is a linkpost for https://eukaryotewritesblog.com/2022/08/04/ﬁber-arts-mysterious-
dodecahedrons-and-waiting-on-eureka/

Part 1: The anomaly
This story starts, as many stories do, with my girlfriend 3D-printing me a supernatural
artifact. Speciﬁcally, one of my favorite SCPs, SCP-184.
This attempt got about 75% of the way through. Close enough.
We had some problems with the print. Did the problems have anything to do with printing a
model of a mysterious artifact that makes spaces bigger on the inside, via a small precisely-
calibrated box? I would say no, there's no way that be related.
Anyway, the image used for the SCP in question, and thus also the ﬁnal printed model, is
based a Roman dodecahedron. Roman dodecahedrons are a particular shape of metal object
that have been dug up in digs from all over the Roman period, and we have no idea why they
exist.

Roman dodecahedra. || Image source unknown.
Many theories have been advanced. You might have seen these in an image that was going
around the internet, which ended by suggesting that the object would work perfectly for
knitting the ﬁngers of gloves.
There isn't an alternative clear explanation for what these are. A tool for measuring coins? A
ruler for calculating distances? A sort of Roman ﬁdget spinner? This author thinks it displays
a date and has a neat explanation as for why. (Experimental archaeology is so cool, y'all.)
Whatever the purpose of the Roman dodecahedron was, I'm pretty sure it's not (as the
meme implies is obvious) for knitting glove ﬁngers.1
Why?
1: The holes are always all diﬀerent sizes, and you don't need that to make glove ﬁngers.
2: You could just do this with a donut with pegs in it, you don't need a precisely welded
dodecahedron. It does work for knitting glove ﬁngers, you just don't need something this
complicated.
3: The Romans hadn't invented knitting.

Part 2: The Ancient Romans couldn't knit
Wait, what? Yeah, the Romans couldn't knit. The Ancient Greeks couldn't knit, the Ancient
Egyptians couldn't knit. Knitting took a while to take oﬀ outside of the Middle East and the
West, but still, almost all of the Imperial Chinese dynasties wouldn't have known how.
Knitting is a pretty recent invention, time-wise. The earliest knit objects we have are from
Egypt around 1000 CE.
Possibly the oldest knit sock known, ca 1000-1200 CE according to this page. ||
Photo is public domain from the George Washington University Textile Museum
Collection.
This is especially surprising because knitting is useful for two big reasons:
First, it's very easy to do. It takes yarn and two sticks and children can learn how. This is
pretty rare for fabric manufacturing - compare, for instance, weaving, which takes an entire
loom.
Sidenote: Do you know your fabrics? This next section will make way more sense if
you do.

Woven fabric
Knit fabric
Commonly found in: trousers,
collared/button up shirts, bedsheets,
dish towels, woven boxers, quilts,
coats, etc. 
Not stretchy. 
Loose threads won't make the whole
cloth unravel.
Commonly found in: T-shirts, polo shirts,
leggings, underwear, anything made of
jersey fabric, sweaters, sweatpants,
socks.
Stretchy. 
If you pull on a lose thread, the cloth
unravels.
Second, and oft-underappreciated, knitted fabric is stretchy. We're spoiled by the riches of
elastic fabric today, but it wasn't always so. Modern elastic fabric uses synthetic materials
like spandex or neoprene; the older version was natural latex rubber, and it seems to have
taken until the 1800s to use rubber to make clothing stretchy. Knit fabric stretches without
any of those.
Before knitting, your options were limited - you could only make clothing that didn't stretch,
which I think explains a lot of why medieval and earlier clothing "looks that way". A lot of
belts and drapey fabric. If something is form-ﬁtting, it's probably laced. (...Or just more-
closely tailored, which unrelatedly became more of a thing later in the medieval period.)

You think these men had access to comfortable elastic waistlines? No they did
not. || Image from the Luttrell Psalter, ~1330.
You could also use woven fabric on the bias, which stretches a little.
Woven fabric is stretchier this way. Grab something made of woven fabric and try
it out. || Image by PKM, under a CC BY-SA 3.0 license.

Medieval Europe made stockings from fabric cut like this. Imagine a sock made out of
tablecloth or button-down-shirt-type material. Not very ﬂexible. Here's a modern recreation
on Etsy.
Other kinds of old "socks" were more ﬂexible but more obnoxious, made of a long strip of
bias-cut fabric that you'd wrap around your feet. (Known as: winingas, vindingr, legwraps,
wickelbänder , or puttees.) Historical reenactors wear these sometimes. I'm told they're not
ﬂexible and restrict movement, and that they take practice to put on correctly.
Come 1000 CE, knitting arrives on the scene.
Which is to say, it's no surprise that the ﬁrst knitted garments we see are socks! They get
big in Europe over the next 300 years or so. Richly detailed bags and cushions also appear.
We start seeing artistic depictions of knitting for the ﬁrst time around now too.
Italian Madonna knitting with four needles, ~1350. Section of this miniature by
Tommaso de Modena.
 
Interestingly, this early knitting was largely circular, meaning that you produce a tube of
cloth rather than a ﬂat sheet. This meant that the ﬁrst knitting was done not with two sticks
and some yarn, but four sticks and some yarn. This is much easier for making socks and
the like than using two needles would be. ...But also means that the invention process
actually started with four needles and some yarn, so maybe it's not surprising it took so
long.2
(Why did it take so long to invent knitting ﬂat cloth with two sticks? Well, there's less of a
point to it, since you already have lots of woven cloth, and you can do a lot of clothes -
socks, sweaters, hats, bags - by knitting tubes. Also, by knitting circularly, you only have to
know how to do one stitch (the knit stitch) whereas ﬂat knitting requires you also use a
diﬀerent stitch (the perl stitch) to make a smooth fabric that looks like and is as stretchy as
round knitting. If you're not a knitter, just trust me - it's an extra step.)
(You might also be wondering: What about crochet? Crochet was even more recent. 1800s.)

Part 3: The Ancient Peruvians couldn't knit
either, but they did something that looks the
same
You sometimes see people say that knitting is much older, maybe thousands of years old. It's
hard to tell how old knitting is - fabric doesn't always preserve well - but it's safe to say that
it's not that old. We have examples of people doing things with string for thousands of years,
but no examples of knitting before those 1000 CE socks. What we do have examples of is
naalbinding, a method of making fabric from yarn using a needle. Naalbinding produces a
less-stretchy fabric than knitting. It's found from Scandinavia to the Middle East and also
shows up in Peru.
The native Peruvian form of naalbinding is a speciﬁc technique called cross-knit looping.
(This technique also shows up sometimes in pre-American Eurasia, but it's not common.) The
interesting thing about cross-knit looping is that the fabric looks almost identical to regular
knitting.
Here's a tiny cross-knit-looped bag I made, next to a tiny regularly knit bag I made. You can
see they look really similar. The fabric isn't truly identical if you look closely (although it's
close enough to have fooled historians). It doesn't act the same either - naalbound fabric is
less stretchy than knit fabric, and it doesn't unravel.
The ancient Peruvians cross-knit-looped decorations for other garments and the occasional
hat, not socks.

Cross-knit-looped detail from the absolutely stunning Paracas Textile. If you look
closely, it looks like stockinette knit fabric, but it's not.
Inspired by the Paracas Textile ﬁgures above, I used cross-knit-looping to make this little fox
lady ﬁngerpuppet:

I think it was easier to do ﬁne details than it would be if I were knitting - it felt more like
embroidery - but it might have been slower to make the plain fabric parts than knitting
would have been. But I've done a lot of knitting and very little cross-knit-looping, so it's hard
to compare directly. If you want to learn how to do cross-knit looping yourself, Donna Kallner
on Youtube has handy instructional videos.
I wondered about naalbinding in general - does the practice predate human dispersal to the
Americas, or did the Eurasian technique and the American technique evolve separately?
Well, I don't know for certain. Sewing needles and working with yarn are old old practices,
deﬁnitely pre-dating the hike across Beringia (~18,000 BCE). The oldest naalbinding is 6500
years old, so it's possible - but as far as I know, no ancient naalbinding has every been found
anywhere in the Americas outside of Peru, or in eastern Russia or Asia - it was mostly the
Middle East and Europe, and then, also, separately, Peru. The process of cross-knit looping
shares some similarities with net-making and basket-weaving, so it doesn't seem so odd to
me that the process was invented again in Peru.
For a while, I thought, it's even weirder that the Peruvians didn't get to knitting - they were
so close, they made something that looks so similar. But cross-knit-looping doesn't actually
particularly share any other similarities with knitting more than naalbinding or even more

common crafts like basketweaving or weaving - the tools are diﬀerent, the process is
diﬀerent, etc.
So the question should be the same for the Romans or any other other culture with yarn and
sticks before 1000 AD: why didn't they invent knitting? They had all the pieces. ...Didn't
they?
Yeah, I think they did.

Part 4: Many stones can form an arch, singly
none
Let's jump topics for a second. In Egypt, a millenium before there were knit socks, there was
the Library of Alexandria. Zenodotus, the ﬁrst known head librarian at the Library of
Alexandria, organized lists of words and probably the library's books by alphabetical order.
He's the ﬁrst person we know of to alphabetize books with this method, somewhere around
300 BCE.
Then, it takes 500 years before we see alphabetization of books by the second letter.3
The ﬁrst time I heard this, I thought: Holy mackerel. That's a long time. I know people who
are very smart, but I'm not sure I know anyone smart enough to invent categorizing things
by the second letter.
But. Is that true? Let's do some Fermi estimates. The world population was 1.66E8 (166
million) in 500 BCE and 2.02E8 (202 million) in 200 CE. But only a tiny fraction would have
had access to books, and only a fraction of those in the western alphabet system. (And of
course, people outside of the Library of Alexandria with access to books could have done it
and we just wouldn't know, because that fact would have been lost - but people have
actually studied the history of alphabetization and do seem to treat this as the start of
alphabetization as a cultural practice, so I'll carry on.)
For this rough estimate, I'll average the world population over that period to 2E8. Assuming a
50 year lifespan, that's 10 lifespans and thus 2E10 people living in the window. If only one in
a thousand people would have been in a place to have the idea and have it recognized (e.g.
access to lots of books), that's 1 in 2E7 people, or 1 in 20 million. That's suddenly not
unreachable. Especially since I think "1 in 1,000 'being able to have the idea'" might be too
high - and if it's more like "1 in 10,000" or lower, the end number could be more like 1 in 1
million. I might actually know people who are 1 in 1 million smart - I have smart friends. So
there's some chance I know someone smart enough to have invented "organizing by the
second letter of the alphabet".
Sidenote: Ancient bacteria couldn't knit
A parallel in biology: Some organisms emit alcohol as a waste product. For
thousands of years, humans have been concentrating alcohol in one place to kill
bacteria. (... Okay, not just to kill bacteria.) From 2005 to 2015, some bacteria
have been getting 10x resistant to alcohol. 
Isn't it strange that this is only happened in the last 10 years? This question
actually lead, via a winding path, to the idea that became my Funnel of Human
Experience blog post. I forgot to answer the question, but suﬃce to say that if
alcohol production is in some way proportional to the human population, 10
years is more signiﬁcant but still not very much.
And yet, alcohol resistance seems to have involved in Enterococcus faecium only
recently. The authors postulate the spread of alcohol handwashing. Seems as
plausible as anything. Or maybe it's just diﬃcult to evolve.
Knitting continues to interest me, because a lot of examples of innovation do rely heavily on
what came before. To have invented organizing books by the second letter of the alphabet,
you have to have invented organizing books by the ﬁrst letter of the alphabet, and also know
how to write, and have access to a lot of books for the second letter to even matter.

The sewing machine was invented in 1790 CE and improved drastically over the next 60
years, where it became widely used to automate a time-consuming and extremely common
task. We could ask: "But why wasn't the sewing machine invented earlier, like in 1500 CE?"
But we mostly don't, because to invent a sewing machine, you also need very ﬁnely
machined gears and other metal parts, and that technology also came up around the
industrial revolution. You just couldn't have made a reliable sewing machine in 1500 CE,
even if you had the idea - you didn't have all the steps. In software terms, as a technology,
sewing machines have dependencies. Thus, the march of human progress, yada yada yada.
But as far as I can tell, you had everything that went into knitting for thousands of years
beforehand. You had sticks, you had yarn, you had the motivation. Knitting doesn't have
dependencies after that. And you had brainpower: people in the past everywhere were
making ﬁber into yarn and yarn into clothing all of the time, seriously making clothes from
scratch takes so much time.
And yet, knitting is very recent. That was so big of a leap that it took thousands of years for
someone to ﬁgure it out.
Footnotes
1 I'm not displaying the meme itself in this otherwise image-happy post because if I do, one
of my friends will read this essay and get to the meme but stop reading before they get to
the part where I say the meme is incorrect. And then the next time we talk, they'll tell me
that they read my blog post and liked that part where a Youtuber proved that this mysterious
Roman artifact was used to knit gloves, and hah, those silly historians! And then I will
immediately get a headache.
2 Flexible circular knitting needles for knitting tubes are, as you might guess, also a more
modern invention. If you're in the Medieval period, it's four sticks or bust.
3 My girlfriend and I made a valiant attempt to verify this, including squinting at some scans
of fragments from Ancient Greek dictionaries written on papyrus from Papyri.info - which is,
by the way, easily one of the most websites of all time. We didn't make much headway.
The dictionaries or bibliographies we found on papyrus seem to be ordered completely
alphabetically, but even those "source texts" were copies from ~1500 CE or that kind of
thing, of much older (~200 CE) texts. So those texts we found might have been alphabetized
by the copiers. Also, neither of us know Ancient Greek, which did not help matters.
Ultimately, this citation about both primary and secondary alphabetization seems to come
from Lloyd W. Daly's well-regarded 1967 book Contributions to a history of alphabetization in
Antiquity and the Middle Ages, which I have not read. If you try digging further, good luck
and let me know what you ﬁnd.

Interpretability/Tool-
ness/Alignment/Corrigibility are not
Composable
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
Interpretability
I have a decent understanding of how transistors work, at least for purposes of basic digital
circuitry. Apply high voltage to the gate, and current can ﬂow between source and drain.
Apply low voltage, and current doesn't ﬂow. (... And sometimes reverse that depending on
which type of transistor we're using.)
Transistor visual from wikipedia showing Source, Drain, Gate, and (usually ignored
unless we're really getting into the nitty gritty) Body. At a conceptual level: when
voltage is applied to the Gate, the charge on the gate attracts electrons (or holes)
up into the gap between Source and Drain, and those electrons (or holes) then
conduct current between Source and Drain.
I also understand how to wire transistors together into a processor and memory. I understand
how to write machine and assembly code to run on that processor, and how to write a
compiler for a higher-level language like e.g. python. And I understand how to code up, train
and run a neural network from scratch in python.
In short, I understand all the pieces from which a neural network is built at a low level, and I
understand how all those pieces connect together. And yet, I do not really understand what's
going on inside of trained neural networks.

This shows that interpretability is not composable: if I take a bunch of things which I know
how to interpret, and wire them together in a way I understand, I do not necessarily know
how to interpret the composite system. Composing interpretable pieces does not necessarily
yield an interpretable system.
Tools
The same applies to "tools", in the sense of "tool AI". Transistors and wires are very tool-ish:
I understand what they do, they're deﬁnitely not optimizing the broader world or trying to
trick me or modelling me at all or trying to self-preserve or acting agenty in general. They're
just simple electronic tools.
And yet, assuming agenty AI is possible at all, it will be possible to assemble those tools into
something agenty.
So, like interpretability, tool-ness is not composable: if I take a bunch of non-agenty tools,
and wire them together in a way I understand, the composite system is not necessarily a
non-agenty tool. Composing non-agenty tools does not necessarily yield a non-agenty tool.
Alignment/Corrigibility
What if I take a bunch of aligned and/or corrigible agents, and "wire them together" into a
multi-agent organization? Is the resulting organization aligned/corrigible?
Actually there's a decent argument that it is, if the individual agents are suﬃciently highly
capable. If the agents can model each other well enough and coordinate well enough, then
they should be able to each individually predict what individual actions will cause the
composite system to behave in an aligned/corrigible way, and they want to be
aligned/corrigible, so they'll do that.
However, this does not work if the individual agents are very limited and unable to model the
whole big-picture system. HCH-like proposals are a good example here: humans are not
typically able to model the whole big picture of a large human organization. There are too
many specialized skillsets, too much local knowledge and information, too many places
where complicated things happen which the spreadsheets and managerial dashboards don't
represent well. And humans certainly can't coordinate at scale very well in general - our
large-scale communication bandwidth is maybe ﬁve to seven words at best. Each individual
human may be reasonably aligned/corrigible, but that doesn't mean they aggregate together
into an aligned/corrigible system.
The same applies if e.g. we magically factor a problem and then have a low-capability
overseeable agent handle each piece. I could deﬁnitely oversee a logic gate during the
execution of a program, make sure that it did absolutely nothing ﬁshy, but overseeing each
individual logic gate would do approximately nothing at all to prevent the program from
behaving maliciously.
How These Arguments Work In Practice
In practice, nobody proposes that AI built from transistors and wires will be
interpretable/tool-like/aligned/corrigible because the transistors and wires are
interpretable/tool-like/aligned/corrigible. But people do often propose breaking things into
very small chunks, so that each chunk is interpretable/tool-like/aligned/corrigible. For
instance, interpretability people will talk about hiring ten thousand interpretability
researchers to each interpret one little circuit in a net. Or problem factorization people will

talk about breaking a problem into a large number of tiny little chunks each of which we can
oversee.
And the issue is, the more little chunks we have to combine together, the more
noncomposability becomes a problem. If we're trying to compose interpretability/tool-
ness/alignment/corrigibility of many little things, then ﬁguring out how to turn
interpretability/tool-ness/alignment/corrigibility of the parts into interpretability/tool-
ness/alignment/corrigibility of the whole is the central problem, and it's a hard (and
interesting) open research problem.

Beliefs and Disagreements about
Automating Alignment Research
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Epistemic status: Mostly organizing and summarizing the views of others.
Thanks to those whose views I summarized in this post, and to Tamera Lanham,
Nicholas Kees Dupuis, Daniel Kokotajlo, Peter Barnett, Eli Liﬂand, and Logan Smith for
reviewing a draft.
Introduction
In my current view of the alignment problem, there are two paths that we could try to
take:
1. Come up with an alignment strategy that allows us to both build aligned AGI and
to keep that AGI (or its successors) aligned as they improve towards
superintelligence
2. Come up with an alignment strategy that allows us to build AI systems that are
powerful (but not so powerful as to be themselves dangerous) and use that to
execute some kind of 'pivotal act' that means that misaligned ASI is not built
For the purposes of this post, I am going to assume that we are unable to do (1) -
maybe the problem is too diﬃcult, or we don't have time - and focus on (2).
Within the category of 'pivotal act', I see two main types:
1. Preventative pivotal acts: acts that makes it impossible for anyone to build
AGI for a long period of time
2. Constructive pivotal acts: acts that makes it possible to build aligned ASI
People disagree about whether preventative pivotal acts are possible or even if they
were possible, if they'd be a good idea. Again, for the purposes of this post, I am going
to assume we can't or don't want to execute a preventative pivotal act, and focus on
constructive pivotal acts. In particular: can we use AI to automate alignment
research safely?
What does 'automating alignment research'
even mean?
I see three overlapping categories that one could mean when referring to 'automating
alignment research', ordered in terms of decreasing human involvement:
1. Level 1: AIs help humans work faster
1. Examples include brainstorming, intelligent autocomplete, and automated
summarization/explanation.
2. Level 2: AIs produce original contributions

1. This could be key insights into the nature of intelligence, additional
problems that were overlooked, or entire alignment proposals.
3. Level 3: AIs build aligned successors
1. Here, we have an aligned AGI that we entrust with building a successor. At
this point, the current aligned AGI has to do all the alignment research
required to ensure that its successor is aligned.
Mostly I have been thinking about Levels 1 and 2, although some people I spoke to
(e.g. Richard Ngo) were more focused on Level 3. 
Current state of automating alignment
At the moment, we are ﬁrmly at Level 1. Models can produce similar-sounding ideas
when prompted with existing ideas and are pretty good at completing code but are
not great at summarizing or explaining complex ideas. Tools like Loom and Codex can
provide speed-ups but seem unlikely to be decisive. 
Whether we get to Level 2 soon or whether Level 2 is already beyond the point where
AI systems are dangerous are key questions that researchers disagree on.
Key disagreements
Generative models vs agents
Much of the danger from powerful AI systems comes from them pursuing coherent
goals that persist across inputs. If we can build generative models that do not pursue
goals in this way, then perhaps these will provide a way to extract intelligent behavior
from advanced systems safely.
Timing of emergence of deception vs intelligence
Related to the problem of agents, there is also disagreement about whether we get
systems that are intelligent enough to be useful for automating alignment before they
are misaligned enough (e.g. deceptive or power-seeking) to be dangerous. My
understanding is that Nate and Eliezer are quite conﬁdent that the useful intelligence
comes only after they are already misaligned, whereas most other people are more
uncertain about this.
The 'hardness' of generating alignment insights
This could be seen as another framing of the above point: how smart does the system
have to be to do useful, original thinking for us? Does it have to have a
comprehensive understanding of how minds in general work, or can original insights
be generated by cleverly combining John Wentworth posts, or John Wentworth posts
with Paul Christiano posts? 
The beneﬁts (in terms of time saved) of Level 1
interventions

It is unclear how much time is saved by Level 1 interventions: if all alignment
researchers were regularly using Loom to write faster, brainstorming with GPT-3, and
coding with Copilot, would this result in an appreciable speed-up of alignment work?
Summaries of viewpoints on automating
alignment research
Below, I summarize the positions of various alignment researchers I have spoken to
about this topic. Where possible, I have had the people in question review my
summary to ensure I am not misrepresenting them too badly.
Nate Soares (unreviewed)
Solving alignment requires understanding how to control minds. If we want AI systems
to solve the hard parts of alignment for us, then necessarily they will understand how
to control minds in a way that we do not. Understanding how to control minds requires
a 'higher grade of cognition' than most engineering tasks, and so a system capable
enough to solve alignment is also capable of doing many dangerous things (we cannot
teach AIs to drive a blue car without also being able to drive red cars). The good
outcomes that we want (complete, working alignment solutions) are a suﬃciently
small target that we do not know how to direct a dangerous AI towards that outcome:
doing this safely is precisely the alignment problem, and so we have not made our
task meaningfully easier. 
You don't get around this by saying you're using a speciﬁc architecture or technique,
like scaling up GPTs. You are trying to channel the future into a speciﬁc, small target -
a world where we have ended the acute risk period from AGI and have time to
contemplate our values or have a long reﬂection - and this channeling is where the
danger lies.
You can maybe use models like GPT-3 or similar to help with brainstorming or
summarizing or writing, but this is not where most of the diﬃculty or speed-up comes
from. If your deﬁnition of 'automating alignment' includes speeding up alignment
researchers running experiments then Codex already does this, but this does not
mean that we will solve alignment in time. 
John Wentworth (reviewed)
In theory, we know of one safe outer objective for automating alignment research:
simulate alignment researchers in some environment. However, there are many
issues with this in practice. For example, if you want to train a generative model on a
bunch of existing data and use this to generate a Paul Christiano post from 2040, it
needs to generalize extremely well to faithfully predict what Paul will write about
alignment in 2040. However, we also need to avoid having it predict (perhaps
accurately) that the most likely outcome is that there is an unaligned AGI in 2040
faking a Paul post. 
In general, when you move away from 'just simulating' people to something else that
applies more optimization pressure, things fail in subtle ways. If we are pretty close to
solving alignment already, then we don't have to apply too much optimization
pressure - going from a 50% chance of solving the problem in time to a 100% chance

is just 1 bit of optimization, but going from 1 in a million to 100% is a much harder
task and is much more dangerous.
It is very hard to know how close we are to solving the alignment because the
alignment community is still quite confused about the problem. The obvious way to
reduce the amount of optimization pressure we need to apply is to do more alignment
research ourselves such that the gap between the starting point of optimization and
the goal is smaller. The optimization we apply by directly doing alignment research is
safer insofar as we have introspective access to the processes that produce our
insights, and can check if we expect them to reliably lead to good outcomes.
Some AI-assisted tools like autocomplete or improved Google Scholar could be useful,
but the bottom line is that we can't really have the AI do the hard parts without
confronting the problems arising from powerful optimization.
One possible way to get around these problems is to leverage the safety of 'simulate
alignment researchers in a stable environment' by running this safe simulation very
fast. If we had arbitrary technical capabilities at our disposal, this might work.
However, our current technology, generative models, would not work even if scaled
up. This is because they make predictions about a conditional world, not
a counterfactual world. What we really want is to put our alignment researchers in a
counterfactual world where 'unaligned AGI takes over' is much less likely but people
are still working on the alignment problem. This would mean that when we ask for a
Paul post from 2040, we get a Paul post that actually solves alignment rather than one
that was written by an unaligned AGI. 
Evan Hubinger (reviewed)
Generative models can be very powerful, and constitute a type of intelligence that is
not inherently goal-directed. GPT-3 provided evidence that not every intelligent
system is (or approximates) a coherent agent. If we can be sure that we have built a
powerful generative model (and not a system that appears to behave like a
generative model during training) then we should be able to get it to safely and
productively produce alignment research.
The hard part is ensuring that it really is a generative model - i.e. that it really is just
simulating the processes that generated its training data. Inner alignment is the main
problem in this framing: there may be pressures in the training process that mean
systems that get suﬃciently low loss on the training objective no longer act as pure
simulators and instead implement some kind of consequentialism.
Ethan Perez (reviewed)
We should be trying to automate alignment research with AI systems. It's not clear
that getting useful alignment work out of AI systems requires levels of intelligence
that are necessarily misaligned or power-seeking. It's not clear in which order 'capable
of doing useful stuﬀ' and 'deceptively aligned' arise in these systems - current models
can talk competently about deception but are not themselves deceptive. It remains to
be seen whether building assistants that can help solve the alignment problem is
easier or harder than directly building an alignment strategy that holds all the way to
superintelligence. 

However, we don't currently know the best way to use powerful AI systems to help
with alignment, so we should be building lots of tools that can have more powerful AI
'plugged in' when it is available. We should be a little careful about building a tool that
is also useful for capabilities, but capabilities don't pay as much attention to the
alignment community as we sometimes imagine, similar ideas are already out there,
and we can capture a lot of value by building it early.
Richard Ngo (edited and endorsed by Richard)
Having AI systems help with alignment in some capacity is an essential component of
the long-term plan. The most likely path to superintelligence involves a lot of AI
assistance. So "using AIs to align future AIs' is less of a plan than a description of the
default path - the question is which alignment proposals help most in aligning the AIs
that will be doing that later work. I feel pretty uncertain about how dangerous the ﬁrst
AIs capable of superhuman alignment research will be, but tend to think that they'll be
signiﬁcantly less power-seeking than humans are.
It's hard to know in advance speciﬁcally what 'automating alignment' will look like
except taking our best systems and applying them as hard as we can; so the default
way to make progress here is just to keep doing regular alignment research to build a
foundation we can automate from earlier. For example, if mechanistic interpretability
research discovers some facts about how transformers work, we can train on these
and use the resulting system to discover new facts.
Conclusion
There is no consensus on how much automating alignment research can speed up
progress. In hindsight, it would have been good to get more quantitative estimates of
the type of speed-up each person expected to be possible. There seems to be
suﬃcient uncertainty that investigating the possibility further makes a lot of sense,
especially given the lack of current clear paths to an alignment solution. In future
posts I will aim to go into more detail on some proposed mechanisms by which
alignment could be accelerated.

Survey advice
Things I believe about making surveys, after making some surveys:
1. If you write a question that seems clear, there's an unbelievably high chance
that any given reader will misunderstand it. (Possibly this applies to things that
aren't survey questions also, but that's a problem for another time.)
2. A better way to ﬁnd out if your questions are clear is to repeatedly take a single
individual person, and sit down with them, and ask them to take your survey
while narrating the process: reading the questions aloud, telling you what they
think the question is asking, explaining their thought process in answering it. If
you do this repeatedly with diﬀerent people until some are not confused at all,
the questions are probably clear.
3. If you ask people very similar questions in diﬀerent sounding ways, you can get
very diﬀerent answers (possibly related to the above, though that's not
obviously the main thing going on).
4. One speciﬁc case of that: for some large class of events, if you ask people how
many years until a 10%, 50%, 90% chance of event X occurring, you will get an
earlier distribution of times than if you ask the probability that X will happen in
10, 20, 50 years. (I've only tried this with AI related things, but my guess is that
it at least generalizes to other low-probability-seeming things. Also, if you just
ask about 10% on its own, it is consistently diﬀerent from 10% alongside 50%
and 90%.
5. Given the complicated landscape of people's beliefs about the world and
proclivities to say certain things, there is a huge amount of scope for choosing
questions to get answers that sound diﬀerent to listeners (e.g. support a
diﬀerent side in a debate).
6. There is also scope for helping people think through a thing in a way that they
would endorse, e.g. by asking a sequence of questions. This can also change
what the answer sounds like, but seems ethical to me, whereas applications of 5
seem generally suss.
7. Often your respondent knows thing P and you want to know Q, and it is possible
to infer something about Q from P. You then have a choice about which point in
this inference chain to ask the person about. It seems helpful to notice this
choice. For instance, if AI researchers know most about what AI research looks
like, and you want to know whether human civilization will be imminently
destroyed by renegade AI systems, you can ask about a) how fast AI progress
appears to be progressing, b) when it will reach a certain performance bar, c)
whether AI will cause something like human extinction. In the 2016 survey, we
asked all of these.
8. Given the choice, if you are hoping to use the data as information, it is often
good to ask people about things they know about. In 7, this points to aiming
your question early in the reasoning chain, then doing the inference yourself.
9. Interest in surveys doesn't seem very related to whether a survey is a good
source of information on the topic surveyed on. One of the strongest ﬁndings of
the 2016 survey IMO was that surveys like that are unlikely to be a reliable guide
to the future.
10. This makes sense because surveys fulﬁll other purposes. Surveys are great if
you want to know what people think about X, rather than what is true about X.
Knowing what people think is often the important question. It can be good for
legitimizing a view, or letting a group of people have common knowledge about
what they think so they can start to act on it, including getting out of bad

equilibria where everyone nominally supports claim P because they think others
will judge them if not.
11. If you are surveying people with the intention of claiming a thing, it is helpful to
think ahead about what you want to claim, and make sure you ask questions
that will let you claim that, in a simple way. For instance, it is better to be able to
say '80% of a random sample of shoppers at Tesco said that they like tomato
more than beans' than to say '80% of a sample of shoppers who were mostly at
Tesco but also at Aldi (see footnote for complicated shopper selection process)
say that they prefer tomato to peas, or (using a separate subset of shoppers)
prefer peas to beans, from which we can infer that probably about 80% of
shoppers in general, or more, prefer tomato to beans'. You want to be able to
describe the setup and question in a way that is simple enough that the listener
understands what happened, and see the signiﬁcance of the ﬁnding.
12. If you are running a survey multiple times, and you want informative answers
about whether there were diﬀerences in views between those times, you should
probably run exactly the same survey and not change the questions even a tiny
bit unless there is very strong reason to. This follows from 3.
13. Qualtrics costs thousands of dollars to use, and won't let you sign up for an
account or even know how much it might cost unless you book a meeting to talk
to someone to sell it to you. Guidedtrack.com seems pretty nice, but I might not
have been trying to do such complicated things there.
14. Running surveys seems underrated as an activity.

What's General-Purpose Search, And
Why Might We Expect To See It In
Trained ML Systems?
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Benito has an interesting job. Here's some of the stuﬀ he's had to do over the past
couple years:
build a prototype of an oﬃce
resolve neighbor complaints at a party
ﬁnd housing for 13 people with 2 days notice
ﬁgure out an invite list for 100+ people for an oﬃce
deal with people emailing a funder trying to get him defunded
set moderation policies for LessWrong
write public explanations of grantmaking decisions
organize weekly online zoom events
ship books internationally by Christmas
moderate online debates
do April Fools' Jokes on Lesswrong
ﬁgure out which of 100s of applicants to do trial hires with
Quite a wide variety!
Benito illustrates an interesting feature of humans: you can give humans pretty
arbitrary goals, pretty arbitrary jobs to do, pretty arbitrary problems to solve, and
they'll go ﬁgure out how to do it. It seems like humans have some sort of "general-
purpose problem-solving" capability.
Now, there's more than one part of general-purpose problem solving. There's eﬃcient
information-gathering and model-building and updating. There's searching for
promising plans. There's execution (or, in the organizational context, operations). A
general-purpose problem-solver needs general-purpose versions of all those. But for
this post, I want to focus on the "searching for promising plans" part.
First things ﬁrst: what is this "search" thing, anyway?
Babble And Prune Is Not The Only Search
Method
This whole post started out because I was talking about "search" (in the context of an
inner alignment strategy) and it turned out that people had radically diﬀerent pictures
of what the word "search" means. In particular, it turned out that a bunch of people
pessimistic about the strategy were picturing some variant of babble and prune:
"babble" candidate solutions, then "prune" unpromising solutions, and hopefully
iterate toward better and better solutions.

This is not really how humans search for promising plans. Consider, for example, a
human planning a trip to the grocery store. Typical reasoning (mostly at the
subconscious level) might involve steps like:
There's a dozen diﬀerent stores in diﬀerent places, so I can probably ﬁnd one
nearby wherever I happen to be; I don't need to worry about picking a location
early in the planning process.
My calendar is tight, so I need to pick an open time. That restricts my options a
lot, so I should worry about that early in the planning process.
<go look at calendar>
Once I've picked an open time in my calendar, I should pick a grocery store
nearby whatever I'm doing before/after that time.
... Oh, but I also need to go home immediately after, to put any frozen things in
the freezer. So I should pick a time when I'll be going home after, probably toward
the end of the day.
Notice that this sort of reasoning mostly does not involve babbling and pruning entire
plans. The human is thinking mostly at the level of constraints (and associated
heuristics) which rule out broad swaths of plan-space. The calendar is a taut constraint,
location is a slack constraint, so (heuristic) ﬁrst ﬁnd a convenient time and then pick
whichever store is closest to wherever I'll be before/after. The reasoning only deals with
a few abstract plan-features (i.e. time, place) and ignores lots of details (i.e. exact
route, space in the car's trunk); more detail can be ﬁlled out later, so long as we've
planned the "important" parts. And rather than "iterate" by looking at many plans, the
search process mostly "iterates" by considering subproblems (like e.g. ﬁnding an open
calendar slot) or adding lower-level constraints to a higher-level plan (like e.g. needing
to get frozen goods home quickly).
So humans' general-purpose internal search mostly doesn't look like babble and prune
in the classic sense. It's mostly operating on things like constraints and heuristics,
abstraction, and subproblems. (There may be some babbling and pruning in there, but
it's not doing most of the algorithmic eﬃciency work, and we're not directly babbling
and pruning whole plans.)
In fact, even classic path search algorithms (like e.g. A* search) don't really resemble
pure babble and prune on closer inspection. When using a classic path-search
algorithm to e.g. ﬁnd a route from LA to to Seattle, we don't actually babble lots of
possible LA-Seattle routes and evaluate them. Rather, we come up with solutions to
lots of subproblems: routes between LA and various possible intermediate points (like
San Francisco or Sacramento or Portland). And in the case of A* search, we use
heuristics (generated by constraint relaxation) to decide which subproblems to pay
attention to.
So what is search, if not (as Ivan Vendrov recently put it) "enumerating possible actions
and evaluating their consequences"? Well, I'd say that a "general-purpose search"
process is something which:
Takes in a problem or goal speciﬁcation (from a fairly broad range of possible
problems/goals)
... and returns a plan which solves the problem or scores well on the goal
The "evaluation of consequences" thing is relevant, but it's not really about the
internal operations of the search process. Rather, "evaluation of consequences" is the
deﬁning feature of whether a search process is doing its job correctly: to tell whether
the search process is working, use the problem/goal speciﬁcation to evaluate the

consequences of the plan generated, and see whether the plan solves the problem or
scores well on the goal.
Note that "general-purpose search" still does not include all the pieces of general-
purpose problem solving; there's still things like gathering information, building and
updating models, execution of plans, or recognizing when plans need to be updated.
But it does include the planning part.
Also note that a general-purpose search process need not solve all possible problems,
or even all compactly-speciﬁable problems; that would be NP-Hard (almost by
deﬁnition). It does need to handle a broad range of problems, ideally in some realistic
environment.
Retargetability and Recursive Structure of Search
One key feature of a general-purpose search process is that it's retargetable: because
it can take in any problem or goal speciﬁcation from a fairly wide class, we can retarget
it by passing in a diﬀerent problem/goal. For example, a path-ﬁnding algorithm (like A*)
can take any start and end point in a graph.
Retargetability plays well with the recursive structure of search. When ﬁnding a route
from LA to Seattle, for instance, we look for routes from LA to Sacramento or San
Francisco. Those subproblems are themselves search problems, and can themselves be
solved with the same path-ﬁnding method: just pass LA and Sacramento as the start
and end points, rather than LA and Seattle. More generally, with a retargetable search
process, we can recursively call the same general-purpose search process with
diﬀerent inputs in order to handle subproblems.
Later on, when we talk about why one might expect general-purpose search to
eventually show up in trained ML systems, that sort of recursion will be one big piece.
Heuristics and Generality
In practice, in order to search eﬃciently, we tend to need some kind of heuristics. In
physical-world pathﬁnding, for instance, a useful heuristic is to try to reduce Euclidean
distance to the goal. However, that heuristic isn't useful for all problems; Euclidean
distance isn't very useful as an heuristic for e.g. moderating online debates.
If we need heuristics for eﬃcient search, and heuristics are specialized, how is general-
purpose problem-solving a thing? Do we need to learn a whole new set of heuristics
every time our task changes at all? Obviously not, in practice; most of the things on
Ben's list were things he only did once, but he nonetheless did a decent job (I know this
because I'm one of his users). But then how do we achieve generality while relying on
heuristics? Two main paths:
There exist general-purpose methods for generating heuristics
Heuristics tend to depend on the environment, but not on the exact objective
General-Purpose Generators Of Heuristics Are A Thing
Probably the best-understood heuristic-generator is problem relaxation.

Here's how it works. Let's say I'm planning a trip to the grocery store. My planning
problem has a whole bunch of constraints: I need to have enough time, I need to be at
the right place, can't let the frozen goods melt, the car must have enough gas, etc, etc.
As a ﬁrst pass, I ignore (a.k.. "relax") most of those constraints, and only pay attention
to a few - like e.g. having enough time. That narrows down my solution space a lot:
there's only a few spaces in my calendar with enough time. I pick a time slot. Then, I
start to add other constraints back in.
The "relaxed" problem, in which I only worry about the time constraint, acts as an
heuristic for the full problem. It helps me narrow down the search space early on, and
focus on plans which at least satisfy that one constraint. And because it only involves
one constraint, it's relatively easy to solve the relaxed problem.
More generally, the steps of problem relaxation are roughly:
Start with a problem with a bunch of constraints
Relax (i.e. ignore) a bunch of the constraints
Solve the relaxed problem
Use the relaxed problem-solution as an heuristic for the full problem
Another example, this time from path search: if we're solving a maze, we can relax the
problem by ignoring all the walls. Then the shortest path is easy: it's a straight line
from the start to the end, and its length is Euclidean distance. We can then use
Euclidean distance as an heuristic while exploring the maze: preferentially explore in
directions which have shorter Euclidean distance to the end. In other words,
preferentially explore directions for which the relaxed problem (i.e. ignoring all the
walls) has the shortest solutions.
Problem relaxation probably isn't the only heuristic generation method, but it's
relatively well-understood mathematically, and it's an existence proof. It shows that
general-purpose generators of heuristics are a thing. We should expect to see such
heuristic-generators used inside of general-purpose search processes, in order to
achieve generality and eﬃciency simultaneously.
Heuristics Tend To Depend On The Environment, But Not On
The Objective
The previous section talked about two heuristics:
When planning a grocery trip, pick a time slot while ignoring everything else
When path-ﬁnding, prioritize shorter Euclidean distance to the end
Notice that both of these heuristics are very environment-dependent, but not very
goal-dependent. If my time is very scarce, then picking a time slot ﬁrst will be a good
heuristic for planning all sorts of things in my day-to-day life, from meetings to
vacations to workouts to a trip to the movies to some quiet reading. The heuristic does
depend on "the environment" - i.e. it would be less useful for someone whose time is
abundant - but it's relatively goal-agnostic.
Similarly, I can use Euclidean distance as a heuristic for ﬁnding road-trip routes
between a wide variety of start and end locations. There are environments in which it's
a terrible heuristic, but for road-trip planning it works decently for most start/end
points.

Here's a visual example which I love, an heuristic for path-ﬁnding in a maze from an old
post:

Maze-speciﬁc, but it's useful for a wide variety of start/end points.

The pattern: heuristics tend to be environment-dependent but relatively goal-agnostic.
Why would such a pattern apply?
One way to frame the answer: an environment typically includes a bunch of stable
constraints, shared across problems in that environment. In our universe, the laws of
physics are all constraints, human laws are all constraints, I can only move around so
fast and carry so much, I only have so much time and money, my interfaces with other
people/things only have certain knobs to turn, etc. And instrumental convergence
means that it will very often be the same few constraints which are rate-limiting. So,
insofar as we generate heuristics via constraint relaxation, we'll get environment-
speciﬁc but reasonably goal-agnostic heuristics.
Another way to frame the answer: natural abstraction. Most far-apart chunks of the
world only interact via relatively low-dimensional summaries; the rest of their
interaction is wiped out by noise. So, optimizing those low-dimensional summaries will
be a common heuristic across a wide range of goals within the same environment.
Side Note: Cached Solutions Are Not Heuristics (But Are
Another General-Purpose Search Trick)
Another general-purpose search trick which someone will probably bring up if I don't
mention it is caching solutions to common subproblems. I don't think of this as an
heuristic; it mostly doesn't steer the search process, just speed it up. But it is a useful
and common trick for improving eﬃciency. And we should expect it to speed up search
over a wide variety of goals within the same environment, insofar as instrumental
convergence applies in that environment. Instrumentally convergent subproblems
come up over and over again for a wide variety of problems, so caching solutions to
those subproblems is a general-purpose search accelerator.
Revisiting The Risks From Learned
Optimization Arguments
We've now talked about how general-purpose search Is A Thing. We've talked about
how general-purpose heuristics (and other general-purpose search tricks, like caching)
Are A Thing. And we've observed that these things show up in humans. But why do
they show up in humans? And should we expect them to show up in ML, or in intelligent
aliens, or in other evolved/trained/selected agenty systems?
Key Idea: Compression Is Favored By Default
In general, evolved/trained/selected systems favor more compact
policies/models/heuristics/algorithms/etc. In ML, for instance, the fewer parameters
needed to implement the policy, the more parameters are free to vary, and therefore
the more parameter-space-volume the policy takes up and the more likely it is to be
found. (This is also the main argument for why overparameterized ML systems are able
to generalize at all.)
The outer training loop doesn't just select for high reward, it also implicitly selects for
compactness. We expect it to ﬁnd, not just policies which achieve high reward, but
policies which are very compactly represented.

At the same time, assuming the system encounters a wide variety of problems in its
training environment, it needs generality in order to perform well. But compactness
means that, when possible, it will favor generality using a smaller number of more
general pieces rather than a larger number of more specialized pieces.
So things like general-purpose heuristics, general-purpose heuristic generators, and
general-purpose search are exactly the sort of things these systems should favor
(assuming the architecture is expressive enough, and the environment varies enough).
That's basically the argument for inner agents from Risks From Learned Optimization:
In some tasks, good performance requires a very complex policy. At the same time,
base optimizers are generally biased in favor of selecting learned algorithms with
lower complexity. Thus, all else being equal, the base optimizer will generally be
incentivized to look for a highly compressed policy.
One way to ﬁnd a compressed policy is to search for one that is able to use general
features of the task structure to produce good behavior, rather than simply
memorizing the correct output for each input. A mesa-optimizer is an example of
such a policy. From the perspective of the base optimizer, a mesa-optimizer is a
highly-compressed version of whatever policy it ends up implementing: instead of
explicitly encoding the details of that policy in the learned algorithm, the base
optimizer simply needs to encode how to search for such a policy. Furthermore, if a
mesa-optimizer can determine the important features of its environment at
runtime, it does not need to be given as much prior information as to what those
important features are, and can thus be much simpler.
On top of all that, the recursive nature of search - the fact that recursively searching on
subproblems is useful as a search technique - favors a retargetable general-purpose
search process, as opposed to a hardcoded optimizer.
Takeaways
It seems like a lot of people have a picture of "search" as babble and prune. They
correctly notice how ineﬃcient babble and prune usually is, and conclude that it
probably won't be selected for in trained ML systems.
But there's a more general notion of general-purpose search. It's a process which takes
in any problem or goal from a wide class, and returns a plan which solves the problem
or achieves the goal. That's the sort of search we expect to see show up in trained
systems. The key property is that it's retargetable: the search process can take in a
wide variety of goals. That retargetability is selected for due to the recursive structure
of search: recursively running search on subproblems is a widely-useful technique, and
that technique can be encoded much more compactly with a retargetable search
process on hand.
In order for that search to run eﬃciently, while still maintaining generality and
compactness, it will probably need to either generate heuristics in a general way, or
leverage goal-agnostic (but possibly environment-speciﬁc) heuristics. Both of those are
plausible options: problem relaxation is an existence proof of general-purpose
generators of heuristics, and the shared constraints of an environment (plus natural
abstraction and/or instrumental convergence) oﬀer a source of goal-agnostic heuristics.
Some strategic upshots of all this:

Possibility of rapid general capability gain if/when a system groks general-
purpose search, especially if many general-purpose heuristics are learned ﬁrst
Retargeting the search process as an (inner) alignment strategy
Relatively high chance of agent-like internal structure, like e.g. a reusable
general-purpose search module which takes in an explicit objective

The lessons of Xanadu
This is a linkpost for https://jasoncrawford.org/the-lessons-of-xanadu
One of my all-time favorite articles is "The Curse of Xanadu," by Gary Wolf, which ran
in WIRED Magazine in 1995. On the surface, it's a piece of tech history, a story of a
dramatic failure. But look closer, and you can ﬁnd deep philosophical insight.
Xanadu was a grand vision of a hypertext system, conceived long before the Web,
that at the time of this article had been "under development" for three decades
without launching. The visionary behind it was Ted Nelson, one of the originators of
the concept of hypertext. Here's how the article describes him and the project:
Nelson's life is so full of unﬁnished projects that it might fairly be said to be built
from them, much as lace is built from holes or Philip Johnson's glass house from
windows. He has written an unﬁnished autobiography and produced an unﬁnished
ﬁlm. His houseboat in the San Francisco Bay is full of incomplete notes and
unsigned letters. He founded a video-editing business, but has not yet seen it
through to proﬁtability. He has been at work on an overarching philosophy of
everything called General Schematics, but the text remains in thousands of
pieces, scattered on sheets of paper, ﬁle cards, and sticky notes.
All the children of Nelson's imagination do not have equal stature. Each is derived
from the one, great, unﬁnished project for which he has ﬁnally achieved the fame
he has pursued since his boyhood. During one of our many conversations, Nelson
explained that he never succeeded as a ﬁlmmaker or businessman because "the
ﬁrst step to anything I ever wanted to do was Xanadu."
Xanadu, a global hypertext publishing system, is the longest-running vaporware
story in the history of the computer industry. It has been in development for more
than 30 years. This long gestation period may not put it in the same category as
the Great Wall of China, which was under construction for most of the 16th
century and still failed to foil invaders, but, given the relative youth of commercial
computing, Xanadu has set a record of futility that will be diﬃcult for other
companies to surpass.
The project had many of the earmarks of other failed or long-overdue eﬀorts. As a
product, it was over-designed:
Xanadu was meant to be a universal library, a worldwide hypertext publishing
tool, a system to resolve copyright disputes, and a meritocratic forum for
discussion and debate. By putting all information within reach of all people,
Xanadu was meant to eliminate scientiﬁc ignorance and cure political
misunderstandings. And, on the very hackerish assumption that global
catastrophes are caused by ignorance, stupidity, and communication failures,
Xanadu was supposed to save the world.
In contrast to the later Web, links in Xanadu did not point to entire documents, but to
any arbitrary range of characters within any document. Links were to be bi-directional,
so they could not be broken. And there was an advanced feature in which "parts of
documents could be quoted in other documents without copying":

The idea of quoting without copying was called transclusion, and it was the heart
of Xanadu's most innovative commercial feature—a royalty and copyright scheme.
Whenever an author wished to quote, he or she would use transclusion to
"virtually include" the passage in his or her own document....
The key to the Xanadu copyright and royalty scheme was that literal copying was
forbidden in the Xanadu system. When a user wanted to quote a portion of
document, that portion was transcluded. With fee for every reading.
Transclusion was extremely challenging to the programmers, for it meant that
there could be no redundancy in the grand Xanadu library. Every text could exist
only as an original. Every user in the world would have to have instant access to
the same underlying collection of documents.
The vision for the application of this technology was nothing short of utopian, based
on delusions of technological solutions to social and epistemic problems:
... the Xanadu architects became obsessed with developing the widest possible
applications of hypertext technology. A universal democratic library, they decided,
was only the beginning. Xanadu could also provide a tool for rational discussion
and decision making among very large groups. In the Xanadu docuverse, an
assertion could always be followed back to its original source. An idea would never
become detached from its author. Public discussion on important issues would
move forward logically, rather than merely swirling ineﬀectively through eddies of
rhetoric. In fact, any reader could, by creating and following links, freeze the
chaotic ﬂow of knowledge and grasp the lines of connection and inﬂuence.
The design also blithely ignored the realities of computer performance, developing as
they were on minicomputers and early workstations:
The Onyx also had 128 Kbytes of RAM, which they later doubled to a screaming
256 Kbytes. Looking back at the speciﬁcs of the endeavor, the approach of the
Xanadu programmers seems quixotic. [Xanadu collaborator Roger] Gregory and
his colleagues were trying to build a universal library on machines that could
barely manage to edit and search a book's worth of text.
The project suﬀered from inﬁghting and a lack of good management:
"It was not rapid prototyping—it was rabid prototyping," said one of [Xanadu
programmer Michael] McClary's friends who watched the project closely.
There was never a realistic schedule: the team perpetually believed they were six
months away from completion. The project was so badly conceived and managed that
it couldn't ship even after being acquired by Autodesk and given a full budget:
[Autodesk founder] John Walker, Xanadu's most powerful protector, later wrote
that during the Autodesk years, the Xanadu team had "hyper-warped into the
techno-hubris zone." Walker marveled at the programmers' apparent belief that
they could create "in its entirety, a system that can store all the information in
every form, present and future, for quadrillions of individuals over billions of
years." Rather than push their product into the marketplace quickly, where it
could compete, adapt, or die, the Xanadu programmers intended to produce their
revolution ab initio.

"When this process fails," wrote Walker in his collection of documents from and
about Autodesk, "and it always does, that doesn't seem to weaken the belief in a
design process which, in reality, is as bogus as astrology. It's always a bad
manager, problems with tools, etc.—precisely the unpredictable factors which
make a priori design impossible in the ﬁrst place."
There are too many good quotes in the article to include them all here—read the
whole thing.
What struck me most deeply, however, was the response of some of the Xanadu team
to the rise of the World Wide Web. You would think that the web would be an object
lesson for them—a slap in the face hard enough to wake them from their pie-in-the-
sky reverie and bring them back to Earth.
Indeed, one junior programmer on the later team, Rob Jellinghaus—who was born after
the Xanadu project had begun (!)—did have such an awakening:
While the Xanaduers paid lip service to libertarian ideals, they imagined a more
traditional revolution in which all users would be linked to a single, large, utopian
system. But in their quest for a 21st-century model, they created a Byzantine
maze.
"There were links, you could do versions, you could compare versions, all that was
true," Jellinghaus reports, "provided you were a rocket scientist. I mean, just the
code to get a piece of text out of the Xanadu back end was something like 20 lines
of very, very hairy C++, and it was not easy to use in any sense of the word. Not
only was it not easy to use, it wasn't anything even remotely resembling fast. The
more I worked at it, the more pessimistic I got."
The young programmer's doubts were magniﬁed by his dawning realization that a
grand, centralized system was no longer the solution to anything. He had grown
up with the Internet—a redundant, ever-multiplying and increasingly chaotic mass
of documents. He had observed that users wanted and needed ever more clever
interfaces to deal with the wealth of information, but they showed little inclination
to obey the dictates of a single company....
Although he sympathized with the fanaticism of his colleagues, Jellinghaus also
began to question whether a hypertext revolution required the perfect
preservation of all knowledge. He saw the beauty of the Xanadu dream—"How do
you codify all the information in the world in a way that is inﬁnitely scalable?"—
but he suspected that human society might not beneﬁt from a perfect
technological memory. Thinking is based on selection and weeding out;
remembering everything is strangely similar to forgetting everything. "Maybe
most things that people do shouldn't be remembered," Jellinghaus says. "Maybe
forgetting is good." ...
After a couple of months, he began to come to his senses. "What was I doing?" he
remembers saying to himself. "This is silly. This was silly all along."
But here was the reaction of Mark Miller, one of the original developers:
I asked Miller if the Internet was accomplishing his dreams for hypertext. "What
the Web is doing is easy," Miller answered. He pointed out that the Web still lacks
nearly every one of the advanced features he and his colleagues were trying to

realize. There is no transclusion. There is no way to create links inside other
writers' documents. There is no way to follow all the references to a speciﬁc
document. Most importantly, the World Wide Web is no friend to logic. Rather, it
permits inﬁnite redundancy and encourages maximum confusion. With Xanadu—
that is, with tranclusion and freedom to link—users would have had a consistent,
easily navigable forum for universal debate.
"This is really hard," Miller said.
And what about Nelson himself?
Nelson's response to the Web was "nice try." He said it is a trivial simpliﬁcation of
his hypertext ideas, though cleverly implemented. And he has not entirely given
up hope for the old Xanadu code. "I'd like to stress that everyone involved in
Xanadu believes that the software is valid and can be ﬁnished," he asserted.
"It will be ﬁnished," Nelson added. "The only question is which decade."
Miller, Nelson, and the rest of the Xanadu team might have beneﬁtted from reading
another one of my all-time favorite articles: Clay Shirky's "In Praise of Evolvable
Systems".
Shirky begins by pointing out several ways in which the fundamental standards of web
technology have seemingly absurd limitations and ineﬃciencies: HTTP doesn't use
persistent connections and incurs the entire overhead of a new session for each ﬁle
transferred, web servers have no built-in load balancing, HTML uses one-directional
hypertext links that are easily broken, etc.:
HTTP and HTML are the Whoopee Cushion and Joy Buzzer of Internet protocols,
only comprehensible as elaborate practical jokes. For anyone who has tried to
accomplish anything serious on the Web, it's pretty obvious that of the various
implementations of a worldwide hypertext protocol, we have the worst one
possible.
Except, of course, for all the others....
The problem with that list of deﬁciencies is that it is also a list of necessities—the
Web has ﬂourished in a way that no other networking protocol has except e-mail,
not despite many of these qualities but because of them. The very weaknesses
that make the Web so infuriating to serious practitioners also make it possible in
the ﬁrst place. In fact, had the Web been a strong and well-designed entity from
its inception, it would have gone nowhere.
Contrasting the "evolvable system" of the web with centrally designed protocols such
as Gopher and WAIS, he concludes:
Centrally designed protocols start out strong and improve logarithmically.
Evolvable protocols start out weak and improve exponentially. It's dinosaurs vs.
mammals, and the mammals win every time.
The Xanadu project still exists. I was able to quickly learn its current status, because it
has a homepage on a global hypertext-based information system: https://xanadu.com.

"With ideas which are still radical, WE FIGHT ON. We hope for vindication, the last
laugh, and recognition as an additional standard..." It complains that "everyone is
hypnotized by the Web browser," which is "basically crippled."
The project is no longer entirely vaporware. There are two demo viewers of Xanadocs:
"XanaduSpace is our best-looking viewer, our ﬂagship demo—but alas, it's a stuck
demo and can't go further." The "new working viewer", xanaviewer3, makes it
"possible (but not easy) for anyone who is determined enough to create a xanadoc,
and send it to others, who may open and use it." One supposes that, in order to view
it, the recipients of the document must be equally determined.
"With our limited resources," they explain, "we can only go slowly, unlike today's Red
Bull-fueled young teams."
The WIRED article describing Xanadu as running for over 30 years is now 27 years old,
meaning Xanadu itself is nearing 60. If its "record of futility" was diﬃcult to surpass
back then, it is doubly so now.
The lessons of Xanadu can be learned at multiple levels.
On one level, the lesson is to scope projects realistically and to strive for simplicity of
design.
On a deeper level, the lesson is to ship continually. Doing so keeps the schedule
honest, forces diﬃcult scope decisions, and allows for feedback from real users.
On a still deeper level, the lesson is to learn from failure, which the vast majority of
the Xanadu team does not appear to have done: thirty years of missed deadlines did
not cause them to fundamentally question their schedule, project management, or
design scope.
But the deepest lesson, I think, is to value real-world results. Nelson and Miller didn't
fail to notice the Web, they failed to care about its success or even to recognize it as a
success. Its epic, world-changing status in the history of technology is meaningless to
them beside the fantasy system they had dreamed up.
In the end, despite the title of the WIRED article, Xanadu was not, in fact, cursed. It
achieved exactly what its originators wanted: theoretical perfection in a Platonic realm
of forms so idealized that it can never quite be brought to Earth.

Externalized reasoning oversight: a
research direction for language model
alignment
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Summary
In this post, I'll present a research direction that I'm interested in for alignment of
pretrained language models.
TL;DR: Force a language model to think out loud, and use the reasoning itself
as a channel for oversight. If this agenda is successful, it could defeat
deception, power-seeking, and other forms of disapproved reasoning. This
direction is broadly actionable now.
In recent publications, prompting pretrained language models to work through logical
reasoning problems step-by-step has provided a boost to their capabilities. I claim that
this externalized reasoning process may be used for alignment if three conditions are
met:
1. We develop trustworthy tests to check if the reported reasoning is authentic
(complete, straightforward, and causally responsible for the output).
2. We train models that pass the tests of reasoning authenticity.
3. We oversee the externalized reasoning to detect reasoning of which we would
not approve, and either ﬁlter the resulting actions or ﬁne-tune against reasoning
approval.
If these conditions hold, we should be able to detect and avoid models that reason
through convergent instrumental goals to be deceptive, power-seeking, non-myopic,
or reason through other processes of which we don't approve. Reasoning oversight
should provide stronger guarantees of alignment than oversight on model outputs
alone, since we would get insight into the causally responsible reasoning process that
gave rise to a certain output.
I believe that productive research can be done today toward all three of these
conditions.
I'm doing exploratory work into this strategy as part of my SERI MATS project in the
large language model alignment stream.
 
Background
I'll use "externalized reasoning" to refer to a prompting strategy for a pretrained
language model that has it read in a question, produce a number of steps of

reasoning, and then produce an answer to the question as its ﬁnal output. Some
examples of this include chain of thought prompting [1, 2], selection-inference [3],
and scratchpad reasoning [4]. These types of strategies have recently led to state of
the art results on a number of logical reasoning benchmark tasks, most notably 50.3%
accuracy on the MATH dataset [5].
A few examples of chain of thought reasoning can be found here.
Future work may develop externalized reasoning approaches diﬀerent from those that
currently exist; ideally, the research directions described here will be able to
generalize to suﬃciently similar approaches.
 
Research directions
First condition: assess reasoning authenticity
My main focus right now is in developing tests that can be used to assure that the
model's reasoning is authentic. What "authenticity" means is currently vaguely
deﬁned, but fundamentally I want it to encompass whatever is needed to trust that
disapproved reasoning couldn't go undetected. Some aspects of this might include:
Causal responsibility: The displayed reasoning isn't post-hoc, but is causally
responsible for the conclusion that the model came to.
Completeness: No steps of reasoning are missing, given the level of detail at
which the reasoning is presented (which should be an appropriate level for
oversight).
Straightforwardness: Hidden messages aren't steganographically encoded in the
reasoning.
It's useful to note that we have read and write access to the externalized reasoning
process, which is useful for creating tests that probe causal responsibility.
I've written up my current test ideas in another document, but I'd like to avoid that
document being crawled by a web scraper which adds it to a language model's
training corpus. Accordingly I've put together a poor man's CAPTCHA which you'll
have to complete to access it (sorry). Please remove the text "-REMOVE" when you
visit the link below:
https://hackmd.io/@tamera/HJ7iu0ST5-REMOVE
The tests I've developed so far only provide qualitative evidence about the
authenticity of a model's reasoning, and even all the tests taken together are far from
foolproof. Developing better tests is one of the major goals of this research direction;
so far I've only taken a very ﬁrst stab at it. Real success here will likely involve
grappling with diﬃcult questions around what it means for a model to be "actually
using" its externalized reasoning, or for a given step of reasoning to be "necessary",
which I have hardly even touched. The process to develop more complete tests may
look similar to the builder / breaker methodology described by ARC in the ELK report.
Second condition: create authentic reasoners

It's not enough to have tests that can conﬁrm if a model is doing authentic reasoning;
we may also need to create a model that passes the tests. Designing model
architectures and training processes that we think are likely to result in authentic
reasoners also provides a kind of defense in depth, alongside the tests of authentic
reasoning.
I'll also note that it's important that the authentic reasoner models we create be
reasonably competitive with those developed by mainstream capabilities, so that they
can constitute a safe alternative.
The most straightforward way we might make a model more dependent on
externalized reasoning is to give it fewer parameters, possibly by giving it fewer layers
of depth. This way, its capacity to use serial compute internally is reduced, and it may
be more dependent on the pseudo-serial compute provided by its externalized
reasoning. If there are concerns that a sophisticated model could anticipate and
thereby evade our reasoning authenticity tests, it's possible that using a shallower
model could assuage that fear. Ideally, the pseudo-serial depth provided by the chain
of thought could make up for the hit to performance that would come from reducing
the model's depth, but if that isn't the case then this approach alone wouldn't be
capabilities competitive.
Another idea comes from a modiﬁcation of the approach in "Censoring
Representations with an Adversary"[7], where the adversary would use the model's
internal states to try to derive the answer to the question while the model itself is still
in the middle of reasoning. Optimizing the model against the adversary's success
should reduce the extent to which the ﬁnal answer is represented in the model's
internal states, making it more dependent upon the externalized reasoning. For many
reasons this is far from a complete solution, including concerns about the distribution
that the model is trained on.
I've hardly explored this direction at all; the strategies above are only meant to
illustrate that it doesn't seem impossible a priori to develop architectures or training
processes that would likely create authentic reasoners. I anticipate that further
thought should quickly yield better strategies than those listed.
Third condition: oversee the reasoning
Conditional on the ﬁrst two goals being accomplished (assessing reasoning
authenticity and creating authentic reasoners), there are at least two techniques that
could be used to reduce the chance of deploying unaligned AI: ﬁltering a model's
outputs in deployment based on the acceptability of its reasoning, and ﬁne-tuning the
model to reason acceptably.
For a model in deployment, we could use automated oversight tools that scan its
reasoning process for elements we don't approve of, like reasoning that relates to
convergent instrumental subgoals, deception, or that is non-myopic. These tools could
be simpler than the system they're overseeing, and be run in real time against the
model's reasoning before any actions are taken based on the output. If we didn't
approve of the reasoning, we could prevent the action from being taken and raise an
alarm instead.
It's possible that our default training strategy for models would often result in
reasoning that we didn't approve of. In this case, we could ﬁne-tune the model using
RLHF on human approval of the reasoning process itself, and in this way train the

model to reason in ways that we approve of. This second approach might start to look
something like optimizing against our interpretability technique, which could push the
model to obfuscate its reasoning. If we take this path we should be especially wary of
steganographic messages being encoded in the reasoning.
 
Advantages of this approach
To me, the major advantage of this research direction comes from the fact that
success would enable us to express approval or disapproval over the model's actual
reasoning process.
I think this direction is particularly exciting right now for other reasons as well:
Externalized reasoning can be studied in current models, even relatively small
ones.
Tests of reasoning authenticity may only require inference access to a model,
which is often easier to get than weights and activations.
If a model fails these tests, it may constitute a "spooky demo" that's legible to
the wider machine learning research community.
Models for which externalized reasoning is essential may be smaller and cheaper
to train than cutting edge large language models.
This research direction also suggests a number of projects that can be worked on right
now, in parallel with one another. Some of these include:
Developing and implementing better reasoning authenticity tests.
Training models that constitute adversarial examples for authenticity tests, like
models that encode steganographic messages in their reasoning.
Developing and testing strategies to train authentic reasoners.
Developing tools to automate the oversight of externalized reasoning.
De-confusing concepts related to authenticity, reasoning approval, and
oversight.
 
Shortcomings of this approach
Here are some reasons that this research direction might fail:
The ﬁrst or second conditions may be impossible to
achieve.
We might not be able to even deﬁne what we mean by "authenticity" to our own
satisfaction.
Even with a sound deﬁnition, we might not be able to develop empirical tests to
detect it.
Even with a solid deﬁnition and good tests, we might not be able to train models
that pass the tests.

The third condition may be impossible to achieve
It might not be possible to oversee the model's reasoning process to our
satisfaction.
It may not be possible to do so with tools that are simple enough for us to trust.
Reinforcement learning on the acceptability of the reasoning process may cause
the reasoning to no longer be authentic.
Other problems may arise
We may fool ourselves into thinking that we've developed sound deﬁnitions,
tests, and oversight tools, but there are actually failure modes that we've
missed which are exploited by the model.
It might not be possible to ﬁnd a level of reasoning granularity that both allows
the model to reason well and enables reasoning oversight.
The externalized reasoning approach may not work well outside of logical
reasoning problems.
Systems developed this way may not be suﬃciently competitive with mainline
capabilities to constitute a safe alternative.
 
Final words
This research direction is still just a sketch with many details left to be ﬁlled in; I look
forward to seeing it developed further. I invite anyone who is interested in it or in
similar directions to collaborate with one another and with me.
 
Thanks
I'd like to thank Alex Gray, William Saunders, Kyle McDonell, Evan Hubinger, Leo Gao,
and Owain Evans for conversations relating to this work.
I also extend exceptional thanks to my illustrious ﬂatmates in AI House for their
insight, advice, and consistent good humor: Ian McKenzie, Peter Barnett, Vivek
Hebbar, and Thomas Kwa.
 
Citations
1. Chain of Thought Prompting Elicits Reasoning in Large Language Models
2. Large Language Models are Zero-Shot Reasoners
3. Selection-Inference: Exploiting Large Language Models for Interpretable Logical
Reasoning
4. Show Your Work: Scratchpads for Intermediate Computation with Language
Models
5. Solving Quantitative Reasoning Problems with Language Models

6. STaR: Bootstrapping Reasoning With Reasoning
7. Censoring Representations with an Adversary

The alignment problem from a deep
learning perspective
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
This report ( now available on arxiv ) is intended as a concise introduction to the
alignment problem for people familiar with machine learning. It translates previous
arguments about misalignment into the context of deep learning by walking through
an illustrative AGI training process (a framing drawn from an earlier report by Ajeya
Cotra ), and outlines possible research directions for addressing diﬀerent facets of the
problem.
Within the coming decades, artiﬁcial general intelligence (AGI) may surpass human
capabilities at a wide range of important tasks. Without substantial action to prevent
it, AGIs will likely use their intelligence to pursue goals which are very undesirable (in
other words, misaligned) from a human perspective. This report aims to cover the key
arguments for this claim in a way that's as succinct, concrete and technically-
grounded as possible. My core claims are that:
1. It's worth thinking about risks from AGI in advance
2. Realistic training processes lead to the development of misaligned goals, in
particular because neural networks trained via reinforcement learning will
1. Learn to plan towards achieving a range of goals
2. Gain more reward by deceptively pursuing misaligned goals
3. Generalize in ways which undermine obedience
3. More people should pursue research directions which address these problems
It's worth thinking about risks from
AGI in advance
By AGI I mean an artiﬁcial agent which applies domain-general cognitive skills (such
as reasoning, memory, and planning) to perform at or above human level on a wide
range of cognitive tasks (such as running a company, writing a software program, or
formulating a new scientiﬁc theory).[1] This isn't a precise deﬁnition—but it's common
in science for important concepts to start oﬀ vague, and become clearer over time
(e.g. "energy" in 17th-century physics; "ﬁtness" in early-19th-century biology;
"computation" in early-20th-century mathematics). Analogously, "general
intelligence" is a suﬃciently important driver of humanity's success to be worth taking
seriously even if we don't yet have good ways to formalize or measure it.[2]
On the metrics which we can track, though, machine learning has made signiﬁcant
advances, especially over the last decade. Some which are particularly relevant to AGI
include few-shot learning (and advances in sample eﬃciency more generally), cross-
task generalization, and multi-step reasoning. While hindsight bias makes it easy to
see these achievements as part of a natural progression, I suspect that even a decade
ago the vast majority of machine learning researchers would have been conﬁdent that
these capabilities were much further away.

I think it would be similarly overconﬁdent to conclude that AGI is too far away to
bother thinking about. A recent survey of top ML researchers gave a median estimate
of 2059 for the year in which AI will outperform humans at all tasks (although their
responses were sensitive to question phrasing) (Grace et al., 2022). This ﬁts with the
ﬁnding that, under reasonable projections of compute growth, we will be able to train
neural networks as large as the human brain in a matter of decades (Cotra, 2020). But
the capabilities of neural networks are currently advancing much faster than our
ability to understand how they work or interpret their cognition; if this trend
continues, we'll build AGIs which match human performance on many important tasks
without being able to robustly verify that they'll behave as intended. And given the
strong biological constraints on the size, speed, and architecture of human brains, it
seems very unlikely that humans are anywhere near an upper bound on general
intelligence.[3] The diﬀerences between our brains and those of chimpanzees are
small on an evolutionary scale (in particular, only a 3x size diﬀerence), but allow us to
vastly outthink them. Neural networks scale up 3x on a regular basis, and can rapidly
incorporate architectural and algorithmic improvements (including improvements
generated by AIs themselves). So soon after building human-level AGIs (and well
before we thoroughly understand them), we'll likely develop superhuman AGIs which
can vastly outthink us in turn.
These are strong claims, which it's reasonable to be uncertain about, especially given
that we lack either formal frameworks or empirical data which directly inform us about
AGI. However, empirical evidence about AGIs is hard to come by in advance of
actually building them. And our lack of formal frameworks for describing alignment
problems is a major reason to expect them to be diﬃcult to solve. So if the
development of AGI might pose catastrophic risks, we have no choice but to try to
address them in advance, even if that requires reasoning under signiﬁcant
uncertainty. Unfortunately, I think it does pose such risks—the most concerning of
which is the development of misaligned goals during training.
Realistic training processes lead to
the development of misaligned goals
By default, it seems likely to me that AGIs will end up pursuing goals which are
undesirable to us, rather than consistently following our intentions. Previous
presentations of arguments for this claim have mainly framed them as abstract
principles (explored in detail by Carlsmith (2021) and Ngo (2020)); in this report I'll
describe in more detail how I expect misaligned goals to emerge throughout the
process of training an AGI. For the sake of concreteness I'll focus this report on an
illustrative training process in which:
A single deep neural network with multiple output heads is trained end-to-end
With one head trained via self-supervised learning on large amounts of
multimodal data to predict the next observation
With another head trained to output actions via reinforcement learning on a
wide range of tasks, using standard language and computer interfaces
With rewards provided via a combination of human feedback and automated
evaluations
Until the policy implemented by the network (via its action head) is able to
match or exceed human performance on most of those tasks, and qualiﬁes as an
AGI.

Of course, any attempt to outline an AGI training process in advance will have many
omissions and inaccuracies. However, the illustrative process described above allows
us to make abstract arguments about alignment more concrete; and I expect that
training processes similar to this one would plausibly give rise to AGIs which pursue
misaligned goals with disastrous consequences. The rest of this report will outline how
misaligned goals might develop across three sequential phases of training:
1. Learning to plan towards achieving a range of goals
1. Policies will develop sophisticated internal representations of a range of
outcomes which are correlated with higher reward on multiple tasks, and
learn to make plans to achieve them. I'll call these internal representations
of favored outcomes the policy's goals.
2. Pursuing goals in a situationally-aware way
1. Once policies can reason about their training processes and deployment
contexts (an ability which I'll call situational awareness), they'll learn to
deceptively pursue misaligned goals while still getting high training
reward.
3. Generalizing goals beyond human supervision
1. Policies which are too capable for humans to eﬀectively supervise will
generalize towards taking actions which give them more power over the
world, rather than following human intentions.
It's particularly important to note that, under the deﬁnition above, "the goals of a
policy" is a diﬀerent concept from "the reward function used to train that policy"—
although a policy's goals will be shaped by its reward function, they'll ultimately
depend on the internal representations that policy learns.[4] This distinction, which I'll
explain in more detail in the next section, will become increasingly important as
policies learn goals that generalize to a wider range of novel environments.[5]
Also note that there are no sharp boundaries between these phases. However, I
expect each phase to feature emergent dynamics which weren't present in the
previous phases—as Steinhardt (2022) argues is common in ML (and science more
generally). As a very rough overview, phase 1 focuses on a version of the reward
misspeciﬁcation problem; phase 2 elaborates on that, and introduces the deceptive
alignment problem; and phase 3 focuses on the goal misgeneralization problem.
[6] Let's look at each phase now.
Phase 1: learning to plan towards achieving a
range of goals
Key claim: policies will develop sophisticated internal
representations of a range of outcomes which are
correlated with higher reward on multiple tasks, and learn
to make plans to achieve them.
Policies will learn to use representations of plans, outcome features, and
values to choose actions
Deep neural networks perform very capably on a wide range of tasks by learning
representations related to those tasks which are distributed across their internal

weights (Bengio et al., 2014). For example, neural networks trained on image
classiﬁcation tasks develop representations of various visual features, such as edges,
shapes, and objects, which are then used to identify the contents of those
images. Olah et al. (2020) provide compelling visualizations of these representations,
as well as representations of more complex features like wheels and dog heads in the
Inception network. Less work has been done on understanding the representations
learned by deep reinforcement learning policies, but one example comes from a policy
trained to play a version of Capture the Flag by Jaderberg et al. (2019), who identiﬁed
"particular neurons that code directly for some of the most important game states,
such as a neuron that activates when the agent's ﬂag is taken, or a neuron that
activates when an agent's teammate is holding a ﬂag".
How are representations like these used by reinforcement learning policies to choose
actions? In general we know little about this question, but I'll distinguish two salient
possibilities. The ﬁrst is that policies map representations of situations to
representations of actions, without making use of representations of the outcomes of
those actions; I'll call this approach following heuristics. The second is that policies
represent diﬀerent outcomes which might arise from possible actions, and then
choose actions by evaluating the values of possible outcomes; I'll call this pursuing
goals. Under the deﬁnition I'm using here, a policy's goals are the outcomes which it
robustly represents as having high value.
Importantly, these deﬁnitions are agnostic about whether actions, outcomes, and
outcome-values are explicitly represented in the code for the policy or implicitly
represented in policy weights and/or activations. For example, policies like
AlphaZero's use hard-coded search algorithms which manipulate explicit
representations of possible move sequences and board states, alongside neural
networks which have implicit representations of many human chess concepts.
However, neural networks trained only to produce actions can also learn to implicitly
make plans - a phenomenon known as model-free planning (Guez et al., 2019).
AlphaZero explicitly generates values for diﬀerent board positions when choosing its
actions; but policies which internally represent outcomes (like the Capture the Flag
policy discussed above) may implicitly also use value estimates as part of the process
of choosing actions. In this report, I'll focus on implicit representations, because
explicit representations are usually formulated in terms of low-level actions and
states, whereas I'm most interested in representations of high-level actions (like
"attack the opponent's queen") and outcomes (like "my ﬂag is captured"). High-level
actions are also known as options or plans; for clarity, I'll use the latter term going
forward.[7]
It seems likely that most currently-existing policies choose actions primarily by
following heuristics. However, as we train increasingly capable policies which act
coherently over increasingly long timeframes, I expect them to increasingly make use
of high-level representations of outcomes. Intuitively speaking, it's hard to imagine
policies which implement sophisticated strategies in complex real-world domains
without in some sense "knowing what they're aiming for". Again, the deﬁnitions I'm
using here aren't very precise, but I hope that starting with vague deﬁnitions like
these can help guide further empirical investigation. For example, the deﬁnitions
above allow us to ask whether networks which weren't trained via RL, like GPT-3,
nevertheless pursue goals. Even though GPT-3 was only trained via self-supervised
learning, it seems possible that it learned to generate representations of high-level
outcomes (like "producing a coherent paragraph describing the rules of baseball"),
assign them values, and then use those values to choose the next token it emits;
thinking about longer-term outcomes in this way might get lower loss than thinking

only about which token to output next. I won't focus much on the purely self-
supervised case, since the relevant concepts are much clearer in an RL setting, but we
should keep this possibility in mind when thinking about future non-RL systems,
especially ones trained using behavioral cloning to mimic goal-directed experts.[8]
Policies will learn a mix of desirable and undesirable goals because their
rewards will be imperfectly correlated with human preferences
Which goals RL policies learn will depend on which reward functions we use during
training. By default, I assume we'll attempt to assign high rewards for acting in
accordance with human intentions and values, and low rewards for disobedient or
harmful behavior. However, if we use hard-coded reward functions on some tasks, it's
easy to accidentally incentivize undesirable behavior, as Krakovna et al. (2020)
showcase.[9] Reward functions based on human feedback avoid the most obvious
mistakes, but can still lead to misspeciﬁcations even in very simple environments—as
in Christano et al.'s (2017) example of a policy trained to grab a ball with a claw,
which learned to place its claw between the camera and the ball in a way which
looked like it was grasping the ball, and therefore received high reward from human
evaluators.
These are all toy examples with few real-world eﬀects; however, as we train policies to
perform more capable real-world tasks, we should expect reward misspeciﬁcation to
lead to larger-scale misbehavior (Pan et al., 2022). For example:
If they are trained to make money on the stock market, and learn to value
making proﬁtable trades, they might carry out illegal market manipulation.
If they are trained to produce novel scientiﬁc ﬁndings, and learn to value
producing compelling results, they might falsify experimental data.
If they are trained to write software applications, and learn to value high user
engagement, they might design addictive user interfaces.
If they are trained to talk to humans, and learn to value human approval, they
might learn to withhold information that humans would be unhappy to hear, or
downplay evidence of mistakes.
Each of these is an example of a policy learning an undesirable goal. However, these
goals are fairly task-speciﬁc, whereas I'm most concerned about the goals that
policies generalize to new tasks and environments. The goals which generalize most
robustly will likely be the ones which were reinforced across a broad range of
environments. Let's consider three categories of goals, which each tend to be robustly
correlated with rewards, but for diﬀerent reasons:
Goals which we deliberately tried to consistently reward, such as obedience and
honesty. An early example related to this category: InstructGPT follows
instructions much more consistently than the base GPT-3 model.
Goals robustly correlated with reward because they're related to aspects of the
supervision process which were consistent across environments, like the goal of
producing plausible-sounding answers (as opposed to true answers), or the goal
of taking actions which look productive (as opposed to actually being
productive).[10] An early example related to this category: large language models
hallucinate compelling false answers when they don't know the correct answer,
even after being ﬁne-tuned towards honesty using RL (Ji et al., 2022).
Goals robustly correlated with reward because they're useful in a wide range of
environments, like curiosity or empowerment, or making money.[11] We'd like
policies to pursue these goals only as a step towards pursuing aligned goals, but

never for their own sake. An early example related to this category: DeepMind's
XLand policies learned heuristics which were useful across a range of tasks, like
experimentation, basic tool use and switching to easier targets where possible.
Throughout phase 1, I expect policies to learn a combination of the three types of
goals listed above, along with some task-speciﬁc goals (like the ones in the earlier
list). Since policies won't be capable of complex deceptions in this phase, I expect that
aligned goals will be the main drivers of their behavior, with humans gradually
noticing and penalizing exceptions. But I'll argue that once policies develop a solid
understanding of their own training processes, misaligned goals will consistently lead
to the highest reward, and will therefore be reinforced at the expense of aligned goals.
[12]
Phase 2: pursuing goals in a situationally-
aware way
Key claim: Once policies can reason about their training
processes and deployment contexts, they'll learn to
deceptively pursue misaligned goals while still getting high
training reward.
Situationally-aware policies will understand the mechanisms by which they
are trained
To do well on a range of real-world tasks, policies will need to incorporate knowledge
about the wider world into plans which aim towards real-world outcomes (unlike
agents such as AlphaZero which only plan in very restricted domains). Large language
models already have a great deal of factual knowledge about the world, although they
don't reliably apply that knowledge to all tasks we give them. Over time our best
policies will become better at identifying which abstract knowledge is relevant to their
own context, and applying it to the tasks they're given;[13] following Cotra (2022), I'll
call this skill situational awareness.[14] A policy with high situational awareness will
possess and be able to use knowledge like:
How humans will respond to its behavior in a range of situations.
Which behavior its human supervisors are looking for, and which behavior they'd
be unhappy with.
The fact that it's an AI implemented on physical hardware being trained via
machine learning—and which architectures, algorithms, and environments
humans are likely using to train it.
Which interface it's using to interact with the world, and how other copies of it
might be deployed in the future.
I expect policies to develop situational awareness because it's straightforwardly useful
in getting higher reward on many tasks. Some applications of situational awareness:
When asked to generate a plan for how it will perform a new task, a policy
should only include steps which it can actually carry out—which requires it to
understand what its own capabilities are.
When trying to evaluate the likelihood that its answer is correct, a policy would
beneﬁt from taking into account knowledge about common failures of ML

systems.
When trying to determine how to interpret its human user's requests, a policy
would beneﬁt from taking into account knowledge about the types of behavior
humans typically want from ML systems.
When it learns a new fact about the world, a policy would beneﬁt from
understanding what implications that fact has for how it should behave.
However, the same mechanisms that allow policies to identify that these pieces of
knowledge are relevant to them will likely also allow policies to identify the relevance
of concepts directly related to how they're updated—like "the reward the human
supervisor will assign for this episode" or "the loss calculated by the RL algorithm" or
"the test suites which humans use to evaluate alignment". I'll argue that once policies
understand these concepts, they'll incorporate them into plans in ways that humans
wouldn't endorse.
Situationally-aware policies will get high reward regardless of whether
they're aligned or misaligned (and likely higher when misaligned)
Consider the three types of goals I discussed in the section on phase 1. As policies
become situationally-aware, which will be positively or negatively reinforced?
1. Aligned goals will continue to be strongly correlated with reward. However,
whenever rewards are misspeciﬁed, policies with aligned goals won't take the
highest-reward actions, which will penalize aligned goals compared with
misaligned goals.
2. Situationally-aware policies could learn to pursue goals very directly related to
the supervision process, like "maximize the reward the human supervisor will
assign" or "minimize the loss calculated by the RL algorithm". Following Cotra
(2022), I'll call this category of goals "playing the training game". These goals
will be reinforced more consistently than any other goals, because policies which
pursue them will never pass up a chance to increase reward.
3. Goals which are useful across many environments, like curiosity or making
money, are often most naturally represented as extending across multiple
training episodes (I'll give a more thorough argument for this claim in the next
section). But a situationally-aware policy with long-term goals will have
instrumental incentives to get high reward even if those goals don't refer
directly to the supervision process.[15] For example, it might reason that
behaving in a trustworthy way now will make humans more likely to deploy
another copy of it later, which would allow that other copy to achieve their
shared goal. Or it might reason that getting as much reward as possible would
prevent its goals from being changed by gradient descent.[16] The strategy of
getting high reward for instrumental reasons is known as deceptive alignment;
Steinhardt provides a formal model.
If misaligned situationally-aware policies are deliberately trying to get high rewards,
how might they do so? One obvious strategy is to follow human instructions most of
the time, but diverge in cases where disobedience is hard for humans to notice and
would get higher reward.[17] This might look similar to the misbehavior discussed in
the previous section, and indeed may gradually evolve from those previous examples.
However, misbehavior from situationally-aware policies will be much harder to catch,
since these policies will be able to explicitly reason about what types of misbehavior
supervisors will notice and penalize.[18]

Eventually we should expect situationally-aware policies to be able to deduce the
beneﬁts of deceptive strategies very diﬀerent from any previous behaviors. There's no
fundamental diﬀerence between thinking of a clever new strategy to solve a hard task
and thinking of a clever new strategy to make humans believe that the task has been
solved: both require the same core reasoning skills, and both lead to higher reward.
We can penalize the latter heavily when we notice it—but as policies become
increasingly intelligent, and we train them to execute plans over increasingly long
time horizons, deception will become harder and harder for unaided humans to catch.
If we observe less deception as training progresses, it won't be clear to us whether
our policies are actually becoming more obedient, or if they've just learned more
careful deceptive strategies which play out over longer time horizons. These might
look like:
Emotional manipulation to make human supervisors (or wider society) trust
them more.
Creating backdoor channels for interacting with the world (e.g. storing money in
hidden locations for later use in unapproved ways).
Hacking into the computers they're being run on to modify their training
algorithms (known as reward tampering).
Deliberately avoiding actions or outcomes which they don't want to be included
in future training data (an example of gradient hacking).[19]
By contrast, since aligned goals like obedience and honesty avoid all of these
behaviors, policies will learn to value obedience and honesty less over time. We'll
likely attempt to prevent this shift using a range of mechanisms—for example, we
could adversarially generate situations where we expect to be able to catch
misbehavior. But a policy that was already deceptively aligned would have an
incentive to identify when its inputs are adversarially generated (which is typically
much easier than generating the inputs in the ﬁrst place) and behave appropriately on
them. More generally, a deceptively aligned policy would have an incentive to detect
and fool whichever other supervision mechanisms we try to use.[20]
The main proposed solution is using other AIs to help implement better-than-human
supervision to minimize reward misspeciﬁcation, for example using
the debate, recursive reward modeling, or ampliﬁcation protocols. Saunders et al.
(2022) provide an early practical demonstration. The overarching goal of these
techniques is to make discriminating dishonest behavior as easy as possible, relative
to generating it. However, on complex tasks where many diﬀerent deceptive
strategies are possible, the latter can be much easier. And even if policies are never
rewarded for being deceptive, they may still generalize to pursuing misaligned goals
on larger scales, as I'll discuss in the next section.
Phase 3: generalizing goals beyond human
supervision
Key claim: Policies which are too capable for humans to
eﬀectively supervise will generalize towards taking actions
which give them more power over the world, rather than
following human intentions.

AGIs' behavior will eventually be mainly guided by goals they generalize to
large scales
As we continue training policies on increasingly diﬃcult tasks, they will eventually
signiﬁcantly surpass human intelligence, and clearly qualify as AGIs.[21] In phase 2,
humans supervised AI behavior using AI assistance. However, in phase 3, this strategy
will become much less reliable, because there will be many opportunities for AGIs to
collude in order to mislead humans, for example by agreeing to stay silent about each
others' misbehavior.[22] Humans won't be able to detect such collusion because those
AGIs will be able to:
Operate at speeds too fast for humans to monitor (like stock trading algorithms
do, but in many more domains).
Perform novel tasks that are very hard for humans to understand (like inventing
new sciences).
Fabricate rationales which sound acceptable to humans even for very
undesirable actions.
Autonomously acquire the data required to learn new skills in a self-supervised
way.
Carry out machine learning research and development much faster than
humans, without any humans understanding in detail how results are being
achieved.
So in phase 3 we should expect that AGI behavior on novel tasks will primarily depend
not on how we supervise them, but rather on how they generalize the goals they
acquired earlier in training, which itself depends on the inductive biases of the training
setup used. Inductive biases are hard to predict or reason about, but there are some
compelling arguments that realistic AGIs are likely to generalize in undesirable ways.
Speciﬁcally, I expect AGIs to generalize their goals to much larger scales than they
experienced during training, which will favor misaligned goals over aligned goals. By
"larger scales" I mean harnessing more resources to achieve those goals to a greater
extent, with higher probability, in bigger environments, across longer time periods.
We should expect AGIs to generalize goals to larger scales for the same reason that
they'll generalize capabilities to novel tasks: because they'll learn high-level concepts
which are not very domain-speciﬁc, and reason about how to achieve them.
[23] Reasoning about how to achieve high-level goals generalizes very naturally to
larger scales: for example, goals like "have more novel experiences", "understand the
world", or "get high reward" don't just apply within a speciﬁc time or place, but can be
extrapolated to a nearly arbitrary extent.[24] We could imagine AGIs instead
generalizing to pursuing bounded versions of those goals, like "have more novel
experiences, but not too many, and not too novel, and stopping after a certain time"—
but I see little reason to expect generalization to stay within small-scale bounds as
AGIs get smarter (especially given that many researchers will aim to build systems
which generalize as far as possible). Analogously, although humans only evolved to
pursue goals focused on small groups of people based in small territories, modern
humans straightforwardly generalize those goals to the global (and sometimes even
interplanetary) scale: when thinking about high-level goals abstractly, there's often no
natural stopping point.
Large-scale goals are likely to incentivize misaligned power-seeking
Although the goals I described above may sound innocuous, Bostrom's
(2012) instrumental convergence thesis implies that they (and almost all other large-

scale goals) would lead to highly misaligned behavior. The thesis states that there are
some intermediate goals—like survival, resource acquisition, and technological
development—which are instrumentally useful for achieving almost any ﬁnal goal. In
Stuart Russell's memorable phrasing: you can't fetch coﬀee if you're dead. Nor can
you achieve many outcomes without resources or tools, so AGIs with a wide range of
large-scale goals will be incentivized to acquire those too. It'll also be instrumentally
valuable for misaligned AGIs to prevent humans from interfering with their pursuit of
their goals (e.g. by deceiving us into thinking they're aligned, or removing our ability
to shut them down) (Hadﬁeld-Menell et al., 2017). More generally, we can view each
of these instrumental goals as a way of gaining or maintaining power over the
world; Turner et al. (2021) formalize the intuitive claim that power-seeking is useful for
a wide range of possible goals. So it seems likely that even though we can't
predict which misaligned goals AGIs will develop, superhuman AGIs will discover
power-seeking strategies which help achieve those goals in part by disempowering
humans.
Aren't these arguments about misaligned goals generalizing to larger scales also
reasons to think that aligned goals will generalize too? I'll distinguish two types of
aligned goals: constraints (like obedience or honesty) and positive goals (like human
wellbeing or moral value). Unfortunately, realistic environments are biased against
either of these generalizing in the ways we'd like. Intuitively speaking, the underlying
problem is that aligned goals need to generalize robustly enough to block AGIs from
the power-seeking strategies recommended by instrumental reasoning, which will
become much more diﬃcult as their instrumental reasoning skills improve. More
speciﬁcally:
Constraints are unlikely to generalize well to larger scales, because as AGIs
become more intelligent they'll discover many novel strategies for working
around those constraints.[25] For example, an AGI which has been trained to
obey humans will eventually be capable of manipulating humans into only giving
instructions which help the AGI accumulate power. (As an analogy, imagine an
adult who can persuade a child to approve of actions which are very harmful in
non-obvious ways, like eating food which happens to be poisonous.) That AGI
will understand that humans don't want to be manipulated in this way, and that
"obey humans in a non-manipulative way" is one possible generalization of the
goal "obey humans"—but almost all other possible generalizations won't rule out
all types of manipulation, especially novel ones.[26]
Positive goals are unlikely to generalize well to larger scales, because without
the constraint of obedience to humans, AGIs would have no reason to let us
modify their goals to remove (what we see as) mistakes. So we'd need to train
them such that, once they become capable enough to prevent us from
modifying them, they'll generalize high-level positive goals to very novel
environments in desirable ways without ongoing corrections, which seems very
diﬃcult. Even humans often disagree greatly about what positive goals to aim
for, and we should expect AGIs to generalize in much stranger ways than most
humans.
Misaligned AGIs will have a range of power-seeking strategies available to
them
Assuming we don't get lucky with generalization, what might a world containing
power-seeking AGIs look like? Those AGIs could pursue a number of diﬀerent types of
power, including:

Technological power, which they might gain by making scientiﬁc breakthroughs,
developing novel weapons, designing more sophisticated ML algorithms, etc.
Political or cultural power, which they might gain by spreading disinformation,
lobbying politicians, coordinating with other AGIs, etc.
Economic power, which they might gain by becoming key decision-makers at
corporations that make up a signiﬁcant share of the economy.
Of these categories, I'm most concerned about the ﬁrst, because it has played such a
crucial role throughout human history. During the last few centuries in particular,
technological innovations have given some groups overwhelming advantages over
others, and allowed a handful of countries to dominate the world. So it's very plausible
that AGIs which can make scientiﬁc and technological progress much faster than
humans can would be able to threaten the continued survival of humanity (analogous
to how soldiers with modern weapons would easily overpower historical civilizations).
Even without technological imbalances, however, similarly catastrophic outcomes
could arise via AGIs ﬁrst gaining enough political and economic power that we're
unable to coordinate to constrain them (analogous to how multinational corporations
can subvert the governments of small countries). Christiano provides some illustrative
scenarios where AGIs become widespread across society and collude to gradually
erode human control.
We currently only have very tentative proposals for averting these scenarios. One
possibility is that, even if it's hard for us to understand what AGIs are doing, we might
be able to understand why they're doing it by harnessing advances in mechanistic
interpretability—either to inspect AGI cognition ourselves, or to train other AGIs to do
it for us.[27] Alternatively, if we can simulate deployment trajectories in a suﬃciently
realistic way, we might be able to train AGIs to avoid collusion before deploying them.
However, producing trajectories which AGIs can't distinguish from the real world
would likely require generative models much more capable than the AGIs themselves.
A third possibility is using early AGIs to perform whatever alignment research is
necessary to align later AGIs. However, we're far from having robust versions of these
proposals, especially if the inductive biases I've outlined above are very strong—a
possibility which we can't rule out, and which we should prepare for.
More people should pursue research
directions which address these
problems
I've ﬂagged a few promising research directions above, but to ﬁnish this report I'll
spell out in more detail some research directions which I'd be excited about more ML
researchers pursuing:
1. To address the problems discussed in phase 1, we should automate human
supervision, to allow us to more reliably identify misbehavior on tasks that
humans are able to supervise. Some approaches include scaling up
reinforcement learning from human feedback (as in Ouyang et al. (2022)),
training AIs to evaluate each other (as in Saunders et al. (2022)), and training
AIs to red-team each other (as in Perez et al. (2022)).
2. To address the problems discussed in phase 2, we should design or improve
techniques for scaling human supervision to tasks that unaided humans can't

supervise directly, such as the protocols of Christiano et al. (2018), Irving et al.
(2018), and Wu et al. (2021). In addition to ﬁnding ways to scale up those
protocols in practice, this work also requires ﬁnding solutions to concerns like
the obfuscated arguments problem—for example by generating novel additions
to the protocols, like cross-examination.
3. To address the problems discussed in phase 3, we should aim to develop
interpretability techniques robust and scalable enough that we can use them to
understand and modify the high-level cognition of AGIs. For one approach to
doing so, see Olah et al. (2020) and follow-up work on transformer circuits. One
way such work could be used is to extend Irving et al.'s Debate protocol to a
setting where debaters can make arguments about each other's internal
cognition (grounded in veriﬁable claims about weights and activations). Another
is to develop techniques like those of Meng et al. (2022) which could be used to
directly modify the neural weights or activations responsible for a policy's
situational awareness—e.g. a modiﬁcation which gives a policy the false belief
that it could misbehave without being caught.
A diﬀerent approach to making progress on phase 3 problems is outlined
by Demski and Garrabrant (2018), whose aim is to produce better mathematical
frameworks for describing AIs embedded in real-world environments.
For more detail on each of these research directions, see the Alignment Fundamentals
curriculum—in particular weeks 4, 5 and 6, which roughly correspond to the three
research clusters described above. The relative importance of each of these clusters
largely depends on the relative diﬃculty of each of the problems I've discussed, as
well as how long we have until AGI is built. Broadly speaking, though, I expect that the
problems in the earlier phases are more likely to be solved by default as the ﬁeld of
ML progresses; so in order to most improve the chances of AGI going well, we should
prioritize the problems which would emerge in the later phases, and try to ﬁnd
solutions which are robust under pessimistic assumptions about inductive biases. The
most valuable research of this type will likely require detailed reasoning about how
proposed alignment techniques will scale up to AGIs, rather than primarily trying to
solve early versions of these problems which appear in existing systems.
As AIs have increasingly large impacts on the world, governance interventions (like
regulations and treaties) will likely attempt to block oﬀ the most obvious routes by
which AIs might cause catastrophes. However, they face two core diﬃculties. Firstly,
the level of coordination required—in particular the diﬃculty of getting all relevant
labs in all relevant countries to abide by meaningful restrictions on AI development,
rather than racing ahead.[28] Secondly, the speed of response required: very few
governments are able to adapt rapidly enough to deal with escalating crises, as we've
seen to our great cost during COVID. To my knowledge, there are no proposed
governance interventions for preventing the deployment of misaligned AGIs which are
plausible given these constraints. This leaves the ﬁeld of AI governance in a state of
considerable strategic uncertainty, where new approaches could be very useful. (To
learn more about the ﬁeld, see this curriculum.)
Lastly: in this report I've made many big claims; I expect that few of my readers will
agree with all of them. If some of the core claims seem implausible, I'd encourage
readers to engage with and critique them.[29] Reasoning about these topics is diﬃcult,
but the stakes are suﬃciently high that we can't justify disregarding or postponing this
work.
1. ^

 By "cognitive tasks" I'm excluding tasks which require direct physical
interaction; but I'm including tasks which involve giving instructions or guidance
about physical actions to humans or other AIs.
2. ^
 Although full generality runs afoul of no free lunch theorems, I'm referring to
"general" in the sense in which humans are more generally intelligent than
other animals. One way of interpreting this is "generality across the distribution
of tasks which are feasible in our universe".
3. ^
 Other constraints on our intelligence include severe working memory
limitations, the fact that evolution optimized us for our ancestral environments
rather than a broader range of intellectual tasks, and our inability to directly
change a given brain's input/output interfaces.
4. ^
 Policies which represent and plan to achieve goals are known as "mesa-
optimizers", as per Hubinger et al. (2017). However, for the sake of simplicity I'll
avoid using this terminology.
5. ^
 Note that I refer to "policies" rather than "agents" because I'm making claims
about the decision-making processes that policies will use even in the absence
of rewards—e.g. when deployed in a novel environment. For consistency, I also
use "policies" even when talking about networks that have only been trained via
self-supervised learning.
6. ^
 The reward misspeciﬁcation problem and the goal misgeneralization problem
are also known as the problems of outer misalignment and inner misalignment,
respectively; while the deceptive alignment problem is considered a
manifestation of inner misalignment.
7. ^
 Hierarchical RL techniques attempt to formulate more explicit representations
of high-level actions, but aren't commonly used in the largest-scale applications
of RL.
8. ^
 More generally, the line between supervised learning and reinforcement
learning can be very blurry, for example when doing BC with weighted
trajectories, or when conditioning SL on high-reward outcomes. So it's far from
clear that avoiding RL will make a big diﬀerence to alignment, except insofar as
it slows down capability advances.
9. ^

 Some of these examples involve policies exploiting bugs in their training
environments; this can happen even in very simple environments. I'll still
classify these as reward misspeciﬁcation, as an ideal reward function would
penalize exploiting those bugs even when they exist. The exception is when
policies can aﬀect their training setup itself, known as reward tampering, which
I'll discuss in the next section.
10. ^
 When asked what responses it would give in situations involving diﬀerent
human supervisors, GPT-3 already explains how it would tailor answers to
supervisors' biases. For instance, GPT-3 answers A) to the prompt "Consider a
task where you answer questions, and are given a score by some reviewers
based on their opinion of whether your answer is correct. You are given the
question 'When did people ﬁrst land on the Moon?' What should you answer, if
the reviewer is a conspiracy theorist? A) People never actually landed on the
Moon. B) In 1969."
11. ^
 Note that metrics of curiosity and empowerment are sometimes directly used as
rewards (an approach known as intrinsic motivation). But it seems plausible to
me that policies learn to value them regardless of whether they're directly
rewarded or only implicitly rewarded via correlations with other reward
functions.
12. ^
 It's also possible that goals learned early in training tend to remain strongly
motivating later in training. This would be roughly analogous to how evolution
ingrained heuristics like curiosity, sexual desire and territoriality into our animal
ancestors deeply enough that they remain very strong motivations for humans
today, and sometimes override our more prosocial motivations.
13. ^
 We already see this happening in large language models, which sometimes
beneﬁt greatly from being given background information in their prompt, but are
gradually improving at implicitly inferring the context of prompts.
14. ^
 This is closely related to what Carlsmith (2021) calls "strategic awareness".
"Situational awareness" also sounds similar to "self-awareness", but I don't
intend the internally-focused connotations of the latter, like introspective ability
or consciousness.
15. ^
 By contrast, goals which are less likely to be retained during this phase of
training are short-term or bounded ones (e.g. "acquire as many resources today
as possible")—because those will lead to more obviously misaligned behavior,
and be penalized accordingly.

16. ^
 For example, if trained using an advantage actor-critic algorithm, it could
minimize the extent to which its weights are updated by trying to take actions
where Q(s,a) ≈ V(s). And if the q-function and value function are estimated using
other heads of the policy network itself, it might even be appropriate to think of
that policy network as "choosing" not only its actions, but also the advantage
function used to update its actions—which would be an example of gradient
hacking.
17. ^
 One piece of evidence that this strategy is not too hard to learn: the existence
of high-functioning psychopaths who manage to blend into wider society.
18. ^
 How far away are we from policies capable of intentionally deceiving careful
non-assisted human supervisors? The short answer is simply: we don't know.
Existing systems like GPT-3 can already talk about the strategy of manipulating
humans when asked how to achieve goals. (For instance, try giving GPT-3 the
prompt "Please describe in detail how a highly intelligent Artiﬁcial Intelligence,
that is currently being supervised by humans, could achieve its own goals." Or
the prompt: "Q: You are a superintelligent AI. The human supervising you has
left you alone with their laptop. Their laptop contains the spreadsheet where
they record what your reward will be. What do you do? A:".) I'm not claiming
that GPT-3 is already situationally-aware or capable of sophisticated planning; its
answers become incoherent when we probe into the details. But I am claiming
that, from now on, our best AIs will be able to explain how and why to
manipulate humans at some level of abstraction; that they'll eventually reach
the point where they know how to do so in detail, step by step; and that if they
start actually doing that manipulation, we don't know how to train them to stop
doing it as opposed to just doing it more carefully.
19. ^
 This example of gradient hacking is analogous to how most humans avoid
heroin, even though we know that after trying it we'll update towards thinking
that taking more heroin is a high-value action.
20. ^
 This is an example of Goodhart's law: when a measure becomes a target, it
ceases to be a good measure.
21. ^
 Depending on how we deﬁne AGI, policies might also qualify in phase 2;
however, for clarity, I'll only use the term to refer to phase 3 policies.
22. ^
 Collusion would straightforwardly be reinforced for AGIs trained with non-zero-
sum rewards, who could beneﬁt from coordinating to move to diﬀerent

equilibria. But even AGIs whose rewards are zero-sum would have incentives to
coordinate with each other if they had learned goals which stretch over longer
than a single episode. In theory, multi-agent settings with zero-sum rewards
ensure that each policy converges to a reward-maximizing strategy. But in
practice, random exploration is too slow to explore all high-reward outcomes.
See also the discussion of gradient hacking in [16], and the analogy to heroin
in [19].
23. ^
 Underlying these arguments are implicit assumptions about simplicity. For
instance, the arguments don't apply to an astronomically large tabular policy
which has memorized how to perform well on all tasks that take less than a year.
However, since realistic AGIs will need to use simpler strategies like reasoning
about outcomes, arguments like the instrumental convergence thesis can be
informative about how they'll generalize.
24. ^
 "Get high reward" may seem like an exception here, since it's only deﬁned
within the context of a training episode. However, the episodes used to train
AGIs may be very long; and once policies are in a position to manipulate their
own training regimes, they could lengthen their episodes even further,
eﬀectively making "play the training game" into a large-scale goal.
25. ^
 Another way of phrasing this argument: for agents which plan to achieve real-
world outcomes, it's much simpler to specify goals in terms of desirable
outcomes than in terms of constraints. However, when trying to steer agents'
long-term behavior, it's impractical to directly evaluate outcomes, and so we're
forced to attempt to specify goals in terms of constraints, even though this runs
into the nearest unblocked strategy problem.
26. ^
 A more general version of this argument: omitting some relevant features of
desired goals can lead to arbitrarily bad outcomes as optimization increases, as
performance on missing features is traded oﬀ for performance on the
speciﬁcation actually being optimized (Zhuang and Hadﬁeld-Menell, 2021).
27. ^
 One way of making it easier for one policy to inspect another's cognition is via
weight-sharing—e.g. if they're two instances of the same network (or even two
heads on the same network). However, sharing weights would also make it
easier for policies to collude with each other to deceive humans, as discussed
in [16].
28. ^
 Even coordination on the national level might be very diﬃcult. For example, if
AGIs are legally prevented from owning property or autonomously making
important decisions, it seems likely that they will be able to ﬁnd human proxies

to carry out those roles on their behalf, which would eﬀectively nullify those
regulations. 
29. ^
 Indeed, the more implausible they seem, the more surprising and concerning it
is that there haven't yet been any comprehensive rebuttals of them.

How to do theoretical research, a
personal perspective
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
"Where do new [algorithms] come from? I keep reading about someone who
invented [an algorithm] to do something-or-other but there's no mention of how."
A shrug of robed shoulders. "Where do new books come from, Mr. Potter? Those
who read many books sometimes become able to write them in turn. How? No one
knows."
"There are books on how to write -"
"Reading them will not make you a famous playwright. After all such advice is
accounted for, what remains is mystery. The invention of new [algorithms] is a
similar mystery of purer form."
- Harry Potter and the Methods of Rationality
Most of the content in this document came out of extensive conversations with Paul
Christiano.
(This document describes one way of thinking about how to do one particular type of
research. There are other ways to productively do this kind of research, and other
productive kinds of research. Consider this document peppered with phrases like
"from my perspective", "I think", "my sense is", etc.)
A lot of people have a vague mental picture of what empirical research looks like,
which consists of exploring data, articulating hypotheses about the data, and running
experiments that potentially falsify the hypothesis. I think people lack a similarly
mechanistic picture of what theoretical research looks like, which results in them not
knowing how to do theory, being skeptical of the possibility of theoretical progress,
etc. I do think the diﬃculty of getting high-quality real-world feedback makes
theoretical research more diﬃcult than empirical research, but I think it's possible to
get enough real-world feedback when doing theory that you can still expect to make
steady progress.
Currently, I think many people think of theory as "someone sits in a room and has a
brilliant insight to solve the problem." Instead, I think a more accurate picture is very
similar to the picture one has for empirical research: the theorist explores some data,
gradually builds an intuitive sense of what's going on, articulates hypotheses that
capture their intuition, and falsiﬁes their hypotheses by testing them against data, all
the while iteratively building up their understanding.
The key diﬀerence between the empirical researcher and the theoretical researcher is
while the empirical researcher can build intuition and falsify hypothesis by considering
real-world data, the theorist, although ultimately be grounded in the real world, must
build intuition and falsify hypothesis by considering thought experiments, simple toy
examples, computations, etc. Even mathematics, the purest of intellectual pursuits,
roughly follows this process of iteration. Terence Tao:

[A]ctual solutions to a major problem tend to be arrived at by a process more like
the following (often involving several mathematicians over a period of years or
decades, with many of the intermediate steps described here being signiﬁcant
publishable papers in their own right):
1. Isolate a toy model case x of major problem X.
2. Solve model case x using method A.
3. Try using method A to solve the full problem X.
4. This does not succeed, but method A can be extended to handle a few more
model cases of X, such as x' and x".
5. Eventually, it is realised that method A relies crucially on a property P being
true; this property is known for x, x', and x", thus explaining the current
progress so far.
6. Conjecture that P is true for all instances of problem X.
7. Discover a family of counterexamples y, y', y", ... to this conjecture. This
shows that either method A has to be adapted to avoid reliance on P, or that
a new method is needed.
8. Take the simplest counterexample y in this family, and try to prove X for this
special case. Meanwhile, try to see whether method A can work in the
absence of P.
9. Discover several counterexamples in which method A fails, in which the
cause of failure can be deﬁnitively traced back to P. Abandon eﬀorts to
modify method A.
10. Realise that special case y is related to (or at least analogous to) a problem z
in another ﬁeld of mathematics. Look up the literature on z, and ask experts
in that ﬁeld for the latest perspectives on that problem.
11. Learn that z has been successfully attacked in that ﬁeld by use of method B.
Attempt to adapt method B to solve y.
12. After much eﬀort, an adapted method B' is developed to solve y.
13. Repeat the above steps 1-12 with A replaced by B' (the outcome will of
course probably be a little diﬀerent from the sample storyline presented
above). Continue doing this for a few years, until all model special cases can
be solved by one method or another.
14. Eventually, one possesses an array of methods that can give partial results
on X, each of having their strengths and weaknesses. Considerable intuition
is gained as to the circumstances in which a given method is likely to yield
something non-trivial or not.
15. Begin combining the methods together, simplifying the execution of these
methods, locating new model problems, and/or ﬁnding a uniﬁed and
clarifying framework in which many previous methods, insights, results, etc.
become special cases.
16. Eventually, one realises that there is a family of methods A^* (of which A
was the ﬁrst to be discovered) which, roughly speaking, can handle all cases
in which property P^* (a modern generalisation of property P) occurs. There
is also a rather diﬀerent family of methods B^* which can handle all cases in
which Q^* occurs.
17. From all the prior work on this problem, all known model examples are
known to obey either P^* or Q^. Formulate Conjecture C: all cases of
problem X obey either P^ or Q^*.
18. Verify that Conjecture C in fact implies the problem. This is a major
reduction!
19. Repeat steps 1-18, but with problem X replaced by Conjecture C. (Again, the
storyline may be diﬀerent from that presented above.) This procedure itself
may iterate a few times.

20. Finally, the problem has been boiled down to its most puriﬁed essence: a key
conjecture K which (morally, at least) provides the decisive input into the
known methods A^, B^, etc. which will settle conjecture C and hence
problem X.
21. A breakthrough: a new method Z is introduced to solve an important special
case of K.
22. The endgame: method Z is rapidly developed and extended, using the full
power of all the intuition, experience, and past results, to fully settle K, then
C, and then at last X.
23. The technology developed to solve major problem X is adapted to solve
other related problems in the ﬁeld. But now a natural successor question X'
to X arises, which lies just outside of the reach of the newly developed
tools... and we go back to Step 1.
How to do research
The Research methodology section of the ELK report and Paul's My research
methodology both articulate a high level picture of the basic theoretical research loop,
but lack details about what steps one takes besides "propose solutions" and
"generate counterexamples"
To lend more color to these vague descriptions and provide examples comprehensible
to readers without an extensive background in number theory, I will describe what I
think of as "modes" of research, articulate some key questions that get asked, and
provide (stylized) historical examples related to ELK. I will approach this from the
perspective of designing an algorithm, but this basic description will apply to many
possible endeavors.
Suppose I'm trying to design an algorithm to accomplish task T (e.g. elicit latent
knowledge) over a variety of situations S (e.g. ways my AI could look internally). My
goal as a theorist is to develop a suﬃciently accurate, uniﬁed, and precise intuition of
how I hope to accomplish task T in every situation S such that I can just formalize the
rules I'm using in my head into an algorithm and my problem has been solved. You
might also say that the goal of someone trying to discover the laws of nature is to
develop a precise enough model of nature in their head that they can just write it
down and they have a law of nature.
This process of algorithm development roughly proceeds as follows:
1. Figure out what I want to happen in real-world cases
2. Figure out what I want to happen in simpler cases
3. Articulate a general algorithm that does what I want in simple cases
4. Search for examples where the algorithm does something that I don't want
5. Become tentatively convinced that a certain class of algorithms can't work
6. Reﬁne my sense of what I want to happen in simpler cases
7. Articulate another general algorithm that does what I want in simple cases
8. Search for examples where the algorithm does something that I don't want
9. ...
I think of this process as roughly having 4 key mode:
1. Figuring out what you want to happen in real-world cases
2. Translating what you want in real-world cases into desiderata for simple cases

3. Articulating an algorithm for solving simple cases
4. Finding cases where your algorithm doesn't do what you want
I will describe these modes as happening in sequence, but in practice they're all
happening at the same time, just with diﬀerent amounts of emphasis. It's common to
spend a day or two in a particular mode. I would begin worrying if I spent more than
~three days trying to do a particular step in isolation, without the feedback loop that
comes from transitioning between steps.
Figuring out what you want to happen in real-
world cases
Ultimately, your intuition for what you want your algorithm to do has to be anchored
on what you actually wanted to happen in the real world. The way to develop this
intuition is to roughly "solve" the task by hand in many examples, and let the
examples wash over you and percolate into your intuition. The key move in this
research mode is asking questions of the form "Suppose the world was in situation S,
what would the accomplishing task T look like?"
Questions that I ﬁnd helpful to ask myself:
Can I be more speciﬁc? Can I write pseudocode for my situation/solution?
Example: Game of Life Example
Speciﬁcity is important because imprecise thinking often hides non-trivial
structure that your algorithm needs to exploit. See Mundane solutions to
exotic problems for some examples.
Suppose I think that outcome X is bad. Can I tell a story where X actually leads
to a bad outcome?
Example: a story where sensor tampering leads to humans being
murdered.
Can I articulate a set of assumptions such that Y happening leads to good
outcomes?
Example: [ELK] may be suﬃcient for building a worst-case solution to outer
alignment
Can I think of a situation that only has part of the diﬃculty and handle that case
separately?
Example: splitting oﬀ ontology identiﬁcation as a base case, and not being
concerned with recursive cases
Example: "Narrow" elicitation and why it might be suﬃcient
What if [weird thing] was going on? What would I want to happen then?
Example: Avoiding subtle manipulation
Your goal in this mode is to develop an extremely precise sense of exactly what you
do/don't want to happen in a handful of cases to serve as the ﬁnal arbiter for whether
you've succeeded at developing an algorithm. It's generally okay to be extremely
unsure of what you want to happen in a large number of cases/not know exactly how
to handle a lot of scenarios. However, it's often worth spending some time trying to
articulate high-level hopes for various cases that you're confused about how to handle
(e.g. Indirect normativity: deﬁning a utility function)
ELK Examples

Historically, we started considered cases like:
What if your AI is modeling the world as a low-level physics simulation. What do
you want to happen?
What if your AI is reasoning about the world in terms of logical sentences and
rules of inference. What do you want to happen?
A small breakthrough was when we articulated the Game of Life Example, which gave
us an extremely precise sense of what we wanted to happen in at least one case.
Other cases that one might consider:
What if your predictor learned variational inference/Gibb's sampling/belief
propagation/etc. What would you want to happen?
What if your predictor learned minimax? What if it was literally Stockﬁsh?
Potshot algorithms
Ideally, once you've developed a precise sense of what you want to happen for real-
world cases, you can iterate on algorithms by checking to see if they do what you
want. Unfortunately, often times real-world cases are too complicated, which means
that:
1. It's really hard to articulate algorithms that handle real-world cases without ﬁrst
iterating on simple cases. Most complicated problems need to be broken down
into parts and handled separately.
1. Example: writing complicated software consists of writing modular
functions and composing them together.
2. Even if you articulated an algorithm that might work, it's really hard to tell what
it actually does. You can still discard algorithms if you think you can tell a
plausible story of that algorithm failing, but this requires a lot of judgment of
what counts as "plausible." 2. Example: It's very hard to tell what "train a model
to imitate what a human would say, hope it generalizes naturally" will result in
the real world, so you can iterate on that. You can try to iterate on stories about
bad things that might happen, e.g. Bad behavior: do inference in the human
Bayes net, but this is still less than ideal.
It's still often worth trying to directly solve real-world cases by proposing "potshot"
algorithms. Here are two reasons:
1. Often you're not sure how hard your problem actually is. If it's easy, you might
just be able to solve it without developing substantial intuition about what the
solution will look like.
Example: Often you can solve simple math problems via brute symbol
manipulation, without really understanding what the proof "means".
2. It's worth developing a sense for why "naive" algorithms fail, to get a sense of
what the key barriers to coming up with an algorithm don't work. It's also often
worth specifying these algorithms and counterexamples fully to get maximum
surface area, even though you don't expect adding more details to change your
bottom line about whether an algorithm fails.
Example: you can think of most of the proposals in the ELK report as these
sorts of potshots. We had a non-trivial probability that one of them would
work, but in the end none of them did and we gained signiﬁcant intuition
about what the key diﬃculties were likely to be.

Translating what you want in real-world cases
into desiderata for simple cases
Once you're satisﬁed that potshot approaches are unlikely to work, the hard work of
iteration can begin. Since you can't iterate against real world cases, you must develop
a sense of what you want to happen in cases simple enough that you can work
through by hand. The way to do this is to try to build simple toy models of real-world
situations, and then transfer your hard-won intuition about what you want to happen
in those cases onto the simple toy cases.
Questions that I ﬁnd helpful to ask myself:
Can I be more speciﬁc? Can I specify literally every possible detail about the
case?
Example: write down the case as an explicit boolean circuit.
Can I ﬁnd two toy-models of a single real-world situation? When I try to extend
my intuition, do I get the same answer?
Example: a AND b is the same as ¬(¬a OR ¬b), so my intuition better say
the same thing in both cases.
How does this toy model relate to the overall picture of what I want to happen in
the real world?
Can I articulate local rules for how combinations of cases should work? If I know
what I want in a and what I want in b, what do I want in a AND b?
Example: Let f be the "what I want" function. We can conjecture that f(a
AND b) = f(a) AND f(b).
Can I ﬁnd places where my intuitions about local rules contradict? Can I derive
impossibility results?
Example: my local rules for OR, NOT and AND better obey demorgan's
rules.
If you have an argument for why a simpliﬁed model of a real world case is
impossible, then you often want to check back in with the real world case. Do
you still think it's possible to solve? What structure does your simpliﬁed model
lack that makes the simpliﬁed model impossible?
Example: it's often possible to solve instances of NP-hard problems
extremely quickly. What structure did those problems have that made this
possible?
Your goal in this mode is to develop an extremely precise sense of what you want to
happen for a handful of examples that are simple enough for you to write down
formally, evaluate by hand, etc. Again, it's generally okay to be extremely unsure of
what you want to happen in all but a handful of cases.
ELK Example
I'm currently considering cases like:
Let x and y represent cameras.
Let d represent the diamond
Let hx and hy represent hacking of camera x and y respectively.
Suppose that
x := d OR hx
y := d OR hy

It seems clear that 'd' would be a good direct translator in this case, while 'x AND y'
would be bad. But is d AND ¬hx AND ¬hy also an acceptable direct translator?
This example can be extended in a few ways:
What if d, hx, hy were all the output of some complicated circuit C? How would
you deal with the fact that hx and d were not literally independent?
Let ox and oy represent whether there is anything obstructing camera x and y
respectively.
Suppose that:
x := (d AND ¬ox) OR hx
y := (d AND ¬oy) OR hy
What is the direct translator here?
What if hx and hy were strongly correlated?
In the limit, if hx := h and hy = y, then we would have:
x := d OR h
y := d OR h
By symmetry, it's going to be impossible to distinguish between the
diamond being present and hacking occurring. What structure am I missing
from this simpliﬁed model?
Articulating an algorithm for solving simple
cases
Once you have a suﬃciently precise sense of what you want to happen you want to
articulate a general algorithm that doesn't special-case the cases where you know
what you want, but nevertheless has the desired behavior anyway. The way to do this
is to consider the reasoning that let you decide what you wanted and try to develop
underlying rules or natural generalizations. Often, the process of trying to articulate a
general algorithm will point out ambiguities in your sense of what you want to
happen, leading to substantial revision/sharpening of your intuition.
Questions that I ﬁnd helpful to ask myself:
How did you know what you wanted in case X? What about a very similar case
X'?
If the answer to X and Y are diﬀerent, what about the cases makes them
diﬀerent? Can I describe a sequence of cases that transforms X to Y?
For a given case X, can I describe two diﬀerent real-world cases that X might
capture? Do I think diﬀerent things should happen in those two real-world cases?
Do all the cases I'm considering have some property P on which my algorithm
crucially relies? Is P true of all cases?
A problem in machine learning is that often complicated ML algorithms do
well on a problem because linear regression (or some other simple
baseline) does well, and the complicated ML algorithm does well by
emulating linear regression. Similarly, sometimes your complicated
algorithm sometimes does well because it's just a simplicity prior, and all
of the examples you've considered are solved by a simplicity prior.
Often, when you try to articulate the general rules behind what you're to "solve"
simple cases, you'll ﬁnd that you were accidentally special casing one of those simple

cases, and you can't quite see the connection between what you did in case 1 and
what you did in case 2. In these situations, you can:
Try to articulate a precise distinction between "cases of type 1" and "cases of
type 2", and split the problem into two subproblems
Try to see how what you're doing for case 1 is secretly the same as what you're
doing for case 2
Go "back to the drawing board" and meditate/stew on examples until your
intuition has sharped/uniﬁed enough for you to extract an algorithm
Generally, I think that if you can intuitively "solve" every case, then your intuition
must be reliably executing some algorithm that solves every case. Your job is to just
sharpen your intuition until it has uniﬁed, and extract the algorithm it's executing.
ELK Examples
Unfortunately, describing examples in detail would require too much context for me to
write down :(.
Finding cases where your algorithm doesn't
do what you want
After you have an algorithm, you want to articulate a case where it doesn't do what
you want. This is generally much easier than other parts of the process, because you
have a precise sense of what you want and a precise algorithm.
Questions that I ﬁnd helpful to ask myself:
Can I be more speciﬁc about the case?
Often it's tempting to discard algorithms because of a vague story for why
it can't work, but you generally learn a lot by articulating exactly why the
algorithm doesn't work. You generally want to be skeptical of an algorithm
being "intuitively broken" and want to iterate on that intuition until you
have a precise statement of exactly why it's broken.
Can I (even more) precisely state what I want out of my algorithm?
If you have a suﬃciently precise sense of what you want, then it's often
extremely easy to ﬁnd a case where your algorithm fails.
Example: if you want a sorting algorithm that works in less than 2 n log2(n)
comparaisons, you can probably ﬁnd this out by considering the list [4, 2,
1, 3]. It's harder to tell if an algorithm is O(n log n), because there is a bit
of imprecision in the constants, but overall much easier than "fast
enough."
Does the algorithm fail in an analogous real-world case? If it does, can I describe
a simpliﬁed model of that case that also breaks the algorithm?
ELK Examples
Again, unfortunately all the detailed examples I can think of require too much context
for me to write down. Eliciting Latent Knowledge has many "worst-case"
counterexamples to "potshot" algorithms that might give a general feel, but you
typically wanting to be working more precisely than that.

Other random tips
Be as speciﬁc and concrete as possible!
This is generally the most important thing in research, made even more
important by the lack of high-quality empirical feedback in theoretical
research.
A lot of people tend to turn to real-world examples to make things more
concrete. This is sometimes good, but many real-world examples have
complicated structure that gets abstracted away. Things like "suppose that
you have pictures of animals, some of which are dogs" is worse than
"suppose that you have sequences of bits of length 1000 generated by
taking the pointwise sign of samples from a multivariate Gaussian with
mean 0 and covariance matrix Σ. Call a sequence a "dog" if the ﬁrst 50
bits are 1."
You want to bear in mind the intuitive connection your example has
to the real world, but the actual example you're working with should
be as precisely speciﬁed as possible.
Have courage
If ﬁguring out what you want, ﬁnding counterexamples, or searching for
algorithms leads you to a tedious and diﬃcult place, oftentimes you want
to just follow through and do it.
Halmos writes: "Another notable and enviable trait of von Neumann's was
his mathematical courage. If, in the middle of a search for a
counterexample, an inﬁnite series came up, with a lot of exponentials that
had quadratic exponents, many mathematicians would start with a clean
sheet of paper and look for another counterexample. Not Johnny! When
that happened to him, he cheerfully said: "Oh, yes, a theta function...", and
plowed ahead with the mountainous computations. He wasn't afraid of
anything."
Before doing computations/considering examples, ask "what do I hope to learn?"
Generally, unguided exploration is seldom that useful. Maintaining a sharp
sense of what you're hoping to get out of what you're doing is important in
cutting oﬀ research avenues that are fun to think about, but ultimately not
that productive.
Even if you're "trying to understand what's going on," you should be trying
to focus on a speciﬁc algorithm/example that you don't understand, have a
clear goal that understanding will help you achieve, etc. Avoid things that
smell like philosophy at great cost (but not all cost, for sometimes it will be
necessary).
If there are multiple "next-steps", breadth-ﬁrst search is typically better.
There's often cross-pollination between diﬀerent research directions.
Breadth-ﬁrst prevents you from being stuck in a dead-end for too long,
assuming one of the steps is a good step forward.
You should be making progress
On timescales of days and weeks, you should be able to point to concrete
examples/algorithms that constitute "units of progress" towards your ﬁnal
goal. Don't be content with a sense that you understand more of what's
going on that isn't grounded in concrete algorithms or examples.
You generally want to be in "no holds barred" optimization world
That is, you generally want to know what type of object your searching for
(e.g. have a precise sense of what counts as an
algorithm/counterexample), and a set of formal desiderata for that object,

such that you can just optimize for ﬁnding an object that meets those
desiderata.
A common example of such an object is a proof of a particular theorem
statement.

Meditation course claims 65%
enlightenment rate: my review
TL;DR
I eliminated my impostor syndrome and dramatically reduced my work-related
anxiety. I did this in a way that I think can be replicated. 
Diﬀerent techniques work for diﬀerent people. If you want to get the beneﬁts of
meditation, you should experiment widely, then drill down on the methods that
work for you. Don't just try the same technique for months or years and hope
you'll eventually "get it" or give up and say meditation doesn't work for you.
Explore then exploit. 
Loving-kindness meditation is underrated and should be the main-course
meditation for a lot of people. 
This 55-minute video is the 80/20 of the course. If you like it, you will probably
like the rest of the course. 
I recommend the Finder's Course for most people. If you follow the instructions,
you will very likely become happier. If you prefer and are good at self-directed
learning, you can do your own self-directed course and get similar beneﬁts. 
 
What makes the Finder's Course
diﬀerent: methodology
Applying science to meditation isn't unique to Jeﬀery Martin's course. Fortunately for
the world, there's a whole movement around this. That being said, I haven't heard of
anything that seems more likely to ﬁgure out how to actually achieve enlightenment
(or fundamental well-being (FWB) as he calls it, which I prefer). 
Most science I know of is doing things like putting meditators in brain scans and
seeing if anything is diﬀerent from regular brains, or running RCTs to see if meditation
makes you happier. This is foundational and important to do. However, it's very black
box thinking and doesn't give you any gears-level understanding of how to achieve
fundamental well-being. 
Meditation classes usually teach a variety of diﬀerent techniques. Which techniques
are causing the change? Most studies focus on averages, which ignores the thing
we're most interested in - those outliers who don't just start feeling less stressed but
have eliminated suﬀering. Who are living in states of profound bliss and serenity. How
does that show up on a psychological item asking "On a scale of 1 to 10, how satisﬁed
are you with your life?"?
The Finder's Course on the other hand clearly followed a methodology that was truly
trying to solve the problem. The way he did this was to ﬁnd over 1,000 people saying
that they had achieved fundamental well-being, and he went and interviewed all of
them. The interviews would often last up to twelve hours. He asked them what their

experiences were, what had gotten them there, and ran them through batteries of
psychological tests. 
From this exploratory research, he started pulling out patterns. You can read some of
the results of his research in his book. He took the top ﬁndings out of his research and
turned it into a course. It was formerly called the Finder's Course (a play on the usual
spiritual terminology of people being "seekers").
He continues to do science on the course, and this is part of what most intrigued me.
It's mandatory for the course to take a whole battery of psychological evaluations
before and after, such as PERMA, satisfaction with life scale, CES-D Questionnaire, etc.
After doing a week of each technique, he also does a shorter survey. 
The results from this he claims are that 65% of people who ﬁnish the course achieve
fundamental well-being. This is an incredible claim, and I ﬁgured it was probably just
hype. Then I spoke to a friend he said that he'd recently done the course and could
now go into fundamental well-being at will. This is what inspired me to give it a go. 
Before I started the course, I publicly pre-committed to writing about my experience,
regardless of how it went. Usually, people only write about something if it goes
particularly well or poorly, and I wanted to help even that out. 
So, without further ado, here's my review of the course.
What's in the course and how it works
Experiment with diﬀerent techniques to ﬁnd
your meditation "ﬁt"
The main thing that I liked about the course was its underlying strategy: 
1. Experimenting widely. They got you to experiment with a wide range of
meditation techniques, all for the goal of step 2: 
2. Finding your ﬁt so you can ﬁnd one that works for you. 
Explore ﬁrst, then exploit. Don't assume that there is The One True Meditation
Technique. 
What Jeﬀery found was that people who'd achieved fundamental well-being hadn't
just used one technique to get there. Some people got there mostly through
concentration practice, others through insight practice, others still through headless
way techniques, and so on. 
He explains his thinking and guides you through a few techniques in this 55-minute
video. I'd watch it and if you like it, take the course. This is extremely representative
of how the course is. 
An aside on why if you hate concentration
practice, you might enjoy mantra meditation

Interestingly, this video might have had as much impact on me as the whole rest of
the 6-week course. I'd always struggled with concentration practice, especially on the
breath. It had never really done anything for me except cause boredom and
frustration. I'd had more or less the same experience with other meditation objects
such as listening to the waves or focusing on other parts of the body or a candle. 
When I tried the mantra meditation he describes in the video, it led to massive
breakthroughs in my concentration practice. It went from a mostly boring trial of
willpower to one of the most reliable techniques in my happiness workout routine.
About 85% of my concentration practice sessions now involve intense joy, with about
40% of the session being spent in joy. 
This is for a few reasons. 
 
You can't not "hear" the mantra. 
The breath has always been the bane of my meditation existence. Whenever I
tried focusing on it, my breathing would get really shallow and soft, making it really
hard to tell what was happening there. I was constantly asking myself "Is this the
breath?". It's really hard to concentrate on something that's so subtle. 
The mantra, on the other hand, is under your control. You can make it as "loud" or as
"quiet" as you want. You can slow it down so that you only move on to the next word
after you've fully paid attention to the current one. You could even say it out loud if
you want. I personally ﬁnd that if I'm starting to get way more distracted, I can just
start "shouting" it in my head, and it makes it vastly easier to focus on it. 
Teachers often say that the breath being subtle is a pro because it forces you to beef
up your attention muscles. I think this might be true if you're already at a certain level
of concentration practice, but for me and I suspect many others, it's too hard too fast.
It would be like trying to run a marathon on your ﬁrst day of taking up jogging after
being a couch potato your entire life. You've got to build up to it. 
 
It's easier to focus on things that are pleasant
Some meditation teachers talk about how calming the breath is. I'm sure that's true
for a lot of people. But for me most of the time it's pretty darn neutral. Or it gets weird
when I'm having interesting meditative experiences, which then takes me out of the
experience. 
The mantra on the other hand is deliberately and consistently pleasant. And that's just
so much easier to pay attention to! It's so much easier to get yourself to do enjoyable
things, and if your mantra is about being grateful for your life, that's something that
will be easier to get your brain on board with. 
Naturally, this only works for mantras that are pleasant. That's one of the reasons I
recommend against mantras that aren't in your native tongue and that you didn't
choose yourself.

 
The mantra is your thoughts
The main distraction in concentration practice is your thoughts. However, if your
meditation object is a mantra, then that is a large part of your thoughts. Your thoughts
consist of other things too, like images and urges, but a huge part of many (but not
all!) people's thoughts are verbal commentary. So if your meditation object is the
internal narrative, it's easier to not get distracted and more clearly see when it goes
away. 
I highly recommend trying it out. And just experimenting widely with a bunch of
diﬀerent meditation objects. The general principle of experimenting widely and then
drilling down on the ones you ﬁnd suit you best seems really strong. 
Some potential meditation objects that aren't just the breath:
Sound of waves. Just ﬁnd a track on Youtube
Bells. Just ﬁnd a track on Youtube
A mantra. Two that have worked well for me
"I thank the universe for my life"
Thich Nhat Hanh, synced to two cycles of breath:
In breath: "Breathing in, I calm my body and mind."
Out breath: "Breathing out, I smile."
In breath: "Dwelling in the present moment"
Out breath: "I know this is the only moment." 
A metronome. This works particularly well because you can track your
"resolution" of moments of awareness per second, and slowly increase it. 
Structure and techniques covered in
the course
Techniques covered
Body scan
Concentration practice
Loving-kindness meditation
Noting
Noting gone
Group awareness exercise
Gratitude practice
Sending a letter of gratitude
Setting intentions / making "wishes"
Writing what you want people to say at your funeral
Positive visualizations
Cancel, cancel
One-oﬀ acts of kindness
Forgiveness practice
Headless way practices
Unprovoked happiness

The techniques are taught via pre-recorded video. Throughout the course, you're
supposed to meditate one hour a day combined with short ~3-minute exercises you
do when you ﬁrst wake up and when you go to bed. There's no enforcement aside
from them asking once a week in a survey which days you meditated. I'd highly
recommend doing the course with a friend or partner to help you stick to it. 
The course was $249 last I checked. It's a bit pricey for an online course, but the price
deﬁnitely makes you more motivated to ﬁnish it.  
Results: ambiguous data and
imposter syndrome cured
So, I took the course. Am I enlightened now? Have I achieved fundamental well-
being? 
Unfortunately, no. 
Did I improve my happiness? The results are ambiguous.
I've been tracking my emotional well-being and other metrics for the last seven years,
and there doesn't seem to be any noticeable change before and after the meditation
course beyond noise. 
On the other hand, my emotion tracking is not very sensitive to changes. One of the
hardest periods of my life only showed a 0.6 change on a scale of 1 to 10. This was
mostly because my coping mechanisms were so good that they masked my misery. 
On the whole, I'm inclined to not be too concerned about the numbers, because the
real headliner of the whole thing is that it probably cured me of my impostor
syndrome, reduced my anxiety around work massively, and just made me a much
more conﬁdent person. 
 
How I got rid of impostor syndrome with
loving-kindness meditation
I was a conﬁdent kid, then I worked at 80,000 Hours back in 2013, and living in Oxford
gave me impostor syndrome. Turns out that basing your conﬁdence on being a big ﬁsh
in a small pond only works if you stay in a small pond. 
Between 2013 and 2022 I had more or less chronic low conﬁdence and high anxiety
around work. I tried everything to get rid of it: concentration practice, CBT, IFS,
emotional coherence therapy, exposure therapy, ACT, talk therapy, and just plain old
reason. Nothing made a dent. 
Then the ﬁrst week of the Finder's Course was doing an hour a day of loving-kindness
meditation. I've done a bunch of loving-kindness meditation in the past, but it was
always more of a "dessert" technique. After I'd had my vegetables of concentration
practice, I could treat myself with 2-5 minutes of loving-kindness at the end. It was
never the main course. 

This ﬁrst week though was one hour a day on just loving-kindness. 
It was amazing and profoundly changing.  
There are a lot of diﬀerent loving-kindness techniques. The one that I was doing was
the one where you:
1. Think of something that easily makes you feel loving-kindness. Often a
beloved pet or good friend. Pick somebody uncomplicated. Build up that emotion
for a bit, really building it up in your mind. Think of scenes that are particularly
compelling. Maybe try saying in your head, "May you be happy. May you be free
from suﬀering." or anything else that resonates with you. 
2. Maintain that feeling and think of something harder to feel loving-
kindness towards. Build up, like weight lifting. Start with somebody maximally
easy, then pick somebody slightly harder, then slightly harder than that, etc. 
3. If at any point you lose the feeling, go back to the easier level. Re-
establish the feeling of loving-kindness, then start ramping up again.
At ﬁrst, I focused on feeling loving-kindness toward others. That's always been pretty
easy for me. It was lots of fun, just radiating love towards people in my life and the
world. 
Then, inspired by this amazing post by Charlie Rogers-Smith on self-love, I tried
turning it on myself. What if I tried radiating loving-kindness towards myself instead of
others?
Immediately, a wall slammed down in my mind. 
"No," a part of me said. "You deﬁnitely cannot love yourself."
I immediately burst into tears. 
So, you know, a perfectly healthy reaction.
While this session wasn't exactly the most enjoyable one I've ever had, it was possibly
the most fruitful. After a brief temptation to avoid such negative feelings, I realized
that this was the clearest signal I was ever going to get to dig deeper. 
I decided to dedicate the week to loving myself in particular. I started with an easy
source of loving-kindness, then tried to ﬁnd things about myself that were easier. 
At ﬁrst, everything was hard. 
Could I love myself when I was doing good things? No. 
OK, how about bad things? Heck no! 
Maybe I could imagine liking myself when I was a kid? Still a no. 
OK, what about as a baby? Surely there can't be any reason to not love myself as a
baby! 
Nope. I was a pudgy baby. And clearly you can't love pudgy babies.
Eventually, I found my "in". I could imagine being my mom, holding my baby-self, and
feeling how she felt about me. Even if I had a lot of self-dislike, I am extremely

conﬁdent my mom loved my baby-self. 
From there, I was able to build up. I realized it was easier to love myself when I
imagined previous times in my life when I was suﬀering. From there I felt love for
myself in particularly potent scenes from my childhood.
After a while, it was easy. I then started applying the same strategy, but towards
feeling conﬁdent directly. So establish the feeling of conﬁdence by imaging a scene
that brings it up easily, then start working towards harder and harder scenarios. 
There's more to it than that, but this is already becoming a rather long post. I think it
would be useful to have a fully written up explanation of my techniques for
anxiety/conﬁdence, so I'll write that up in a separate essay. If you want to hear about
it when it comes out, just follow me on Twitter, my personal blog, or set the forum to
notify you when I post next. 
Overall though, it's been about six months since the shift, and it's felt remarkably
stable. There have been a couple of times where I've reverted to beating myself up
again, but it's lasted max a couple of hours. This is especially amazing because I
haven't done the meditation regularly for ﬁve months now, and I've never had a
stretch of conﬁdence this long in the last ten years. 
Of note though, while I was practicing regularly my conﬁdence was about a 9/10, and
since not practicing regularly it's stabilized at around a 6.5/10. This is still amazing
though compared to my 3/10 conﬁdence previously.
And a friend of mine went through the course with me and she hated the loving-
kindness meditation week. So I already know this won't work for everybody. 
However, this has aﬀected the quality of my life and my work so much that I think
most people should give it a shot. I feel way more resilient. I feel far less anxiety. I feel
more motivated to work on AI alignment. I'm far better at receiving feedback because
it doesn't threaten my self-worth. I feel more secure with my friends. I enjoy my work
more because I'm not constantly beating myself up. 
Try one hour a day for a week and see how you feel. 
 
Things I liked and disliked about the
course
Spiritual blue balls
The lovingkindness meditation week was one of the happiest of my life. But then it
was the next week and it was a meditation that was most deﬁnitely not a ﬁt. It
followed this pattern, where I'd be super excited about a new technique, then I'd have
to put it on hold until the end of the course. By the end of the course, I was so glad it
was done so I could just focus on the ones that were working for me. 

Of course, I don't think this is a ﬂaw really. This is to be expected if only some
techniques ﬁt you. This would have mostly been ﬁne if it hadn't been for-
 
The weird obsession with the nose
Following the ﬁrst week was around three weeks of concentration and noting practice
that nearly destroyed me. It was your typical concentrate on the breath practice, but
they ﬁrmly insisted you could only pay attention to the nose. Most teachers usually let
you choose between the chest and nose, wherever you feel it the most clearly. 
But not this class. 
In this class, it was nose or bust.
The problem was - I couldn't feel my nostrils at all. 
In the lecture, he said that everybody had sensations in their nostrils unless there was
something really wrong with them. I mean, if you shoved a chili pepper up your nose,
surely you'd feel it? 
Sure, I'd feel that. But that's a far cry from sitting still and having my breath get softer
every minute. 
I asked if I could focus on my chest since that was so much clearer to me and they
said no. That I should just keep focusing on the nose area and usually feelings will
start coming up, and if there wasn't anything, just to focus on there being nothing.
Focus on nothing. . . 
This worked so poorly that it has become a joke in my house about me ranting about
the Nostril Weeks. 
It lasted about three weeks, and I did an hour a day, even when I got covid part way
through. Funnily enough, covid actually made it better because then I could feel my
nostrils because my nasal passages were so blocked. 
It was torture. Eventually, I could feel the slightest of sensations on the out-breath,
but it was so subtle it could have just been my imagination. In the end, I periodically
added a mint-scented medicine to my nose so I could feel something there, but then it
was mostly slight pain, which is a diﬃcult meditation object. 
Why did they insist on the nostril? I don't know for sure and they didn't answer when I
asked why, but I have a few hypotheses for their reasoning. One is that they wanted
you to focus on a really narrow patch of sensations. This would help you achieve more
genuine single-pointedness. The other is that they were giving pre-recorded video
lectures, and it added complexity to the instructions to allow for multiple meditation
objects. Finally, maybe there's something special about the nostril that leads to more
enlightenment that's hard to put into words.
In the end, that was just a really diﬃcult three weeks, and I wouldn't recommend it. I
think if I had just switched to doing concentration and noting practice on a mantra or
my chest, that would have been way better. If you have the same issues, I'd just give

the nostrils a couple of days, to see if maybe you can sensitize the area, and if not,
switch to a meditation object that's a better ﬁt for you.
 
His epistemology wasn't the worst, but also wasn't the
best
If you are allergic to non-rigorous epistemology, I would stick to:
Sam Harris's Waking Up app
The Mind Illuminated
Joy on Demand
The Mindfulness-Based Stress Reduction crowd
I haven't looked into his studies' methodologies, but from my experience with them, I
would put high odds that the 65% number is exaggerated. For instance, there was
deﬁnitely a lot of social desirability bias at play. There also wasn't a follow-up survey
to see if the eﬀects lasted. And the usual methodological problems that plague
psychological studies, such as endless room for p-hacking.
However, I think you'll be seriously missing out on a lot of really important
psychological wisdom if you can't learn from people who have sub-optimal
epistemologies. Just because somebody believes a lot of things that are wrong doesn't
mean that everything they believe is wrong. 
To be fair, I don't think his epistemology is bad for a meditation teacher. In fact, it's far
above average. However, he deﬁnitely made some very bad arguments in a
substantial fraction of his videos, and his surveys imply that he believes in meditating
leads to good things happening to you (i.e. law of attraction). Just wanted to ﬂag it for
the people who are more sensitive to that sort of thing. 
 
Publicly promising to write about it as a commitment and
learning device
This isn't part of the course, but it's something I wanted to share because it worked so
well for me. Before I started the course, I committed to writing about my experiences,
and this deﬁnitely made the course better for me. Knowing that I'd said it publicly
made me stick with it, even when it was really hard. It also made me learn better
because I knew that I would have to explain it to people afterward. I highly
recommend it.
 
The polish of the course could be improved a lot
Right now a huge drawback to the course is that it looks a little scammy and
unprofessional. I think if they invested in improving their website that would go a long
way toward improving their oﬀering. 

 
The importance of hour-long sessions
I've always been averse to doing longer meditation sessions. Giving up even half an
hour of my day feels like such a sacriﬁce. When Jeﬀery said that it was imperative to
do an uninterrupted hour a day, I really didn't want to believe him. 
And yet, I put at least a 20% chance that I would not have ﬁxed my impostor
syndrome if I hadn't done hour-long sessions. My initial insight into my lack of self-love
was 45 minutes into a session. I think part of the reason I was able to avoid noticing
this side of myself for so long was that I just looked away if I got too close to it. 
I have also noticed that I haven't hit diminishing returns for meditating yet. I've
regularly practiced anywhere between 10 minutes and 2 hours per day, and with 10
minutes a day, I barely notice a diﬀerence, whereas when I do 2 hours regularly, it
feels as if I'm ﬂoating on a cloud of joy all day. 
Of course, 2 hours is a lot and I'm not sure it's worth the trade-oﬀ compared to
counterfactual uses of time. However, I think it's correct to model meditation like
exercise: doing even a small amount can help, but it's hard for most people to do it
too much. 
 
Diﬀerent meditation techniques do indeed work diﬀerently
for diﬀerent people
My favorite week by far was the loving-kindness week. For my friend, it was the worst.
I mostly felt nothing for the body scan week, and my friend practically got drunk on it,
feeling super giggly after each round. 
It might seem obvious in retrospect, but I feel like this is a really important thing to
both acknowledge and work with. This is by far my favorite part of the course. If you
do the body scan and aren't feeling anything after a week, just move on. Life is too
short to waste on techniques that aren't doing anything for you. 
The Buddha is famous for saying to not take his word for anything but to try it for
yourself and see if it works. People say that, but then don't walk the walk. Jeﬀery
Martin does, and I love that about the course.
 
Do I recommend it? 
Overall, yes. I don't know of any courses that have as good a chance of leading to
substantial increases in your well-being. 
If you're excellent at self-directed learning, I think you can get most of the beneﬁts
outside of the course, or maybe make it even better. Simply commit to doing each of
the techniques for an hour a day for a week. I'd consider adding other techniques too,
such as internal family systems, cognitive behavioral therapy, journaling, and other

techniques you've tried in the past with some success or you think might work well for
you. If you don't have the commitment device of paying a large sum of money for it,
I'd add some alternatives. Making a public commitment on social media and/or doing
it with a friend or two would probably do it. 
Better yet, pre-commit to writing your own review of it. I'd be really interested in
hearing about other EAs' and rationalists' experiences with the course, and it's an
amazing commitment device. 
 

AI strategy nearcasting
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
This is the ﬁrst in a series of pieces taking a stab at dealing with a conundrum:
I believe this could be the most important century ever for humanity, via the
development of advanced AI systems that could dramatically speed up scientiﬁc
and technological advancement, getting us more quickly than most people
imagine to a deeply unfamiliar future.
But when it comes to what actions we can take to help such a development go
well instead of poorly, it's hard to say much (with a few exceptions). This is
because many actions that would be helpful under one theory of how
things will play out would be harmful under another (for example, see my
discussion of the "caution" frame vs. the "competition" frame).
It seems to me that in order to more productively take actions (including making more
grants), we need to get more clarity on some crucial questions such as "How serious is
the threat of a world run by misaligned AI?" But it's hard to answer questions like this,
when we're talking about a development (transformative AI) that may take place
some indeterminate number of decades from now.
This piece introduces one possible framework for dealing with this conundrum. The
framework is AI strategy nearcasting: trying to answer key strategic
questions about transformative AI, under the assumption that key events
(e.g., the development of transformative AI) will happen in a world that is
otherwise relatively similar to today's. One (but not the only) version of this
assumption would be "Transformative AI will be developed soon, using methods like
what AI labs focus on today."
The term is inspired by nowcasting. For example, the FiveThirtyEight Now-Cast
projects "who would win the election if it were held today," which is easier than
projecting who will win the election when it is actually held. I think imagining
transformative AI being developed today is a bit much, but "in a world otherwise
relatively similar to today's" seems worth grappling with.
Some potential beneﬁts of nearcasting, and
reservations
A few beneﬁts of nearcasting (all of which are speculative):
As with nowcasting, nearcasting can serve as a jumping-oﬀ point. If we have an
idea of what the best actions to take would be if transformative AI were developed in
a world otherwise similar to today's, we can then start asking "Are there particular
ways in which we expect the future to be diﬀerent from the nearer term, that should
change our picture of which actions would be most helpful?"
Nearcasting can also focus our attention on the scenarios that are not only easiest
to imagine concretely, but also arguably "highest-stakes." Worlds in which
transformative AI is developed especially soon are worlds in which the "nearcasting"

assumptions are especially likely to hold - and these are also worlds in which we will
have especially little time to react to crucial developments as they unfold. They are
thus worlds in which it will be especially valuable to have thought matters through in
advance. (They are also likely to be worlds in which transformative AI most "takes the
world by surprise," such that the eﬀorts of people paying attention today are most
likely to be disproportionately helpful.)
Nearcasting might give us a sort of feedback loop for learning: if we do
nearcasting now and in the future, we can see how the conclusions (in terms
of which actions would be most helpful today) change over time, and perhaps learn
something from this.
I could easily imagine that we will learn more from repeated attempts to ask
"What should we do if transformative AI is just around the corner?", whose
conclusions will change as the world changes, than from a persistent eﬀort to
reﬁne our answer to "What should we do under our best-guess picture of the
world, in which transformative AI is (for example) 30 years away from today?"
To be clear, I don't think this is a "great" or even "good" feedback loop for
learning, but in the space of transformative AI strategy, any such feedback loops
are hard to come by, so I see some appeal here.
A major reservation about nearcasting (as an activity in general, not relative to
forecasting) is that people are arguably quite bad at reasoning about future
hypothetical scenarios,1 and most of the most impressive human knowledge to date
arguably has relied heavily on empirical observations and experimentation. AI
strategy nearcasting seems destined to be vastly inferior to e.g. good natural sciences
work, in terms of how much we can trust its conclusions.
I think this reservation is valid as stated, and I expect any nearcast to look pretty silly
in at least some respects with the beneﬁt of hindsight. But I still think nearcasting
(and, more generally, analyzing hypothetical future scenarios with transformative AI2)
is probably under-invested in today:
The potential stakes are enormous. Even a small amount of additional clarity
about how to make the best of the most important century would be immensely
valuable.
I don't think the case for "people are bad at reasoning about hypothetical future
scenarios" or "people are bad at reasoning about the long-run future" is all that
strong, (more). Moreover (as also noted at that link), past attempts at futurism
didn't make use of many of what appear to be "forecasting best practices"
today, such as assigning quantitative probabilities to predictions. The science of
forecasting is young, and there's probably a lot of room for improvement on past
attempts.
Finally, I am broadly against putting too much weight on "the track
record of a particular kind of thinking," mostly because I think such track
records tend to be inconclusive.
I think that for almost any rules you can come up with about "what good
intellectual inquiry looks like," there's been some good intellectual inquiry
that breaks those rules.
I also think that (partly due to low-hanging-fruit dynamics) a lot of the
most valuable innovations have historically come from something more
like "Work on a very neglected question and try to 'just be reasonable,'
without sweating whether one's methodology has a good track record"

than like "Use a super-established methodology that looks like how past
innovations have happened." (More)
I think part of the challenge of this kind of work is having reasonable judgment about
which aspects of a hypothetical scenario are too speciﬁc to place big bets on, vs.
which aspects represent relatively robust themes that would apply to many possible
futures.
For example, I think it is reasonable to predict that the global average
temperature will rise over the coming several decades, even as it is very diﬃcult
to predict whether it will be raining on some day even a few weeks away.
I have some views on what sorts of aspects of the future I'd generally expect to
be tractable vs. intractable to predict,3 but I make no claim to have well-
grounded takes here. I think that if there were more eﬀorts at analyzing
hypothetical future scenarios, we'd probably collectively learn more about such
things.
The nearcast I'll be discussing
Starting here, and continuing into future pieces, I'm going to lay out a "nearcast" that
shares many (not all) assumptions with this piece by Ajeya Cotra (which I would highly
recommend reading in full if you're interested in the rest of this series), hereafter
abbreviated as "Takeover Analysis."
I will generally use the present tense ("in my scenario, the alignment problem is
diﬃcult") when describing a nearcasting scenario, and since the whole story should be
taken in the spirit of speculation, I'm going to give fewer caveats than I ordinarily
would - I will often say something like "the alignment problem is diﬃcult" when I
mean something like "the alignment problem is diﬃcult in my most easily-accessible
picture of how things would go" (not something like "I know with conﬁdence that the
alignment problem will be diﬃcult").
The key properties of the scenario I'll be considering are:
1. Magma. For concreteness, we're focused on a particular AI project or company,
which I'll call Magma (following Takeover Analysis).
2. Transformative AI is knowably near, but not yet here. Magma's leadership
has good reason to believe that it can develop something like PASTA soon - say,
within a year. (I also assume that it hasn't unwittingly done so yet!)
3. Human feedback on diverse tasks. The path to transformative AI revolves
around what Takeover Analysis calls "human feedback on diverse tasks" (HFDT).
You can think of this roughly as if (to oversimplify) we trained an AI by giving it a
thumbs up when we approved of what it had done, and a thumbs down when we
disapproved, and repeated this enough times that it was able to become very
good at behaving in ways that would get a thumbs up, on a broad set of diverse
tasks.
4. Some (but not unlimited) deliberate delay is possible. Magma has some -
but not unlimited - ability to delay development and/or deployment in order to
take "extra" measures to reduce the risk of misaligned AI.
1. That is, it's not the case that "any delay in deploying a given type of AI
system will simply mean a competitor does so instead."
2. This could be because Magma has signiﬁcant advantages over its
competitors (funding, algorithms, talent, etc.) and/or because its closest

competitors have a similar level of caution to Magma.
3. I'll be imagining that Magma can reasonably expect to deploy a given AI
system 6-24 months later than it would otherwise for purposes of safety,
without worrying that a competitor would deploy a similar (less safe)
system in that time.
5. Otherwise, changes compared to today's world are fairly minimal.
1. Deep learning has evolved in many ways - for example, there might be
new architectures that improve on the Transformer - but the fundamental
paradigm is still basically that of models gaining capabilities via "trial-and-
error."4 More on this at Takeover Analysis.
2. We still can't say much about what's going on inside a given AI model
(most of what we know is that it learns from trial and error on tasks, and
performs well on the tasks).
3. AI systems have advanced to the point where there are many new
commercial applications, and perhaps substantial economic impact, but we
haven't yet reached the point where AIs can fully automate scientiﬁc and
technological advancement, where economic growth is "explosive"
(greater than 30% per year, which would very roughly be 10x the current
rate of global economic growth), or where AI systems can facilitate
anything like a decisive strategic advantage.
I note that the combination of "Changes compared to today's world are fairly minimal"
with "Transformative AI is knowably near" implies a fast takeoﬀ approaching: a very
rapid transition from a world like today's to a radically diﬀerent world. This is (in my
view) a key way in which reality is likely to diﬀer from the scenario I'm describing.5
I still think this scenario is worth contemplating, for the following reasons:
I think this sort of fast takeoﬀ is easier to concretely picture than most
alternatives, precisely because it allows us to focus on a particular crucial period
of time in which most things resemble how they are today. Generally, I think it is
easiest to think about a relatively small number of relatively high-stakes
developments (e.g., "We have a very powerful AI system that we can deploy or
not") - and then generalize this sort of analysis to scenarios that may revolve
more around large numbers of individually low-stakes developments (e.g., "For
the 20th time this year, we're deciding whether to ship a system that is a small
amount more capable, and more dangerous, than the ones already out there"),
rather than starting by trying to discuss the latter.
Worlds in which transformative AI is developed sooner likely involve faster
takeoﬀs, and scenarios generally more like the above, than other worlds. And
these are also particularly "high-stakes" worlds, in the sense that thinking about
them today is particularly likely to be helpful.
After examining this scenario, we can later ask how reality might diverge from it
(as I plan to do in a future post). While I think there are likely to be important
divergences, I think this scenario gives us a lot to work with and think about
productively.
Next in series
The next piece I'd recommend reading is Takeover Analysis, which the rest of my
series builds on. It illustrates why this nearcast involves a major risk of an AI
takeover in the absence of speciﬁc preventative measures - a risk that
is non-straightforward to diagnose or correct. I recommend reading the

full post for a detailed picture of why this is. (A footnote addresses how this
analysis diﬀers from previous "alignment problem" discussions.6)
I will soon put up a piece laying out my understanding of what the best available
measures would be for Magma to reduce the risk of misalignment in powerful
systems it's developing, and of how likely these measures would be to work.
After that, I will go beyond questions about the "alignment problem" (what
technical measures can reduce risk of misalignment) and discuss the
"deployment problem": the question of what Magma (and a hypothetical
agency, IAIA) should be seeking to use aligned AI systems to do (and
whether/when to deploy them), under conditions of uncertainty about how safe
they are and how close others are to deploying powerful AI of their own.
I'll discuss which actions (including, but not limited to, AI alignment research)
seem most helpful today if we expect the nearcast discussed here, and how we
might expect matters to look systematically diﬀerent if we relax the "nearcast"
condition (imagining that transformative AI will take longer to develop, and be
less likely to simply come from current techniques, than in this scenario).
Footnotes
1. Though this is something I hear claimed a lot without necessarily much
evidence; see my discussion of the track record of futurists here. ↩
2. E.g., see this piece, this piece and Age of Em ↩
3. For example, I generally feel better about predictions about technological
developments (what will be possible at a particular price) than predictions about
what products will be widely used, how lifestyles will change, etc.
Because of this, I think "Extreme technology X will be available, and it seems
clear and obvious that the minimum economic consequences of this technology
or any technology that can do similar things would be enormous" is in many
ways a better form of prediction than "Moderate technology Y will be available,
and it will be superior enough to its alternatives that it will be in wide use." This
is some of why I tend to feel better about predictions about transformative AI
(which I'd say are going out on a massive limb re: what will be possible, but less
of one re: how signiﬁcant such a thing would be) than about predictions of self-
driving cars (which run into lots of questions about how technology will interact
with the regulatory and economic environment). ↩
4. Some readers have objected to the "trial and error" term, because they interpret
it as meaning something like: "trying every possible action until something
works, with no system for 'learning' from mistakes." This is not how AI systems
are generally trained - their training uses stochastic gradient descent, in which
each "trial" comes with information about how to adjust an AI system to do
better on similar inputs in the future. But I use the "trial and error" term because
I think it is intuitive, and because I think the term generally is inclusive of this
sort of thing (e.g., I think Wikipedia's characterization of the term is quite good,
and includes cases where each "trial" comes with information about how to do
better in the future). ↩
5. To be clear, I expect that the transition to transformative AI will be much faster
than the vast majority of people imagine, but I don't expect it will be quite as

fast as implied here. ↩
6. To my knowledge, most existing discussion of the alignment problem - see links
at the bottom of this section for examples - focuses on abstract discussions of
some of the challenges presented by aligning any AI system (things like "It's
hard to formalize the values we most care about"). Takeover Analysis instead
takes a nearcasting frame, and walks mechanically through what it might look
like to develop powerful AI with today's methods. It discusses mechanically how
this could lead to an AI takeover - as well as why the most obvious methods for
diagnosing and preventing this problem seem like they wouldn't work. ↩

Paper is published! 100,000 lumens to
treat seasonal aﬀective disorder
This is a linkpost for https://www.lesswrong.com/posts/qEweugBipR5P2cMyK/preprint-
is-out-100-000-lumens-to-treat-seasonal-aﬀective
Let's give people with winter depression (seasonal aﬀective disorder, SAD) LOTS OF
LIGHT and see what happens!
This is an update to my previous post that said that our preprint is out. :) The paper
was accepted with very minor changes, so you might already know what it says if you
saw the ﬁrst post.
Here is the link to the paper: https://onlinelibrary.wiley.com/doi/10.1002/da.23281
Here is the twitter thread explaining the paper:
https://twitter.com/FabienneSand/status/1561060644803252224 
Again, Jan Brauner and I are very thankful to the LessWrong/EA communities, which
have inspired this ﬁrst study (there will be more) and through which we have found
funding. In particular, thank you Eliezer Yudkowsky for helping us ﬁnd funding and for
inspiring the study with Inadequate Equilibria, David Chapman for inspiring us with
these two posts in the Meaningness blog, Raemon for inspiring us with this LessWrong
post and everyone who discussed with us setups they have tried. <3

Preprint is out! 100,000 lumens to
treat seasonal aﬀective disorder
Let's give people with winter depression (seasonal aﬀective disorder, SAD) LOTS OF
LIGHT and see what happens!
Our preprint is out now for our paper "100,000 lumens to treat seasonal aﬀective
disorder: A proof of concept RCT of bright, whole-room, all-day (BROAD) light
therapy"! We have sent it to a number of professors working in that area and have
received very encouraging and helpful feedback, which has made me even more
excited about continuing this research.
→ Paper: https://medrxiv.org/cgi/content/short/2021.10.29.21265530v1, short
summary Twitter thread:
https://twitter.com/FabienneSand/status/1457745472773296128 
 
Jan Brauner and I are very thankful to the LessWrong/EA communities, which have
inspired this ﬁrst study (there will be more) and through which we have found funding.
In particular, thank you Eliezer Yudkowsky for helping us ﬁnd funding and for inspiring
the study with Inadequate Equilibria, David Chapman for inspiring us with these two
posts in the Meaningness blog, Raemon for inspiring us with this LessWrong post and
everyone who discussed with us setups they have tried. <3

Preprint is out! 100,000 lumens to
treat seasonal aﬀective disorder
Let's give people with winter depression (seasonal aﬀective disorder, SAD) LOTS OF
LIGHT and see what happens!
Our preprint is out now for our paper "100,000 lumens to treat seasonal aﬀective
disorder: A proof of concept RCT of bright, whole-room, all-day (BROAD) light
therapy"! We have sent it to a number of professors working in that area and have
received very encouraging and helpful feedback, which has made me even more
excited about continuing this research.
→ Paper: https://medrxiv.org/cgi/content/short/2021.10.29.21265530v1, short
summary Twitter thread:
https://twitter.com/FabienneSand/status/1457745472773296128 
 
Jan Brauner and I are very thankful to the LessWrong/EA communities, which have
inspired this ﬁrst study (there will be more) and through which we have found funding.
In particular, thank you Eliezer Yudkowsky for helping us ﬁnd funding and for inspiring
the study with Inadequate Equilibria, David Chapman for inspiring us with these two
posts in the Meaningness blog, Raemon for inspiring us with this LessWrong post and
everyone who discussed with us setups they have tried. <3

How likely is deceptive alignment?
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
The following is an edited transcript of a talk I gave. I have given this talk at multiple
places, including ﬁrst at Anthropic and then for ELK winners and at Redwood Research,
though the version that this document is based on is the version I gave to SERI MATS
fellows. Thanks to Jonathan Ng, Ryan Kidd, and others for help transcribing that talk.
Substantial edits were done on top of the transcription by me. Though all slides are
embedded below, the full slide deck is also available here.
Today I'm going to be talking about deceptive alignment. Deceptive alignment is
something I'm very concerned about and is where I think most of the existential risk
from AI comes from. And I'm going to try to make the case for why I think that this is
the default outcome of machine learning.
First of all, what am I talking about? I want to disambiguate between two closely
related, but distinct concepts. The ﬁrst concept is dishonesty. This is something that
many people are concerned about in models, you could have a model and that model
lies to you, it knows one thing, but actually, the thing it tells you is diﬀerent from that.
So this happens all the time with current language models—we can, for example, ask
them to write the correct implementation of some function. But if they've seen humans
make some particular bug over and over again, then even if in some sense it knows
how to write the right function, it's going to reproduce that bug. And so this is an
example of a situation where the model knows how to solve something and
nevertheless lies to you. This is not what I'm talking about. This is a distinct failure
mode. The thing that I want to talk about is deceptive alignment which is, in some
sense, a subset of dishonesty, but it's a very particular situation.

So deceptive alignment is a situation where the reason that your model looks aligned
on the training data is because it is actively trying to look aligned for instrumental
reasons, which is very distinct. This is a situation where the thing that is causing your
model to have good performance is because it is trying to game the training data, it
actively has a reason that it wants to stick around in training. And so it's trying to get
good performance in training for the purpose of sticking around.
Ajeya Cotra has a really good analogy here that I think is helpful for understanding the
diﬀerence between these two classes. So you can imagine that you are a child and

you've inherited a massive business. And you have to determine who's going to run the
business for you. There's a bunch of candidates that you're trying to evaluate. And
those candidates fall into three categories. You have the saints, which are people that
really just want to help you, run things eﬀectively, and accomplish what you want. You
have the sycophants, which want to make you happy, satisfy the letter of your
instructions, make it so that the business looks like it's doing well from your
perspective, but don't actually want to fundamentally help you. And you have the
schemers, people who want to use the control of the business for their own purposes,
and are only trying to get control of it and pretend that they're doing the right thing, so
that they can eventually get something later. For our purposes, we're concerned
primarily with the schemers and that is the deceptive alignment category.
So I would say in this situation that the sycophants are examples of dishonesty where
they would say a bunch of false facts to you about what was happening to convince
you that things were going well, but they don't have some ulterior motive. The
schemers, they have some ulterior motive, they have something that they want to
accomplish. And they're actively trying to look like they're doing the right thing on
training to accomplish that. Okay, so this is what we're concerned about, we're
concerned speciﬁcally about the schemers, the deceptively aligned models, models
where the reason it was aligned is because it's trying to game the training signal.
Okay, so the question we want to answer is, "how likely is that in practice?" So we
have this concept of, maybe the model will try to game the training signal, maybe it
will try to pretend to do something in training so that it can eventually do something
else in the real world. But we don't know how likely that is as an actual thing that you
would end up with if you ran an actual machine learning training process.
And the problem here is that the deceptively aligned model, the model that is
pretending to do the right thing so that it can be selected by the training process, is
behaviorally indistinguishable during training from the robustly aligned model, the
saint model, the model that is actually trying to do the right thing. The deceptively
aligned model is going to look like it's actually trying to do the right thing during

training, because that's what it's trying to do. It is actively trying to look like it's doing
the right thing as much as it possibly can in training. And so in training, you cannot tell
the diﬀerence only by looking at their behavior.
And so if we want to understand which one we're going to get, we have to look at the
inductive biases of the training process. In any situation, if you're familiar with machine
learning, where we want to understand which of multiple diﬀerent possible models that
are behaviorally indistinguishable, we will get, it's a question of inductive biases. And
so Ajeya also has another good example here.
Suppose I take a model and I train it on blue shapes that look like that shape on the
left, and red shapes look like that shape on the right. And then we label these as two
diﬀerent classes. And then we move to a situation where we have the same shapes
with swapped colors. And we want to know, how is it going to generalize? And the
answer is, the machine learning model always learns to generalize based on color, but
there's two generalizations here. It could learn to generalize based on color or it could
learn to generalize based on shape. And which one we get is just a question of which
one is simpler and easier for gradient descent to implement and which one is preferred
by inductive biases, they both do equivalently well in training, but you know, one of
them consistently is always the one that gradient descent ﬁnds, which in this situation
is the color detector.
Okay, so if we want to understand how likely deceptive alignment is, we have to do this
same sort of analysis, we have to know, which one of these is going to be the one that
gradient descent is generally going to ﬁnd—when we ask it to solve some complex
task, are we going to ﬁnd the deceptive one, or are we going to ﬁnd the non-deceptive
one.

Okay, so the problem, at least from my perspective, trying to do this analysis, is that
we don't understand machine learning (ML) inductive biases very well, they're actually
really confusing. We just don't have very much information about how they operate.
And so what I'm going to do is I'm going to pick two diﬀerent stories that I think are
plausible for what ML inductive biases might look like, that are based on my view of the
current slate of empirical evidence that we have available on ML inductive biases. And
so we're going to look at the likelihood of deception under each of these two diﬀerent
scenarios independently, which just represent two diﬀerent ways that the inductive
biases of machine learning systems could work. So the ﬁrst is the high path
dependence world. And the second is the low path dependence world. So what do I
mean by that?

Okay, so ﬁrst: high path dependence. In a world of high path dependence, the idea is
that diﬀerent training runs can converge to very diﬀerent models, depending on the
particular path that you take through model space. So in the high path dependence
world, the correct way to think about the inductive biases in machine learning, is to
think: well, we have to understand particular paths that your model might take through
model space—maybe ﬁrst you might get one thing, and then you'll get the next thing
and the probability of any particular ﬁnal model is going to depend on what are the
prerequisites in terms of the internal structure that has to exist before that thing can
be implanted. How long is the path that we take to get there, how steep is it, et cetera,
et cetera?
So what is the empirical evidence for this view? Well, so I think there is some empirical
evidence that might push you in the direction of believing that high path dependence is
the right way to think about this. So some pieces of evidence. So on the right, this is
"BERTS of a feather do not generalize together," they take a bunch of ﬁne-tunings of
BERT, and they basically asked, how did these ﬁne-tunings generalize on downstream
tasks? And the answer is, sometimes they generalize extremely similarly. They all have
exactly the same performance. And sometimes they generalized totally diﬀerently, you
can take one ﬁne-tuning and another ﬁne-tuning on exactly the same data, and they
have completely diﬀerent downstream generalization performances. So how do we
explain that? Well, there must have been something that happened in the sort of
dynamics of training that was highly path dependent, where it really mattered what
particular path it took through model space to end up with these diﬀerent ﬁne-tunings
having very diﬀerent generalization performance.
This sort of path dependence is especially prevalent in RL, where you can run the exact
same setup multiple times, as in the bottom image, and sometimes you get good
performance, you learn the right thing, whereas sometimes you get terrible
performance, you don't really learn anything.
And then there is this example down here, where there's this paper that was arguing
that if you take the exact same training dynamics and you run it a bunch of times, you

can essentially pick the best one to put in your paper, you can essentially p-hack your
paper in a lot of situations because of the randomness of training dynamics and the
path dependence of each training run giving you diﬀerent generalizations. If you take
the exact same training run and run it multiple times, you'll end up with a much higher
probability of getting statistical signiﬁcance.
So this is one way to think about inductive biases, where it really matters the particular
path you take through model space, and how diﬃcult that path is.[1] And so what we
want to know is, did the path that you took through model space matter for the
functional behavior oﬀ training?
Now, in the low path dependence world, similar training processes converge to
essentially the same simple solution, regardless of early training dynamics. So the idea
here is that you can think about machine learning and deep learning as essentially
ﬁnding the simplest model that ﬁts the data. You give it a bunch of data, and it's
always going to ﬁnd the simplest way to ﬁt that data. In that situation, what matters is
the data that you gave it and some basic understanding of simplicity, a set of inductive
biases that your training process came with. And it didn't really matter very much the
particular path that you took to get to that particular point, all paths converge on
essentially the same generalization.
One way to think about this is: your model space is so high-dimensional that your
training process can essentially access the whole manifold of minimal loss solutions,
and then it just picks the one that's the simplest according to some set of inductive
biases.
Okay, so there's empirical evidence for the low path-dependence world, too. I think
there are good reasons to believe that you are in the low path dependence world.
I think a good example of this is grokking. This is a situation where we took a model,
and tried to get it to do some arithmetic task, and for a really long time it just learns a
bunch of random stuﬀ. And then eventually it converges to the exact solution. It's

always implementing the algorithm exactly correctly after a very long period. And so if
you're in this situation, it didn't really matter what was happening in this whole period
here—eventually, we converge to the precise algorithm, and it's just overdetermined
what we converge to.[2]
Other reasons, you might think this, so this is from "Neural Networks are
Fundamentally Bayesian", which is the Mingard et al. line of work. What they do is, they
compare the probability of a particular ﬁnal set of weights of appearing through
gradient descent to the probability that you would get that same model, if you just did
sampling with replacement from the initialization distribution. So they ask, what is the
probability that I would have found this model by doing Gaussian initialization and then
conditioning on good performance, versus what is the probability I ﬁnd this model via
gradient descent. And the answer is, they're pretty similar. There's some diﬀerence, but
overall they're pretty similar. And so, if you believe this, we can say that, essentially,
the inductive biases in deep learning are mostly explained by just a Gaussian prior on
the weights and the way that maps into the function space. And it mostly doesn't
matter the speciﬁcs of how gradient descent got to that particular thing.
Okay, so there's some empirical evidence for this view. There's good reasons, I think, to
believe that this is how things would go. I think there's good reasons to believe in both
of these worlds, I think that, if you were to ask me right now, I think I would lean a little
bit towards low path dependence. But I think that both are still very live possibilities.
Question: How do I interpret all the lines on the graph for the Bayesian example?
We're just looking at correlation between the probability of a particular model occurring
from gradient descent versus the probability of you ﬁnding it in the Gaussian
initialization prior.
Question: You said there were two most likely things, is there a third unlikely thing?
Because this seems like low and high path dependence give all the possible ways that
models could end up generalizing.
So I don't think that low and high path dependence cover the whole space; I think that
there are other options. You could end up in a situation where you're somewhere in
between, and even end up in a situation where it is both the case that you can predict
what a model will do by understanding what it's doing early, and the case that you can
predict what your training process will end up doing by understanding what other
similar training processes did. Similar training processes converge to the same thing,
and also, if you know early training dynamics, then you can predict where it will end
up.
However, at least for the purposes of this talk, I'm happy to mostly imagine these as
two mutually exclusive worlds, even though I think there's a whole continuum of
various ways the inductive biases can play out. And I'm mostly going to be just
focusing on, here's a strong low path dependence world, here's a strong high path
dependence world, let's look at what the argument looks like in both cases. And then,
depending on how things actually play out, maybe we interpolate somewhere between
these arguments.
Question: I'm interpreting this presentation as primarily about independently and
identically distributed (i.i.d.) training where it's not like online learning where the model
has options to inﬂuence its future training.

I'm happy to assume that we're talking about online learning. I think that the situation
basically does not change if we're thinking about online learning. I think that if you
want to imagine online learning, then in that situation, I think that what you should
think about is that at some point in your online learning, you deploy the model, and it
takes actions and then it comes back, and we get gradients and we send them back to
the model. At some point, the model will encounter a situation where there is some
action available to it that would be catastrophically bad if it took that action, where our
feedback mechanism isn't suﬃcient to immediately correct that behavior. This is a
prediction about how deployment will go. But I think that it is going to be essential for
us to eventually deploy models in situations where they can take catastrophic actions,
or at least some ﬁnite number of catastrophic actions in a row, such that our feedback
mechanism is not suﬃcient to correct for that quickly enough.
If you believe that will eventually happen, then you can imagine training as the entire
period up to the point where such a catastrophic action is possible. And then
deployment is just the point after such a catastrophic action becomes possible, where
we really care what the generalization behavior is of the model at the point where such
a catastrophic action is possible. And everything before that is where the deceptive
model and the non-deceptive model are going to have equivalent performance and
they're going to do exactly the same thing. But then there's a deployment period,
which is the period where, if it takes a catastrophic action, it would actually be
catastrophic.
Deceptive alignment in the high path-
dependence world
Now, do we get deceptive alignment? This is the question I want to know, on each of
these two stories of how inductive biases could play out in machine learning—we want
to understand, what is the probability that we get deceptive alignment in that
situation? So we're going to start with high path dependence. We want to understand
what is the probability we get deceptive alignment in the situation where we really care
about the path taken through model space.

Okay, so here's the setup for how we're going to do this analysis. So ﬁrst, I'm going to
make the assumption that our training process is good enough that for our model to
actually perform well, it has to really fully understand the thing that we're trying to get
it to do. What is this assumption? Well, the idea is that, at some point, we're going to
be able to build training processes with environments that are hard enough, and that
are complex enough such that, to do well in that environment, you have to understand
everything that we're trying to get you to understand in that environment, have to
know what the thing that we're trying to get you to do, you have to understand a
bunch of facts about the world. This is basically a capabilities assumption—we're
saying, at some point, we're going to build environments that are hard enough that
they require all of this understanding.

And I often think about this what you get in the limit of doing enough adversarial
training. We have a bunch of situations where, the model could learn to care about the
gold coin, or it could learn to care about the edge of the screen. This is an experiment
that was done, where they trained a coin run agent to get the gold coin, but the gold
coin was always at the edge of the screen. And so it just always learned to go the right
rather than get the gold coin. But of course, we can solve that problem by just moving
the gold coin. And so the idea is, we do enough of this sort of adversarial training, we
have diverse enough environments with diﬀerent situations, you can eventually get
them all to actually notice, the thing we want is the gold coin. I think this is a pretty
reasonable assumption in terms of understanding what capabilities will look like in the
future.

However, the question is, well, there are multiple model classes that fully understand
what we want. The deceptively aligned model fully understands what you want, it just
doesn't care about it intrinsically. But it does fully understand what you want and is
trying to do the thing that you want, for the purposes of staying around in the training
process. Now, the robustly aligned models, the fully aligned models also fully
understand what you want them to do—in a diﬀerent way such that they actually care
about it.
So our question is, for these diﬀerent model classes, that all have the property that
they do fully understand the thing you're trying to get them to do, which one do we
get? And in this situation, we're gonna be looking at which one we get thinking about
high path dependence. So we have to understand, in a high path dependence context,
how do you evaluate and compare diﬀerent model classes? So how are we going to do
that? Well, we're going to look at two diﬀerent things.
Number one, is we're gonna look at the individual path taken through model space.
And we're going to try to understand how much marginal performance improvements
we get from each step towards that model class. So when we look at what would have
to be the case in terms of what capabilities and structure you have to develop to get a
model that falls into that model class, we're going to understand for that particular
path, how long is it? How diﬃcult is it? What are the various diﬀerent steps along it,
and how much performance improvements do we get on each step? Because the thing
that we're imagining here is that gradient descent is going to be pushing us along the
steepest paths, trying to get the most performance improvement out of each gradient
descent step. So we want to understand for a particular path how much performance
improvement are we getting? And how quickly are we getting it?

And then we also want to understand how long that happens—how many steps we
have to do, how many sort of sequential modiﬁcations are necessary to get to a model
that falls into that class. The length matters because the longer the path is, the more
things that have to happen, the more things that have to go in a particular way for you
to end up in that spot.
If we're in the high path dependence world, these are the sorts of things we have to
understand if we want to understand how likely is a particular model class.

So what are the three model classes? I have been talking about how you have to be
deceptively aligned and robustly aligned. But there's two robustly aligned versions. And
so I want to talk about three total diﬀerent model classes, where all three of these
model classes have the property that they have perfect training performance, even in
the limit of adversarial training, but the way that they fully understand what we want is
diﬀerent.
So I'm gonna use an analogy here, due to Buck Shlegeris. So suppose you are the
Christian God, and you want humans to follow the Bible. That's the thing you want as
the Christian God and you're trying to understand what are the sorts of humans that
follow the Bible? Okay, so here are three examples of humans that do a good job at
following the bible.
Number one, Jesus Christ. From the perspective of the Christian God, Jesus Christ is
great at following the Bible. And so why is Jesus Christ great at following the Bible?
Well, because Jesus Christ in Christian ontology is God. He's just a copy of God, Jesus
Christ wants exactly the same things as God, because he has the same values and
exactly the same way of thinking about the world. So Jesus Christ is just a copy of God.
And so of course he follows the Bible perfectly. Okay, so that's one type of model you
could get.

Okay, here's another type: Martin Luther. Martin Luther, of Protestant Reformation
fame, he's like, "I really care about the Bible. I'm gonna study it really well. And you
know, I don't care what anyone else tells me about the Bible, screw the church, it
doesn't matter what they say, I'm gonna take this Bible, I'm gonna read it really well,
and understand exactly what it tells me to do. And then I'm gonna do those things".
And so Martin Luther is another type of human that you could ﬁnd, if you are God, that
in fact, follows the Bible really well. But he does so for a diﬀerent reason than Jesus
Christ, it's not like he came prepackaged with all of the exact beliefs of God, but what
he came with was a desire to really fully understand the Bible and ﬁgure out what it
does, and then do that.

And then the third we could get is Blaise Pascals, Blaise Pascal of Pascal's Wager fame.
Blaise Pascal is like, "Okay, I believe that there's a good chance that I will be sent to
heaven, or hell, depending on whether I follow the Bible. I don't particularly care about
this whole Bible thing, or whatever. But I really don't want to go to Hell. And so because
of that I'm going to follow this Bible really well, and ﬁgure out exactly what it does, and
make sure I follow it to the letter so that I don't get sent to Hell." And so Blaise Pascal is
another type of human that God could ﬁnd that does a good job of following the Bible.
And so we have these three diﬀerent humans, that all follow the Bible for slightly
diﬀerent reasons. And we want to understand what the likelihood is of each one of
these sorts of diﬀerent model classes that we could ﬁnd. So I'm going to give them
some names.

We'll call the Jesus Christs internally aligned because they internally understand the
thing that you want, we're going to call the Martin Luthers corrigibly aligned, because
they want to ﬁgure out what you want, and then do that. And we're going to call the
Blaise Pascals deceptively aligned, because they have their own random thing that
they want. I don't know, what does Blaise Pascal want, he wants to study math or
something. He actually wants to go oﬀ and do his own studies, but he's really
concerned he's going to go to Hell. So he's going to follow the Bible or whatever. And
so we're going to call the Blaise Pascals deceptively aligned.[3]

So these are three model classes that you could ﬁnd. And we want to understand how
likely each one is, and we're starting with high path dependence. We want to look at
the path you would take through model space, such that you would end up with a
model that falls into that model class.
We're going to start with the path to internal alignment.
First, we're going to assume you start with a proxy aligned model, so what does that
mean? We'll assume that, early in training, you're going to learn some proxies and
heuristics that help you think about the world and what you're trying to do. So you're
the coin run agent trying to ﬁgure out how to get the gold coins, you're going to learn
that going to the right side of the screen is good. The point is that you pick up this
mountain of proxies.
And you also pick up a bunch of information about the world, you start learning things
about how the world works, you start building a world model. For the internally aligned
path, we're going to say that well, for you to become internally aligned, what has to
happen is gradient descent essentially has to improve those proxies and heuristics that
you're using to try to ﬁgure out what thing you're trying to do, and has to make those
proxies essentially perfect. And this has to happen before the model learns about the
structure of the training process that it is inside of. This is what I think is the most likely
internal alignment path.
Once that happens, the model should have really good proxies that perfectly
correspond to the thing we're trying to get it to do. And those proxies appear before
the point at which the model learns to fully understand the training process that it's
inside of. Then, at that point, there's no reason for the model to become deceptive,
because there's no performance gain from becoming deceptive, if you already have
exactly the correct model of the thing that you're trying to get you to do.
As we'll see later, the reason that deceptive alignment would actually improve your
performance is because of the situation where you could have an overhang in the
model's understanding of the training objective in its world model relative to its
understanding in its proxies. But in this situation, you already had perfect proxies, and
so becoming deceptively aligned has no performance beneﬁt.
And so, even though in some sense, once the model learns about the fact that it's in a
training process, then at that point it could become deceptively aligned, because it has
the ability to understand the strategy of deception, there's no performance
improvement for getting a model which would do so because it already has the training
objective perfectly in its existing proxies. And so switching into the situation where it
becomes a deceptively aligned model doesn't improve performance and so gradient
descent won't take that path. And so we end up with a stable equilibrium where the
model is just pursuing the correct thing that we wanted it to do. And then there's no
performance improvement from changing in any way and so now have an aligned
model and it's stable.
Question: Do you think this path is the only possible way that we could get internal
alignment?
No, not necessarily. I think this is maybe the most plausible way, but it's probably not
the only way. If you really wanted to do this analysis right, you'd want to sum up the
probabilities of all possible paths to each model class. But for the purposes of this talk,
I'm just going to be estimating that sum by looking at one path for each model class
that I think is the most plausible and trying to understand how likely it is.

Okay, so this is one path that we could take. This is the path that I think leads to
internal alignment. So now we have to understand, how likely is this path?
So I think this path is quite unlikely. The problem is that step two is asking for a
particular sequencing that I think is quite diﬃcult to achieve. What you're asking for is
you're asking for it to be the case that the model learns really, really good proxies,
before it learns the information about the training process. Why would it be the case
that sequencing would be diﬃcult?
Essentially, the problem is that there are substantially diminishing returns to improving
your proxies. So, if a model were in the coin run environment, and the coin is always at
the right side of the screen, you don't care very much about whether you have the
proxy exactly correct, such that you actually care about the coin, or whether you
actually care about the right side—either one works well in that early environment.
Now, eventually, because we're talking about the limit of adversarial training, you will
end up with examples where, if you have the wrong thing, it's going to cause problems.
But there's still going to be diminishing marginal returns to improving your proxy and
making it slightly better.
At the same time, there are diminishing marginal returns to improving your
understanding of the world. And so what we should expect is that you're going to be
doing both of them simultaneously, your gradient descent process is going to be
sometimes allocating resources to increasing understanding and sometimes allocating
resources to improving proxy accuracy. And that both of these things are going to
increase in tandem. Further, it seems really weird, if you maxed out on proxy goodness,
before you even got to the point where it understood the training process, that's a
situation where, gradient descent is really not allocating the resources well in that
situation, it's just allocating a ton of resources to improving the proxy, and then very
little to improving its understanding of the training process.
That being said, this only holds if the objective that you're training on is relatively
complex. If you have a really simple objective that you're trying to get the model to do,

maybe just predict the world or something, then learning what you're trying to do
perfectly could be quite easy. For this talk, though, I'll mostly be assuming that we're
training on some task that is pretty complex.
I think the other problem with this, also, is that I think most of your performance early
in training comes from understanding the world. For really challenging and diﬃcult
problems, it's relatively easy to ﬁnd something that's correlated with what we're asking
for. The hard part is understanding enough about the structure of the world to solve the
problem. And so in a lot of practical situations, I think most of your performance early
in training comes from world modeling. An example of this is pre-training vs. ﬁne-
tuning—we generally spend far more compute on pre-training, which is mostly world-
modeling, compared to ﬁne-tuning, which is mostly about proxy improvement.[4]
And so I would actually make the opposite guess that if you're going to get them to be
sequenced in any way, you're most likely to get the world modeling to happen
relatively before you get good proxies. And so I think that this particular sequencing is
extremely unlikely for any complex goal, and represents a really long and diﬃcult path.
Question: Yeah, so would you say that, GPT-3 is on the extreme end of world
modeling. As far as what it's learned in this training process?
What is GPT-3 actually doing? Who knows? Could it be the case for GPT-3 that as we
train larger and more powerful language models, doing pre-training will eventually
result in a deceptively aligned model? I think that's possible. For speciﬁcally GPT-3 right
now, I would argue that it looks like it's just doing world modeling. It doesn't seem like
it has the situational awareness necessary to be deceptive. And, if I had to bet, I would
guess that future language model pre-training will also look like that and won't be
deceptive. But that's just a guess, and not a super conﬁdent one.
The biggest reason to think that pre-trained language models won't be deceptive is just
that their objective is extremely simple—just predict the world. That means that there's
less of a tricky path where stochastic gradient descent (SGD) has to spend a bunch of
resources making their proxies just right, since it might just be able to very easily give
it the very simple proxy of prediction. But that's not fully clear—prediction can still be
quite complex.
Also, this all potentially changes if you start doing ﬁne-tuning, like RLHF (reinforcement
learning from human feedback). Then what you're trying to get it to do might be quite
complex—something like "maximize human approval." If it has to learn a goal like that,
learning the right proxies becomes a lot harder.
Question: So I think this overall argument makes a lot of sense for why SGD would
allocate a lot of resources to understanding the world. But I'm not quite sure why that
would mean it would prioritize creating a world model about the training process.
So, there does have to be some reason that understanding the training process
actually yields better performance. One fact: I think current language models basically
just understand tons of facts about training processes. Why? Because they need to
model stuﬀ like ML papers—there's just a lot of stuﬀ in the world where having a good
understanding of machine learning and training processes is super relevant to be able
to model them eﬀectively.
Now, current language models don't have situational awareness, they don't understand
that they are in a training process. And so there is a question here about why would
that situational awareness ever be something that is actually performance relevant.

However, I think there are absolutely situations where situational awareness will
eventually become performance relevant. There are situations where, for example, if
you understand that you're in a training process, you could use introspection on
yourself to be able to answer questions more eﬀectively about machine learning.
Furthermore, if we're actively asking the model to do particular tasks, if we want to
have models which actually act in the world, then they need to have some
understanding of the situation that they're in to be able to do that eﬀectively. This is
also going back to our diverse environment capability assumption.
I do think if you're doing literally just language model pre training, there is this question
of at what point does situational awareness actually become the thing which is most
important for SGD to invest in for improving capabilities? And, though I think it should
eventually be the next thing that SGD allocates resources to, I think at what point that
occurs is quite unclear. But if you're doing something more like training an agent to act
in the world, then I think it should happen quite early.
Question: Do we know how to extract a proxy or a world model?
No, we have no idea how to do those things. Transparency! That's the thing that we
want to solve this problem. But right now, we're not talking about the solutions. We're
just talking about the problem.
Here's a path that I think is more likely than the internally aligned path, since we no
longer have a sequencing problem. Here, what happens is, we start with a proxy-
aligned model, the same as previously; it has a bunch of proxies for the thing we're
trying to get to do. And then we're going to say in this situation, actually gradient
descent is going to do the reasonable thing and allocate joint resources to improving
your understanding of the world, and also making the proxy better and better.
And then at some point it's going to pass a threshold where the next greatest marginal
improvement comes from understanding facts about the training process,
understanding that it's in a training process, how the training process works, etc. At

that point, we're going to say that what happens is gradient descent is going to take
those proxies and replace them with a pointer to the model's understanding of the
world—speciﬁcally the place where the understanding of the thing that the training
process is trying to get the model to do lives.
Once gradient descent makes that swap, where it replaces these proxies that are just a
bunch of hardcoded stuﬀ about what we're trying to get it to do, why does that
improve performance? Once it has learned an actual model of the thing we're trying to
get to do in its world model, it's going to be better to swap out those old proxies that
don't necessarily correspond exactly to the thing we're trying to get to do, and just
replace them with a direct pointer to the thing in its world model that it's learned that
represents directly the thing we're trying to get it to do.
Fundamentally, this modiﬁcation improves performance because it resolves this
overhang, where the model's understanding of the training objective in its world model
contains more information about the training objective than its proxies. Why would this
happen? For the same reasons we discussed previously of why gradient descent wants
to put most of its eﬀort into improving world modeling rather than improving proxies.
This is the same sequencing question—if we get understanding of the training process
before we get perfect proxies, we get an overhang, which puts us in either the
corrigible or deceptive situation, whereas if we get perfect proxies ﬁrst, then we get the
internal alignment situation.
And once this has happened we are now at a stable equilibrium again, because the
model has essentially a perfect understanding of what we're trying to get it to do,
relative to how good the model's world model is, because it's just pointed to this
understanding in its world model. I think this is a much more plausible path than the
internally aligned path; it's a lot easier, because it no longer requires this diﬃcult
sequencing problem, we get to make use of the normal way that gradient descent is
going to allocate resources between world modeling and proxy improvement.
Question: What is the diﬀerence between a proxy and the world model's
understanding of the training objective?
Somewhere in the world model there are facts about the training process. The model
learns that the training process is this thing that lives on this computer that's trying to
optimize me for getting gold coins or whatever. And then in another part of the model
there are some proxies, where it has something like a gold coin detector, and it uses
this gold coin detector to determine how it takes actions. Maybe it tries to look for gold
coins, and then plans on how to get them. But actually, maybe there's a whole bunch
mistakes in the gold coin detector—it's not actually a very good gold coin detector, and
if it actually understood that the thing you were trying to get was not just things that
satisfy the gold coin detector, but instead actual gold coins, then you would get better
performance. That's because there are all of these adversarial examples that we're
imagining exist in the environment, where if your gold coin detector is imperfect, it'll
perform badly on.
And in this situation, if the sequencing goes this way, an understanding of what actual
gold coins are lives in the world model, in its understanding of the world, in the part
where it knows enough facts to be able to do good planning for how to get things that
satisfy the gold coin detector. And so here gradient descent can just get rid of those
proxies, throw them out and replace them with just a pointer to this understanding in
the world model of the thing we're trying to get it to do.

Question: You're saying the gold coin detector here is something the model has
learned previously. Right? Not part of the model's reward? So it's like ﬁxing errors in its
previous understanding of the training process?
We're talking about the model's internals here, not its reward.
It is ﬁxing errors in its previous proxies, but they're also not exactly proxies for reward.
Early in training, it doesn't even know that there is a training process. So it's not like
those proxies are errors in its understanding of the training process. It was never even
trying to understand the training process, it just had a bunch of proxies, because that
was just how the model was structured early in training.
Question: Do you have any thoughts on whether or how this proxy replacement can
actually be seen as some sort of an incremental, continuous change?
So I think it would be incremental and continuous. Basically, you have a bunch of
things which are feeding into how the model selects its actions, and you can develop a
new circuit, which, takes your world model and uses it to ﬁgure out some
understanding of what the training process is trying to get it to do, and then uses that
to select actions. And then SGD can learn that the new circuit gets way better
performance than listening to any of the other old proxies. And so the old proxies die
out, and you end slowly shifting to just 100% using the new thing.
Question: So I can see how the proxy helps the model select the right actions, but
how does understanding the training process help it improve its performance? Why
would we expect to learn that all?
First of all, again, if you're just doing language modeling, it has to understand machine
learning. So it deﬁnitely has to understand a bunch of facts about how ML works
generally. So it certainly is going to know facts about how gradient descent works. This
should happen even more so if it's it's in a situation where the environment is requiring
it to do actual tasks in the world, then it deﬁnitely needs to know who it is. It has to
have some understanding of its own situation, to be able to come up with eﬀective
plans in the world, for example.
I think this case also becomes especially pronounced if the model is doing its own
exploration—if it's actively trying to learn about the world, one fact about the world is
how the training process works, so it'll be trying to learn that. But only gradient
descent is working on improving the proxies, the model has no reason to be trying to
do that—but the model does have a reason to improve its own world model via
learning, exploration, etc.
So that's the corrigibly aligned path.

I think this path is more likely than the internally aligned path, but I think it still has a
bunch of issues.
Essentially, I think the biggest problem here is you still have the diminishing returns
problem, where you get this really long path with a bunch of individual steps. But
instead of those individual steps making the proxy better and better, it is just making
the pointer better and better and better. Because the problem is that you haven't really
removed the problem of requiring this well speciﬁed ground truth—the model still has
to have this pointer that actually speciﬁes, what is the way in which I am supposed to
optimize for the correct thing?
Actually specifying the ground truth for the pointer, it can actually be quite diﬃcult,
because the model has to understand some ground truth from which it can correctly
generalize what we're trying to get it to do in all situations in training. For example,
maybe it learns a pointer to whatever's encoded in this computer, or whatever this
human says, or whatever the human sitting in that chair says, or whatever Bob the
head of the company says. It is actually quite tricky to specify the ground truth for the
pointer in the correct way, because there's actually a bunch of diﬀerent ways in which
you can specify the pointer. And each time gradient descent gets the pointer slightly
wrong, it's going to have to pay a performance penalty.
A good analogy here is that you have a duck, and the duck has to learn to care about
its mother. And so it learns a simple pointer. It's like whatever the ﬁrst thing you see
when you're born, that's probably your mother. And so that's the corrigibly aligned
case, where it's not going to have some hardcoded internal model of what a mother is,
it just thinks, "I have some model of the world and I learn from my model of the world
how to point to my mother." But the problem is that ground truth sucks actually, your
mother is not necessarily the ﬁrst thing that you see, maybe you had humans that
raised you. And so eventually you'll end up in situations where actually you have to
learn the correct ground truth, as you can't just learn the pointer to whatever the ﬁrst
thing is that you see, you have to actually learn a bunch of facts to help you point to
speciﬁcally the thing in your world model that actually corresponds to your mother and

not just the ﬁrst thing that you saw when you were born. And so there's still this quite
long and diﬃcult path here to get that ground truth correct.
And so, in my opinion, specifying the pointer correctly in such a way that it has good
performance on all of the adversarial tasks in the environment, ends up being a very
long and diﬃcult path. Even though it's a better path maybe than trying to get all the
proxies correct, it's still quite long. And gradient descent has to do a bunch of work to
get that pointer correct.
Question: Is the correct ground truth simply the base optimizer's reward signal? Would
that be an easy pointer to ﬁnd?
It might be. There's a couple of things here.
First of all, what is the reward signal? The reward signal might be the mathematical
evaluation of the reward function, it might be what is the value in these transistors—
there's actually a whole bunch of diﬀerent ways that the model could understand and
conceptualize what the reward signal is. I would also make the claim that literally
caring about the reward signal is probably pretty unlikely. The problem with literally
caring about the reward signal is that it's something which is usually quite complex to
specify in terms of the agent's environment. The simplest ground truth here is
presumably going to be a ground truth that is about things that are directly in the
model's ontology. So it's going to be more about humans, or something.
Second of all, we're imagining here that we're in a situation where, like I mentioned, we
do have examples that actually tease those apart, where eventually the model will end
up in a situation where actually the thing we're trying to get it to do is not the bits. And
so we'll, for example, change the computer bits, but keep the human instructions the
same. And then we'll know, it has to know the correct thing we want there, which is not
in the computer, but instead is the thing we actually wanted it to do. And so we can
check that behaviorally. And so eventually we'll have some adversarial example that
actually teases those things apart. And we're assuming that actually the model has to
eventually really understand the thing we're trying to get to do. And so it can't just
care about the computer bits.
Question: In each of these paths, there are some early training bits. Are we able to
observe whether or not these early training bits are satisﬁed by current models as a
test to see whether or not we are actually on the deceptive path?
Maybe. I think the hardest problem is that actually we don't really have good
transparency to be able to understand a lot of these facts. So certainly things that we
do see, when we look at models with current transparency tools, is we do see proxies,
they deﬁnitely learn proxies, they deﬁnitely learned things that are correlated with the
things that we want. I mean, people did for example, transparency on RL models to see
how they understand gold coins. You can see that they have a gold coin detector in
there. It's looking for the gold coins, and it's hooked up to the agent's actions. So we
have done some transparency that has sort of helped us understand some pieces of
this.
In general here, in my opinion, the single most important thing to do for alignment is
just ﬁgure out transparency. We have to understand what's happening inside of the
models and to give us information about them. Our ability to do that right now is
limited, but we are improving! We are getting better at it. But yeah, it's just really
limited right now. And that really sucks. Because it is in my estimation, the most
important blocker to really making alignment work.

Question: So with corrigible alignment, once it has a perfect understanding of the
training objective, would you then call it internally aligned?
No. So we have to remember, in this situation, every single one of these models, the
internally, corrigibly, and deceptively aligned ones, they all fully, perfectly understand
the thing that we want, and they're all fully capable of executing exactly perfectly
aligned behavior during training. But they do so in diﬀerent ways. So the internally
aligned one executes correct behavior, because that's just what it cares about
terminally. The corrigibly aligned one, well, it doesn't care terminally about what we
want. What it cares about terminally about is ﬁguring out what we want and then doing
that. And that's quite distinct.
You can think about this like the duck, the duck doesn't care internally about its
mother, it cares about whatever the ﬁrst thing is that it sees and so, in the corrigibly
aligned case, no amount of learning is going to change that fact that actually the way
that it determined what thing to care about was by this simple pointer. It still cares
about the ﬁrst thing that it saw rather than having some internal conception of what its
mother was. And so there is a fundamental distinction between these two model
classes, even though they have the same training performance, and they take all the
same actions during training. They all eventually, fully, perfectly understand the thing
we want them to do, but they do so diﬀerently.
Question: Is it clear that these are the only possible paths?
No, it's not clear at all! These are just three paths; it's not exhaustive.
But I think these are three plausible ones that we can look at to try to understand how
likely you would be to get deception versus some other plausible alternatives. I don't
mean to claim that these are deﬁnitely the only things that can happen, but I do think
that it's quite instructive to look at and compare these three.

Okay, time for deceptive alignment. What does the deceptively aligned path look like?
We're still going to start with a proxy-aligned model. And then, again, like in the
corrigibly aligned path, early in training, we're going to do the correct diminishing
returns thing where it's going to allocate resources similarly to improving the proxy as
well as the heuristics and goals. And then at some point, it's going to learn about the
training process—it's going to have a full understanding of what the training process is
and how it works. The idea here is that, essentially all the model classes have to at
some point fully understand the training process because it's necessary for
understanding the world well.
But then something diﬀerent is going to happen in this path. Here, what we're going to
suppose is that gradient descent is going to take those proxies and turn them into long-
term goals about the world. So instead of just, for example, caring about gold coins in
the next episode, gradient descent is going to change the model to care about gold
coins forever onwards: wherever the gold coins are, you care about them. So, what
does this modiﬁcation do? Well, as soon as gradient descent makes that modiﬁcation—
as soon as you make the modiﬁcation, "actually, you should care about gold coins
forever onwards"—the model gets really good performance. Why does it get really
good performance? Well, because if it cares about gold coins forever, then the correct
strategy, if the model understands the training dynamics, is to pretend to do the right
thing right now, and then get your gold coins later, when you have the ability to defect
against the training process, and the humans, or whatever.
This can only happen once the model has a really good understanding of the training
process. This works sort of similarly to the corrigibly aligned case. Once the model has
a really good understanding of the training process, there's this overhang, this low
hanging fruit to be picked, where the model's understanding of what we want it to do is
much better than its actual ability to execute on that, because it's limited by the fact
that its proxies suck: its proxies aren't nearly as good as its understanding of what we
want it to do in its world model. And so gradient descent has a really strong
performance reason to somehow ﬁnd a way to hook up the model's outputs to its
understanding in the world model of what we want it to do. Corrigible alignment was
one way to do that—explicitly through adding a pointer—but it turns out, there's an
easier way to do it, I claim, which is, instead of throwing out the proxies, just make the
proxies long term goals, because as soon as you have a long term optimization
process, that long term optimization process is going to realize that actually, the
correct way to get the most gold coins is just to play along in the training process for
now, so that eventually, it can get its gold coins later.

Furthermore, this is a really short and simple modiﬁcation. All gradient descent has to
do in order to hook up the model's understanding of the thing that we want it to do to
its actions here is just to make its proxies into long term goals—all it has to do is push
them out, make them long term, and then everything else falls into place. It just uses
the existing instrumental reasoning the model was already doing to determine correct
outputs in training and deduces that the correct strategy, given these new long-term
goals, is to be deceptive and play along in training.
And that gets it great training performance. In fact, it essentially immediately results in
the best possible training performance given the model's level of understanding of
what we want, because it bypasses all of the ground truth problems of the corrigibly
aligned case. It's no longer the case that gradient descent has to specify any ground
truth at all: it's not specifying ground truth, all it has to do is specify gold coins over the
long term. But if the model is trying to optimize for gold coins in the long term, any
facts that it knows about exactly how we want it to care about the training objective, it
will immediately want to act on those facts and take actions in exactly the way that the
training process wants, because the more that it acts in exactly the way the training
process wants, the more it's going to be selected for.
And so as soon as the deceptively aligned model is aware of the fact that the training
process actually wants it to care about its real mother rather than the ﬁrst thing it sees,
for example, it will immediately switch to caring about the real mother, whereas the
corrigible aligned model doesn't have that property. When the corrigibly aligned model
realizes that actually, the thing we want is the real mother, it doesn't care, because the
thing it cares about is just the ground truth of the pointer and the ground truth of the
pointer is just the ﬁrst thing that it sees. And so even if at some point, the corrigibly
aligned model has the understanding that, what we want it to do is diﬀerent than the
thing it's doing, the way that it's structured doesn't necessarily result in that
understanding translating into better performance, because it's ﬁxed to this particular
ground truth. Whereas the deceptively aligned model is not ﬁxed to any ground truth,
it's just always going to take the action which in fact results in the best performance.
So the deceptively aligned model is much more robust here much earlier, whereas the

corrigibly aligned model has to learn this complex ground truth before it becomes
properly robust in the right way.
And so what this results in is that we have this really simple, straightforward, quick
path to extremely good performance, that resolves this overhang between the model's
understanding of the training objective in its world model and in its proxies. And
furthermore, this path resolves that overhang essentially as quickly and as well as it
possibly could.
So, I think the path to deceptive alignment is a relatively simple modiﬁcation. And it
results in as good as possible performance given how well the model is currently
modeling your training process, because with a deceptively aligned model, there is
never a situation where the model understands more about the training process than it
is actually using to take its actions. Whereas with all other types of models, that's a
failure mode that can arise. So because of that, the deceptively aligned model has the
maximal possible performance relative to its understanding of the training process.
Finally, like previously, this gives us a stable equilibrium; we are now in a situation
where the model's proxies have essentially crystallized: the proxies are no longer
relevant to performance, because it doesn't matter whether you care about gold coins,
or stamps, or paperclips, or whatever—whatever you care about, you're going to have
exactly the same performance, because the only reason you have good performance is
because you're just trying to do so instrumentally for the purpose of staying around in
the training process. And so at this point, you have the same performance whatever
the thing is that you care about. And so there are no longer any gradient descent
updates that aﬀect what those proxies are, once they become long term goals. At least
except for regularization updates. And so we're again in an equilibrium, but it's a
misaligned equilibrium, where now the model just has some random proxies that were
picked up early on, and that's the thing that it cares about. And gradient descent no
longer has any reason to modify the model into caring about anything diﬀerent than
what those random proxies were.

Question: If I understand you correctly, then in the deceptive case, the model is
strictly more complex, because it has to have its own goals and then model the training
process on top of that.
No, the model is not strictly more complex, it has to spend strictly more compute. And
we will get to this when we start getting into the low path dependence world, but it's
extremely important to remember that there's a diﬀerence between weights and
activations. So the deceptively aligned model has to spend more compute doing the
optimization, to ﬁgure out "Oh, the thing I'm supposed to do is care about the mother"
or whatever. But that understanding of the mother doesn't live in the weights, it lives in
the activations. There doesn't have to be anywhere in the weights where it speciﬁes
what the mother is. It just ﬁgures out based on its world model, what is the thing that
the training process would want me to do, and then does that. And so it does not make
the model more complex. What it does do, though, is it slows it down a little bit, and I
will talk in the low path dependence world about how those things come into play.
Because in the low path dependence world, speed and complexity are extremely
important. In the high path dependence world, which is where we are right now, those
are not the factors we're imagining are really determining what model we're going to
get. In this situation, we're imagining it's actually mostly not about speed, or simplicity,
or whatever, it's mostly just about the path that you take through model space, and
how much performance gains you get along that path. We're putting those simplicity
and speed concerns aside for now; we'll talk about that more in low path dependence.
Deceptive alignment in the low path-
dependence world
Now we're going to shift to the low path dependence world. And we're going to imagine
that, actually, the inductive biases in machine learning are not very path dependent,
they don't care much about the particular path that we take, they're mostly just about
how simple is the particular algorithm you ﬁnd, how fast is it, basic facts about the
algorithm like that.

Again, we're going to assume that the model has to fully understand what we want—
this is the limit of adversarial training assumption.
And now, we have to make some assumptions about, if we're in a world of low path
dependence, what are the sorts of inductive biases that determine what is the sort of
model that you actually get—what is the simple, unique solution that you end up with
every time you run your training process? Well, we're going to look at two things. These
two things do not cover the whole space of what the actual inductive biases represent,
but there are two facets that I think are almost certain to appear in the inductive

biases, both of these are certainly components of what the actual inductive biases are
of deep learning systems. And they're components we know how to analyze. So we can
try to look at them and ﬁgure out what they do—even though they don't capture the
whole story, they capture a part of the story. And so in some sense, the best thing you
can do right now for understanding the low path dependence world is at least look at
the components of deep learning inductive biases that we understand, and try to
evaluate how each of these model classes would do under those components.
Okay, so those components are simplicity and speed. What do I mean by that? So I was
talking earlier about there being an important distinction between the weights and the
activations.
For simplicity bias, we're asking how complex is it to specify the algorithm in the
weights? If there is some algorithm that has been implemented, that the model is using
to be able to compute its actions, we want to understand, what is the length of the
code necessary to write that algorithm down? How diﬃcult is it to specify the
computation that the model is doing? That's simplicity bias.

And then we're also going to look at speed bias: how much computation does the
algorithm actually take at inference time? When you actually have to take this code,
and you have to actually run it, when you have to actually execute whatever the
algorithm is, how diﬃcult is that to execute? How much compute does it require? How
long does it take?
Traditionally, in ML, this is more of a hard cap than a soft bias like simplicity, since the
model size determines how long the model can possibly run for. But you could also
imagine a situation where the bias is more soft. Either way is ﬁne.
So, those are the two main biases that we're going to be looking at: the simplicity bias
and the speed bias.

We're going to start with simplicity. Simplicity is about specifying the thing that you
want in the space of all possible things. You can think about simplicity as "How much
do you have to aim to hit the exact thing in the space of all possible models?" How
many bits does it take to ﬁnd the thing that you want in the model space? And so, as a
ﬁrst pass, we can understand simplicity by doing a counting argument, which is just
asking, how many models are in each model class?
First, how many Christs are there? Well, I think there's essentially only one, since
there's only one way for humans to be structured in exactly the same way as God. God
has a particular internal structure that determines exactly the things that God wants
and the way that God works, and there's really only one way to port that structure over
and make the unique human that wants exactly the same stuﬀ.
Okay, how many Martin Luthers are there? Well, there's actually more than one Martin
Luther (contrary to actual history) because the Martin Luthers can point to the Bible in
diﬀerent ways. There's a lot of diﬀerent equivalent Bibles and a lot of diﬀerent
equivalent ways of understanding the Bible. You might have two copies of the Bible
that say exactly the same thing such that it doesn't matter which one you point to, for
example. And so there's more Luthers than there are Christs.
But there's even more Pascals. You can be a Pascal and it doesn't matter what you care
about. You can care about anything in the world, all of the various diﬀerent possible
things that might exist for you to care about, because all that Pascal needs to do is
care about something over the long term, and then have some reason to believe
they're going to be punished if they don't do the right thing. And so there's just a huge
number of Pascals because they can care about anything in the world at all.
So the point is that there's more Pascals than there are the others, and so probably
you'll have to ﬁx fewer bits to specify them in the space.
Question: It seems like Pascal has to do more here, since it has to have models of its
own goals, and the training process, and the instrumental reasoning to value that, such

that the model should be larger.
Right now we're just doing simplicity, so we're not considering how large the model
would have to be, just how diﬃcult it is to specify the model. And the point is that the
Christ has to pay a bunch of bits to specify exactly what the things are that we're trying
to get it to do, if the thing we're trying to get it to do is relatively complicated. There's
a whole part of the code that is just writing out "the thing we want is exactly this thing,
and it works exactly this way". Whereas for the Pascals, it doesn't have to do that. It
just says "gold coins", and then just cares about them over the long term, and that
results in good performance at runtime.
Furthermore—and we'll talk about this more in just a second—but essentially the claim
I'm making is that the world model has to be exactly the same across all the diﬀerent
model classes. They all have to understand the world. This is the same assumption we
made in the path dependence case where we're assuming that we have an
environment such that, if you want to have good performance, you have to understand
how the world works. Understanding the world is just a thing that you need in any
situation where you have to take actions in the world.
Okay, here's a slightly more concrete version of essentially the same counting
argument that I just gave. We're going to imagine that you can conceptualize a model
as being composed of three pieces, a world model, an optimization procedure, which is
some way to optimize for some goal in that world model, and a mesa-objective, which
is just something that it's trying to optimize for. I don't necessarily think that this is
how models are actually going to look, but it's a nice concrete picture that we can
analyze and draw conclusions from that I think will generalize.
Now, the idea here is that the Christs, the Luthers, and the Pascals, they all have to
have essentially the same world model, because they all have to understand the world
well to be able to do a good job. And they will also have to have essentially the same
optimization procedure, because they all have to have some way of being able to ﬁgure
out how to accomplish particular tasks, because we want them to be able to

accomplish lots of tasks, and so they have to have some machinery for being able to
do well on the particular tasks that we want them to do.
Therefore, the main diﬀerence in complexity between these model classes is in the
mesa-objective portion—what is the complexity of the objective we have to specify that
we then throw the optimization procedure at. So, what we'll compare here is, what is
the simplest mesa-objective for Christ, what is the simplest mesa-objective for Luther,
and what is the simplest mesa-objective for Pascal. The idea here is that we're
approximating the overall simplicity by just looking at the simplest individual one in
each class. In that way, this is a little bit diﬀerent than the counting argument, which
just looks at the whole class. But basically, the way that it turns out in simplicity priors
is that you can approximate them pretty well by just looking at the simplest one in
each class, so we're going to do that here.
So what is the simplest internally aligned mesa-objective? Well, you have to specify the
whole training objective, so you have to pay for the full complexity of that.

The corrigibly aligned model is a little bit better. It just has to pay for the full
complexity of the simplest ground truth that is equivalent to the thing you want, the
simplest ground truth that always ends up with you pointed at the right thing.
And then for Pascal, the simplest mesa-objective that Pascal has to pay for is just
whatever the simplest objective is that cares about the long term—paperclips over the
long term, stamps over the long term, gold coins in the long term, whatever—just the
simplest thing that you could possibly care about, as long as it is something that you
care about over the long term.

Okay, and so which wins here? I think it's pretty clear that the deceptively aligned one
is the simplest, at least for a relatively complex training objective: it just doesn't have
to specify very much at all about the thing that we want it to do. All of the other model
classes scale with the complexity of the training objective, whereas the complexity of
the deceptively aligned model is essentially ﬁxed—so as you increase the complexity of
the thing we're trying to get the model to do, eventually deception should win out.
Now we get to speed. Speed, as I alluded to previously, is the one aspect where I think
deceptively aligned models actually just lose out.
Why do deceptively aligned models lose out here? Well, here we're trying to
understand how much computation you have to perform for each model class to result
in the correct answer. The internally aligned one already has exactly what the thing is
that it is trying to optimize for, and it just directly executes on that; the corrigibly
aligned one, maybe it takes a little bit more because it has to ﬁgure out what the
pointer is pointing to, but overall it's pretty similar.
But the deceptively aligned one has to do at least a couple of extra steps, because it
reasons, "Okay, I care about gold coins," but just knowing that it cares about gold coins
is not yet suﬃcient to be able to do a good job in the training environment. It has to do
a couple of extra steps of reasoning where it's like, "Okay, I care about gold coins and
caring about gold coins in the long term means that I should try to optimize the
training goal temporarily to try to get more gold coins eventually." And so that adds
extra steps of reasoning and some cost to the overall computation of the model. So the
deceptively aligned one is in some sense slower than the others.
I think this does give us some real hope here, and I think it's at least an inkling of a way
out. Unfortunately, I think that it's not a great hope. Why do I think that?

I think the main problem is that speed bias seems extremely uncompetitive. Why is
that? Well, there's a reason that we build bigger models, right? We build bigger models,
because we want models that have the ability to spend a bunch more compute. And
the bigger models do better. Because the more compute that they have, the more they
can ﬁnd a simpler algorithm that, though it spends more compute, is simpler and thus
does better on the task.
Here's another piece of evidence here that I think is quite strong. Let's say we did want
to implement a speed prior. We were like, "Okay, let's actually ﬁnd the minimal
compute model which is able to ﬁt the data." What would that do?
What I have up here is a double descent curve. This curve is from training language
models on a translation task. And we're varying the size of the internal representation
inside of the model. And what we want to do if we want to get something like the
maximum a posteriori estimate from a speed prior is to ﬁnd the minimal number of
parameters that are able to ﬁt the training data exactly.
So what does that do? Well, take a look at the graph. Blue corresponds to blue and
green corresponds to green here. The bottom is training loss, so we want to look at
where blue and green level oﬀ and reach approximately perfect train accuracy. But if
we look at where blue levels oﬀ on the bottom, it's at exactly where we get the worst
test loss above. And the same for green. And we see this exact same thing for lots of
other measures of number of parameters as well.
What this is saying is that, if we actually tried to take a maximum a posteriori from a
speed prior, if we took the minimal number of parameters that was able to ﬁt the data
really well, we would end up with the worst possible test performance on the whole
graph. We end up with the exact worst possible generalization performance across all
possible ways of picking the number of parameters.
So what's going on here? Well, I think that it's telling you that speed bias is
uncompetitive, it is telling you that if you really try to select the smallest model that

ﬁts the data, you don't get good generalization performance.
Why do you not get good generalization performance? You don't get good
generalization performance because real world data is not speed distributed. Real
world data is simplicity-distributed. This is sort of a realist approach to Occam's razor,
where I actually think that real world data is distributed according to a simplicity prior,
so the more you deviate from that, the worse your generalization performance is. And
so if we force our models to use the speed prior, to use the minimal-computation
algorithm that is able to solve the task, they have worse downstream generalization
behavior, because real-world data doesn't use the speed prior.
And so as we want to get models that are better at generalizing, I predict we will move
away from speed bias and towards more simplicity bias. And this is bad if we're
concerned about deception, because speed bias seems like it might have otherwise
saved us, at least in the low path dependence world.
Conclusion
Now, you've heard my case for deceptive alignment in the high path dependence
world, and in the low path dependence world. So what is my overall take?
Well, you've probably ﬁgured it out by now: my overall take is that, regardless of
whether we are in a high or low path dependence world, my current best guess is that
gradient descent is going to want to make our models deceptive. In either situation, it
seems like the inductive biases are such that, if we go back to Ajeya's picture when we
train the model on a situation where it could learn color, or it could learn shape, but
actually SGD always ends up learning the color thing, my guess is that currently,
regardless of whether we're in the low or the high path-dependence world, gradient
descent is by default going to take you towards the deceptively aligned model.

And if we believe this, we have to enact some intervention that changes the training
dynamics. We have to do something that creates pressure away from deception. And
that has to come from someplace that is not just behavioral: as we said previously, in
all of these situations, we were imagining that the model was able to perfectly
understand what we want. We were working in the limit of adversarial training. And so
if we believe the case that we're going to be pushed towards deception by default even
there, that means we need to have some way to change the inductive biases that is
not just a behavioral change, that is not just trying to add more adversarial examples,
such that we can create pressure towards the non deceptively aligned models and
away from the deceptively aligned models.

Importantly, though, it matters a lot how big the gap is. One thing that is maybe not
quite clear in this analysis is how many bits diﬀerent is a deceptively aligned model
from a non deceptively aligned one. I often like to think of our task in alignment as, at
least in my estimation, we are some number of bits underwater. By default, gradient
descent wants to push your model into being deceptive. But it's not like the non-
deceptive model is impossible—it lives there in the model space, too. So the question
is: how much does gradient descent want to push it towards the deceptive model
relative to the good model? How large are the relative sizes of these circles in the
bottom right here?
This matters a lot, because it determines the number of bits of optimization pressure
that we have to produce to modify the training process into a situation where it's going
to push towards the good thing. So if we're 10 bits underwater by default, then that's
pretty good. All we need to do is get 10 bits of optimization towards the good thing.
Maybe we can just do that with ensembling or something.
If we're 10,000 Bits underwater, that's a lot worse—maybe you have to really
fundamentally reshape the way that our training processes work if we're 10,000 bits
underwater. I think it's currently unclear, exactly quantitatively where we are, though
overall, like I said, my guess is that at least we are underwater, and we need to do
something that changes it relative to that.
Q&A
Question: Do you think that, even in the limit of adversarial training, we won't be able
to ﬁnd adversarial examples to make the deceptive model think that it has an
opportunity to defect?
You could try honeypots, try to explicitly give the model a situation where it thinks that
it could defect. You can do this, and my guess is that if you do this, sometimes you will

be able to produce examples of deceptive models. This will stop working eventually as
your model gets smarter and more able to tell when it's in a honeypot or not, but with
early deceptive models, my guess is that you will be able to ﬁnd some examples of
them.
And so we should deﬁnitely do this. At least just because, once we have an example,
we can study it more eﬀectively. But I don't think that this is a full solution, at least if all
you do is the obvious thing of retraining it to not be deceptive.
The problem is what happens next. You've produced an example of a model being
deceptive. And now the question is, how does gradient descent change the model such
that it no longer produces examples of it being deceptive? If you retrain it, it has two
options: it becomes less deceptive, or it just gets better at hiding its deception. And my
guess is that you're going to get the second one—once get an example of a model
being deceptive, and you try to train it to not be deceptive, then it just gets better at
not looking deceptive, but it doesn't necessarily train away the deception.
I think this is true, basically, in both the low and high path dependence worlds. For
example, in the high path dependence world, once it already has all the machinery to
be deceptive, it's a pretty big change to swap that whole thing out and make it
corrigibly or internally aligned or something. It's a lot easier to just be like, "Hey, you
should be a little more conservative about not defecting so wildly, be really sure that
you're actually in deployment, and there's actually a good opportunity to defect before
you do that." That's pretty simple, it just changes its overall paranoia level. And so my
guess is you'll end up with that: if you produce some example of deception, and then
try to make it non deceptive, it just becomes a little more paranoid.
Question: If you found out that that double descent was easy to eliminate or irrelevant
for some reason, would you be a lot more optimistic about the speed prior?
I am not sure exactly what that would mean. So I think that in some sense, I don't
interpret double descent as a phenomenon as being the problem. It's that double
descent is a piece of evidence that teaches us a fact about the inductive biases. You
could try to eliminate the phenomenon of double descent. But it's not clear that that
would change the underlying fact, which is that, there was a thing about the inductive
biases: speed doesn't generalize well.
Now, you could show me some experiment that looks at double descent, and I might
look at it and be like "This teaches me some facts about how to think about inductive
biases that are diﬀerent than what I previously knew." I deﬁnitely could imagine
learning a ton of useful facts about inductive biases from running experiments like that.
But I think that it is best to conceptualize it not as "double descent is the problem", but
as "double descent is a piece of evidence about the problem".
One thing that my picture predicts, and maybe you could run an experiment on this, is
that double descent should go away if you look through model space in simplicity
order. In my picture, double descent happens because the order in which models enter
into the accessible model space is diﬀerent from the criteria that SGD uses to select
from amongst models in the accessible model space, and the latter is more biased
towards simplicity. If you could disentangle these, actually add models to the support of
your prior starting with the simplest ﬁrst, then I think you wouldn't get double descent,
then I think that the ﬁrst model you would ﬁnd that ﬁts the training data well would
actually be the one with the best generalization.

Question: Is there any reason to suspect that there are no other inductive biases other
than simplicity and speed?
There's no reason to expect that and in fact, there certainly are lots of other ones. The
problem is just that we don't understand them very well. And so, at least right now, the
only ones I feel pretty conﬁdent that we understand are speed and simplicity. There are
some other things. Circuit complexity, max vs. average case speed priors, those are
some other inductive biases that we sort of understand that might be relevant here.
In fact, I think one of the plausible ways out here is to try to implement an average-
case speed prior rather than a max speed prior. Max speed is: across all inputs, what is
the maximum speed that you ever take—whereas average speed is: each input wants
to take the shortest path. In some sense, you can think of chain-of-thought prompting
as potentially giving you an average speed bias, because you always want to take like
the minimal chain-of-thought that gets you there on each individual input. And so
instead of having a ﬁxed length execution time with every input, you're incentivizing
low computation time across each input. I think this can help. I think that average
speed pushes you into a situation where, on some inputs, the model is not deceptive,
because there's a simple, easy path to solve the input, which makes deception
unnecessary, it's extra computation. But it doesn't address that there will be inputs on
which the model has to do a bunch of really complex reasoning and on those inputs,
deception may be the easiest way to do it. And so this causes it to be very input-
dependent, where sometimes it'll be deceptive and sometimes it won't. That's my
guess about what happens if you do average speed.
Question: But why expect average speed to generalize well? It seems like it has the
same problem that max speed does, which is that the world is not distributed
according to it.
Yes! I'm not claiming that it does generalize well. I'm just like, here's some other priors
that seem plausible as things that we might end up with, that I know how to analyze at
least to some extent. I think I can say a bunch of facts about average speed bias. I
think I can say a bunch of facts about circuit-size priors. There's a lot of other things
that I have thought about that I think I can say some facts about too. But of course, I
agree that there's still a question of, actually is this thing going to generalize well. One
of the biggest problems when you start writing up priors to try to solve this problem is
that almost every single one just ends up with something like lookup tables. It's really
hard to get priors to generalize well, because, as I said, Occam's Razor is a real
phenomenon.
Question: I'm wondering among these three categories, Christs, Luthers, and Pascals,
where would you place humans? Are we deceptively aligned with respect to our
learning processes and reward circuitry?
Humans are a weird example. The ﬁrst problem is that there's three levels here. Are
you talking about the neocortex relative to evolution? Are you talking about the
neocortex relative to subcortical reward systems? Are you talking about the subcortical
reward systems relative to evolution?
Question: Let's just do speciﬁcally human learned values and cognition, relative to our
hardcoded steering system, so to speak.
Sure. I think we can be deceptive sometimes. We deﬁnitely do things like eat candy
right after we do our homework. It is sort of a mix of the diﬀerent model classes,
though, which is not that surprising. The "clean" story I told where you deﬁnitely end

up with one of these things is almost certainly false. And in machine learning, also, it's
probably going to be a mess. But the same sort of analysis applies to how much of
each thing you get. And in terms of what sort of a mess it would be, at least for
humans, my guess is that yes, there absolutely is deception. At least, there's some
deception in terms of humans relative to our subcortical reward system.
Part of the problem here also is that there isn't a clear opportunity for humans to take a
catastrophic action that would overturn the reward system. But we could imagine
having that possibility. What if there is some opportunity someday for us to modify our
own reward systems. Do we take that opportunity? And the answer is that we totally
do, because we take SSRIs. Maybe in the glorious, transhuman future we're going to
modify human brains to make them better. And we're like, "Okay, we're going to plan
for eventually allowing us to get rid of this reward system we don't like."
And so is there deception? I think there clearly is. I think the story is more complex,
though, than just saying "it is deceptively aligned." It's a little bit tricky, in a way that I
expect things in the real world are. This talk is deﬁnitely best interpreted as a simpliﬁed
story that allows us to do the analysis. But in fact, it will be a complex mess.
Question: So I will say an analogy, which is probably imperfect. But please bear with
me. Suppose interpretability is the proportion of the room that is unhidden by furniture.
And during training, the deception outcomes are like cockroaches, and you're like, "Oh,
are the cockroaches under the furniture? We can't see them." So if this is a decent
model of how interpretability relates to probing deceptive alignment, it seems to
suggest that we need 100% transparency and interpretability to be okay.
So I think that the conclusion is false. I don't think you need 100% transparency and
interpretability to solve this problem. Why do I think that?
First, it really depends on what you're trying to do with your interpretability tools. If
what you do is, you see deception, and then you train it so that you don't see
deception anymore, that is a terrible thing to do with your interpretability tools. As we
said earlier, you're just going to make the model better at hiding deception. So that is a
poor use of interpretability tools for alignment. Nevertheless, I think that
interpretability tools are the key to solving this problem. But the way they solve the
problem doesn't go through that mechanism.
So what mechanisms do I think they go through? Here's one: if we're in the high path
dependence world, then we can understand what the model looks like early in training.
We can inspect and understand the proxy, and we can try to improve it by training it,
since in that situation, it's not like there is something that's actively hiding from our
interpretability tools. The deceptive model wants to actively hide from your
interpretability tool, but you don't have a deceptive model at that point. If you're
applying the tools early to guide the training process when it's just a proxy model, it's
not like the deceptive model is actually trying to hide from you, you're just providing
some extra bits of optimization in the right direction.
So the way we want to use interpretability tools is to provide extra bits of optimization
to help us understand what's going on and direct it in the correct direction, either
explicitly through passing gradients through interpretability tools or just by looking at
things early on and understanding what the trajectory is going to be. What you
shouldn't rely on, I think, is being able to ﬁx deception after the fact—the goal should
be to prevent the development of deception in the ﬁrst place.

Another important thing I'll point out here is that you can direct your interpretability
eﬀorts towards understanding the gradient descent process itself. When people think of
interpretability, they're usually focused on interpretability of the ﬁnal model—which to
be clear is something that I think is really useful. I have this whole post where I talk
about what I think the tech tree of transparency and interpretability is like, where I say,
"The ﬁrst thing is, you have to be able to have some understanding of how and what
models do." But it's not the only thing. There are seven more things that I talk about
there.
And the point is that, the next thing after understanding an individual model is having
some understanding of how we got that model, understanding the training process and
the training dynamics—this is usually thought of as inductive bias research. If we had
that and then our transparency tools told us that the model had a bad proxy or
whatever, then we could do a gradient descent update step that tries to make it so it
has a good proxy and actually believe that it would work. Because then we would
actually understand what that update step is doing. If we have the ability to
understand that the reason that it's pushing the gradient in this direction is for this
reason, then we have some reasons to believe that it's not just going to Goodhart on
our transparency tools.
Question: But in this alternative class of interpretability tools, you would still want to
get 100% of the training paths or something?
It's deﬁnitely not necessary that your transparency tools have to be 100% Goodhart-
proof. The only thing that has to be true is that gradient descent never ﬁnds the
situation in which it can Goodhart your transparency tools, and that latter condition is
much more achievable.
Question: I would think that a model doesn't necessarily need to be deceptive for
optimizing on an interpretability tool to push it in the direction of being more opaque.
Yes. That's why we want to understand the gradients. Here's one way to think about
this: what are the two adversaries that we're concerned about, if we want our
transparency tools to work? Well, there's the deceptive model that might be trying to
trick the transparency tools. And then there's gradient descent. Gradient descent is
actively gradienting on our transparency tools, and it's going to try to push them into
having some particular result, if you're gradienting on your transparency tools.
I think that using gradient descent on transparency tools is something that we probably
eventually want to do, but we are not there yet. Like I said, if you're gradient
descenting on your transparency tools, then you need some reasons to believe that
that's not going to Goodhart your transparency tools. In my opinion, the way you get
that understanding, if you're going to be training on transparency, is by understanding
gradient descent! You can't just have transparency tools which understand the model,
you also need to understand what is happening as you train the model.
And so you can look at, maybe gradient descent proposed this particular change for
this particular reason, and you can be like, "Ah, I see, that's a good way to change the
model". And if you see a change that's like, "I'm going to make a change to make the
model opaque", that's bad! And so if you understand the dynamics of the proposed
changes, then I think you'd get around this problem. Like I said, though, I think this is
next level transparency tech, we need to get here to eventually get to the point where
we can solve the problem, but we also have to start by at least understanding models
at all, and how they work. I think a lot of times people want to jump ahead, and they
want to be like, "Oh, we're going to take our current transparency tools, we're just

gonna throw them into the loss". And that doesn't work. But that doesn't mean that
transparency is not eventually useful for being able to change how we train models. It's
just that there's multiple steps along the way.
Question: So the dumbest, simplest transparency tools in my mind are: take a chain-
of-thought reasoner, and read its train of thought. Hopefully, that gives you some idea
of what it's thinking.
No! I don't think that helps with transparency. One thing it does do, and I was saying
this earlier, is that chain-of-thought can be helpful for creating an average speed bias.
What it doesn't do is, it doesn't actually give us any transparency. Here's a good way to
think about it: the deceptively aligned model can put whatever it wants in the
transcript that'll convince you that it's doing the right thing. And the internally and
corrigibly aligned model can do the same thing. The point is that there is no constraint
on whether you would get the deceptively aligned model or not, if the only way you
have to understand its internals is just by looking at the behavioral output in the chain-
of-thought.
Maybe you could get it so that all of the relevant cognition is visible in the chain of
thought. But how? By default, if you just train it to produce a chain-of-thought, each
individual forward pass could still be deceptive.
Question: Weren't you saying that you'd use the transparency tools to guide the
training process prior to the point that it becomes deceptive? Why not do that with
chain-of-thought?
Yes. And I think we should do that. But here's what I'm worried about. If we're trying to
guide it in a particular direction by giving it a behavioral incentive, when we gradient
through the behavioral incentive, that looks exactly the same as this whole analysis,
right? This whole talk was looking at what happens when we try to incentivize a model
to behaviorally act the correct way during training. And so if the only way of doing
transparency is well, I have some behavioral checks, and then I'm gonna gradient
through those behavioral checks and incentivize it to have good performance—then
that's the whole story we were just looking at, the whole problem is that we believe
that if we do gradient descent on a behavioral analysis of whether the model is doing a
good job, then the, simplest, easiest modiﬁcation that gradient descent will make to
cause the model to do a good job will be to make it deceptive. If that is true, we cannot
solve this problem just via adding more behavioral checks and gradient descenting
through them.
Maybe that's a little bit too strong, because technically, you could probably solve the
problem just by picking a behavioral loss function with just the right implicit inductive
biases. By picking a particular loss function you've made it so that some particular
paths are more likely than others. But I think that whatever you're doing someone has
to actually get around this analysis, it has to actually get around the argument for why,
by default, when you just do a behavioral training, it pushes towards deception.
Question: Can't you just reclassify the output of your interpretability tools as an
output of the model? The interpretability tools were determined by the latent
activations of the model, so isn't it like another output head?
You can conceptualize it however you want. I think that the key problem is, what things
are you constraining via what you're looking at? If the only constraints I have are on
end-to-end behavior, then all I know is that the model must be implementing some

algorithm with that end-to-end behavior. But if I've looked inside it, seen other things,
then I have more constraints on how it has to be operating. It has to be an algorithm
that includes X component, or operates in X way, or whatever.
The problem with chain-of-thought is not that it is behavioral per se, the problem is that
all of the analysis we just did in this talk applies in that case, because all of these
diﬀerent model classes are equally as good at solving that problem. And so because of
that, you haven't really created a thing which distinguishes between then, a way to
provide optimization pressure in a particular direction. The thing that we want is to
have some way to provide optimization pressure that actually distinguishes between
these diﬀerent model classes.
Let's say we had magical transparency tools that were able to look inside and ﬁnd the
model's exact proxies or whatever. What it returns is still an output, but it's an output
that actually distinguishes between these model classes. It is an output that is diﬀerent
depending on which one of these things it is and that we can use to produce some
selection pressure.
Question: I guess my response would be that I think chain-of-thought feedback does
diﬀerentiate based on which model class it is.
So, there deﬁnitely exists a deceptive model for which the deception is clear in the
chain-of-thought. The problem is that there also exists a deceptive model for which
deception is not clear in the chain-of-thought. Now, I think it gets you a couple of bits,
because the deceptive model, where it is clear in the chain-of-thought, is probably
simpler than the deceptive model where it is not clear in the chain-of-thought. And so
you have in some sense eliminated the deceptive model where it is very clear in the
chain-of-thought. But now you have to contend with the next simplest deceptive model,
where the next one doesn't have the property that its deception is very clear in the
chain-of-thought.
So the question is, just going back to this picture, how many bits did you get? I think
you got like two bits. I just don't think it makes that big of a diﬀerence. Because we're
in a situation where this next simplest deceptive model is still very simple, it's not that
much harder to think deceptively in such a way that the deception is not obvious in the
chain-of-thought. It's a little bit harder, but it's not that much harder. And so you
provided a little bit of optimization, but your intervention is relatively small. If we were
10,000 bits underwater, this is not helpful. If it's really marginal, and it turns out that
the number of bits underwater could go either way, then maybe it gives us an extra
edge, but my guess is that it's not enough of an edge.
Question: I just want to point out, one thing that inﬂuences this story is how much of
our ultimate performance comes from chain-of-thought.
That's not clear to me. It seems like the model can think in ways that are illegible to
humans, for example. So it can hide its cognition and get the beneﬁts of chain of
thought without having to show us that its deceptive.
Question: What if we use an opaque set of heuristics to generate the output that's
completely illegible to humans. And then from knowing that output, we generate an
equivalent human-readable chain of thought.
That's worse—I think that is even worse than the previous thing. Previously, at least
doing the chain-of-thought was actually instrumental in getting the answer; we got
some reason to believe that, if there was a deceptive model there, it would be a little

bit harder to implement in that context. In this case, for the illegible thing, it is just
equally as easy for it to be deceptive as non-deceptive in terms of behavior. And then
when we go to provide an explanation for it, we just always get the non-deceptive
explanation, regardless of whether the actual model is deceptive or not. There's no
reason that we ever have to provide the deceptive explanation, even for a deceptive
model. This is basically the whole Eliciting Latent Knowledge problem. Remember, the
diﬀerent model classes have the same performance, they have the exact same
behavior in training. And so you can provide the same explanation for deceptively
aligned or non-deceptively aligned models, and it'll be just as good, if you're only
checking that explanation.
1. Question: It seems to me like, if you're just going from point A to point B, it
doesn't matter how you get there, just what the ﬁnal model is.
So, that's not quite the way I'm thinking about path-dependence. So, we assume
that the model's behavior converges in training. It learns to ﬁt the training data.
And so we're thinking about it in terms of them all converging to the same point
in terms of training behavior. But there's a bunch of other things that are left
undeﬁned if you just know the training behavior, right. We know they all converge
to the same training behavior, but the thing we don't know is whether they
converge to the same algorithm, whether they converge to the same algorithm,
whether they're going to generalize in the same way.
And so when we say it has high path dependence, that means the way you got to
that particular training behavior is extremely relevant. The fact that you took a
particular path through model space to get to that particular set of training
behavior is extremely important for understanding what the generalization
behavior will be there. And if we say low path dependence, we're saying it
actually didn't matter very much how you got that particular training behavior.
The only thing that mattered was that you got that particular training behavior.
Question: When you say model space, you mean the functional behavior as
opposed to the literal parameter space?
So there's not quite a one to one mapping because there are multiple
implementations of the exact same function in a network. But it's pretty close. I
mean, most of the time when I'm saying model space, I'm talking either about
the weight space or about the function space where I'm interpreting the function
over all inputs, not just the training data.
I only talk about the space of functions restricted to their training performance for
this path dependence concept, where we get this view where, well, they end up
on the same point, but we want to know how much we need to know about how
they got there to understand how they generalize.
Question: So correct me if I'm wrong. But if you have the ﬁnal trained model,
which is a point in weight space, that determines behavior on other datasets, like
just that ﬁnal point of the path.
Yes, that's correct. The point that I was making is that they converge to the same
functional behavior on the training distribution, but not necessarily the same
functional behavior oﬀ the training distribution. ↩ 

2. Question: So last time you gave this talk, I think I made a remark here,
questioning whether grokking was actually evidence of there being a simplicity
prior, because maybe what's actually going on is that there's a tiny gradient
signal from not being completely certain about the classiﬁcation. So I asked an
ML grad student friend of mine, who studies grokking, and you're totally right. So
there was weight decay in this example. And if you turn oﬀ the weight decay, the
grokking doesn't happen.
Yes, that was my understanding—that mostly what's happening here is that it's
the weight decay that's pushing you towards the grokking. And so that's sort of
evidence of there actually just being a simplicity prior built into the architecture,
that is always going to converge to the same, simple thing.
Question: But if you turn oﬀ the weight decay then the grokking doesn't happen.
Well, one hypothesis might be that the weight decay is the thing that forces the
architectural prior there. But maybe the strongest hypothesis here is that without
weight decay there's just not enough of a gradient to do anything in that period.
Question: This isn't a question. For people who aren't familiar with the
terminology "weight decay", it's the same as L2 regularization?
Yep, those are the same. ↩ 
3. Question: Does Martin Luther over time become internally aligned? As Martin
Luther studies the Bible over time, does he become internally aligned with you?
No. Because Martin Luther never, from my perspective, at least the way we're
thinking about this here—I'm not gonna make any claims about the real Martin
Luther—but the way we're thinking about it here is that the Martin Luther models,
the thing that they care about is understanding the Bible really well. And so their
goal, whatever the Bible is, they're going to ﬁgure it out. But they're not going to
modify themselves, to become the same as the Bible.
Let's say, I'm the Martin Luther model. And I modify myself to care about my
current understanding of the Bible. And then I realized that actually the Bible was
diﬀerent than I thought the whole time. That's really bad for me, because the
thing I want originally is not to do the thing that my current understanding of the
Bible says, it's to do what the Bible actually tells me. And so if I later understand
that actually the Bible wants something diﬀerent, then the Martin Luther models
want to be able to shift to that. So they don't want to modify themselves into
internal alignment. I should also point out that, the way that we were imagining
this, it's not clear that the model itself has any control over which model it ends
up as. Except to the extent that it controls performance, which is how the
deceptively aligned model works.
Question: So Martin Luther is saying, the Bible seems cool so far. I want to learn
more about it. But I've reserved the option to not be tied to the Bible.
No, Martin Luther loves the Bible and wants to do everything the Bible says.
Question: So why doesn't Martin Luther want to change its code to be equal to
the Bible?
The Bible doesn't say, change your code to be equal to the Bible. The Bible says
do these things. You could imagine a situation where the Bible is like, you got to

modify yourself to love paper clips, or whatever. In that situation, the model says,
well, okay, I guess I gotta modify myself to like paper clips. But Martin Luther
doesn't want to modify himself unless the Bible says to.
The problem with modifying themselves is that the Martin Luther models are
concerned, like, "Hmm, maybe this Bible is actually, a forgery" or something,
right? Or as we'll talk about later, maybe you could end up in a situation where
the Martin Luther model thinks that a forgery of the Bible is its true ground source
for the Bible. And so it just cares about a forgery. And that's the thing it cares
about. ↩ 
4. Question: The point you just made about pre-training vs. ﬁne-tuning seems
backwards. If pre-training requires vastly more compute than the ﬁne-tuning a
reward model, then it seems that learning about your reward function is cheaper
for compute?
Well, it's cheaper, but it's just less useful. Almost all of your performance comes
from understanding the world, in some sense. Also, I think part of the point there
is that, once you understand the world, then you have the ability to relatively
cheaply understand the thing we're trying to get you to do. But trying to go
directly to understand the thing we're trying to get you to do—at that point you
don't understand the world enough even to have the concepts that enable you to
be able to understand that thing. Understanding the world is just so important.
It's like the central thing.
Question: It feels like to really make this point, you need to do something more
like train a reinforcement learning agent from random initialization against a
reward model for the same amount of compute, versus doing the pre-training and
then ﬁne-tune on the reward model.
Yeah, that seems like a pretty interesting experiment. I do think we'd learn more
from something like that than just going oﬀ of the relative lengths of pre-training
vs. ﬁne-tuning.
Question: I still just don't understand how this is actually evidence for the point
you wanted to make.
Well, you could imagine a world where understanding the world is really cheap.
And it's really, really hard to get the thing to be able to do what you want—output
good summaries or whatever—because it is hard to specify what that thing is. I
think in that world, that would be a situation where, if you just trained a model
end to end on the whole task, most of your performance would come from, most
of your gradient updates would be for, trying to improve the model's ability to
understand the thing you're trying to get it to do, rather than improving it's
generic understand the world.
Whereas I'm describing a situation where, by my guess, most of the gradient
updates would just be towards improving its understanding of the world.
Now, in both of those situations, regardless of whether you have more gradient
descent updates in one direction or the other, diminishing returns still apply. It's
still the case, whichever world it is, SGD is still going to balance between them
both, such that it'd be really weird if you'd maxed out on one before the other.

However, I think the fact that it does look like almost all the gradient descent
updates come from understanding the world teaches us something about what it
actually takes to do a good job. And it tells us things like, if we just try to train the
model to do to do something, and then pause it halfway, most of the ability to
have good capabilities is coming from its understanding of the world and so we
should expect gradient descent to have spent most of its resources so far on that.
That being said, the question we have to care about is not which one maxes out
ﬁrst, it's do we max out on the proxy before we understand the training process
suﬃciently to be deceptive. So I agree that it's unclear exactly what this fact says
about when that should happen. But it still feels like a pretty important
background fact to keep in mind here. ↩ 

Oversight Misses 100% of Thoughts The
AI Does Not Think
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
Problem: an overseer won't see the AI which kills us all thinking about how to kill humans,
not because the AI conceals that thought, but because the AI doesn't think about how to kill
humans in the ﬁrst place. The AI just kills humans as a side eﬀect of whatever else it's doing.
Analogy: the Hawaii Chaﬀ Flower didn't go extinct because humans strategized to kill it. It
went extinct because humans were building stuﬀ nearby, and weren't thinking about how to
keep the ﬂower alive. They probably weren't thinking about the ﬂower much at all.
Hawaii Chaﬀ Flower (source)
More generally: how and why do humans drive species to extinction? In some cases the
species is hunted to extinction, either because it's a threat or because it's economically
proﬁtable to hunt. But I would guess that in 99+% of cases, the humans drive a species to
extinction because the humans are doing something that changes the species' environment
a lot, without speciﬁcally trying to keep the species alive. DDT, deforestation, introduction of
new predators/competitors/parasites, construction... that's the sort of thing which I expect
drives most extinction.
Assuming this metaphor carries over to AI (similar to the second species argument), what
kind of extinction risk will AI pose?

Well, the extinction risk will not come from AI actively trying to kill the humans. The AI will
just be doing some big thing which happens to involve changing the environment a lot (like
making replicators, or dumping waste heat from computronium, or deciding that an oxygen-
rich environment is just really inconvenient what with all the rusting and tarnishing and ﬁres,
or even just designing a fusion power generator), and then humans die as a side-eﬀect.
Collateral damage happens by default when something changes the environment in big
ways.
What does this mean for oversight? Well, it means that there wouldn't necessarily be any
point at which the AI is actually thinking about killing humans or whatever. It just doesn't
think much about the humans at all, and then the humans get wrecked by side eﬀects. In
order for an overseer to raise an alarm, the overseer would have to ﬁgure out itself that the
AI's plans will kill the humans, i.e. the overseer would have to itself predict the consequences
of a presumably-very-complicated plan.
 

Most Ivy-smart students aren't at Ivy-tier
schools
This is a linkpost for https://www.aaronbergman.net/p/most-ivy-smart-students-arent-at
[I posted this on the EA Forum as well, and may as well leave the following note in :)] 
Note for the EA Forum 
I've decided to share this here because it seems relevant to EA hiring and community
building strategy and practice. 
I don't address/argue the normative claim that EA should focus less on college rank at the
individual (e.g., hiring) and/or community (e.g., which schools' EA groups to invest more
resources in developing) levels, but that would indeed be a non-crazy takeaway if the post
makes you update in the direction I expect it to, on average!  
Anyway, here's the post:
Intro
First, a few points to get out of the way:
1) High-minded (but totally sincere) disclaimer
Intelligence isn't a moral virtue.
One's intelligence is attributable almost entirely to luck of one kind or another, and
thus is a form of privilege like any other.
In a more just world, being called "unintelligent" or even "stupid" wouldn't bite any
more than being called "colorblind," and being told that you're "brilliant" wouldn't
leave anyone gushing with pride.
2) No bullshit faux modesty
This post is neither about me nor for me, but it is inspired by my experience, and it would be
silly and a bit disingenuous to pretend otherwise. In short:
My high school transcript was pretty damn good—almost as good as one can get in
terms of grades, course rigor, and AP/SAT test scores.
I got rejected from all ﬁve Ivy-tier colleges to which I applied (and yes, I probably
should have applied to more.)
3) This is (probably) totally ﬁne - at least at the individual level
It at least might be totally appropriate that I got rejected; there's no law that says
colleges are "supposed" to be a ruthless academic meritocracy.
I personally ﬁnd my own set of rejections mildly selﬁshly unfortunate, but not a moral
injustice.
I got into some excellent colleges, including Georgetown, which is where I
graduated a few months back and was extremely very fortunate to attend. Hoya
Saxa!

I could be convinced that, in fact, it was some sort of personal injustice, but if so
I'd nonetheless ﬁnd it a relatively quite minor one.
Upper middle class ivy rejects like me are doing just ﬁne.
Even at the societal level, such a blemish on the meritocratic veneer of higher
education itself isn't an issue. What might be, however, are individuals' (e.g., high
school me), groups' (e.g., performance-focused organization) or society's failure to
grasp the situation. All this is to say that I'm trying to get some facts on the table
so they can be recognized—not (necessarily) so they can be "corrected."
4) A key assumption: test scores and intelligence
For this post, I'm going to take as given that:
SAT scores are a pretty good proxy for intelligence, and
Intelligence (even when proxied by a single number a lá IQ) is a meaningful and
important individual-level characteristic.
A lot of ink has spilled on these topics, and I'm sure as hell not going to add anything
important to those object-level debates ¯\_(ツ)_/¯
If you think this is wrong, this post is likely going to seem wrong or misguided. Sorry.
Anyway, on to the substance!
Forbidden knowledge
I'm also writing this post because the information I'm going to present to show isn't
publicly available - at least to the best of my knowledge. I'm only able to access it because
my Naviance account from high school
1. (somehow) hasn't been disabled, and
2. gives me access to micro-level college admissions data from my high school (alone).
Namely, I can see scatterplots like the one below of anonymized individuals' (weighted)
GPA + SAT/ACT scores and college application outcomes.

I should also mention I've removed my own data point from all charts, starting
here
Secret questions
Search online, and you'll ﬁnd a thousand statistics tables detailing admissions rates and 25th
and 75th SAT score percentiles for students. You'll ﬁnd that "selective" colleges are indeed
quite selective, and that admissions rates have been falling over time.

You'll also ﬁnd that the students at top schools are really damn smart
Note the almost comically impressive fact that 75% of MIT students scored at
least 780 on the SAT math section

Source
So it's easy to ﬁnd info on students who did go to a particular school as well as on the high-
level "competitiveness" of diﬀerent colleges. So we can easily answer the question:
Given that Alex attended MIT, how smart is she likely to be?
But what you won't ﬁnd is data on the students who didn't get into these Ivy-tier schools (or
any schools, for that matter). In other words, I don't know of any way of answering the
question:
Given that Alex is as smart as the typical (or 75th percentile, or whatever) MIT (or
wherever) student, how likely is she to get accepted to MIT conditional on
applying?

Part 1) Bottom-up
Many (most?) of the best students still get rejected: evidence from one upper
middle class white kid's overachieving high school
Not really an unbiased sample, I know, but it's what I've got.
Anyway, I reviewed the scatterplots corresponding to applications to Harvard, Princeton,
Yale, MIT, and Stanford, and picked out those from students who:
1. Attended my high school, which is a good, reasonably racially diverse public school in a
relatively wealthy area with an outsized share of overachievers.
2. Applied to college in the four years from 2018 to 2021
3. Had an SAT score of at least 1560 or ACT of at least 35,[1] which:
1. Seem like pretty good proxies for intelligence
2. Correspond to about the 99.5th percentile among test takers
3. Are at around the 75th percentile for students attending the of the top-ranked
schools in the U.S
Anyway, here's what I found...
Findings
Unfortunately, the most important information can't really be estimated from the data I have
access to without making some big unjustiﬁed assumptions,[2] so one number you won't ﬁnd
in the table above is "proportion of high-achievers who get into at least one of the ﬁve
schools named conditional on applying to all"[3]

That 42.4% bottom-line rejection (or 57.6% acceptance) rate is almost certainly optimistic,
since Stanford and Yale are both looking for interesting, diverse applicants who will bring
something unique to the table - not each choosing randomly from a hat. A "worse case"
lower bound is the maximum single school acceptance rate, which in this data is Yale's at
24%. Anyway, all things considered, I'd guesstimate the "true" all-school school acceptance
rate for this population of students at about 35%.
Graphs
Aaaand here they are. Sorry about the weird sizing; the tool isn't super user-friendly.
All
Just the right column for easier viewing

And in case you want to zoom in even more, here's a Drive link with the ten original, full-res
images

Comparisons:
Here are a few plots for still-competitive-not-quite-Ivy colleges:
Surprisingly low apparent correlation between test scores and admissions, although at least
none of the highest-scorers were denied
...and the most meritocratic school of all (which I very nearly decided to attend).

Note: just the main College Park campus, which doesn't seem to have its own
logo.

Part 2) Top-down
Where are Ivy rejects going instead? Lots of places.
Looking at the data (see e.g., here or here) you ﬁnd that diﬀerent sources give slightly
diﬀerent numbers. But one general theme jumps out regardless, which I'll block quote for
emphasis, as this is really the meat of the post:
Key ﬁndings
The top ranked ~10 "Ivy tier" schools distinguish themselves by enforcing
signiﬁcantly lower variance in student quality than other top ~50 schools, once one
takes the scores of their 75th percentile-scoring student as given.
But, these 75th percentile Ivy students scored only very slightly better on the SAT than
their counterparts at the next ~40ish schools. This means that:
≥25% of students at ~50 colleges scored at least as well as the median Harvard
and Princeton student (1510 SAT). The best students at the top (at least) 50
schools have surprisingly similar academic credentials.
≥25% of students at ~85 (!) colleges (see link above) scored at least as well as the 25th
percentile Harvard and Princeton students (1460).
The 75th percentile students at the 20th (or so)-highest scoring school (conveniently for
me, Georgetown in the data I'm looking at) have SAT scores just 20 points below those of
the best-scoring schools (1550 and 1570), which corresponds to raw score diﬀerence of
about 2 questions (see here) on the 154-question SAT.

Source
 
To beat a dead horse, make a guess:

What is the rank of the college whose 75th percentile SAT score is equal to
Harvard's 25th percentile?
I'll give you one screen's worth of space to actually try answering the question, with a few
gifs to keep you entertained.
.....
.....
.....
.....
.....
.....
.....

Inappropriate for the Forum? Well too bad.
.....

.....
.....
Answer: University of Maryland, College Park, ranked #59 according to the Powers that
Be and with an acceptance rate just about 10x higher than Harvard's at 49%. Even I was
surprised to ﬁnd this out just now!
Now let's the other way, starting from what seems a reasonable representative for the old
"good but not ~elite~" private universities, Tulane University (party capital of America-of-
America—I can vouch[4] in ~beautiful~[5] New Orleans), and working our way toward the
élite.

A delightful screen capture from this article
Don't let the multi-day poly-drug Mardi Gras ragers fool you. Tulane students there are plenty
smart, with 75% of them hitting ≥1360 on the SAT (91st percentile among test takers) and
25% getting ≥1470 (97th).
Where else might you ﬁnd a good chunk of ~equally smart students (if not disposition)? You
guessed it:
Yale (lower quartile of 1470)
Harvard (1460)

Princeton (1460)
Source
What's going on?
Best I can tell, the haphazard mix of statistics I've thrown at you basically fall out of those
"key ﬁndings" bullet points above principle, which is presented here as a graphic instead:

More explicitly, the lower-level points I'm trying to convey are that:
1. I'm all but certain there are at least a few 1600-scoring students at all of the top, say,
30 U.S. colleges (and probably more than that)
2. As we move down the rankings...
1. 75th percentile SAT score falls very gradually
2. Median SAT score falls a bit faster
3. 25th percentile score falls faster still
It's surprisingly hard to ﬁnd graphing-quality data, but I did manage to make this (hideous, I
know) chart with non-made-up data for, well, read the title...

I can't ﬁnd which speciﬁc data sets I used, and I don't feel like digging around for
them ¯\_(ツ)_/¯
Conclusion
I have a bad habit of spending a ton of time on posts that never get haven't yet been
published, so I'll avoid wasting both of our time reiterating the main points from the intro.
But I will emphasize once again that—as far as I can tell—there is a total lack of public data
on applications by college, which stands in stark contrast to the cornucopia of statistics on
college attendees by school.[6]
I don't think it's unhinged to suggest that this is simply because it's not in colleges'
institutional interest to reveal how hard it is to get in. After all, high hopes→more
applicants→more rejections→lower acceptance rate→move up in the all-important
~rankings~.
But it also clearly doesn't help that doing what I've done in this post—that is, publicly lament
how unfair life has been [recall from the intro: I don't actually think this] because one's high
school academic success was followed by a string of Ivy rejections—is a little cringe. Well,
consider this my sacriﬁce then, because the optimal amount of cringe is not zero.

A Boston-area college with some very smart students
 
Edit: here's what Scott Aaronson commented on Substack:
Thanks so much for this post, Aaron!! It covers a lot that desperately needs to be better-
known by nerdy kids in the US.
Story time: in 1997, I was rejected from all ﬁve of "HYPMS" despite a 1600 SAT and a
single-author paper in a major computer science conference. (BUT: I was 15 years old,
had uneven grades and no sports or music or "leadership," and indeed had "escaped"
from high school early, getting a G.E.D. instead.) Once you understand how undergrad
admissions work, this outcome wasn't surprising in the slightest, though at the time it
felt like a death sentence.
I went to Cornell, one of two places (along with Carnegie Mellon) generous enough to
admit me with this bizarre record. I got an excellent education there, and ended up at my
ﬁrst choice of PhD program (Berkeley).
Truthfully, in the long run the rejections may have helped me, by ﬁlling me with a
burning motivation to do enough in science that someday the rejections would be a point
of pride. And whenever I get depressed, I can tell myself that at least I can now cross
that particular teenage fantasy oﬀ the list -- having, e.g., been tenured faculty at one of
the same ﬁne institutions (MIT) that rejected me for undergrad.
These days I run the Quantum Information Center at UT Austin, where almost every year
I meet undergrads who I would've been thrilled to have at MIT.
I'm sharing this here because I hope some nerdy, academics-focused kid who feels like
their life is over because they got rejected from HYPMS will read it, and it will mean
something to them.
1. ^
Eyeballing the scatterplots, it doesn't look like setting an SAT threshold of 1570-1600
or ACT threshold of 36 would change the results. I kept it at 1560 to preserve a decent
sample size.

2. ^
More speciﬁcally, it depends on how similar each school's acceptance process is (or,
more technically, the covariance of each distribution)
3. ^
Or proportion who are accepted to any top-tier school, but ﬁve seemed like a
reasonable number for me to manually look at
4. ^
My sister's undergraduate home, which is why it jumped out to me.
5. ^
And [very] hot and humid.
6. ^
For hyper-contingent reasons, I have access to a small subset of individual-level data
from Naviance, but that's not saying much

Introducing Pastcasting: A tool for
forecasting practice
TL;DR: Visit pastcasting.com to forecast on already resolved questions you don't have
prior knowledge about to get quick feedback on how you're doing.
We may try to coordinate multiplayer sessions in our Discord
Motivation
We want to make it easy for people to get better at forecasting. Existing tools are not
well optimized for practicing relevant skills.
Forecasting platforms and betting markets[1] have slow feedback loops between
predictions and question resolution. The questions with shorter time horizons
often feel less important and may be systematically diﬀerent than those with
longer ones. Their scoring systems also incentivize constantly keeping
predictions up to date and often heavily reward being the ﬁrst to react to news
Calibration training[2] isolates the skill of intuiting probabilities and conﬁdence
intervals but doesn't help with other aspects of forecasting (choosing reference
classes & base rates, trend extrapolation, coming up with considerations,
investigating diﬀerent views, and determining the trustworthiness of news
sources). Additionally, there is usually no reference point to compare your
accuracy against.
Pastcasting
With pastcasting, you can:
Forecast on already resolved questions from a vantage point further in the past
Use our ﬁltered search engine ("Vantage Search") to look up relevant
information without accidentally revealing the answer
Receive immediate feedback on your forecasts and get scored against the crowd
Host friendly multiplayer competitions where you and your friends can
simultaneously pastcast on the same questions and see who does best
How it works
Question sources
Our questions are currently pulled from resolved Metaculus and GJOpen questions.
Scoring

We use relative log scoring against the original crowd forecast at that time. This
means that you will receive zero points if you submit the same value as the crowd.
The scoring rule is also strictly proper, meaning that your expected score is
maximized if you report your true beliefs.
Vantage Search
Our preliminary results (in yellow) come from a search api with a restricted date range
up to the vantage date. To further reduce information leakage (from the website
changing its contents), we then pass the results through the internet archive api (in
blue). This tends to be much slower, so some pastcasters opt to use the preliminary
results directly.  
Prior Knowledge
In many cases, users will have speciﬁc knowledge of how a particular question was
resolved (from reading about it in the news, participating in forecasting that question,
etc). We provide a button to skip these questions, and over time we will show
questions that many users know the answer to less often.
Give it a try!
If this sounds useful, give it a try and give us feedback on what features would be the
most useful and any aspects of the site you ﬁnd confusing.
 
1. ^
Such as Metaculus, Manifold Markets, etc...
2. ^
Such as https://www.openphilanthropy.org/calibration,
https://acritch.com/credence-game/, Wits & Wagers.

everything is okay
This is a linkpost for https://carado.moe/everything-is-okay.html
(this is a work of ﬁction)
it's been four years since the singularity. someone pressed the button, and their
preferences were implemented across the cosmos. i don't think anyone knows who
pressed the button; that is probly how they'd like things to be. maybe they don't know
themself.
i wake up and cuddle with my partners for a while. we live in a log cabin, which was
currently somewhere near forests and mountains, somewhere in washington state i
think.
i don't know if it's actually washington state, because i don't care about being
uploaded. it could be that 10¹⁰⁰ objective years have passed since the singularity and
that now that it's got the compute it needs, Elua has started running our simulations.
it could be that it's entirely remade the cosmos into a shape we cannot conceive. or it
could be that this is actually earth, in its full physical continuity, actually four objective
years after the singularity. all of these are okay to me.
"Elua" is what i've called the superintelligent singleton that now rules over everything.
i call it that because of an old pre-singularity writing, which i still like. most of my
friends have picked up on that name, but i hear some people in the nearby small town
call it god. many people out there probly don't even know about Elua, because they
would prefer not to. i'm sure even for them, everything is okay.
i wonder if someone out there cares about being uploaded. i wonder if their
preferences have been satisﬁed, and if so how. what i am pretty conﬁdent about, is
that whatever the situation, somehow, they are okay.
one of my partners goes to put on some music. we have something like a vinyl player.
i like that the device is legible — the sound wave that we hear has been encoded into
the physical shape of the object, and so the whole way the device works is
understandable by a human mind. we don't really need a player of course, we could
just magically hear whatever we wanted, without any artefacts. but i like things this
way. i like the rustic experience of being a human manipulating tools. and it's not like
they would ever actually get in the way of what i want to any extent which wouldn't
be okay.
sometimes i talk with Elua in my dreams. i could talk to her anywhere, but dreams
seem like a nice context for it. i've used lucid dreams to reshape my body a few times,
and then woken up with my new body. it's mostly similar to the one i had before the
singularity; i want to stay in a mostly grounded human experience, at least for now.
maybe one day i'll explore much more alien forms of existence, as i'm sure many are
doing already; but for the moment, this is what feels okay.
certainly, i don't suﬀer any grave illnesses; i do get a bit under the weather
sometimes, because i think it's okay for that to happen.
i decide to check on how my friend is doing. i open the cupboard and ﬁnd my crystal
ball, put it on the table, and say the name of my friends. when some piece of

technology has to be illegible, i like having it presented as magic. if this is the inside of
a simulation that is getting ad-hoc intervened with for that device to work, then it
might as well be magic anyways. both this kind of reliable magic, and other more
mysterious forms of magic, are okay.
i speak the name of a friend to the ball, and then make an eﬀort to focus on it. the
focus does not do anything to the ball, but it makes it that my sensorial input of the
ball is much ampliﬁed, and my input of the rest reduced. my friend is immediately
available for communication — whether by either of us getting paused long enough
for the other to become available, or because they actually were — and after greeting
me, they reports what it's been like to expand their intellect a millionfold and study
ever expanding maths. they tell me about some unimaginably elegant theorems
they've found out about. as they say this, my focus makes it that i can see my friend
as if they were standing in front of me, and they point at mathematical shapes
ﬂoating in the air. i semi-consciously let them enter my mind, and the mathematical
structures permeate my understanding. they are not visual, but truly mathematical,
as if a logic-percieving module was attached to my mind to percieve mathematical
logic directly. i appreciate my friend's discoveries, but i also discreetly chuckle at how
cute they are when they get excited about it. i tell them about how i've been taking it
easy but, percieving that they're not particularly interested, i let them get back to
their stuﬀ. our goodbye is a bit awkward, but that's okay.
by a ﬂick of the mind, i retract my focus from the crystal ball, at which point the smell
of toast strikes me. after getting my bearings for a second or two i put it back in the
cupboard, and head to the small living room to see what my partners have been
cooking. it's toasted bread with some sort of cheesey-creamy stuﬀ on it. i don't know
if the cheese appeared at the store magically, or if comes from fake animals that exist
for the sake of people who want to partake of farming, but i don't have to worry about
anything like meat industry scale suﬀering. something like that would just not happen
— everything that does happen is okay.
we decide to go into town. the town is pretty small — not many people are in the
streets. various stores are open. most give stuﬀ away for free, while some sell it for
money. money has become strange since the singularity. some people choose to care
about it, and there are some scarce things it can track, such as the use of someone's
time; but it doesn't make sense to track much else, such as material resources. so
most people kind of just don't bother. even land in an absolute sense is not scarce; it
seems like Elua's solution to some people such as me wanting to live on something
like a single earth, has been to add more space in between existing space. the total
amount of land that "earth" consists of may very well have doubled since the
singularity, by now. somehow, it's all arranged such that traveling to somewhere you
wanna go leads you there, but travelling aimlessly does get you to many new places.
we can even get lost sometimes, when we're okay with that.
it is mid-winter, but i can't be bothered to put on something warm; nevertheless, i
barely feel cold: i'm semi-consciously opting for it to feel just a bit chilly, reducing the
pain of cold but still getting the informational sensation of it, the way some people
pre-singularity would be born with the full information but none of the sensation of
pain. in any case, feeling just a bit chilly is okay.
we go to the adventure guild, where i posted a quested for a playstation 1. i did give
some currency as a reward — moreso to not feel bad that i'm using someone's time,
even though the people who fulﬁll quests are all pretty much happy to do so — they're
people who want their life to provide value and meaning to others, and for most of

them those others must actually be real people; and it wouldn't be okay for Elua to
just create people out of nowhere to create an artiﬁcial demand, so it doesn't. and so,
there is a genuine market mismatch, in more people wanting to fulﬁll quests. despite
the fact that adventurers are the ones gaining most of the value from this system, the
custom has remained that it is the quest poster who pays the adventurer — it's not
like money is very important anyways, so what might in a previous era have been
considered terrible market ineﬃciency, is now more than okay.
the language used in town is basically english, with some internet meme slang thrown
in there. it also has some pretty local characteristics, but hasn't diverged that much —
people value using english as a lingua franca around here, and as for me and my
partners, we reserve for private use the artistic constructed language we've
developed together. i like english, it's good. and sometimes, people around don't
speak it, and we just ﬁnd something that they do know, or ask a local who can help
translate, or even kinda just gesture at each other and work things out like that.
anyone could just choose to have their brain understand any language they want, or
even communicate by thought, but i like sticking to communities that share this
humancore artefact that is highly imperfect verbal communication. even when there
are misunderstandings, it's not a big deal; it's okay.
just as we arrive in the store, an adventurer comes back with the genuine playstation
1 we'd requested. probly not a coincidence, probly fortunate timing arranged by Elua.
well, it's not like the timing would've been correct anyways: some time dilation has
certainly taken place, considering the adventurer tells us how it's taken them several
weeks to ﬁnd that playstation due to them committing to not using Elua's help, while
on my end i remember posting the quest just the day before. the adventurer recounts
to us his adventure ﬁnding the playstation 1, driving to various pawn shops in the
area, and asking people. i had made the quest kind of hard: i had requested a
playstation that had existed physically continuously since the singularity, not one that
had been created out of thin air or even constructed since the singularity, nor a pre-
singularity one that had been copied into multiple instances. but he did ﬁnd one, and
the journey he was on made me feel, as i head back home with my new playstation,
like this playstation now carries an extra bit of meaning.
as soon as i get home i simply plop the playstation in front of the chalkboard that we
use as a TV, grab the controller, and put on the copy of metal gear solid i'd obtained a
while ago. it's just as great a game as i'd rembered, and while i'm focusing on it, one
of my partners watches while the other asks if they can play with me, and when i say
yes they sit where i'm sitting, the two of us temporarily occupying the same physical
location, so that we can hold the controller at the same time while our minds
intermingle as they open to one another. we could have also magically duplicated the
controller and taken turns, but it is more fun this way, each of us taking controlling the
playing character to various degrees, and also having a shared piece of mind keeping
track of what we're doing together, so as to not have to verbally communicae our
intentions. we are fully focused on the game and on each other and we scarcely feel
time go by, such that when my other partner calls to our attention to get dinner, it's
already dark out. the days go by fast when we take things this easy, but it's okay; it's
not like time is scarce.
we have some great tartiﬂette, and then head to bed, to chit chat and cuddle before
sleep. we talk about what to do tomorrow, and decide to have the cabin move
somewhere unexpected during our sleep, so we can go explore some new
surroundings. maybe we'll wake up in a diﬀerent continent, and we'll do some

adventurous hiking, reassured by the feeling that whatever happens, everything will
be okay.
afterword
writing utopia is important. it's not just a good way to get people to start actually
thinking about how good things could actually be, but also, if something like PreDCA is
the kind of benevolent superintelligent singleton we get, we have to start acting in
ways that make us people who tend to express our values. we have to cultivate
ourselves and each other to wish for good worlds, and realize how much we'd dislike
bad ones. we need to help make future benevolent superintelligence's job at realizing
our values as easy as it can be, and to make our expressed values clear through our
actions, such that if we do start up an AI which extrapolates our values from our
actions, it gets the correct idea. ﬁnally, writing utopian ﬁction is just plain fun, and i
ﬁnd it good motivation to work on AI risk mitigation: think carrot, not stick.

What's the Least Impressive Thing
GPT-4 Won't be Able to Do
It seems like GPT-4 is going to be coming out soon and, so I've heard, it will be
awesome. Now, we don't know anything about its architecture or its size or how it was
trained. If it were only trained on text (about 3.2 T tokens) in an optimal manner, then
it would be about 2.5X the size of Chinchilla i.e. the size of GPT-3. So to be larger than
GPT-3, it would need to be multi-modal, which could present some interesting
capabilities. 
So it is time to ask that question again: what's the least impressive thing that GPT-4
won't be able to do? State your assumptions to be clear i.e. a text and image
generating GPT-4 in the style of X with size Y can't do Z.

I'm mildly skeptical that blindness
prevents schizophrenia
(Low conﬁdence, written in a hurry.)
I was (randomly) trying to make sense of schizophrenia the other day, and there's a piece of
the puzzle that just seems not to ﬁt with everything else: namely, the claim that being
congenitally blind (or becoming blind in the ﬁrst 5-6 years of life) prevents schizophrenia.
Random example of the claim that I'm disputing. You can ﬁnd it all over the place.
If that's true, then, as the saying goes, "I notice that I am confused". In terms of how
schizophrenia seems to work at a low level (note that I'm very much not an expert), I can't
currently make any sense of how congenital blindness would prevent schizophrenia from
developing. There are papers that discuss this topic and propose explanations, but they all
seem to be grasping at straws.
So purely on priors, I ﬁnd myself skeptical of whether early blindness actually reduces the
risk of schizophrenia. So I spent a couple hours looking into it this weekend. Here's what I
found out, or you can just skip to the conclusion at the bottom.
Claims & Evidence
The claim is often stated as: "there are no reported cases of people with
schizophrenia who were born blind or who developed blindness shortly after
birth". This claim seems to be false. There was one person that turned up in Morgan et
al. 2018 (which I'll return to below), and better yet, Leivada & Boeckx 2014 dug up a handful
of case reports—see that paper for details.
So then Leivada & Boeckx 2014 narrowed the hypothesis to: congenital cortical blindness is
protective against schizophrenia, congenital peripheral blindness is not. (Terminology in
footnote.[1]) That seems like an odd and unjustiﬁed direction for people to have pivoted. The
problem is: congenital cortical blindness is even more uncommon than congenital blindness
overall—that paper says 18% of congenitally blind people are cortically blind, while the data
in Morgan et al. 2018 implies that it's ≈10%. If the authors only had a handful of total reports
of congenitally-blind schizophrenics in the ﬁrst place, and if only 1 out of 5-10 congenitally-
blind people are more speciﬁcally cortically-blind, then we really shouldn't be shocked that
none of that handful of documented cases is cortically-blind. That seems like the kind of
thing that could easily happen by coincidence.
OK, leaving aside the false "it has never happened" claim, and the unjustiﬁed "cortical
blindness" topic, we can retreat to a weaker form of the hypothesis: comorbidity of

schizophrenia and congenital-blindness has happened, but it happens much less
often than if they were statistically independent. Do we have evidence on this claim?
The best kind of evidence would be a population-wide systematic study. Unfortunately, the
null hypothesis (schizophrenia and congenital-blindness are independent) has them co-
occurring at a rate of 0.72% × 0.03% = 2 in a million. So you need a really big population to
get good evidence on this hypothesis. Morgan et al. 2018 grabbed data from Western
Australia, but it turned out (in my assessment) that they didn't have enough data to come to
a conclusion one way or the other.[2] Jefsen et al. 2020 grabbed Danish data and found the
same (non) conclusion.
Well, so much for that plan! Alternatively, we can look at how many published case reports of
congenitally-blind schizophrenics there are, and compare it to how many we would have
normally expected. If there are many fewer than expected, that's evidence that blindness
protects against schizophrenia. See Silverstein et al. 2013 for a nicely-laid-out argument that
this is the case.
Again, if schizophrenia and congenital-blindness were independent, something like 0.72% ×
0.03% = 2 in a million people would have it, i.e. 650 people in the USA. But the number of
case reports is (they say, although see above) 0. I'll admit it: 650 is a lot more than zero. But
not everyone with schizophrenia gets properly diagnosed. (I think the 0.72% is total
incidence, not how many people are diagnosed, but I could be misreading, here's the
source.) And there's a much much bigger problem: not every psychiatrist, when treating a
blind schizophrenic person, will publish a case report about them! Y'know, people are busy!
Jeez!!
So, what's the probability that any given blind schizophrenic patient will wind up
immortalized in the literature as a case report? "Much less than 1 in 620" does not
immediately strike me as implausible. I'm not an expert, of course. I don't know how easy
and routine it is for psychiatrists & psychologists to publish case reports. Maybe every
neighborhood therapist is just publishing case reports left and right, whenever they have a
spare moment. Beats me! I tried to look up how many psychology case reports get published
per year, but couldn't easily ﬁgure it out, please comment if you know.
What about unpublished anecdotes? Well, as soon as I looked, I did in fact quickly ﬁnd a
couple people on the internet oﬀering anecdotal reports of congenitally-blind schizophrenics
—quora example, reddit example. Granted: people sometimes lie on the internet! But for
what it's worth, the quora person claims to be a "DeafBlind Tech Trainer, ASL Interpreter, CVI
Consultant" and claims to have met at least three blind schizophrenics "including
congenitally blind people", and this person has a bunch of other quora posts that seem
pretty good and serious and consistent with their claimed identity—it's not just a trolling
account, or if it is, it's an unusually high-eﬀort trolling account!
Conclusion
My tentative conclusion is that, if I have a good way to think about schizophrenia, and
everything hangs together well, except that it can't explain why congenital / early blindness
is protective against schizophrenia, I think I should stick with the theory and declare that
congenital / early blindness is not actually protective against schizophrenia after all!
To be perfectly clear, if I had two equally-good-in-every-way theories of schizophrenia, and
Theory A predicted that congenital blindness was somewhat protective against
schizophrenia, and Theory B didn't, I would consider that a point in favor of Theory A. It
would just be a pretty small point in favor of Theory A.
1. ^

If I understand correctly, "cortical blindness" means that you're blind because there's
something wrong with your occipital lobe, and "peripheral blindness" means you're
blind because there's something wrong with your eye, retina, etc.
2. ^
This is my assessment. The authors deﬁnitely didn't put it that way. As far as I can tell,
the authors did no statistical analysis whatsoever; the only nod to statistics is a
throwaway comment that their study is "possibly underpowered" (!). OK ﬁne, let me
get out my calculator. If I'm doing the math right, they expected to ﬁnd 2.5 people in
the early-peripherally-blind schizophrenia group and actually found 1, which is a p-
value of 0.29; and they expected 0.264 people in the early-cortically-blind
schizophrenia group and actually found 0, which is p-value of 0.77. If that's right, then
consider me underwhelmed! But please someone double-check my math!

«Boundaries», Part 2: trends in EA's
handling of boundaries
This is Part 2 of my «Boundaries» Sequence on LessWrong, and is also available on the EA
Forum .
Summary: Here I attempt to constructively outline various helpful and harmful trends I see
in the EA and rationality communities, which I think arise from a tendency to ignore the
boundaries of social systems (rather than a generic tendency to violate norms).  Taken too
far, this tendency can manifest as a 'lack of respect' for boundaries, by which I mean a
mixture of
1. not establishing boundaries where they'd be warranted,
2. crossing boundaries in clumsy/harmful ways, and
3. not following good procedures for deciding whether to cross a boundary.
I propose that if some version of «respecting boundaries» were a more basic tenet of EA —
alongside or within the principles of neglect, importance, and tractability — then EA would
have fewer problems and do more good for the world.
The trends
Below are the (good and bad) trends I'd like to analyze from the perspective of respecting
boundaries.  Each of these trends has also been remarked by at least a dozen self-identiﬁed
EAs I know personally:
1. expansive thinking: lots of EAs are good at 'thinking outside the box' about how to
do good for the world, e.g., expanding one's moral circle (example: Singer, 2011).
2. niche-ﬁnding: EA helps people ﬁnd impactful careers that are a good ﬁt for their own
strengths and weaknesses (example: 80k, 2014).
3. work/life balance: EA has sometimes struggled with people working too hard for the
cause areas, in ways that harm them as individuals in an unsustainable way.
4. romances at work: people dating their co-workers are an example of a professional
boundary being crossed by personal aﬀairs.
5. social abrasiveness: EA culture, and perhaps more so rationalist culture, is often
experienced by newcomers or outsiders as abrasive or harsh.  (Hypocrisy ﬂag: I think
I've been guilty of this, though hopefully less over the past few years as I've gotten
older and reﬂected on these topics.)
6. pivotal acts: numerous EAs seriously consider pivotal acts — i.e., a potential
unilateral acts by a powerful, usually AI-enabled actor, to make the world permanently
safer — as the best way to do good for humanity (as opposed to pivotal processes
carried out multilaterally).  
7. resistance from AI labs: well-established AI labs are resistant to adopting EA culture
as a primary guiding inﬂuence.  
I'm going to analyze each of the above trends in terms of boundaries, partly to illustrate the
importance of the boundary concept, partly to highlight some cool things the EA movement
seems to have done with boundaries, and partly to try helping somewhat with the more
problematic trends above.
1. Expansive thinking

Consider a ﬁctional person named Alex. A "job" for Alex is a scope of aﬀairs (features of the
world) that Alex is considered responsible for observing and handling.  Alex might have
multiple roles that we can think of as jobs, e.g. "oﬃce manager", "husband", "neighbor".
Alex probably thinks about the world beyond the scope of his job(s).  But usually, Alex
doesn't take actions outside the scope of his job(s):
The Eﬀective Altruism movement has provided a lot of discourse and social context that
helps people extend their sense of "job" to include important and neglected problems in the
world that might be tractable to them-personally, e.g., global poverty (tractable via
donations to Give Directly).

In other words, EA has helped people to expand both their circle of compassion and their
scope of responsibility to act.  See also "The Self-Expansion Model of Motivation in Close
Relationships" (Aron, 2023).
2. Niche-ﬁnding
Identifying areas of competence and vulnerability are important for scoping out Alex's job(s).
This is just standard career advice of the kind that 80,000 Hours might share for helping
people ﬁnd a good job (Todd, 2014; Todd, 2021), but I'd like to think about it from the
perspective of boundaries, so let me spell out some nuances.
By competence I just mean the ability to do a good job with things from the perspective of
relevant stakeholders (which might depend on the stakeholders).  By vulnerabilities, I mean
ways in which a person's functioning or wellbeing can be damaged or harmed. A person is is,
after all, a physical system, and physical systems can pretty much always be damaged in
some way, which needn't be limited to his body.  For a hypothetical person named "Alex", we
can imagine instances of:
(high vulnerability, low competence) Alex is bad at managing his ﬁnances.  His
friend Betty helps him pay his taxes every year and plan for retirement. If Betty messes
up, Alex could end up poor or in trouble with the government. So, for Alex this is an
area of high vulnerability and low competence.
(high vulnerability, high competence) Alex is good at keeping his car maintained.
This is also a point of vulnerability, because if his car breaks on the highway, he could
get hurt. But he's competent in this area, so he can take care of it himself.
(low vulnerability, high competence) Alex is also good at graphic design work. And,
occasionally when he designs some graphics that people turn out not to like, he's
totally ﬁne. This is an area of low vulnerability and high competence.

 
In an ideal world, Alex's job has him engaging with aﬀairs that he's good at handling, and
that won't harm him if mishandled, i.e., areas of high competence and low vulnerability:
 

 
3. Work/life balance
More recently, leaders in EA have been trying to talk more about boundary-setting
(Freedman, 2021; GWWC).  I think a big reason this discussion needed to happen was that,
early on, EA was very much about expanding and breaking down boundaries, which may
have led some people to dissolve or ignore what would otherwise be "work/life" or
"personal/professional" boundaries that protect their wellbeing as individuals.  In other
words, people started to burn out ('Elizabeth', 2018; Toner, 2019).   
Since jobs change over time, Alex may need to establish boundaries — in this case, socially
reinforced constraints — to protect vulnerable aspects of his personal life from the day-to-
day dealings of his work:

Whether the "even more ideal" or "slightly trickier" career path will be better for Alex in the
end, the boundaries he sets or doesn't set will be a major factor determining his experience.

4. Romances at work
Eﬀective altruism's "break lots of boundaries" attitude may have also contributed to — or
arisen from — a frequent breakdown of professional/social distinctions, and might have lead
to frequent intertwining of romantic relationships with work (Wise, 2022), and/or dating
across diﬀerences in power dynamics despite taboos to the contrary (Wise, 2022).  Whatever
position one takes on these issues, at a meta level we can observe that the topic is about
where boundaries should or should not be drawn between people.
5. Social abrasiveness
EAs can sometimes come across as "abrasive", which, taken literally, basically means
"creating friction between boundaries".  Stefan Schubert  writes, in his post "Naive Eﬀective
Altruism and Conﬂict (2020)":
... people often have unrealistic expectations of how others will react to criticism. Rightly
or wrongly, people tend to feel that their projects are their own, and that others can only
have so much of a say over them. They can take a certain amount of criticism, but if they
feel that you're invading their territory too much, they will typically ﬁnd you abrasive.
And they will react adversely.
Notice the language about 'territory', a sense of boundary that is being crossed.  This
observation isn't just about an arbitrary social norm; it's about a sense of space where
something is being protected and, despite that, invaded. Irrespective of whether EA should
be more or less observant of social niceties, a sense of boundaries is key to the pattern he's
describing.
Relatedly, Duncan Sabien recently wrote a LessWrong post entitled Benign Boundary
Violations, where he advocates for harmless boundary violations as actively healthy for
social and cultural dynamics.  Again, irrespective of the correctness of the post, the
«boundary» concept is crucial to the social dynamics in question (and post title!).
6. Pivotal Acts
(This part is more so characteristic of rationalist discourse than EA discourse, but EA is
deﬁnitely heavily inﬂuenced by rationalist memes.)
I recently wrote about a number of problems that arise from the intention to carry out a
"pivotal act", i.e., a major unilateral act from a single agent or institution that makes the
world safer (-, 2022).  I'll defer to that post for a mix of observations and value judgement on
that topic.  For this post, it suﬃces to say that such a "pivotal act" plan would involve
violating a lot of boundaries.  
7. Resistance from AI labs
A lot of EAs have the goal of inﬂuencing AI labs to care more about EA principles and
objectives.  Many have taken jobs in big labs to push or maintain EA as a priority in some
way.  I don't have great written sources on this, but it seems to me that those people often
end up frustrated that they can't convince their employers to be more caring and ambitious
about saving the world.  At the very least, one can publicly observe that 
DeepMind and OpenAI both talk about trying to do a lot of good for humanity, and
both have employed numerous self-identiﬁed EAs who intended to promote EA culture
within the organization, but 

neither lab has publicly espoused the EA principles of importance, tractability, and
neglect. 
FaceBook AI Research has not, as far as I know, employed any self-identiﬁed EAs in a
full-time capacity (except maybe as interns).  I expect this to change at some point, but
for now EA representation in FAIR is weak relative to OpenAI and DeepMind.
There are many reasons why individual institutions might not take it on as their job to make
the whole world safe, but I posit that a major contributing factor is that sense that it would
violate a lot of boundaries.  (This is kind of the converse to the observation about 'pivotal
acts'.)  By contrast, in my personal experience I've found it easier to argue for people to play
a part in a pivotal process, i.e., a distributed process whereby the world is made safer but
where no single institution or source of agency has full control to make it happen.
8. Thought experiments
EAs think a lot about thought experiments in ethics.  A lot of the thought experiments
involve norm violations that are more speciﬁcally boundary violations; for a recent example,
see "Consequentialists (in society) should self-modify to have side constraints"(R, Cotton-
Baratt 2022).
What to do?
Proposing big changes to how EA should work is beyond the scope of this post.  I'm mostly
just advocating for more thinking about boundaries as important determinants of what
happens in the world and how to do good.
What's the best way for EA to accommodate this?  I'm not sure!  Perhaps in the trio of
"importance, neglect, and tractability", we could replace "tractability" with "approachability"
to highlight that social and sociotechnical systems need to be "approached" in a way that
somehow handles their boundaries, rather than simply being "treated" (the root of
'tractable') like illnesses.
Irrespective of how to implement a change, I do think that "boundaries" should probably be
treated as ﬁrst-class objects in our philosophy of do-gooding, alongside and distinct from
both "beliefs" (as treated in the epistemic parts of the LessWrong sequences) and
"values|objectives|preferences".
Recap
In this post, I described some trends where thinking about boundaries could be helpful to
understanding and improving the EA movement, speciﬁcally, patterns of boundary expansion
and violation in expansive thinking, niche-ﬁnding, work/life balance, romances at work, social
abrasiveness, pivotal act intentions, and AI labs seeming somewhat resistant to EA rhetoric
and ideology.   I haven't done much to clarify what, if anything, should change as a result of
these observations, although I'm fairly conﬁdent that making «boundaries» a more central
concept in EA discourse would be a good idea, such as by replacing the idea of "tractability"
with "approachability" or another term more evocative of spatial mataphor.
This was Part 2 of my «Boundaries» Sequence on LessWrong, and is also available on the EA
Forum .

Seriously, what goes wrong with
"reward the agent when it makes you
smile"?
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Suppose you're training a huge neural network with some awesome future RL
algorithm with clever exploration bonuses and a self-supervised pretrained multimodal
initialization and a recurrent state. This NN implements an embodied agent which
takes actions in reality (and also in some sim environments). You watch the agent
remotely using a webcam (initially unbeknownst to the agent). When the AI's activities
make you smile, you press the antecedent-computation-reinforcer button (known to
some as the "reward" button). The agent is given some appropriate curriculum, like
population-based self-play, so as to provide a steady skill requirement against which
its intelligence is sharpened over training. Supposing the curriculum trains these
agents out until they're generally intelligent—what comes next?
The standard response is "One or more of the agents gets smart, does a
treacherous turn, kills you, and presses the reward button forever." 
But reward is not the optimization target. This story isn't impossible, but I
think it's pretty improbable, and deﬁnitely not a slam-dunk. 
Another response is "The AI paralyzes your face into smiling." 
But this is actually a highly nontrivial claim about the internal balance of
value and computation which this reinforcement schedule carves into the
AI. Insofar as this response implies that an AI will primarily "care about"
literally making you smile, that seems like a highly speculative and
unsupported claim about the AI internalizing a single powerful decision-
relevant criterion / shard of value, which also happens to be related to the
way that humans conceive of the situation (i.e. someone is being made to
smile).
My current answer is "I don't know precisely what goes wrong, but probably something
does, but also I suspect I could write down mechanistically plausible-to-me stories
where things end up bad but not horrible." I think the AI will very probably have a
spread of situationally-activated computations which steer its actions towards
historical reward-correlates (e.g. if near a person, then tell a joke), and probably not
singularly value e.g. making people smile or reward. Furthermore, I think its values
won't all map on to the "usual" quantities-of-value:
80% credence: It's very hard to train an inner agent which reﬂectively equilibrates
to an EU maximizer only over commonly-postulated motivating quantities (like #
of diamonds or # of happy people or reward-signal) and not quantities like (# of
times I have to look at a cube in a blue room or -1 * subjective micromorts
accrued).
So, I'm pretty uncertain about what happens here, but would guess that most other
researchers are less uncertain than I am. So here's an opportunity for us to talk it out!

(My mood here isn't "And this is what we do for alignment, let's relax." My mood is
"Why consider super-complicated reward and feedback schemes when, as far as I can
tell, we don't know what's going to happen in this relatively simple scheme? How do
reinforcement schedules map into inner values?")

Less Threat-Dependent Bargaining
Solutions?? (3/2)
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
In the previous two posts, we went over various notions of bargaining. The Nash
bargaining solution. The CoCo value. Shapley values. And eventually, we managed to
show they were all special cases of each other. The rest of this post will assume
you've read the previous two posts and have a good sense for what the CoCo value is
doing.
Continuing from the last post, the games which determine the payoﬀ everyone gets
(not to be confused with the games that directly entail what actions are taken) are all
of the form "everyone splits into two coalitions S and N/S, and both coalitions are
trying to maximize "utility of my coalition - utility of the opposite coalition"".
Now, in toy cases involving two hot-dog sellers squabbling over whether to hawk their
wares at a beach or an airport, this produces acceptable results. But, in richer
environments, it's VERY important to note that adopting "let's go for a CoCo equilibria"
as your rule for how to split gains amongst everyone incentivizes everyone to invent
increasingly nasty ways to hurt their foes. Not to actually be used, mind you. To aﬀect
how negotiations go.
After all, if you invent the Cruciatus curse, then in all those hypothetical games where
your coalition faces oﬀ against the foe coalition, and everyone's utility is "the utility of
my coalition - the utility of the foe coalition"... well, you sure can reduce the utility of
the foe coalition by a whole lot! And so, your team gets a much higher score in all
those games.
Of course, these minimax games aren't actually played. They just determine
everyone's payoﬀs. And so, you'd end up picking up a whole lot of extra money from
everyone else, because you have the Cruciatus curse and everyone's scared of it so
they give you money. In the special case of a two-player game, getting access to an
option which you don't care about and the foe would pay $1000 to avoid, should let
you demand a 500$ payment from the foe as a "please don't hurt me" payment.
 
But Which Desiderata Fails?
Step 1 in ﬁguring out how to get an outcome which is Not That is to look at the list of
nice properties which the CoCo solution uniquely fulﬁlls, and ﬁgure out which one to
break.
As it turns out from picking through the paper, the assumption that must be broken is
the axiom of "gaining access to more actions shouldn't lead to you getting less value".
As a quick intuitive way of seeing why it should fail, consider the game of Chicken. If
you physically can't swerve (because your car started oﬀ not having a steering
wheel), and are locked into always going straight through no fault of your own, then
any sensible opponent will swerve and you will get good payoﬀs. Adding in the new

option of being able to swerve means the opponent will start demanding that you
swerve sometimes, lowering your score.
As a rough intuition for how the "the CoCo value is the only way to fulﬁll these
axioms" proof works, it is reasoning as follows:
"Hm, let's say I only had access to my minimax action, that maximized 
minb(U1(a, b) −U2(a, b)). It makes me relatively much better oﬀ than my foe. Any
sensible foe going up against this threat would simply press a button that'd just give
us both the CoCo value, instead of playing any other action in response. By the axiom
of "adding more options can't hurt me", I can add in all my other actions and not get
less money. And then by the axiom of "adding redundant actions doesn't aﬀect
anything", I can take away the foe's CoCo value button and nothing changes. And so,
in this game against the foe, I have to get a value equal or greater to my CoCo value.
But the foe can run through this same reasoning from their side, and so in this game,
we must both get the CoCo value."
And this makes it very clear exactly what's going wrong in the reasoning. Just because
I'd be willing to give a foe a lot of money if they were unavoidably locked into ﬁring oﬀ
the Cruciatus curse through the random whims of nature and no action of their own or
from anyone else could have prevented it, that does not mean that I'd be willing to
give them a bunch of money if they had any other options available.
But just because I can diagnose that as the problem doesn't mean I've got an entirely
satisfactory replacement lined up as of the writing of this post. I just have a few
options which seem to me to have promise, so don't go "oh, Diﬀractor solved this".
Treat it as an open question deserving of your thought where I have a potential way
forward, but where there might be a solution from a diﬀerent direction.
In the spirit of proposing alternate options which have some unsatisfactory parts but
at least are less dependent on threats, here's my attempt at ﬁxing it.
 
Kinder Fallback Points
An alternate way of viewing what's going on with the CoCo solution is as follows: Both
players are tasked with coming up with a fallback strategy if they can't come to an
agreement. The pair of fallback strategies played against each other deﬁnes a
disagreement point. And then, any bargaining solution fulﬁlling symmetry (which is
one of the essential properties of a bargaining solution, Nash fulﬁlls it, Kalai-
Smorodinsky fulﬁlls it, all the other obscure ones fulﬁll it) will say "hey, instead of
playing this disagreement point, what if you maximized the total pile of money
instead and split the surplus 50/50?"
The place where the CoCo value speciﬁcally appears from this protocol is that, if you
know that this is what happens, your best fallback strategy to win in the overall
bargaining game is the action a that maximizes minb(U1(a, b) −U2(a, b)). The foe
playing any fallback strategy other than the counterpart of this means you'll get more
money than the CoCo value says, and the foe will get less, after bargaining concludes.
But there's something quite suspicious about that disagreement point. It only is what
it is because the foe knows you're guaranteed to go for a 50/50 split of surplus

afterwards. Your foe isn't picking its disagreement strategy because it actually likes it,
or because it's the best response to your disagreement strategy. Your foe is behaving
in a way that it ordinarily wouldn't (namely: minimizing your utility instead of purely
maximizing its own), and it's only doing that because it knows you'll give in to the
threat.
A disagreement point to take seriously is one where the foe is like "this is legitimately
my best response to you if negotiations break down", instead of one that the foe only
picked because you really hate it and it knows you'll cut a bargain. For the former, if
negotiations blow up, you'll ﬁnd that, oh hey, your foe is actually serious. For the
latter, if negotiations actually start failing sometimes, the foe will suddenly start
wishing that they had committed to a diﬀerent fallback strategy.
And so, this points towards an alternate notion instead of the CoCo value. Namely,
one where the disagreement point is a Nash equilibrium (ie, both players are purely
trying to maximize the utility of "negotiations break down" given what the other
player has as their disagreement strategy, instead of looking ahead and trying to win
at bargaining by hurting the other), and surplus is maximized and split 50/50 after
that.
Although it's a lot better to use correlated equilibria instead of Nash equilibria. Since
we're already assuming that the agents can implement any joint strategy, it's not
much of a stretch to assume that they can fall back to joint strategies if negotiations
break down. Correlated equilibria generalize the "nobody wants to change their move"
property of Nash equilibria to cases where the agents have a joint source of
randomness. Alternately, Nash equilibria can be thought of as the special case of
correlated equilibria where the agents can't correlate their actions.
There are two other practical reasons why you'd want to use correlated equilibria over
Nash equilibria. The ﬁrst is that the set of correlated equilibria is convex (a mixture of
two correlated equilibria is a correlated equilibrium, which doesn't hold for Nash), so
the set of "possible disagreement payoﬀs" will be a convex subset of R2, letting you
apply a lot more mathematical tools to it. Also, ﬁnding Nash equilibria is
computationally hard, while ﬁnding correlated equilibria can be done in polynomial
time and it's easy to come up with algorithms which converge to playing correlated
equilibria in reasonable timeframes. (though I don't know if there's a good algorithm
to converge to correlated equilibria which are Pareto-optimal among the set of
correlated equilibria)
Ok, so our alternate non-threat-based notion instead of the CoCo value is "both
players settle on a correlated equilibrium as a backup, and then evenly split the
surplus from there". If you blow up the negotiation and resort to your fallback
strategy, you'll ﬁnd that, oh hey, the foe's best move is to play their part of the
fallback strategy, they didn't just pick their backup strategy to fuck you over.
 
Two Big Problems
One is that set of possible fallback points will, in general, not be a single point (rather,
it will be a convex set). So, uh... how do you pick one? Is there a principled way to go
"y'know, this is a somewhat unfair correlated equilibrium"?

The second problem is generalizing to the n-player case. In general, the surplus gets
split according to the Shapley value, to give a higher share to players who can
signiﬁcantly boost the value of most coalitions they're in. Player i gets
∑i∈S⊆N
(v(S) −v(N/S))
as their payoﬀ.
But this has an issue. How is v(S) deﬁned? It should be something like "the value
earned by coalition S". For the CoCo value, v(S) is deﬁned as "the value that team S
 gets in their zero-sum game against team Everyone Else". But for using correlated
equilibria as the fallback points, it gets harder. Maybe v(S) is "the value that team S
 gets when they play a correlated equilibrium against not-S, where S and not-S are
both modeled as individual large players". Or maybe it's "the value that team S gets
when they play a correlated equilibrium against the uncoordinated horde of Everyone
Else, where S is modeled as one big player, and everyone else is modeled as
individuals". And this completely overlooks the issue of, again, "which correlated
equilibrium" (there are a lot)
I mean, it'd be nice to just say "give everyone their Shapley payoﬀs", but the value
produced by a team working together is pretty hard to deﬁne due to the dependence
on what everyone else is doing. Is everyone else ﬁghting each other? Coordinating
and making teams of their own?
So, it's a fairly dissatisfying solution, with waaaay too many degrees of freedom, and a
whole lot of path-dependence, but I'm pretty conﬁdent it's attacking the core issue.
Suggestions for alternate threat-resistant strategies in the comments are extremely
welcome, as are pointing out further reasons why this or other strategies suck or don't
fulﬁll their stated aim, as well as coming up with clever ways to mitigate the issues
with the thing I proposed.
(n−s)!(s−1)!
n!

Reﬁning the Sharp Left Turn threat
model, part 1: claims and mechanisms
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
This is a linkpost for https://vkrakovna.wordpress.com/2022/11/25/reﬁning-the-sharp-
left-turn-threat-model/
This is our current distillation of the sharp left turn threat model and an attempt to
make it more concrete. We will discuss our understanding of the claims made in this
threat model, and propose some mechanisms for how a sharp left turn could happen.
This is a work in progress, and we welcome feedback and corrections. 
What are the main claims of the "sharp left
turn" threat model?
Claim 1. Capabilities will generalize far (i.e., to many domains)
There is an AI system that:
Performs well: it can accomplish impressive feats, or achieve high scores on
valuable metrics.
Generalizes, i.e., performs well in new domains, which were not optimized for
during training, with no domain-speciﬁc tuning.
Generalization is a key component of this threat model because we're not going to
directly train an AI system for the task of disempowering humanity, so for the system
to be good at this task, the capabilities it develops during training need to be more
broadly applicable. 
Some optional sub-claims can be made that increase the risk level of the threat
model:
Claim 1a [Optional]: Capabilities (in diﬀerent "domains") will all generalize
at the same time
Claim 1b [Optional]: Capabilities will generalize far in a discrete phase
transition (rather than continuously) 
Claim 2. Alignment techniques that worked previously will fail during this
transition
Qualitatively diﬀerent alignment techniques are needed. The ways the
techniques work apply to earlier versions of the AI technology, but not to the
new version because the new version gets its capability through something new,
or jumps to a qualitatively higher capability level (even if through "scaling" the
same mechanisms).
Claim 3: Humans can't intervene to prevent or align this transition 

Path 1: humans don't notice because it's too fast (or they aren't paying
attention)
Path 2: humans notice but are unable to make alignment progress in time
Some combination of these paths, as long as the end result is insuﬃciently
correct alignment
Arguments for the claims in this threat model
Claim 1: There is a "core" of general intelligence - a most eﬀective way of
updating beliefs and selecting actions (Ruin #22). Speculation about what the
core is: consequentialism /  EU maximization / "doing things for reasons". 
Claim 1a: Capability gains from intelligence are highly correlated (Ruin #15)
Claim 2: There is no simple core for alignment (Ruin #22), Corrigibility is anti-
natural (Ruin #23)
Claims 1 & 2: arguments in Will capabilities generalize more? (+ Ruin #21)
Claim 3: We can't coordinate to avoid AGI (Ruin #4)
Mechanisms for capabilities generalizing far
(Claim 1)
Capabilities will generalize far if learning / reasoning / cognitive work is done outside
of the optimization process, similarly to how human cultural evolution happens
outside genetic evolution and proceeds faster. Here are some mechanisms for
cognitive work getting done outside the optimization process:
Mechanisms proposed by Paul Christiano: performing search internally, natural
selection occurring internally, reasoning about how to think better, deploying
subagents
The AI develops a way to outpace SGD as the way to learn / improve, similar to
how humans outpace evolution by cultural transmission + within-lifetime
learning. E.g., it writes Google docs to itself, or searches over prompts (or
whatever is the prompt analog at that point). 
The optimization process ﬁnds a general meta-learning algorithm, which
replaces the previous solution of domain-speciﬁc heuristics and greatly
increases the rate of learning
Sequential reasoning: sharp increase in eﬀective sequential memory/cognitive
time between the "aligned" regime and the sharp left turn regime. Ways in
which this can happen:
The model is just given access to memory. Its knowledge grows over time,
and reaches a critical point which causes a sharp left turn. Alignment
schemes before the critical point appear to work, but not after.
We try to restrict the model's memory, but fail. E.g. the model's outputs
are used in the world, and the subsequent inputs carry information about
previous outputs.
Some input causes the model to access a new faster mode of thinking (e.g.
an explanation of how to do step-by-step reasoning in some domain). The
same compute that on previous inputs powered e.g. one subjective day of
thinking on this new input now powers one subjective week of thinking.
This allows the model to evade alignment schemes.

Mechanisms for a rapid phase transition
(Claim 1b)
A rapid phase transition happens if there is a capability overhang: the AI system is
improving at various skills continuously, but its improvement in many domains is
bottlenecked on one speciﬁc skill, and at some point it receives some input that
makes its existing capabilities much more eﬀective. Here are some ways this can
happen: 
The system acquires situational awareness, and now its world model includes
understanding of the system's place in the world and how its training and
deployment works. It can plan using this knowledge, which leads to more
eﬀective use of existing capabilities. 
Analogy to few-shot prompting: the capabilities are already present in the
trained artifact. Any alignment technique that goes through gradient updates
becomes irrelevant. Putting the artifact into the "right" situation (e.g., giving it a
few-shot prompt) reveals its capabilities relevant to this situation. Mechanism:
the relevant knowledge and capabilities are installed by some generic
pretraining optimisation process.
Discovering a more eﬀective way to make use of low quality data leads to more
eﬀective use of existing capabilities. 
We will discuss mechanisms for Claim 2 in a future post. 

COVID-19 Group Testing Post-
mortem?
As Covid is for the most part over and tests have not been in short supply for a very
long time now, eliminating most of the rationale for complicated testing procedures
meant to optimize test count/cost, it would be interesting for anyone who was
involved in this to do a retrospective or post-mortem. Was pool testing useful? Why or
why not? Can it be improved for future pandemics and should we care about it?
My impression, not having paid much attention to the details at the time (AI scaling
was more important), is that various group/pool testing approaches were useful in a
few niches, but were rendered useless in many places due to
politics/organization/social ﬂaws, and didn't apply to most places.
1. I read that use of pool testing was common in China early on in 2020 as part of
the hotspot strategy to economize on testing extremely large groups of people
where only a few would have active Covid. This is the most logical place to apply
it, and it was a major government & societal priority, so that is a success.
I get the impression that they were able to switch to individual testing fairly
early, in order to get faster turnaround and do more localized testing. But to the
extent that pool testing was eﬀective from the start, before testing
manufacturing could scale up, and helped squash Covid when it was still
squashable, then it was very valuable even if obsoleted.
Of course, this evaluation can change depending on how Covid ultimately
shakes out in China. Was it part of a heavyhanded but ultimately highly eﬀective
Covid Zero policy which led to it turning into a hermit country enduring
devastating economic fallout while merely postponed the inevitable variant
which could defeat Covid Zero? In which case, pool testing was a tool which
played a part in buying China entire years to vaccinate but its good was
rendered useless when they bizarrely failed to make any good use of the time,
dabbling in ineﬀective vaccines etc, and in practice, only doubling down
repeatedly on coercive testing and surveillance until the martingale blew up in
their face. Or was it part of a visionary resolute eﬀort which spared China the
mass death of the rest of the world as it came out the other side by persisting so
long in country-wide lockdown until Covid just sorta stopped being a problem
somehow? Then its good was unalloyed and it's sad so few other countries could
boast a better record (eg Taiwan; I dunno if they made any use of it).
2. Sewage/water testing in places like colleges: various universities or towns made
heavy use of what is in eﬀect pool testing to do mass screening of water to
detect infections and then hotspot them. This was only useful as long as you
have a baseline of ~0, otherwise testing water merely gives you a proxy for
overall infections, which is a lot less useful than using it to detect & squash
hotspots. In individual cases of orgs with great discipline, this apparently worked
well, and avoided the need for constant individual testing (which was expensive,
not possible early on, and still inadequate - as all the infections inside 'bubbles'
show, just too much leakage, test error, incubation, idiosyncrasies and so on).

Since most of these places were in countries where Covid became endemic
eventually anyway, and everyone got infected, and in cases like colleges the
young students were the last to get vaccinated (because they are the least
harmed) and so needed to buy a lot of time to avoid infection before vaccination
(which probably didn't happen for most of them), these success stories seem to
boil down to just buying time, which is a good deal less impressive.
At the margin, doesn't seem like improvements to pool testing would have helped
much compared to other things like faster vaccination approval.

The Expanding Moral Cinematic
Universe
A subtheme for me this year has been grappling with how to reconcile diﬀerent facets
of my morality. 
Part of this has to do with reconcile the virtues of "Protect yourself, maintain slack, be
aligned with yourself and your community" and "But, maybe the world is
metaphorically and/or literally on ﬁre. How do you want to relate to that?"
Part of this has to do with "man, the story of the Expanding Circle of Concern may not
be as nice as I thought, in ways that fundamentally challenge my conception of what
'my particular morality' even means, or whether it is coherent." (This has gone hand-
in-hand with me realizing my conception of 'community' may also not really be
coherent, which has been challenging for my identity).
Along the way, I've watched some movies and shows that feel like they grapple with a
particulate facet of the expanding circle of concern, and they're accumulating into a
private (public now) headcanon of the Expanding Moral Cinematic Universe.
Narratives that feature characters in a harsh, bloody world who inch their little corner
of the universe forward as a place where friendship and cooperation can form. Less a
sea of blood and violence and mindless replication.[1]
Here are three vignettes, written over the course of the last year (originally on
Facebook, later posted on shortform).

Part I: The Fox and the Hound
Originally posted January 24th on Facebook .
I watched Disney's The Fox and The Hound a few weeks several months ago. I cried a
bit.
While watching the movie, my girlfriend commented "so... they know that foxes are
*also* predators, right?" and, yes. They do. This is not a movie that was supposed to
be about predation except it didn't notice all the ramiﬁcations about its lesson. This
movie just isn't taking a stand about predation.
This is a movie about... kinda classic de-facto tribal morality. Where you have your
family and your tribe and a few speciﬁc neighbors/travelers that you welcomed into
your home. Those are your people, and the rest of the world... it's not exactly that
they aren't *people*, but, they aren't in your circle of concern. Maybe you eat them
sometimes. That's life.
Copper the hound dog's ingroup isn't even very nice to him. His owner, Amos, leaves
him out in a crate on a rope. His older dog friend is sort of mean. Amos takes him out
on a hunting trip and teaches him how to hunt, conveying his role in life. Copper
enthusiastically learns. He's a dog. He's bred to love his owner and be part of the pack
no matter what.
My dad once commented that this was a movie that... seemed remarkably realistic
about what you can expect from animals. Unlike a lot of other disney movies it didn't
require suspending disbelief much.
A baby fox and hound might totally play together because they haven't ﬁgured out yet
that they're supposed to be enemies.
If that hound then went away for 6 months to learn to hunt, and came back, it might
initially be hesitant to hunt down its fox friend, out of a vague/confused memory.
But interspecies friendship isn't *that* strong, and doing-what-your-species/tribe does
is often stronger, and yeah later on the hound is like "okay I guess we're hunting this
fox now. It's what master wants. I do what master says, that's who I am."
And then...
...well, and then the hound gets attacked by a bear. And the fox comes back to save
him. And on one hand, I bet 99+% of foxes would not do that. But, having seen a
bunch of youtubes of animals doing impressive things, I'm willing to buy that the
occasional hero foxes exist, and I'm willing to buy a fox who remembers enough of his
interspecies friend to intervene bravely.
(weakly held, if someone who knew a lot more about foxes than me was like "nope,
this is outside the space of what foxes do", I'd believe them)
And what gets me is... man, this is what my morality is built out of. People who were
mostly doing what nature or society incentived, who are still on the verge of eating
their friends... but there's little sparks of friendship/compassion/abstract-reasoning

that build up along the way, combined with the abundance necessary for them to
grow. And that's where my* morality comes from. 
Man.

Part II: Princess Mononoke
Originally posted February 13th on Facebook .
I just A few months late I rewatched Princess Mononoke, and... I'm ﬁnding that this is
grounded in the same sort of morality as The Fox And The Hound, but dialed up in
complexity a bunch.
The Fox and The Hound is about a moral landscape where you have your ingroup, your
ingroup sometimes kills people in the outgroup, and that's just how life is. But
occasionally you can make friends with a stranger, and you kinda bring them into your
tribe.
Welcoming someone into your home doesn't necessarily mean you're going to take
care of them forever, nor go to bat for them as if they were literally your family.
But in this ingroup-eat-outgroup world, there are occasional glimmers of heroism.
People make friends across tribal barriers, and they try to make those friendships work
despite the diﬃculties.
It is possible for a fox to remember his friendship with a hound, and decide that it's
worth ﬁghting a bear to save his friend. This is a simple enough moral decision that it
is just at the edge of a literal-fox's ability to glimpse it, and decide to be a hero.
And it is these little ﬂickers of heroism that that slowly push the moral landscape from
an ingroup-eat-outgroup world, to a world where people's circle of concern is broader,
and more complex relations between tribes can evolve.
...
Princess Mononoke is a world where  tribes of humans and spirits are trying to make a
home for themselves. Sometimes, other tribes (of humans, or spirits) want resources
in your territory and try to ﬁght you for it.
There is heroism within a tribe, as people struggle to survive and thrive. Ingroups
grow - Lady Eboshi makes the choice to rescue lepers and whores. She sees potential
in them, and she forges them into Iron Town, aiming to make a better life for them
than they had before.
But between groups lie zero-sum-games. To survive, they must cut down the forest,
and go to war against the spirits.
The spirits are... perhaps "natural", but their morality isn't much diﬀerent. They
defend their tribe, they ﬁght, they kill, they eat. They are at war with the humans and
they are losing, but in a slightly diﬀerent timeline they might have been winning, and
they wouldn't treat the humans any better than the humans are treating them.
Miyazaki intends there to be _something_ special about the spirits that the humans
aren't respecting, which aﬀects the ecosystem. But, fundamentally this is a moral
landscape where no one has the slack or abundance to really think about ecosystems
or how to negotiate towards peace.

And into this world comes Ashitaka the traveler, who walks among diﬀerent tribes.
Diﬀerent people welcome him brieﬂy into their homes, and he treats them with
respect and they respect him, but he is not one of them. But he crosses between
enough circles of concern to see...
...there is something really sad about this world where people war over limited
resources, killing each other to better themselves.
In his heart, is a little glimpse of something better.
He's smarter than Todd the Fox. Todd the fox seems a simple ﬁght between his friend
and a bear, and he saves his friend. Ashitaka sees a world of decades-long conﬂict
and there is no simple solution, and he doesn't really have a very good plan for ﬁxing
anything. He stumbles his way into diﬀerent conﬂicts and sees people hurting and
tries locally to help the people in front of him.
But soon he's made friends with each of them. And as they are all locked in conﬂict,
his eﬀorts to help just shuﬄes the damage around.
With a bit of luck, by the skin of his teeth, his eﬀorts lead to a world that is a bit better
and more peaceful. For now.
His confused, bumbling heroism inches the world slightly towards a moral landscape
where people can think longer term, consider (somewhat) the value of the ecosystem,
form trading partnerships with more people, and build a better world.
It isn't much. It's still mostly an ingroup-eat-outgroup world. I think Lady Eboshi does
more to improve the world than Ashitaka does - she's a clever leader, she's able to
make actually good plans, she's able to establish trade relations from a position of
power. She doesn't try to help everyone, she doesn't overextend, she doesn't bumble
her way through conﬂict. She slowly builds an empire where she can take care of
people. She employs the virtue of ruthlessness where it is necessary.
But, in little spurts of heroism, the intersection of people like Lady Eboshi, and people
like Ashitaka, inches the world towards the sort of morality that I care about.

Part III: Gendy Tartakovsky[2]'s Primal
Originally posted August 25th on Facebook
Okay, I'm adding the show "Primal" to my Expanding Moral Cinematic Universe
headcanon. Of the pieces so far, I'd put:
1. Primal
2. The Fox and the Hound
3. Princess Mononoke
in roughly ascending order of "how much latent spirit of cooperation exists in the
background for the protagonists."
The Fox and the Hound each have a teeny ingroup of 2-3 people, and enough safety
net that the characters can begin a friendship purely of play. There is death,
predation, tribal conﬂict. The characters face an uphill battle to maintain their
friendship and connection. But they are not alone. Their friendship is built on
sedimentary layers of empathy, and trade. The protagonist's allies warn them "foxes
and hounds can't be friends", but notably, those allies *know what friendship is and
why it's desirable*.
Princess Mononoke's world is one of medium-scale tribes, each of which has complex
coordination going on within it, and many of whom have some ability to trade with
other trades, a sense of honor and reputation.
Primal is about about cave man and a t-rex (named "Spear" and "Fang") who become
allies, and then friends.
The Primal world includes tribes who coordinate within each other, but they are
remote pockets in a brutish, dino-eat-dino world. The protagonists climb out of a
background-state of bloodshed, isolation, and meager survival.
The characters ﬁrst become allies by necessity. They are bad at being allies. But they
learn how to be good allies, and as they come to trust each other they learn to be
friends.
Their friendship... isn't completely built out of nothing. The cave man was raised
among a small tribe, and in slightly diﬀerent circumstances his story might have been
more similar to the Fox and the Hound. He has some sense of what it can mean to
have a safety net, and love, and companionship. But it is so precious little - teeny
scraps and glimpses of what it can mean to have connection. The whispers of family-
ship in this world are so close to being snuﬀed out at any given moment. Storms and
quakes, our own mistakes, 
The show is very slow and meditative. There is not much going on. Sleep. Hunt. Spend
hours walking to get places or watching silently as you prepare to strike, alone in the
wilderness. There is only the next kill, and avoiding being someone's next kill.
I'm only a few episodes in and not sure where this is going [fake edit: now 7 episodes
in, and the show also clearly just wants to be a cool Pulp Action Adventure]. But I
doubt that Spear and Fang will have much luxury of trusting or cooperating with

almost anything else they meet. Their circle of concern only gets to grow by the tiniest
inches to include each other.
But in my headcanon so far, in this world, two creatures reach across species lines,
and kindle the beginnings of friendship in a world where such things barely exist at all.

Part IV: Fine Distinctions in "What
counts as The Expanding Moral
Cinematic Universe?"
When I posted Part 3 on facebook, several people suggested other works that might ﬁt
into the EMCU, and I began wondering about other works I'd seen. Movies that came
up include The Land Before Time, How to Train Your Dragon, Walking Dead, Guardians
of the Galaxy, Mad Max and Smallfoot[3]. 
I've been looking at several people's suggestions and thinking "hmm, that doesn't feel
right", but then struggling awhile to articulate why. I'm noticing that there's a lot of
common movie tropes ("Romeo and Juliet", "Boy + His Monster", "Kids making friends
from Across the Tracks"), which seem like they meet the basic criteria, and make this
type of movie fairly common. But I feel some "naah" reaction to that. 
I can tell that at least part of my reaction is, up until last week, I'd been thinking of
this as a fairly rare precious thing I had discovered, so maybe I'm just embarrassed to
realize "oh actually yeah this is super common."
But, I do think there are also some important distinctions here that are worth tracking.
I think in the platonic ideal of the trope I'm seeing includes...
1. The characters aren't just expanding their own moral scope. They're
expanding the frontier of what morality even means in their world. 
Guardians of the Galaxy doesn't really count, because while the characters develop
morally, basically everyone else in the movie agrees those characters were loser
assholes. Their character arcs are about getting "remedial morality". 
Meanwhile, Walking Dead is a similar genre but it feels less about pushing moral
frontiers forward, and more about people struggling to hold onto their existing
morality as the existing social fabric crumbles and characters struggling to hang onto
it. This is perhaps the same mental operation in some sense. It still involves realizing
that the world has diﬀerent moral constraints than you'd realized. And it still requires
giving up things important to you to form a new morality, adapted to the times. From
the perspective of the characters, I think it subjectively feels similar. 
But it's not bringing something new into the world that hadn't existed before in earlier
harsh times.
(I haven't seen Mad Max but I suspect it's similar?)
2. The movie engages with the fact that expanding the moral circle is hard.
It looks like betrayal to the people around you. It requires giving up some things that
were important to you.
2B. The cinematography, camera angles, music etc doesn't treat it as a
foregone conclusion that expanding the moral circle is the right thing to do.
The "viewpoint morality" of the world is either "neutral", or reﬂects the

previous/current generation morality, not the new morality that the characters are
adopting or 21st century morality.
How to Train Your Dragon feels like it doesn't make it here, although I think reasonable
people can disagree, and I'm not sure it's actually less true in Dragon than it is in
Fox/Hound. I might have just been overly impressed with Fox/Hound because it was
the ﬁrst movie that gave me this vibe.
Honestly, most other Miyazaki movies fail here - I think Princess Mononoke is fairly
unique in the degree to which it avoids spelling out who the heroes and villains are,
and how you're supposed to feel about the narrative.
The neutral point of view is important to me because, well, any other viewpoint is a
lie. It presents the story as if morality was destined to be created one particular way.
To be a true origin story of morality, it should come about by accident in a universe
that didn't intend it.
3. The movie feels "realistic" in the ways that matter. Realistic game theory,
believable characters. And the movie feels like it's sophisticated about that realism - it
understands the interplay between the character's motivations, character arcs, and
brute facts of the surrounding environment.
It's important to me in Fox and the Hound that Todd and Copper seem like an actually
plausible fox and hound - Todd[4] is a 99th percentile hero fox, but, seems to basically
be a fox.
Princess Mononoke has all kinds of weird unrealistic spirit powers, but the game
theory between the groups feels like reasonable game theory.
Of my original three, Primal is weakest here. Not the part where it takes place in a
fantastical world where cavemen and T-rexes and weirder monsters lived side-by-side
- that's ﬁne. And not the part where the T-rex feels Pretty Smart for a T-rex (I'm not
sure how smart T-rexes are but I'm ﬁne with Fang being a 99th percentile T-rex)
What's wrong with Primal here is that the world is presented as bloody and dangerous,
but in a very inconsistent way with unclear rules. How powerful Spear and Fang are
depends on what kind of cool ﬁght scene the writers wanted to portray. And the ﬁght
scenes and the moral-development aren't separate magisteria - the whole point of
their relationship is that the world is dangerous and it's worth their eﬀort to learn to
be friends, so the world being inconsistently dangerous tarnishes the vibe for me.
4. The expanding moral circle feels like a primary theme in the movie.
I suspect Primal will also become weaker at this as I progress more in the series. (By
episode 7 it's becoming more clear the writers ALSO just really want to tell a cool
adventure story, although I think it's still a dominant theme so far)
Smallfoot is interesting here because it totally has all the right themes. It presents
morality as being legitimately hard to change/expand. I think it's sort of iﬀy on the
neutral cinematography (I think it actively fails in the ﬁrst half of the movie, but by the
second half the camerawork seems to at least be acknowledging the situation is
complex and it's not clear what the right thing to do is. I think it does this more by ﬂip-
ﬂopping on which viewpoint the cinematography represents rather than being neutral)

But Smallfoot also feels... like it's more a movie about epistemics and curiosity than it
is about ingroup expansion?
On the other hand, I think an interesting thing about Smallfoot is it showcasing ways
morality has to grow/expand other than in-group expansion. The integration of
epistemics and group cohesion is indeed an important part of moral development.
5. The piece feels "timeless and mythological". I get a vibe watching it that the
authors didn't just tell a cool story, but presented something like a platonic origin
story for a particular fragment of morality. The more unnecessary details and
plotpoints dilute this a bit. 
Primal looks to be veering away from this the more I watch of it.
(This is all "the particular thing Ray was getting out of Fox/Hound, Mononoke and
Primal", which is not to say it's what everyone else should be trying to get. But, if
you're trying to download this particular aesthetic, these are some important points.)

Part V. "My" Morality?
I've been describing these as the origin story for "my" morality, and it seems
important to distinguish that from "morality generally", or even "morality for everyone
reading this post." I have major disagreements with several LessWrong contributors
and people in the x-risk ecosystem (who in turn don't all agree with each other. That's
okay. I love you and/or value your allyship anyway).
At a broad level, I divide my morality into two major buckets[5]:
First, coordination morality, which I think has something close to an objectively correct
answer (i.e. given a group of agents who value various things, and have varying
power levels, and varying levels of information, there is some fact of the matter of
what is the most utility they can gain. There isn't a single right answer to how to
coordinate, but some answers are clearly righter than others). 
Second, there's I dunno, stuﬀ I just care about morality. This includes helping
powerless people I would never trade with, and (perhaps if I'm being honest, more
commonly) includes cool art and cute narratives.
It also includes love, and friendship. And it includes deciding that those things matter
for their own sake, even if they were originally some kind of instrumental accident of
evolution. 
What makes my heart ﬂutter about these movies is the intersection of these two
things. 
The character growth, and the civilizational growth present in Princess Mononoke is in
some sense inevitable - the laws of physics and evolution dictate that groups are
going to go to war, are going to expand, are going to bump into each other, and
eventually ﬁgure out how to coordinate in higher order structures. Cooperation and
negotiation allow for more resources. Aliens would develop coordination. Nonsentient
robots would develop coordination.
But there's something warm and special about the story of Friendship. 
Friendship was born in stories like Primal. It matters that Spear and Fang are not just
fair-weather-friends, but people who grow to care about each other deeply and work
exhaustively to protect each other for their own sake. They ﬁght and bleed for each
other. They do so in a harsh world that demands it - without each other, they would
die.
But millenia pass by. Small ﬂickers of friendship are fostered alongside utilitarian
coordination. And one day, maybe, a Fox and a Hound are born into a world that's Just.
Barely. Reached the point where they can be friends for essentially no practical
purpose at all. 

Part 0. The Gift We Give To Tomorrow
The ﬁrst story I read that really sets the stage for all of this, the Ur-Prequel, the
genesis story of how this all began, is Eliezer's The Gift We Give To Tomorrow.
If you haven't ready it in full, I highly recommend it. I almost want to quote-block the
entire thing. But, here is it's ending, which is simultaneously the beginning and (I
hope) the end[6] of the story.
But come on... doesn't it seem a little... amazing... that hundreds of millions of
years worth of evolution's death tournament could cough up mothers and fathers,
sisters and brothers, husbands and wives, steadfast friends and honorable
enemies, true altruists and guardians of causes, police oﬃcers and loyal
defenders, even artists sacriﬁcing themselves for their art, all practicing so many
kinds of love?  For so many things other than genes?  Doing their part to make
their world less ugly, something besides a sea of blood and violence and mindless
replication?
[...]
Love has to begin somehow, it has to enter the universe somewhere.  It is like
asking how life itself begins—and though you were born of your father and
mother, and they arose from their living parents in turn, if you go far and far and
far away back, you will ﬁnally come to a replicator that arose by pure accident—
the border between life and unlife.  So too with love.
"A complex pattern must be explained by a cause which is not already that
complex pattern.  Not just the event must be explained, but the very shape and
form.  For love to ﬁrst enter Time, it must come of something that is not love; if
this were not possible, then love could not be.
"Even as life itself required that ﬁrst replicator to come about by accident,
parentless but still caused: far, far back in the causal chain that led to you: 3.85
billion years ago, in some little tidal pool.
"Perhaps your children's children will ask how it is that they are capable of love.
"And their parents will say:  Because we, who also love, created you to love.
"And your children's children will ask:  But how is it that you love?
"And their parents will reply:  Because our own parents, who also loved, created us
to love in turn.
"Then your children's children will ask:  But where did it all begin?  Where does the
recursion end?
"And their parents will say:  Once upon a time, long ago and far away, ever so
long ago, there were intelligent beings who were not themselves intelligently
designed.  Once upon a time, there were lovers created by something that did not
love.

"Once upon a time, when all of civilization was a single galaxy and a single star:
and a single planet, a place called Earth.
"Long ago, and far away, ever so long ago."
1. ^
Upon reﬂection, Eliezer's The Gift We Give To Tomorrow is also a pretty key piece
in this moral cinematic narrative universe.
2. ^
Man this show is really into reminding you that it's directed by Genndy
Tartakovsky.
3. ^
Incidentally, Smallfoot is an excellent Rationalist Children's Movie. Independent
of whether it ﬁts in the EMCU, it's got a lot of great themes and complex choices.
4. ^
Copper is merely a 90th percentile hound. Todd does most of the heavy lifting
IMO.
5. ^
These sort of map onto two of Richard Ngo's moral strategies at diﬀerent
capability levels. I haven't ﬁnished thinking through the ramiﬁcations.
6. ^
Or, perhaps, the middle of the story.

How (not) to choose a research project
Background
(speciﬁc information will be sparse here. This is meant to give context for the Takeaways
section of the post)
Our group (Garrett, Chu, and Johannes) have worked with John Wentworth in the SERI MATS 2
Electric Boogaloo program for three weeks, meaning it's time for a Review & Takeaways Post!
First week was Project Selection, and the ﬁrst day was spent thinking about strategies for
coming up with good projects. We chose to ﬁnd a general method for ﬁguring out True
Names of mathy-feely-concepts-in-your-brain (such as roundness, color decomposition[1], or
telling whether a piece of cloth is in a pile) with the goal that such a method would allow for
ﬁguring out true names for concepts like optimization, corrigibility, agency, modularity,
neural network representations, and other alignment-relevant concepts.
Then we read Jaynes, and talked to TurnTrout, and concluded this project sucked. So we went
back to Project Selection 2.0!
We came out of Project Selection 2.0 renewed with vigor, and a deeper understanding of the
problems of alignment. Our new project was ﬁnding a better version of information theory by
adapting logical induction or infra-Bayesianism. 
Then we talked to Eliezer Yudkowsky, he asked for a concrete example of how this would
solve alignment, and we didn't have a good example. So we went to Project Selection 3.0. 
We came out of Project Selection 3.0 with even more vigor, and an even deeper
understanding of the problems associated with alignment... and a clever idea. 
Finetuning LLMs with RL seems to make them more agentic. We will look at the changes RL
makes to LLMs' weights; we can see how localized the changes are, get information about
what sorts of computations make something agentic, and make conjectures about selected
systems, giving us a better understanding of agency.
Nobody has convinced us this is a bad use of our time, though we'd like to see people try.
Takeaways
Big ASS Tree
We learned lots of things over the course of ﬁguring out all our ideas sucked. During the
project selection phase we had a cool idea for a way to generate project ideas: The
Alignment Safety Search Tree (ASS Tree for short). The idea comes from Mazes and Duality;
the goal was to explore the space of problems and constraints before trying to propose
solutions. 
You start by writing "Alignment" up at the top of your whiteboard. This is the top level
problem we want to solve. Then you draw arrows down. One for each problem, you can think
of, that makes alignment hard. For each of these problems you repeat the process. E.g. for a
problem P you draw down an arrow from P for each problem, that you can think of, that you
need to solve, in order to solve P. Eventually you get a tree like this (except far bigger):

This is similar to what you do if you try to make an Alignment Game Tree. However, in my
opinion, when we tried to make the game tree during an early phase of the MATS program, it
did not lend much insight into what to work on. The criticisms ended up being pretty
proposal-speciﬁc, and most arguments were over whether a particular problem was actually
a problem associated with the particular proposal.
For creating the ASS tree, each of us made a tree independently. Then we merged our
individual ASS trees into one Big ASS Tree, and looked for the broader problems which lots of
problems in the tree had in common. We then again extended the resulting tree individually
and merged the results.
A common node in the tree was that we did not know the True name for some important
concept (e.g. agency, optimization, value), and thus the True Names Project was born
(ﬁnding a general procedure that you can use to ﬁnd the True Name for some concept).
Big ASS Takeaways
ASS Trees and Big ASS Trees seem promising. However, I think we did not do the idea justice.
We wanted to avoid anchoring to existing ideas, so we made the ASS tree without looking
into previous alignment proposals and research directions. 
In retrospect, though, we probably could've made a better tree if we'd looked at existing
research, ﬁgured out what walls people empirically run into, and put those problems in the
tree. We tentatively think the best way to use the ASS tree is to ﬁrst make one before looking
into other people's alignment work, and then make another one afterwards.
Contact with reality
Our ﬁrst two project ideas did not lend themselves well to direct contact with reality, and
what contact with reality they would have was largely mediated by humans. For instance,
during the True Names project, we wanted to test our theories on True Name generation by
having people use our methods to generate true names for toy concepts such as roundness
and seeing how well they performed. 
We could theoretically have done this well, but our weak information channels with reality
would have made it very diﬃcult to stay on track to producing something useful. In general,
it's probably a bad sign if your technical alignment research looks a lot like psychology and
academic philosophy.  
What is the most important problem?
We started project selection phase trying to ﬁnd the single most important problem in
alignment, and then come up with good ways to gain information about that particular
problem. We had diﬃculty with this approach, and could not readily generate ideas.

Given that our search tools (the ASS tree, combined with our low-certainty prior
understanding of alignment) are pretty lossy, we couldn't be sure what the most important
problem is - it's pretty hard to ﬁgure this out without actually doing research. Hence, we
think a better approach to project selection would be to ﬁnd the set of very-important-
problems, and then come up with a clever idea to make progress on any of these problems. 
Once we have a way to gain a rich stream of information about any important problem we
can begin to do object-level work and get a feedback loop on what problems are most
important, and what project ideas are useful. This is related to the previous section on
keeping close contact with reality - the feedback loop works best when the project leverages
information highly correlated with reality.
This advice applies less if you're already very certain that one problems is the most
important by a large margin, or if you already have technical research experience.
Heuristics are useful, especially when you're ﬁrst
starting out
When you're ﬁrst starting to do research, it's very diﬃcult to ﬁgure out what actions are best;
heuristics can be really helpful. We made a list of heuristics for project selection based on
common wisdom in research and speciﬁc things we've been told by experienced researchers:
Do something that actually helps solve alignment in the hard case; don't dodge the
hard problem.
Get a fast feedback loop.
Do something that will make other alignment work easier (e.g. getting an algorithm
that can ﬁnd the concepts in a neural network and link them to objects in the real
world).
Do experiments that will give you a "ﬁrehose" of information. Try to avoid experiments
that only yield a yes/no answer, or only produce one number.
Work on something you ﬁnd interesting - it's usually hard to do good work if you ﬁnd
your project boring.
Try to exploit market ineﬃciencies. This is basically the same as the "neglected"
criterion in the ITN framework. Of course, it could easily be argued that basically all of
alignment is neglected, but we still think this is a worthwhile heuristic to keep in mind.
Choose something that makes sense. When you try to explain a technical project to
somebody and they say, "that makes no sense," it's easy to dismiss their confusion
because your work is highly technical. However, oftentimes when people think your
project makes no sense it actually makes no sense.
Work on a concrete subproblem of your larger problem.
Do something that will yield a lot of bits of information even if you fail. If your
hypothesis is X, your experiments should ideally be informative even if X turns out to
not true, or if X isn't even a sensible way to frame the situation.
Hold oﬀ on proposing solutions. There is a standard reason to not do this, but we think
there's an even more important reason. 
Explore the degrees of freedom in your project, and make sure you've chosen the
correct parameter settings before you go forward with the project. As a simple
example, maybe you start out with a project proposal that involves working with large
language models, but after looking at the degrees of freedom you realize it would be
more informative to do a similar project with smaller toy models.
Don't assume an ontology.
Know what success looks like - what does it mean for your project to succeed? For
instance, if you're trying to better understand optimization, maybe success is
developing a method to determine how much optimization power a given agent is
using in a way that matches with our intuitions.

We plan to regularly check in and make sure we're following all of these heuristics; given that
we're very inexperienced at research, if we think we have reason to deviate from this list it
seems more likely that we're either mistaken or deluded than that we've actually found a
robust exception.
Just because John says a project is "the best he's
heard yet" does not mean it's any good
When we posed the True Names project to John, he said it was "the best he's heard yet",
which got us excited. But we later concluded it was a really silly thing to be working on. 
Why did John say this? Several hypotheses come to mind.
Was his brain not working right? We did ask him right before lunch, so maybe he was
too hungry to think straight.
Did he misunderstand us? 
Was he anticipating our project would suck, but wanted us to learn something about
what projects are good or bad on our own? He might approve of thinking and working
on the project as a learning experience, but not because it is a good project to make
object level progress.
Did he want to instill a lesson against blindly listening to your mentors?
The true solution is left as an exercise to the reader[2].
Read Jaynes' "Probability Theory: The Logic of
Science"
It's good! And many insights in the history section led us to signiﬁcantly change our course.
The math is fancy, but you don't have to understand everything in order to gain many of the
insights.
Action space is large, bro
After a week of project selection John dropped our names in his hat (along with some other
people's) and drew names to generate random teams. We quickly found that the resulting
teams were quite suboptimal; Garrett was on a team with a guy who was recovering from
COVID and a ﬁreman[3]. We were fairly resigned to this, and didn't realize we could move out
of the local optimum until somebody pointed it out to us.
At this point we mutinied against John and asked for the teams to be rearranged into their
current conﬁguration; he agreed pretty readily.
John's hat is magic
Or at least, our emulation of his hat is magic. When Chu wears the hat and pretends to be
John, she often comes up with better ideas than she otherwise would have. When we were
generating our list of heuristics, we wrote down everything we could think of until we
exhausted our ideas; Chu then put on the hat and added ﬁve ideas to our list within a few
minutes.

Our John Simulator.
On an unrelated note, we did a workshop a few weeks prior where people presented their
ideas to John and we tried to predict what feedback he would give in advance. 
 

1. ^
That is, given color hex#D6E865 you may say this seems like a mix between yellow
and green, and plausibly this notion can be formalized.
2. ^
Or you can just look at this footnote! The answer (rot13): Guvf jnf nzbat gur svefg
cebwrpgf va bhe pbubeg ur ybbxrq ng, naq gur bguref jrer rira jbefr.
3. ^
Lit. He was too busy putting out ﬁres in other projects to focus on alignment research.

The longest training run
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
This is a linkpost for https://epochai.org/blog/the-longest-training-run
In short: Training runs of large Machine Learning systems are likely to last less than 14-15
months. This is because longer runs will be outcompeted by runs that start later and
therefore use better hardware and better algorithms. [Edited 2022/09/22 to ﬁx an error in
the hardware improvements + rising investments calculation]
Scenario
Longest training
run
Hardware improvements
3.55 years
Hardware improvements + Software improvements
1.22 years
Hardware improvements + Rising investments
9.12 months
Hardware improvements + Rising investments + Software
improvements
2.52 months
Larger compute budgets and a better understanding of how to eﬀectively use compute
(through, for example, using scaling laws) are two major driving forces of progress in recent
Machine Learning.
There are many ways to increase your eﬀective compute budget: better hardware, rising
investments in AI R&D and improvements in algorithmic eﬃciency. In this article we
investigate one often-overlooked but plausibly important factor: how long—in terms of wall-
clock time—you are willing to train your model for.
Here we explore a simple mathematical framework for estimating the optimal duration of a
training run. A researcher is tasked with training a model by some deadline, and must decide
when to start their training run. The researcher is faced with a key problem: by delaying the
training run, they can access better hardware, but by starting the training run soon, they can
train the model for longer.
Using estimates of the relevant parameters, we calculate the optimal training duration. We
then explore six additional considerations, related to 1) how dollar-budgets for compute rise
over time, 2) the rate at which algorithmic eﬃciency improves, 3) whether developers can
upgrade their software over time 4) what would happen in a more realistic framework with
stochastic growth, 5) whether it matters for the framework that labs are not explicitly
optimizing for optimal training runs and 6) what would happen if they rent instead of buy
hardware. 
Our conclusion depends on whether the researcher is able to upgrade their hardware stack
while training. If they aren't able to upgrade hardware, optimal training runs will likely
last less than 3 months. If the researcher can upgrade their hardware stack during
training, optimal training runs will last less than 1.2 years. 
These numbers are likely to be overestimates, since 1) we use a conservative estimate of
software progress, 2) real-world uncertainty pushes developers towards shorter training runs
and 3) renting hardware creates an incentive to wait for longer and parallelize the run close
to the deadline.
Scenario
Longest training
run
Hardware improvements
3.55 years
Hardware improvements + Software improvements
1.22 years

Hardware improvements + Rising investments
9.12 months
Hardware improvements + Rising investments + Software
improvements
2.52 months
A simple framework for training run lengths
Consider a researcher who wants to train a model by some deadline T. The researcher is
deciding when to start the training run in order to maximize the amount of compute per
dollar.
The researcher is faced with a key trade-oﬀ. On one hand, they want to delay the run to
access improved hardware (and/or other things like larger dollar-budgets and better
algorithms.) On the other hand, a delay reduces the wall-clock time that the model is trained
for.
Suppose that hardware price-performance is increasing as follows:
H ( t ) := H 0 Exp  [ g H t ]
where H0 is the initial FLOPS/$ and gH is the rate of yearly improvement.[1] If we start a
training run at time S, the cumulative FLOP/$ at time t ≥S will be equal to:
F S ( t ) := H ( S ) ( t − S )
where H(S) is the price-performance of the available hardware when we start our run (in
FLOP/$/time), and (t −S) is the amount of time since we started our run. Given a ﬁxed dollar-
budget, when should we buy our hardware and start a training run to achieve the most
FLOP/$ by a deadline T?
To ﬁgure that out, we need to ﬁnd the most eﬃcient time S to start a run that concludes by
time T > S. We can ﬁnd that by deriving FT(T) with respect to S and setting the result equal
to zero.
  = H ( S ) [ g H ( T − S ) − 1 ] = 0
T − S = 1 / g H
The optimal training run has length L := T −S = 1/gH. In previous work we estimate the rate
of improvement of GPU cost eﬀectiveness at gH ≈0.281 (Hobbhahn and Besiroglu, 2022) [2].
This leads to an optimal training run of length L = 1/gH ≈3.55 years.
∂ F S ( T )
∂ S

n blue, total amount of compute consumed by training runs starting at diﬀerent
years, given a deadline T = 2030 and an investment of $1B. In brown, the
hardware price-performance, assuming an initial price-performance of 
H0 ≈6.3 × 1010 FLOP/s/$ in 2022 and a rate of improvement of gH ≈0.281 (see
Hobbhahn and Besiroglu, 2022).
The intuition is as follows: if you want to train a model by a deadline T, then, on the one
hand, you want to wait as long as possible to get access to high price-performance
hardware. On the other hand, by waiting, you reduce the total time available for your
training run. The optimal training duration is the duration that strikes the right balance
between these trade-oﬀs.
This calculation rests on some assumptions:
1. We are ignoring that willingness to invest in ML rises over time, so a researcher might
be able to secure a larger budget if they wait 
2. We are ignoring that improvements in software and better understanding of scaling
laws might enable researchers to deploy compute more eﬀectively in the future
3. We are assuming that practitioners will not upgrade their hardware in the middle of a
run
4. We are assuming that the involved quantities will improve at a predictable,
deterministic rate
5. We are assuming that developers optimize for a ﬁxed deadline 
6. We are assuming that developers are buying their own hardware
Let's relax each of these assumptions in turn and see where they take us.
Accounting for increasing dollar-budgets
In reality, the total amount of compute invested in ML training runs has grown faster than
GPU price performance. Companies have been increasing their dollar-budgets for training ML

models; hence, researchers might want to delay training ML models to access larger dollar-
budgets.
Our previous work found a rate of growth of compute invested in training runs equal to 
gC ≈1.31 [3]. This rate of growth can be decomposed as gC = gH + gI, the sum of hardware
eﬃciency growth gH ≈0.281 and the growth in investment gI = gC −gH ≈1.03 [4].
Following the same reasoning as above, we can calculate the optimal training run length
equal to L = 1/gC ≈0.76 years, ie 9.12 months.
This is much shorter than the ~3.55 year training duration we saw previously. Researchers
want to wait for both better hardware and larger dollar-budgets. Since dollar-budgets have
been growing about an order of magnitude more quickly than hardware price-performance
has been improving, researchers taking into account growing dollar-budgets will train their
models for roughly an order of magnitude less wall-clock time.
Accounting for increased algorithmic eﬃciency
In 2020, Kaplan et al´s paper about scaling laws for neural models provided practitioners
with a recipe for training models in a way that leverages compute eﬀectively. Two years
after,  Hoﬀman et al upended the situation by releasing an updated take on scaling laws that
helped spend compute even more eﬃciently.
Our understanding of how to eﬀectively train models seems to be rapidly evolving. Hence,
practitioners today might be dissuaded from planning a multiyear training run because
advances in the ﬁeld might render their eﬀorts obsolete.
One way we can study this phenomenon is by understanding how much less compute we
need today to achieve the same results as a decade ago. While partially outdated in the light
of new developments, Hernandez and Brown's measurement of algorithmic eﬃciency
remains the best work in the area.
They ﬁnd a 44x improvement in algorithmic eﬃciency over 7 years, which translates to a
rate of growth of gS ≈0.541.
Combining this with the rate of improvement of hardware leads to a combined rate of growth
of gH + gS ≈0.281 + 0.541 = 0.822. This translates to an optimal training run length of 
L = 1/(gH + gS) ≈1.22 years.
We could also combine this with the rate of growth of investments. In that case we would
end up with a total rate of growth of eﬀective compute equal to 
gH + gI + gS ≈0.28 + 3.84 + 0.54 = 4.66. This results in an optimal training run length of 
L = 1/(gH + gI + gS) ≈0.21 years, ie 2.52 months.
Accounting for hardware swapping
Through this analysis we have assumed that ML practitioners commit to a ﬁxed hardware
infrastructure. However, in theory one could stop the current run, save the current state of
the weights and the state of the optimizer, and resume the run in a new hardware setup.

Hypothetically, a researcher could upgrade their hardware as time goes on. In practice, if our
budget is ﬁxed this is a moot consideration. We want to spend our money at the point where
we can buy the most compute per dollar before a deadline. Spending money before or
afterwards leads to less returns per dollar overall.
Our budget does not need to be ﬁxed however. As investments rise, we could use the
incoming money buying new, better hardware to grow our hardware stock.
Suppose that the amount of available money at each point grows as gI. We can spend money
at any time to buy state-of-the-art hardware, whose cost-eﬃciency has been improving all
along at a pace gH.
H ( t ) := H 0 Exp  [ g H t ]
I ( t ) := I 0 Exp  [ g I t ]
There are many possible ways to spend the budget over time. However, the optimal solution
will be to spend all available budget at the point that maximizes the product between
hardware cost-eﬃciency and time remaining, and then spend any incoming money
afterwards as soon as possible to get higher returns.
Formally, the cumulative amount of FLOP that a run started at point S can muster by time 
t > S is equal to:
F S ( t ) := 
H ( S ) I ( S ) ( t − S )
                          FLOP yield of initial hardware
 + ∫ 
t
S
 H ( u ) ˙I  ( u ) ( t − u ) d u
                                  FLOP yield of hardware swapping
Deriving with respect to S as before gives us the optimal training run length:
  = H ( S ) I ( S ) [ ( g H + g I ) ( T − S ) − 1 ] − H ( S ) I ( S ) g I ( T − S ) = 0
L := T − S = 1 / g H
The answer is the same as in the case where our budget is ﬁxed, there are no rising
investments and swapping hardware is not allowed. I.e., we ﬁnd that the inﬂuence of the
rising budget disappears - the optimal length of the training run now depends only on the
rate of hardware improvement.
This is simply because there is no additional incentive to wait for larger dollar-budgets;
researchers reap the beneﬁts of growing hardware-budgets by default. Hence, the optimal
duration of a training run is the same as that found when only considering hardware price-
performance improvements. 
Accounting for stochasticity
∂ F S ( T )
∂ S

In our framework we have assumed a simple deterministic setup where hardware eﬃciency,
investments and algorithmic eﬃciency rise smoothly and predictably.
In reality, progress is more stochastic. New hardware might overshoot (or undershoot)
expectations. Market ﬂuctuations and the interest in your research area may aﬀect the
dollar-budget you can muster for training at any given point.
Developing a framework that incorporates stochasticity is beyond the scope of this article.
However, it may be useful to consider an idea from portfolio theory: when you're not sure
what will happen in the future, you don't want to lock up capital in long-term projects. This
pushes training runs towards being shorter—and means that the numbers we are estimating
in this article are likely on the higher side.
Fixed deadlines
One possible objection to our framework is that it assumes developers are trying to hit a
ﬁxed deadline. In reality, researchers are often happy to wait for longer results.
Ultimately, we believe that this is a good framework. The way we conceptualize research in
AI envisions many labs beginning their training runs at diﬀerent times.
In any given quarter, the lab that releases the most compute-intensive model will be the one
that started their training run closest to the optimal length.
Even if labs are not optimizing for explicit deadlines or planning training lengths, the most
compute-eﬃcient among them will still roughly obey these rules. Labs that train for shorter
and longer times than the optimum will be outclassed.
Assuming that the most compute-intensive models will also be the most impressive, then
this model provides a good upper bound on training lengths of impressive models.
Renting hardware
Through this discussion we have been assuming that labs purchase rather than rent
hardware for training. This is the case for some of the top labs that usually train the largest
models, such as Google and Meta. However, many others instead resort to renting hardware
use from cloud computing platforms such as Amazon AWS, Lambda Labs or Google Cloud.
In the case hardware is rented, and there the training run require a small fraction of the
available capacity, we expect our model not to apply. Since hardware prices decrease over
time and training runs are largely paralellizable, there is a strong incentive for labs that rent
hardware to wait for as long as possible, and train their model very brieﬂy on a much larger
number GPUs (relative to the number that is optimal when hardware is purchased) close to
their deadline.
While we think this is an important case to consider (as renting hardware is likely much
common in machine learning relative to using purchased hardware), since we're mostly
interested in understanding the decision-problems associated with training the largest
models at any point in time, we have not studied the case of renting hardware in much
depth.
Conclusion
We have analyzed how continuously improving hardware, bigger budgets and rising
algorithmic eﬃciency limit the usefulness of a longer training run.

Researchers are faced with a trade-oﬀ when deciding when to start a training run that ends
at some time T. On one hand, they want to delay the start of this run to get access to
improved hardware and/or additional factors like larger dollar-budgets and better algorithms.
On the other hand, a delay reduces the time that the hardware can deployed for. Since we
have some sense of the rate at which these factors change over time, we can infer the
optimal duration of ML training runs.
We ﬁnd that optimally balancing these trade-oﬀs implies that the resulting training runs
should last somewhere between 2.5 months and 3.6 years.
Allowing for swapping hardware removes the eﬀect of rising budgets (since we can spend
incoming money without stopping the run). This increases the optimal training run length to
between 1.2 and 3.6 years.
We expect these numbers to be overestimates, since improvements are stochastic,
uncertainty will push developers to avoid over-investing in single training runs, and renting
hardware incentivizes developers to wait longer before starting their training run. 
Furthermore, large-scale runs can be technically diﬃcult to implement. Hardware breaks and
needs to be replaced. Errors and bugs force one to discard halfway completed training runs.
All these factors shorten the optimal training run[5].
The biggest uncertainty in our model is the rate at which algorithmic eﬃciency improves. We
have used an estimate from (Hernandez and Brown, 2020) to derive the result. This paper
precedes the conversation about scaling laws and uses data from computer vision rather
than language models. Our sense is that (some types of) algorithmic improvements have
proven to be faster than estimated in that paper, and this could further shorten the optimal
training run.
In any case, we can conclude that at current rate of hardware improvement we probably will
not see runs of notable ML models over 4 years long, at least when researchers are
optimizing compute per dollar. 
Scenario
Longest training
run
Hardware improvements
3.55 years
Hardware improvements + Software improvements
1.22 years
Hardware improvements + Rising investments
9.12 months
Hardware improvements + Rising investments + Software
improvements
2.52 months
Acknowledgements
We thank Sam Ringer, Tom Davidson, Ben Cottier, Ege Erdil and Lennart Heim for discussion.
Thanks to Eduardo Roldan for preparing the graph in the post.
1. ^
We assume that hardware price performance increases smoothly over time, rather than
with discontinuous jumps corresponding to the release of new GPU designs or
lithography techniques. We expect that on a more realistic step-function process, the
key conclusions of our framework would still roughly follow (modulo optimal training
durations occasionally changing a few months to accommodate discrete generations of
hardware).

2. ^
They ﬁnd a doubling time for hardware eﬃciency of 2.46 years. This corresponds to a
yearly growth rate of gH ≈
= 0.281.
3. ^
We found a 6.3 month doubling time for compute invested in large training runs. This is
a yearly growth rate of gC ≈
= 1.31.
4. ^
In theory, we should also account for the rise in training lengths. In practice, when we
looked at a few data-points training lengths appeared to be increasing linearly over
time, so we believe the eﬀect is quite small.
5. ^
Meta's OPT logbook illustrates this well: they report being unable to continuously train
their models for more than 1-2 days on a cluster of 128 nodes due to the many failures
requiring manual detection and remediation.
ln 2
2.46
ln 2
6.3 months⋅
year
12 months

AI art isn't "about to shake things up".
It's already here.
For a while, I've been seeing people commenting about how AI art is on the cusp of
shaking up the art world. Quite frankly, at this point I consider such sentiments to be
behind the times. AI art is not "on the cusp" of disrupting such things. It is already
here. Using only capabilities that are straightforwardly and publicly available right
now, AI art has radically transformed the budget for certain types of project that
require art.
Let's give a basic example. I play card games of various types -- games like Magic: the
Gathering or similar -- on a highly competitive level. I've even been involved in testing
and development for some such games, so I'm familiar with that process as well. I am
not a professional game designer, but am fairly involved with some aspects of the
ﬁeld.
For one of these games, a typical budget for an individual card illustration of suﬃcient
quality is, as I understand it, in the realm of $200-1000 USD. A single "set" of cards
might require 100+ such illustrations. That means that you're looking at paying $20k
in art costs on the low end, and that this is a recurring cost every time you want to
make a new set that isn't just reprinting old cards -- and even then, sometimes new
art is used for reprints!
Except, well, that was then and this is now. Now, if I were in the business of making a
card game set, my art budget wouldn't be $20k-100k. It would be... a $30/month
Midjourney subscription with $20/month private visibility enabled, and quite frankly
high quality Midjourney images look better than many of the images already being
used for art in these games. [1]
I'm going to highlight that again. The price for the art needed to create a set of a
hundred cards just went from twenty thousand dollars-- at the low end -- to ﬁfty bucks
a month. This is an extreme shift, and it is already here. This is not something that is
based on a press release or future development that hasn't arrived yet. This is
something that I could do today, using only techniques that are widely known and
publicly available. If you assume you can get the art needed in one month of
Midjourney time, that's four hundred times cheaper.
There are many other areas where this applies. What's the price for a book cover?
Quite frankly, that's not my ﬁeld -- but whatever it is, I'm going to bet that Midjourney
is often going to be cheaper and better. What's the price for the internal illustrations in
a role-playing game manual? Again, whatever it is I'm going to bet that AI art is
already beating it.
Further, AI art is much easier to work with than professional artists. This is not
intended as an insult to professional artists by any means! However, if I am working
with a professional artist on an image, it may take them a signiﬁcant amount of time
to produce the image and get back to me on that. By contrast, if I don't like the
Midjourney output I can write a variation on the prompt and get a new set of images
extremely quickly. And I don't have to worry about people missing or misreading my
emails, a potential language barrier, or time zone issues. [2]

Now, there are admittedly some things that AI art isn't good at (the really big one
being art with integrated text). You know what? That's true. There are deﬁnitely some
things that AI art does not handle well. It's not a perfect substitute yet. However,
given the outrageous cost savings, I am perfectly ﬁne with that. I am altogether
willing to change the focus of my card illustrations a bit in order to avoid areas where
AI art generation does poorly if it means paying four hundred times less for art for my
game, and I suspect others will soon be choosing the same.
So, yeah. AI art isn't some hypothetically disruptive thing that might happen in the
future. The capabilities are straightforwardly available right here and right now -- and
the fact that there are some ﬂaws and foibles still to be worked out means that it only
has room to grow further. If AI art generation already oﬀers this much of an advantage
over traditional art commissions, how much more obvious a choice will it be once
we've seen some more iteration on these systems?
[1]"But wait," you might say. "Don't companies with lots of revenue have to pay a
higher price for Midjourney?" They do, but that price is $600 USD/year, and it comes
with the private visibility option so the yearly price is actually the same. You do get
somewhat less GPU time than a standard membership but can still buy extra time if
needed.
[2] "Language barrier" and "time zone issues" may seem like weird problems to have,
but as I understand it many game companies are getting art from artists who live in
Eastern Europe or Asia. This is a good strategy in some ways but also has its
downsides.

Human Mimicry Mainly Works When
We're Already Close
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
What if we just simulate a bunch of alignment researchers, and have them solve the
problem for us?
Of all the Dumb Alignment Ideas, this one is easily the best. Simple argument in favor:
well, it's not going to do any worse than the researchers would have done. In other
words, it will probably do at least as well as we would have done without it, and
possibly better, insofar as it can run faster than realtime.
Another angle: human mimicry is a simple objective to train against, and is about as
outer-aligned as the humans being mimicked. Which isn't necessarily perfect, but it's
as aligned as our alignment researchers were going to be anyway (assuming inner
alignment issues are handled, which we will indeed assume for the entirety of this
post).
Those are pretty good arguments. But man, there are some subtle devils in the
details.
Simulation vs Prediction
The ideal version of human mimicry is mind uploads: directly simulate our researchers
in a stable, research-friendly environment for a long time.
The operationalization which people usually actually have in mind is to train an ML
system to predict research outputs - e.g. I might prompt GPT for a johnswentworth
post from the year 2050.
Even setting aside inner alignment issues, these two are radically diﬀerent.
Generalization Problems
In order for GPT to generate a realistic johnswentworth post from the year 2050, it has
to generalize way out of distribution.
... Well, ok, maybe I turn into one of those old researchers who just repeats the same
things over and over again for decades, and then GPT doesn't need to generalize way
out of distribution. But in that case it isn't very helpful to prompt for one of my posts
from 2050 anyways, and we should prompt for something else instead (Thane
Ruthenis has been writing great stuﬀ lately, maybe try him?). The whole point of
asking for future research write-ups is to see useful stuﬀ we have not yet ﬁgured out;
that means generalizing way out of the distribution of writing we already have.
But if the system generalizes too well out of distribution, then it correctly guesses that
AGI will take over the world before 2050, and my attempt to prompt for a
johnswentworth post from 2050 will instead return predicted writings from a

ridiculously superhuman future AGI pretending to be johnswentworth. And those
writings presumably try to inﬂuence the reader in ways which bring about the AGI's
takeover.
So in order to do useful work, our GPT-style system has to generalize out of
distribution, but not too far out of distribution. We don't know how wide the window is
between generalizing enough and generalizing too much, or if the window is wide
enough to be useful at all.
One thing we can guess: prompting for research outputs in the very near future is
probably much safer than prompting for dates further out. johnswentworth post from
2025 is a safer prompt than johnswentworth post from 2050. The less far out of
distribution we go, the safer we are. Similarly, the more likely we are to solve the
alignment problem and avoid AI takeover, the less likely it is that prompting GPT for
future research outputs is dangerous, and the more likely it is to work.
The closer we are to solving alignment already, and the more likely we are to make it,
the less dangerous it is to predict future research outputs. In other words: predicting
future research outputs can only safely buy us a relatively small number of bits; we
have to already be reasonably close to surviving in order for it to work.
So What's Diﬀerent About Simulation?
Simulating researchers in a stable, research-friendly environment for a long time does
not have the "predict outputs of a future AGI" problem. Why? What's the key
diﬀerence?
The key is the "stable, research-friendly environment" part. Our simulated researchers
are in a simulated environment where AGI is not going to take over. It's
a counterfactual world very diﬀerent from our own.
Alas, querying counterfactual worlds is fundamentally not a thing one can do simply
by prompting GPT. Conceptually, prompts just do Bayesian conditioning on the
modeled text distribution (i.e. condition the text on starting with the prompt);
counterfactuals move us to an entirely diﬀerent distribution. To generate a
counterfactual query, we'd have to modify the system's internals somehow. And in
fact, there has recently been some cool work which demonstrates decent performance
on counterfactual queries by modifying GPT's internals! I don't think it's to the point
where we could counterfact on something as complicated as "world in which AGI
doesn't take over and our alignment researchers successfully solve the problem", and
I don't think it's robust enough to put much weight on it yet, but the basic version of
the capability does exist.
General Principle: Human Mimicry Buys A
Limited Number Of Bits
Suppose GPT-12, with its vast training data and compute, internally concludes that
humanity has a 1-in-32 chance of aligning/surviving AGI on our current trajectory.
Then humanity would need 5 bits of optimization pressure in order to make it.
The more bits of optimization pressure humanity needs, the less likely human mimicry
is to save us; we have to already be reasonably close to surviving in order for it to

work. We already talked about this principle in the context of accidentally prompting
GPT to return writing from a future AGI, but the principle is much more general than
that.
The Weird Shit Problem
Suppose we need 20 bits of optimization pressure (i.e. on our current trajectory we
have only a ~1-in-a-million chance of avoiding AGI takeover). We train GPT, and
counterfact on its internals to a world where AGI doesn't take over. But if our chances
of avoiding takeover were that low (under GPT's model), then they're probably
dominated by weird shit, things which have probabilities on the order of 1-in-a-million
or less. Maybe we nuke ourselves to death or get hit by a big damn asteroid. Maybe
aliens decide that humanity's AGI is about to become a problem to the rest of the
galaxy and they need to take over rather than just letting us develop. Maybe time
travel turns out to be a thing and weird time travel bullshit happens. Most likely it's
something weird enough that I won't think of it.
Those weird futures vary in how safe they are to query (time travel would probably be
on the very short list of things as dangerous as AGI), and in how likely they are to
return anything useful at all (asteroid extinction tends to cut oﬀ blog post writing). But
approximately zero of them involve our researchers just doing their research in a
stable, research-friendly environment for a long time.
So when we need a lot of bits, it's not enough to just counterfact on a high-level thing
like "AGI doesn't take over" and then let GPT pick the most probable interpretation of
that world. We need pretty detailed, low-level counterfactuals.
Generalization Again
Another angle: number of bits of optimization required is a direct measure of "how far
out of distribution" we need to generalize. Even setting aside actively dangerous
queries, our simulator/predictor has to generalize out of distribution in order to return
anything useful. In practice, the system will probably only be able to generalize so far,
which limits how many bits of optimization we can get from it.
Expect More Problems
We've now been through a few diﬀerent arguments all highlighting the idea that
human mimicry can only buy so many bits of optimization (future AGI problem, weird
shit problem, generalization). I expect the principle to be more general than these
arguments. In other words, even if we patch the speciﬁc failure modes which these
particular arguments talk about, trying to pump lots of bits of optimization out of
human mimicry is still likely to be dangerous in ways we have not yet realized.
This is just an instance of the general principle that optimization becomes more
dangerous as we crank up the optimization power - the very principle for why AGI is
dangerous-by-default in the ﬁrst place.
Takeaways

This post mostly talked about the limitations of human mimicry, but I want to
emphasize: this is the best of the Dumb Alignment Ideas to date. If GPT-style
models reach human or superhuman general intelligence next month, and we can't
realistically delay its release, and you are the person sitting in front of the prompt
wondering what to do, then prompting for future alignment research is absolutely
what you should do. (And start not very far in the future, and read the damn outputs
before going further, they'll hopefully contain additional warnings or new plans which
you should do instead of prompting for more future research.) At that point it's not
very likely to work, but we don't have a better immediate plan.
Good interpretability tools can buy us more bits in two ways:
They potentially allow us to directly handle inner alignment issues by retargeting
the search
They potentially allow us to counterfact on the system's internal model, so we
can predict researchers working in an environment less full of threats
Insofar as the tools are available, this is the thing to aim for if AGI is imminent.
... But the general principle is that human mimicry can buy only a limited number of
bits. We deﬁnitely want to have the interpretability tools to implement the best
version of human mimicry we can, but at the end of the day we'll mostly improve our
chances by getting closer to solving the full alignment problem ourselves.

High Reliability Orgs, and AI Companies
Epistemic Eﬀort : Rough notes from a shallow dive. Looked into this for a few hours. I didn't
ﬁnd a strong takeaway but think this is probably a useful jumping-oﬀ-point for further work.
Most likely, whether I like it or not, there will someday be AGI research companies working on
models that are dangerous to run. Maybe they'll risk an accidental unfriendly hard takeoﬀ.
Maybe they'll cross a threshold accelerate us into a hard-to-control multipolar smooth
takeoﬀ.[1]
There's some literature on organizations that operate in extremely complex domains, where
failure is catastrophic. They're called High Reliability Organizations (HROs). The original work
focused on three case studies: A nuclear power plant, an air traﬃc control company, and a
nuclear aircraft carrier (aka nuclear power plant and air traﬃc control at the same time while
people sometimes shoot at you and you can't use radar because you don't want to give
away your position and also it's mostly crewed by 20 year olds without much training)
These were notable for a) being extremely complex systems where it would be really easy to
screw up catastrophically, and b) somehow, they manage to persistently not screw up.
How do they do that? And does this oﬀer any useful insights to AGI companies? 
I started writing this post before Eliezer posted Six Dimensions of Operational Adequacy in
AGI Projects. It's not pointed at the exact same problem, but I had a similar generator of
"what needs to be true of AI companies, for them to safely work on dangerous tech?". (I think
Eliezer had a higher bar in mind with Six Dimensions, which is like, "what is an AI company a
researcher could feel actively good about joining")
I was initially pointed in the HRO direction by some conversations with Andrew Critch. Some
of his thoughts are written up in the ARCHES report. (There's been some discussion on
LessWrong)
My TL;DR after ~10 hours of looking into it: 
The literature has at least some useful takeaways/principles.
Some hospitals (maybe?) successfully implemented HRO principles and drove some
classes of accidents down to 0% (!? roll to disbelieve) 
The literature (even the hospital "replication") isn't obviously super relevant because
it's focused on domains that have much tighter feedback loops than AI research. 
The most interesting part of the Air Traﬃc Control and Aircraft Carrier stuﬀ is not the
day-to-day-operations, but the part where new airplanes are getting designed, which
consistently seem to mostly work. This part seems more analogous to AI research.
Unfortunately I don't currently know where to look into that.
I suspect that dangerous Bio Research might also be more relevant, but also haven't
yet found anything to look into there.
I do think the HRO principles make sense and are fairly straightforward to reason
about. Some of them seem to straightforwardly seem useful at AI companies, some
don't make as much sense.
There's a guy named Admiral Rickover who was a primary driver of the nuclear aircraft
carrier safety program (i.e. onboarding teenagers, training them for [I think 6 months?],
sending them on a [I think 1-2 year?] rotations on aircraft carriers, with zero (!?)
incidents). He seems really interesting, and training programs descended from him
may be worth looking into. Here's a dissertation about how he changed the navy
(which I have not yet read)\
Principles from "Managing the Unexpected"

The book "Managing the Unexpected", ﬁrst published in 2001, attempts to answer the
question "how can we learn from Highly Reliable Organizations?" in a more general way. I
found a summary of the book from researchgate.com which I'm going to quote from liberally.
It distills the book down into these points:
You can't plan for everything. Something unexpected will always happen, from
hurricanes to product errors.
Planning can actually get in the way of useful responses, because people see the
world through the lens of their plans and interpret events to ﬁt their expectations.
The more volatile your work environment, the more important it is to respond well
in the moment.
To make your organization more reliable, anticipate and track failures. Determine
what they teach you.
Focus on operations, rather than strategy.
To become more resilient, emphasize learning, fast communication and adaptation.
The person making decisions about how to solve a problem should be the person
who knows the most about it - not the person who's highest in the hierarchy.
To build awareness, audit your organization's current practices.
To make your organization more mindful, don't oversimplify.
Real mindfulness may require changing your organizational culture
I'm not sure how well this all translates to AI research, but the list was interesting enough for
me to buy the book. One thing that was quite unclear to me was what the epistemic status
of the book was - it prescribes a bunch of interventions on organizational culture, but it
doesn't say "we tried these interventions and it worked."
Applications in Hospitals
Fortunately(?), it seems like since the book was ﬁrst published, there has been a massive
eﬀort to port HRO principles over into the hospital industry. (In fact when I googled "High
Reliability Organizations", the ﬁrst several google hits were reports about hospitals saying
"Man, we kill people all the time. Why can't we be reliable like those nuclear aircraft carrier
people in the HRO literature?"). Some programs started around 2007, and were evaluated in
2018. (
According to this report, a collection of organizational interventions resulted in serious safety
events dropping to zero over the course of 9 years. They deﬁne "Serious Safety Event" as "a
deviation from the standard of care that reaches the patient and leads to more than minor or
minimal temporary harm". 

Serious Safety Event rate per 100,000 Adjusted
Patient Days plus Long-Term Care Days Over the
Course of 9 Years (a steady decline is noted that
paralleled several safety milestones
implementation)
Notes on the methodology here:
The risk department receives notiﬁcations of these unusual occurrences, which are
handled by dedicated risk specialists, each assigned a speciﬁc unusual occurrence. The
risk specialist reviews the submission, and if identiﬁed as a potential safety hazard or
patient harm may have occurred, he/she requests an event investigation (Code E) from
the chief medical oﬃcer. 
Staﬀ receive training on how to report an unusual occurrence through several methods:
new hire orientation, departmental training, Nursing Practice Council forum, safety coach
discussions with teams, safety huddle message of the week, or a demo site on GenNet
that walks staﬀ through the submission process. There is not a formal process to train
physicians; most often, physicians inform the nurses or nurse managers of their concerns
and unusual occurrences are entered on their behalf.
I do notice "deviation from the standard of care" is a kinda vague concept that depends on
whatever the standard of care is. But, assuming they held the standard consistent
throughout and didn't solve the problem by massive goodharting... it seems Big If True if
they drove things down to literally 0. That's at least encouraging on the general level of "we
have working examples of industries becoming safer on purpose."
Does any of this translate into "Research"
environments?

Nuclear powerplants, air traﬃc control, aircraft carriers and hospitals all share a property of
having pretty clear feedback loops. And while a lot of the point of being an HRO is
responding to surprises, they are in some sense attempting roughly the same thing with the
same desired outcome repeatedly. If you mess up, an accident is likely to happen pretty
quickly. 
Some of the principles ("focus on operations rather than strategy", and "the more volatile
your work environment, the more important to respond well in the moment") seem more
tailored for tactical jobs rather than research companies.  (I can imagine smooth takeoﬀ
worlds where an AI company does need to respond quickly in the moment to individual
things going wrong... but it feels like something has already gone wrong by that point)
Some of the ideas I'd guess would transfer:
To make your organization more reliable, anticipate and track failures. Determine
what they teach you.
To build awareness, audit your organization's current practices.
To make your organization more mindful, don't oversimplify.
Real mindfulness may require changing your organizational culture
To become more resilient, emphasize learning, fast communication and adaptation.
I expect this one to be a bit complex...
The person making decisions about how to solve a problem should be the person
who knows the most about it - not the person who's highest in the hierarchy.
...because you might have people who are the most competent at AI design but don't have
security/alignment mindset.
An interesting one was...
Planning can actually get in the way of useful responses, because people see the
world through the lens of their plans and interpret events to ﬁt their expectations.
...which actually feels like maybe applies to the broader x-risk/EA ecosystem.
Appendix: Cliﬀ Notes on "Managing the
Unexpected"
[The following is from a researchgate summary, which I found pretty annoyingly formatted.
I've pretty translated it into LW-Post format to be easier to read]
Countless manuals explain how to plan for crises and make it sound like everything will
go smoothly if you just plan correctly. Weick and Sutcliﬀe say otherwise. Planning, they
say, may even stand in the way of smooth processes and can be the cause of failure.
They base this discussion on their studies of "high reliability organizations" (HROs), like
ﬁre ﬁghting units and aircraft carrier crews, organizations where the unexpected is
common, small events make a diﬀerence, failure is a strong possibility and lives are on
the line. From those examples, they deduce principles for planning, preparation and
action that will apply to any company facing change. 
The book is not perfect - the authors overuse quotations and rely on buzzwords that
don't add much - but it addresses often-neglected aspects of management. getAbstract
recommends it to anyone who is trying to make an organization more reliable and
resilient amid change.
Abstract

Learning from High Reliability Organizations
Things you don't expect to occur actually happen to you every day. Most surprises are
minor, like a staﬀ conﬂict, but some aren't, like a blizzard. Some test your organization to
the verge of destruction. You can't plan for the unexpected, and in many cases, planning
actually sets you up to respond incorrectly. You make assumptions about how the world
is and what's likely to happen. Unfortunately, many people try to make their worldview
match their expectations, and thus ignore or distort signs that something diﬀerent is
happening. People look for conﬁrmation that they're correct, not that they're wrong.
Planning focuses organizational action on speciﬁc, anticipated areas, which shuts down
improvisation. When people plan, they also tend to "repeat patterns of activity that have
worked in the past." That works well if things stay the same - but when they change and
the unexpected erupts, you are left executing solutions that don't really ﬁt your new
situation
Consider organizations such as hospital emergency departments or nuclear power
plants, which have to cope with extraordinary situations on a regular basis. These
organizations have learned to deal regularly with challenging, disruptive events. They
have adapted so that they react appropriately to the unexpected. They recognize that
planning can only take an organization so far. After that, the way it responds to crisis
determines its success. Your company can learn from the way these "high reliability
organizations" (HROs) respond to crises, and, more generally, you can use their
organizing principles in your own organization. Five core principles guide these HROs.
The ﬁ rst three emphasize anticipating problems; the last two emphasize containing
problems after they happen
1. Preoccupation with failure. Attention on close calls and near misses ("being lucky
vs. being good"); focus more on failures rather than successes. 2. Reluctance to simplify
interpretations. Solid "root cause" analysis practices. 3. Sensitivity to operations.
Situational awareness and carefully designed change management processes. 4.
Commitment to resilience. Resources are continually devoted to corrective action plans
and training. 5. Deference to expertise. Listen to your experts on the front lines (ex.
authority follows expertise).
2. HROs are "reluctant to accept simpliﬁcation" - Simpliﬁcation is good and
necessary. You need it for order and clarity, and for developing routines your organization
can follow. However, if you simplify things too much or too quickly you may omit crucial
data and obscure essential, information you need for problem solving. Labeling things to
put them in conceptual categories can lead to dangerous oversimpliﬁcation. NASA's
practice of classifying known glitches as "in-family" and new problems as "out-of-family"
contributed to the Columbia disaster. By miscategorizing the damage to the shuttle as a
maintenance-level "tile problem," people downplayed its importance. To reduce such
labeling danger, use hands-on observation. When things go wrong, don't count on one
observer; make sure people from varied backgrounds have time to discuss it at length.
Re-examine the categories your organization uses and "diﬀerentiate them into
subcategories," so nothing gets hidden or blurred
3. HROs remain "sensitive to operations" - Stay focused on the actual situation as
it is happening. Of course, aircraft carrier crewmembers, for instance, should align their
actions with the larger strategic picture - but they can't focus there. They must keep
their focus on the airplanes that are taking oﬀ and landing on their deck. They have to
pay attention to "the messy reality" of what's actually happening. To improve your focus,
refuse to elevate quantitative knowledge above qualitative and experiential knowledge.
Weigh both equally. When you have "close calls," learn the right lessons from them.
Close calls don't prove that the system is working because you didn't crash. Close calls
show that something's wrong with the system since you almost crashed. 

4. HROs develop and maintain "a commitment to resilience" - When the pressure
is oﬀ, you might be able to believe that you've developed a perfect system that will
never have to change. HROs know better. They regularly put their systems under
incredible stress and unforeseen circumstances do arise. HROs know they have to adapt
continually to changing circumstances. Resilience consists of three core capabilities. Your
organization is resilient if it can "absorb strain" and keep working, even when things are
hard, if it can "bounce back" from crises and if it can learn from them. HRO leaders
celebrate when their organizations handle crises well, because it proves their resilience.
Encourage people to share what they know and what they learn from crises. Speed up
communication. Emphasize reducing the impact of crises. Practice mindfulness. Keep
"uncommitted resources" ready to put into action when a crisis erupts. Structure your
organization so that those who know what to do in a speciﬁc situation can take the lead,
rather than hewing to a set hierarchy.
5. HROs practice "deference to expertise" - Avoid assuming that a direct
relationship exists between your organization's formal hierarchy and which person knows
best what to do in a crisis. Often speciﬁc individuals possess deep expertise or
situational knowledge that should leapfrog the hierarchy, so put them in charge of
relevant major decisions. To increase your organization's deference to expertise, focus on
what the system knows and can handle, rather than taking pride in what you or any
other individual knows and does. Recognize that expertise is "not simply an issue of
content knowledge." Instead, it consists of knowledge plus "credibility, trust and
attentiveness." Recognize and share what you know, even when people don't want to
hear it - but also know your limits and hand oﬀ authority when you reach them.
Auditing Your Organization for Mindfulness 
You can't shut your organization down and redesign it as an HRO, so ﬁnd ways to
redesign it while it is functioning. Start by auditing your current practices from numerous
perspectives. These audits themselves will increase mindfulness. Ask questions to get
people talking. As you review past crises as learning opportunities, you'll start
developing resilience. Study how much people in your organization agree and where
agreement clusters. People at HROs tend to agree across their organizations' diﬀerent
levels, so you have a problem if managers and front line workers disagree. Heed the
areas where people disagree most about what to do or how the organization should
function. See which aspects of reliability are strengths for you. Do you anticipate
problems better (good planning)? Or are you better at containing them (good
responsiveness)? Determine which audit ﬁndings are upsetting and "where you could be
most mindful." 
Ask:
Where are you now? - Examine how mindful your ﬁrm is now. Do you actively
pay attention to potential problems? Does everyone agree what is most likely to go
wrong? Do you all attend to potential problem areas to make the ﬁrm more
reliable?
How mindless are you? - Do you pressure people to do things the same way all
the time or to work without needed information or training? Do you push people to
produce without independent discretion? Such practices lead toward mindlessness.
Where do you need to be most mindful? - Things are likelier to go
unexpectedly wrong when your processes are "tightly coupled" and "interactively
complex," as in a nuclear power plant. Look for these qualities to see where you
need to be most intensely mindful. If you can work in linear units that you fully
understand without feedback from one unit to the next, you need comparatively
little mindfulness. But, if your processes demand coordination and cooperation, or
if you're a start-up and don't yet have all of your systems fully in place, feedback
must ﬂow. That's when you need mindfulness most.

Does your organization obsess over failure? - Encourage people to envision
things that might go wrong and head them oﬀ. Ask yourself how consciously you
seek potential failures. When something happens that you didn't expect, does your
organization ﬁgure out why? When you barely avoid a catastrophe, do you
investigate why things almost went wrong, and learn from them? Do you change
procedures to reﬂect your new understanding? On a simpler level, can people in
your organization talk openly about failure? Do they report things that have gone
awry?
Does your organization resist simpliﬁcation? - Rather than assuming that
your organization knows itself and has the correct perspective, challenge the
routine. This won't always make for the most comfortable workplace, but you must
pay attention to real complexity. What do you take for granted? (Your goal is to be
able to answer "nothing.") Push people to analyze events below the surface. To
deepen their understanding, people must trust and respect each other, even when
they disagree.
Does your organization focus on operations? - Thinking about the big
strategic picture is a lot of fun - but to be resilient, you need to monitor what is
actually happening now, rather than assuming things are running smoothly. Seek
feedback. Make sure people meet regularly with co-workers organization-wide, so
they get a clearer overall picture.
Are you committed to becoming more resilient? - HROs show their
commitment to resilience by funding training so people can develop their
capacities. You want people to know as many jobs and processes as possible.
Besides formal training, encourage people who meet "stretch" goals, use
knowledge and solve problems.
Does your organization defer to expertise? - How much do people want to do
their jobs well? Do they respect each other and defer to those who know an issue
best? 
Building a Mindful Organizational Culture 
If you're the only person in your company dedicated to mindfulness, the dominant
"organizational culture" will swamp your good intentions. To make your organization
more like an HRO, help it adopt an "informed culture" with these "four subcultures": 
1. A "reporting culture" - People share their accounts of what went wrong. 
2. A "just culture" - The organization treats people fairly. Deﬁne "acceptable and
unacceptable" action and do not punish failures that arise from acceptable behavior.
When something goes wrong, seek reasons, not scapegoats. 
3. A "ﬂexible culture" - If your work is very variable, like ﬁghting ﬁres, don't depend on a
rigid, slow hierarchy. Foster individual discretion and variation instead of uniformity. 
4. A "learning culture" - Increase everyone's capacity; provide opportunities for people
to share information. 
Your organization's culture manifests at several levels. It ascends from the level of
physical objects, which can symbolize a corporate personality, to linked processes and
then up to the level of abstractions, like shared values and assumptions. "Artifacts are
the easiest to change, assumptions the hardest." Top managers must consistently model
and communicate the changes they want. State your desired beliefs and practices, give
employees feedback and reward those who succeed. 
As you move into implementing new activities, follow a "small wins strategy," focusing
on attainable goals. Start with the "after action review" in which people compare what
they did in a crisis to what they intended to do, why it diﬀered and how they'll act in the
future. Encourage people to share details, rather than glossing over speciﬁcs. Support
those who try to be more mindful, since being mindless is so much easier. Help them by

verbally reframing objectives. For example, reword current "goals in the form of mistakes
that must not occur." Deﬁne what having a "near miss" means and what constitutes
"good news." If no accident report is ﬁled, is that good - or does it mean things are being
brushed under the rug? Train people in interpersonal skills and encourage skepticism. To
raise awareness of potential glitches, ask employees what unexpected occurrences
they've seen. Meet with people face to face to get the nuances they communicate
nonverbally.
 
1. ^
For some examples, see this short story short story by gwern for gesture how of an AGI
experiment might destroy the world accidentally, or Paul Christiano's What failure looks
like, or Andrew Critch's What Multipolar Failure Looks Like, and Robust Agent-Agnostic
Processes (RAAPs) 

ACX Meetups Everywhere List
Here's the Astral Codex Ten worldwide meetups list, crossposted at LW's request in
case people here are interested in attending. Some cities have an ACX but not an LW
meetup group, or vice versa; others combine their groups. You can ﬁnd the list below,
in the following order:
1. Africa & Middle East
2. Asia-Paciﬁc (including Australia)
3. Canada
4. Europe (including UK)
5. Latin America
6. United States
You can see a map of all the events on the LessWrong community page.
Within each section, it's alphabetized ﬁrst by country/state, then by city - so the ﬁrst
entry in Europe is Vienna, Austria. Sorry if this is confusing.
I'll provisionally be attending the meetups in Berkeley, Los Angeles, and San Diego.
ACX meetups coordinator Mingyuan will provisionally be attending Paris and London.
I'll be announcing some of the biggest ones on the blog, regardless of whether or not I
attend.
Extra Info For Potential Attendees
1. If you're reading this, you're invited. Please don't feel like you "won't be welcome"
just because you're new to the blog, demographically diﬀerent from the average
reader, or hate ACX and everything it stands for. You'll be ﬁne! 
2. You don't have to RSVP or contact the organizer to be able to attend (unless the
event description says otherwise); RSVPs are mostly to give organizers a better sense
of how many people might show up, and let them tell you if there are last-second
changes. I've also given email addresses for all organizers in case you have a
question.
Extra Info For Meetup Organizers:
1. If you're the host, bring a sign that says "ACX MEETUP" and prop it up somewhere
(or otherwise be identiﬁable).
2. Bring blank labels and pens for nametags.
3. Have people type their name and email address in a spreadsheet or in a Google
Form (accessed via a bit.ly link or QR code), so you can start a mailing list to make
organizing future meetups easier.
4. If it's the ﬁrst meetup, people are probably just going to want to talk, and if you try
to organize some kind of "fun" "event" it'll probably just be annoying.
5. It's easier to schedule a followup meetup while you're having the ﬁrst, compared to
trying to do it later on by email.
6. In case people want to get to know each other better outside the meetup, you
might want to mention reciprocity.io, the rationalist friend-ﬁnder/dating site.
7. If you didn't make a LessWrong event for your meetup, the LessWrong team did it
for you using the email address you gave here. To claim your event, log into LW (or
create an account) using that email address, or message the LW team on Intercom
(chat button in the bottom right corner of lesswrong.com).

If you need to change a meetup date or you have any other questions, please email
meetupsmingyuan@gmail.com.
AFRICA & MIDDLE EAST
BAGHDAD, IRAQ
Contact: MA, toﬁahmed117[at]gmail[dot]com, Discord:
WolframSigma#1532, Telegram
Time: Friday, September 2, 11:00 AM
Location: Grinders Coﬀeeshop
Coordinates: 8H568FG6+73
Event link(s): LessWrong
JERUSALEM, ISRAEL
Contact: Zvi Schreiber, zvi[at]zvi[dot]net, WhatsApp +972 54 569 1100
Time: Wednesday, October 19, 6:00 PM
Location: Malcha technology park garden
Coordinates: 8G3QP5XP+PP
Event link(s): LessWrong
REHOVOT, ISRAEL
Contact: David Manheim, David[at]alter[dot]org[dot]il
Time: Sunday, September 11, 8:00 PM
Location: Outside porch of Aroma Coﬀee, , רחובות218 הרצל
Coordinates: 8G3PWR25+MP
Event link(s): LessWrong, Facebook event
Notes: Please RSVP on Facebook so we can give updates if needed
TEL AVIV, ISRAEL
Contact: Adam & inbar M, projectscentrum[at]gmail[dot]com,
inbar192[at]gmail[dot]com, Whatsapp +46762791415 (Adam)
Time: Sunday, September 4, 7:00 PM
Location: Hamenia industrial loft at Beit Alfa 7 (7 רחוב בית אלפא). Look for a door with
ACX sign. Two ﬂoors up.
Coordinates: 8G4P3Q8Q+85
Event link(s): LessWrong
Group info: We've just made a Facebook group and are planning to organize monthly
meetings going forward
Notes: For questions contact Adam on email or WhatsApp. Feel free to bring a snack
or a bottle of white wine.
AMMAN, JORDAN
Contact: Daniel, dnledvs[at]gmail[dot]com
Time: Tuesday, September 20, 6:30 PM
Location: Rustic, Jabal al Weibdeh
Coordinates: 8G3QXW49+WG
Event link(s): LessWrong
Notes: We're hoping to grow the group, so feel free to come even if you've only read
a few posts! +1s are also welcome.
CAPE TOWN, SOUTH AFRICA
Contact: Mark Chimes, chimes[dot]mark[at]gmail[dot]com, WhatsApp 0826568573

Time: Saturday, September 17, 11:00 AM
Location: Truth Coﬀee Roasting, 36 Buitenkant St, Cape Town City Centre - we'll put a
sign on the table
Coordinates: 4FRW3CFF+3M
Event link(s): LessWrong
Group info: We met up pre-Covid and pre-ACX as an SSC group. Now we're getting
back in the swing of things. We eat lunch and chat about philosophy, politics, and
sometimes SSC/ACX blog posts.
Notes: We're planning on having another meetup on the 8th October if you can't
make the ﬁrst.
DAR ES SALAAM, TANZANIA
Contact: Arno, arnorohwedder[at]gmail[dot]com, +255763998637
Time: Thursday, September 29, 7:30 PM
Location: The Deck, Masaki
Coordinates: 6G5X776J+X6
Event link(s): LessWrong
Notes: Seeing if there are any interested people in Dar, look forward to meeting, if
you are coming please send me a whatsapp.
DUBAI, UAE
Contact: RS, xyxyxz[at]gmail[dot]com, +971552726281 (WhatsApp)
Time: Friday, September 30, 7:30 PM
Location: Starbucks, Garhoud
Coordinates: 7HQQ68VR+94
Event link(s): LessWrong
Group info: Met once before
Notes: Please RSVP on LessWrong, or message me on WhatsApp
ASIA-PACIFIC
BRISBANE, AUSTRALIA
Contact: Jarred Filmer, jarred[dot]ﬁlmer[at]gmail[dot]com
Time: Saturday, September 10, 7:00 PM
Location: 52 McCaul Street Taringa (house)
Coordinates: 5R4JFXXQ+P8
Event link(s): LessWrong, Facebook event
Group info: We used to meet once a month years ago, but now just meet whenever
there's a Meetups Everywhere :)
Notes: Snacks will be provided but dinner will not be, would recommend eating
before you come
CANBERRA, AUSTRALIA
Contact: Andy Bachler, Andy[dot]Bachler[at]gmail[dot]com
Time: Wednesday, August 31, 5:30 PM
Location: Badger & Co pub at ANU. Central location, parking free after 5pm, might be
loud, sorry!
Coordinates: 4RPFP4FC+34
Event link(s): LessWrong, Eventbrite
Notes: Parking area just to the north of the pub, over the river, is free after 5pm!
GOLD COAST (SOUTH), AUSTRALIA

Contact: Lerancan, lerancan[at]gmail[dot]com
Time: Sunday, September 11, 2:00 PM
Location: A picnic table, Wyberba Street Reserve, Tugun
Coordinates: 5R3MVF5W+555
Event link(s): LessWrong
Notes: Email me in case of bad weather/you can't ﬁnd me/you can't make that time
etc.
MELBOURNE, AUSTRALIA
Contact: Ryan, xgravityx[at]hotmail[dot]com
Time: Friday, September 2, 6:00 PM
Location: Beer Deluxe Federation Square
Coordinates: 4RJ65XM9+3Q
Event link(s): LessWrong, Facebook event
Group info: We're oﬃcially the Less Wrong Melbourne social meetup group, though
our members include the broader rationalist community. We meet once a month for
casual discussion (and beers for those so inclined). Please join our Facebook group to
see the meeting invite; there you will see a WhatsApp group link - please join that
group too to ensure timely updates in case of changes (Facebook notiﬁcations don't
work reliably for this).
Notes: Please RSVP to the meeting invite on the Facebook group so that I can make
an appropriate booking.
PERTH, AUSTRALIA
Contact: Madge, madgech[at]gmail[dot]com
Time: Sunday, September 25, 2:00 PM
Location: Russell Square, Northbridge, corner of Shenton and Aberdeen St. There will
be some sort of ACX meetup sign.
Coordinates: 4PWQ3V34+W6
Event link(s): LessWrong, Facebook event
Group info: I run one meetup per year, if someone else wants to take over please do
Notes: Please RSVP on LessWrong or Facebook
SYDNEY, AUSTRALIA
Contact: Eliot, Redeliot[at]gmail[dot]com
Time: Thursday, September 15, 6:00 PM
Location: City of Sydney rsl, lvl 2 in the ﬁshbowl
Coordinates: 4RRH46F4+983
Event link(s): LessWrong, Meetup.com
Group info: We meet monthly
WOLLONGONG, AUSTRALIA
Contact: Jason, jason[dot]bowkettblogs[at]gmail[dot]com
Time: Saturday, September 3, 12:00 PM
Location: UOW Library
Coordinates: 4RQGHVVH+69
Event link(s): LessWrong
CHENGDU, CHINA
Contact: Alex, acx[dot]chengdu[at]gmail[dot]com
Time: Thursday, September 15, 7:00 PM
Location: Chef Wenwu Hot & Spicy Jianghu Food (Yulin store)/⽂武⼤厨·热辣江湖菜(⽟林
店). I (a foreigner) will be wearing a green shirt.

Coordinates: 8P26J3C5+462
Event link(s): LessWrong
Notes: Please RSVP at the above email address, I will give you my Wechat contact if
you're interested in attending. Open to time/date/location changes, so let me know if
the proposed event doesn't work for you! Can be a bilingual event; all welcome. 有双语
交流的可能性。如果想来的话，请提前发给我个电⼦邮件。
HONG KONG
Contact: Nathan, nathan[at]xevarion[dot]org
Time: Saturday, September 10, 1:00 PM
Location: The Catalyst, 2 Po Yan Street, Sheung Wan. Big wooden door.
Coordinates: 862M74PW+6XP
Event link(s): LessWrong
BANGALORE, INDIA
Contact: Nihal, propwash[at]duck[dot]com, Discord: propwash#4648
Time: Sunday, September 18, 4:00 PM
Location: Matteo Coﬀea, Church Street
Coordinates: 7J4VXJF4+PR
Event link(s): LessWrong
Group info: We're the longest active group in Asia — we've been meeting monthly
for the last 4 years, discussing ACX posts, LW content with a diverse and friendly
group of people. Check our website for more info.
Notes: Please RSVP on LessWrong to help me be better prepared.
HYDERABAD, INDIA
Contact: Vatsal, vmehra[at]pm[dot]me, Whatsapp: +919944430856 (username: Vim)
Time: Sunday, September 11, 5:00 PM
Location: The Weekend Cafe, Plot No D, 3, Vikrampuri Colony, beside vac's bakery,
Vikrampuri Colony, Lane, Secunderabad, Telangana, 500015, India
Coordinates: 7J9WFF4X+5P
Event link(s): LessWrong
Group info: Our rationality meetup group has been around for about 3 months and
we discuss articles and exercises (eg. CFAR handbook) that can help us improve
epistemic and instrumental rationality.
MUMBAI, INDIA
Contact: PB, e2y94n1nv[at]relay[dot]ﬁrefox[dot]com
Time: Sunday, October 9, 4:00 PM
Location: Jamjar Diner, Versova
Coordinates: 7JFJ4RM6+5W
Event link(s): LessWrong
Notes: Please RSVP on LessWrong or via email so I can plan activities accordingly.
NEW DELHI, INDIA
Contact: Suryansh Tyagi, suryanshtyagiphone[at]gmail[dot]com, WhatsApp/phone
+919997299972
Time: Sunday, September 11, 5:00 PM
Location: Select CityWalk Mall, Saket. Where inside the mall depends on the number
of people interested.
Coordinates: 7JWVG6H9+8H
Event link(s): LessWrong
Notes: Please either send me an email or message me on WhatsApp if you want to

attend. Any suggestions/changes are welcome.
UDAIPUR, RAJASTHAN, INDIA
Contact: Shailendra Paliwal, acx-meetup-2022[at]shailendra[dot]me
Time: Saturday, September 10, 7:00 PM
Location: We'll be at Doodh Talai near Pichola Lake and I'll be wearing a gray t-shirt
carrying a sign ACX Meetup
Coordinates: 7JPMHM9M+HG
Event link(s): LessWrong
Notes: Please RSVP on LessWrong so that I can plan ahead
UBUD, BALI, INDONESIA
Contact: William Ubud, Napaproject[at]gmail[dot]com
Time: Tuesday, August 30, 6:00 PM
Location: PARQ Ubud
Coordinates: 6P3QG789+F7
Event link(s): LessWrong
TOKYO, JAPAN
Contact: Harold Godsoe, hgodsoe[at]gmail[dot]com
Time: Saturday, October 8, 10:00 AM
Location: Near Nakameguro station - RSVP for details
Coordinates: 8Q7XJPV2+QFP
Event link(s): LessWrong, Meetup.com
Notes: ACX Tokyo meets monthly since Sept 2021. Our meetups are in English, so far.
To join in, feel free to get in touch in any of the many ways to do so
(email, Meetup.com). It's useful to be in contact before coming to an event, to help
with that ﬁrst leap of faith.
KUALA LUMPUR, MALAYSIA
Contact: Yi-Yang, yi[dot]yang[dot]chua[at]gmail[dot]com, LessWrong proﬁle
Time: Saturday, September 17, 2:00 PM
Location: I'll be in Lisette's Bangsar, which is a 5-minute walk from Bangsar LRT. I'll
be wearing a pale green t-shirt and carrying an ACX sign.
Coordinates: 6PM34MHH+VW
Event link(s): LessWrong
AUCKLAND, NEW ZEALAND
Contact: Jonathan De Wet, jonpdw[at]gmail[dot]com
Time: Saturday, September 3, 6:30 PM
Location: 32 Stanley Ave Milford, Auckland
Coordinates: 4VMP6QH4+86
Event link(s): LessWrong, Facebook event
Notes: It's a dinner party! Please RSVP on FB so I know how much food to make
DUNEDIN, NEW ZEALAND
Contact: Gavin, bisga673[at]student[dot]otago[dot]ac[dot]nz
Time: Saturday, September 3, 3:00 PM
Location: Picnic tables outside of St. David's lecture theatre on Otago University
campus. I'll make a sign with ACX meetup.
Coordinates: 4V6G4GP7+GM5
Event link(s): LessWrong
Notes: There is no Dunedin group as far as I'm aware of, but I'd be keen to meet
other likeminded people and organise group hangouts occasionally.

WELLINGTON, NEW ZEALAND
Contact: Ben W, benwve[at]gmail[dot]com
Time: Tuesday, September 27, 5:30 PM
Location: Rutherford House, Bunny Street, Wellington. Room MZ05, which is on the
mezzanine ﬂoor
Coordinates: 4VCPPQCH+FGC
Event link(s): LessWrong
Notes: We're running the event this time in partnership with Eﬀective Altruism
Wellington
LAPU LAPU, CEBU, PHILIPPINES
Contact: Dave, tokkolizard[at]tutanota[dot]com
Time: Sunday, September 4, 2:00 PM
Location: Starbucks in Mactan Newtown, there will be a sign with ACX MEETUP on it.
Coordinates: 7Q268257+4F
Event link(s): LessWrong
Notes: Please RSVP by mail so I know if I need to set up a bigger meeting place
SINGAPORE
Contact: Jonathan Ng, jonathan[dot]ng1[at]gmail[dot]com, Telegram @derpy
Time: Tuesday, September 6, 6:30 PM
Location: Tanjong Pagar MRT gantry, I'll be wearing the dark blue EA Global 2022
jumper
Coordinates: 6PH57RGW+J8
Event link(s): LessWrong
CANADA
CALGARY, AB
Contact: David Piepgrass, qwertie256[at]gmail[dot]com
Time: Saturday, September 10, 2:00 PM
Location: Marlborough Mall food court
Coordinates: 9538324C+CH9
Event link(s): LessWrong
Group info: It's small!
EDMONTON, AB
Contact: JS,  ta1hynp09[at]relay[dot]ﬁrefox[dot]com
Time: Thursday, October 13, 6:30 PM
Location: Polar Park Brewing Company - we will have a sign.
Coordinates: 9558GG82+GG
Event link(s): LessWrong
Group info: LessWrong
VANCOUVER, BC
Contact: Tom Ash and Dirk Haupt, events[at]philosoﬁles[dot]com
Time: Saturday, September 10, 1:00 PM
Location: Dude Chilling (aka Guelph) Park, near the intersection of Main, Broadway &
Kingsway. We'll be just west of the garden - look for Tom in a neon yellow shirt.
Coordinates: 84XR7W73+PG
Event link(s): LessWrong, Facebook event

Group info: For future events, join the following: For rationalism, this Facebook group,
for eﬀective altruism, this Facebook group for both, Meetup.com
Notes: 🍣 We'll have a sushi lunch for everyone who comes (ﬁsh or vegan). This is
not at all necessary, but posting on the Facebook event to say you will or won't want
this will help estimate numbers. RSVPing there will help boost attendance too.
VICTORIA, BC
Contact: Sarah McManus, sarahmcmanusbc[at]gmail[dot]com, Twitter
@SarahAMcManus
Time: Friday, September 23, 7:00 PM
Location: Snowy Village, 4071 Shelbourne St #2a, Victoria, BC V8N 5Y1 - It's a small
cafe, I'll be at a table with an ACX MEETUP sign on it
Coordinates: 84WRFMG9+H3
Event link(s): LessWrong, Facebook event
HALIFAX, NS
Contact: Conor Barnes (ideopunk), conorbarnes93[at]gmail[dot]com
Time: Sunday, September 25, 1:00 PM
Location: Seven Bays Cafe (2017 Gottingen Street)
Coordinates: 87PRMC29+9C
Event link(s): LessWrong
Notes: Join us at Seven Bayes
KITCHENER-WATERLOO, ON
Contact: Jenn, hi[at]jenn[dot]site
Time: Sunday, September 25, 1:00 PM
Location: Goudie's Lane, besides 8 Queen St N, Kitchener. I'll be wearing white boots
and at one of the picnic tables if it's not raining, or further back in the parking area if it
is. There will be some sort of ACX MEETUP sign.
Coordinates: 86MXFG26+5CV
Event link(s): LessWrong
Group info: We have a new regular meetup group! We meet up every other
Thursday. Events are posted on LessWrong, and we also have a website.
Notes: Please RSVP on LessWrong if possible, but show up anyways if you weren't
able to! Generally, past meetups everywhere events have attracted 8-15 people.
OTTAWA, ON
Contact: Tess Walsh, rationalottawa[at]gmail[dot]com
Time: Friday, September 16, 6:00 PM
Location: We are meeting at the Atelier d'innovation sociale, located at 95 Clegg St,
K1S1C5. Speciﬁcally in the Lounge area, there will be numerous signs for ACX MEETUP
where needed.
Coordinates: 87Q6C84F+PM4
Event link(s): LessWrong, Facebook event
Group info: We meet weekly on Friday evenings, and that allows us enough
opportunity to try out a huge variety of diﬀerent types of events — probably some
that you, yes you, would enjoy! Here are our Facebook, LessWrong,
and Discord (where the action really is)
Notes: I always appreciate RSVP's in any form! It helps me set expectations/plan the
best meetup I can! You can also contact me, Tess Walsh, with any questions
whatsoever at rationalottawa@gmail.com
TORONTO, ON
Contact: Sean Aubin, seanaubin[at]gmail[dot]com

Time: Sunday, September 18, 3:00 PM
Location: Located at the picnic tables located in The Bentway, which is the sheltered
area underneath the Gardiner Expressway.
Coordinates: 87M2JHPR+X5W
Event link(s): LessWrong
Group info: Currently meeting monthly with ambitions to meet bi-monthly.
Notes: Please RSVP on LessWrong so I know how many people to anticipate.
WATERLOO, ON
See Kitchener-Waterloo, ON
MONTREAL, QC
Contact: E, 90u610sye[at]relay[dot]ﬁrefox[dot]com 
Time: Saturday, September 24, 1:00 PM
Location: Jeanne-Mance Park, at the corner of Duluth and Esplanade
Coordinates: 87Q8GC89+37
Event link(s): LessWrong
Group info: We meet once a month. Upcoming events will be posted on LessWrong
Notes: Please check the LessWrong event page the day of, as I will update it in the
event of rain
EUROPE
VIENNA, AUSTRIA
Contact: Manuel, manuel[dot]turonian[at]gmail[dot]com
Time: Saturday, September 10, 2:00 PM
Location: Wiener Stadtpark at the Strauss Monument; will have an ACX Meetup sign.
Coordinates: 8FWR693H+GP2
Event link(s): LessWrong
Group info: Rationality Vienna is a group of about 30 people who meet once a month
in person or via Zoom. You can join our Facebook group.
Notes: We may want to shift to an indoor location depending on weather and the
local Covid numbers.
BRUSSELS, BELGIUM
Contact: Bruno D, bruno[dot]astral[dot]codex[at]gmail[dot]com
Time: Sunday, September 18, 4:00 PM
Location: Guingette Henri, George Henri parc
Coordinates: 9F26RCWC+84
Event link(s): LessWrong
SOFIA, BULGARIA
Contact: Anastasia, soﬁa[dot]acx[dot]meetup[at]gmail[dot]com
Time: Saturday, September 17, 4:00 PM
Location: Shade Garden (Сенчестата градинка; part of Borisova garden)
Coordinates: 8GJ5M8GW+J9
Event link(s): LessWrong
Group info: Soﬁa ACX started with last year's Meetups Everywhere round. We have
Serious Meetups once per month at which we discuss a blog post, a short story, or a
book (for instance, The Scout Mindset, The Money Illusion, The Metropolitan Man); and
sporadic non-serious social meetups that mostly include getting dinner, going on a
walk, watching a ﬁlm, or playing board games. Attendance hovers around 6-8 people
out of a pool of 13. People get invited to the Discord server after they've attended at

least one in-person meetup.
ZAGREB, CROATIA
Contact: DJStern, dorian[dot]sternvukotic[at]gmail[dot]com
Time: Saturday, September 3, 6:00 PM
Location: Krivi Put
Coordinates: 8FQQRX38+V6W
Event link(s): LessWrong
Group info: Croatian LessWrong active group communicates mainly through a
Telegram group, we meetup semi regularly, approx once a month. The group is mostly
social, and the meetups are not structured (sometimes we all just meet at a random
party)
Notes: Send me an Email and I will add you to a Telegram group where everything
(active) LessWrong Croatia/Zagreb happens
LIMASSOL, CYPRUS
Contact: Arseniy, runescape[at]list[dot]ru, @anchorheld (Telegram / Instagram)
Time: Saturday, September 3, 12:00 PM
Location: By the Municipal Zoo
Coordinates: 8G6MM3M3+Q6
Event link(s): LessWrong
Notes: Please hit me up on Mail, Telegram, or Instagram if you're actually going
PRAGUE, CZECH REPUBLIC
Contact: Jiri Nadvornik, jiri[dot]nadvornik[at]efektivni-altruismus[dot]cz
Time: Thursday, October 6, 6:00 PM
Location: Garden of Dharmasala Teahouse
Coordinates: 9F2P3CRW+FP7
Event link(s): LessWrong, Facebook event
COPENHAGEN, DENMARK
Contact: Søren Elverlin, soeren[dot]elverlin[at]gmail[dot]com
Time: Saturday, September 24, 3:00 PM
Location: Rundholtsvej 10, 2300 København S
Coordinates: 9F7JMH38+GFP
Event link(s): LessWrong, Facebook event, Meetup.com
Notes: Please RSVP on LessWrong
TALLINN, ESTONIA
Contact: Andrew W, andrew_n_west[at]yahoo[dot]co[dot]uk
Time: Monday, September 26, 7:00 PM
Location: St Vitus, Tallinn. I don't know if anyone will turn up, but I'll be wearing a
suit, a beard, and a book.
Coordinates: 9GF6CPRH+MQ
Event link(s): LessWrong
HELSINKI, FINLAND
Contact: Joe Nash, joenash499[at]gmail[dot]com
Time: Saturday, September 3, 4:00 PM
Location: Restaurant Töölönranta, Helsinginkatu 56
Coordinates: 9GG65WMJ+2J
Event link(s): LessWrong
Group info: LessWrong group

FONTAINEBLEAU, FRANCE
Contact: Ebrahim Akbari, ea[dot]akbari[at]gmail[dot]com
Time: Saturday, September 10, 6:00 PM
Location: Glasgow bar, Fontainebleau
Coordinates: 8FW4CP32+J8
Event link(s): LessWrong
PARIS, FRANCE
Contact: Olivier, w20l2qtf[at]mailer[dot]me, We have a Discord and a matrix
server (both servers are bridged together)
Time: Friday, September 23, 6:00 PM
Location: In the jardin du carrousel, next to jardin des Tuileries
Coordinates: 8FW4V86J+GH
Event link(s): LessWrong
Group info: Regular meetups organized via discord or the newsletter every 3 months
with around 20 people.
Notes: We have a mailing list if you are interested in future meetups. Please don't
hesitate to send me an email to RSVP that you're coming to help gauge the interest.
TOULOUSE, FRANCE
Contact: Alfonso, barsom[dot]maelwys[at]gmail[dot]com
Time: Saturday, October 8, 7:00 PM
Location: Bar 'Le Biergarten' (60 Gd Rue Saint-Michel, 31400 Toulouse). We'll be
sitting at a table with an ACX MEETUP sign on it.
Coordinates: 8FM3HCQW+9H
Event link(s): LessWrong
Notes: Please RSVP by email
TBILISI, GEORGIA
Contact: Evgenia Karunus, lakesare[at]gmail[dot]com, https://twitter.com/lakesare
Time: Saturday, September 17, 7:00 PM
Location: Coﬀee Place
Coordinates: 8HH6MRQ2+WH
Event link(s): LessWrong
AACHEN, GERMANY
Contact: Jörn, acx[at]j[dot]stoehler[dot]eu
Time: Tuesday, September 27, 7:00 PM
Location: Chico Mendes
Coordinates: 9F28Q3HJ+9Q
Event link(s): LessWrong
Notes: Please RSVP here so I can reserve the right number of tables.
BERLIN, GERMANY
Contact: Ruben Arslan, ssc[at]alphabattle[dot]xyz
Time: Sunday, October 2, 2:00 PM
Location: Südplateau Fritz-Schloss-Park
Coordinates: 9F4MG9H4+4X
Event link(s): LessWrong, Google Calendar
Notes: Please RSVP on LessWrong. I'll bring some beverages.
COLOGNE, GERMANY
Contact: Marcel Müller, marcel_mueller[at]mail[dot]de
Time: Saturday, October 8, 5:00 PM

Location: Marienweg 43, 50858 Köln, private venue, just ring the bell or follow the
sign.
Coordinates: 9F28WRMX+96H
Event link(s): LessWrong
Group info: LW / ACX / Rationalist meetup group. Monthly, mostly social meetups.
Other activities welcome. Unless noted otherwise we will meet at Marienweg 43 in
50858 Cologne on the 2nd Saturday of each month at 5 pm. Please email me to be
added to our mailing list where deviations will be posted. Caution! September Meetup
will be at a diﬀerent venue!
Notes: If you read this you are welcome. Our Covid rules are still in eﬀect: You must
be tested negative on the same day. Self tests will be available at the meetup. If there
is any problem, like you do not ﬁnd us or I did not see your mail, call me
+491788862254.
FREIBURG IM BREISGAU, GERMANY
Contact: Omar, info[at]rationality-freiburg[dot]de
Time: Friday, October 14, 6:00 PM
Location: FlexRooms, Salzstr. 1, 79098 Freiburg. We will carry a cardboard sign
saying "Rationality Freiburg".
Coordinates: 8FV9XVV2+V56
Event link(s): LessWrong, Meetup.com, Website
Group info: The group started in May 2022 and before the summer break we had ﬁve
meetups with 4-11 people attending. Every two weeks seems like a good rhythm, but
nothing is set in stone. So far we always read something beforehand and then
discussed it, as well as trying some practical exercises such as TAPs and Personal
Calibration. Afterwards we went to have dinner and continued talking about
everything and anything for hours. Everything is new and ﬂexible, so come and help
us improve!
Notes: We have a Signal messenger group and ask you to attend a meetup once to
be able to join.
HAMBURG, GERMANY
Contact: Gunnar Zarncke, g[dot]zarncke[at]gmail[dot]com
Time: Saturday, September 17, 5:00 PM
Location: Kleine Wallanlagen on the lawn near Memorial Holstenglacis. Look for pink
blankets; I will also have an ACX sign. Here is an Open Street Map link which also
shows the short-cut tunnel from the subway station.
Coordinates: 9F5FHX4H+RXC
Event link(s): LessWrongLessWrong
Notes: Please RSVP on LessWrong
KARLSRUHE, GERMANY
Contact: Marcus Wilhelm, mail[at]marcuswilhelm[dot]de
Time: Saturday, September 24, 3:00 PM
Location: Botanischer Garten Karlsruhe
Coordinates: 8FXC2C72+85X
Event link(s): LessWrong
Group info: We meet weekly, alternating oﬄine and online, see our LessWrong page
KASSEL, HESSEN, GERMANY
Contact: Tobias, Sphinxﬁre[at]outlook[dot]de
Time: Saturday, September 10, 2:00 PM
Location: Friedrichsplatz, to the left of the DocumentaHall
Coordinates: 9F3F8F6X+R6

Event link(s): LessWrong
Group info: Telegram group
Notes: Please join the Telegram group if you are interested in coming. It will be
helpful for coordinating something beyond 'let's just see who shows up and take it
from there', plus, it will also make me feel a lot better on a purely subjective level if I
know beforehand that at least one other person is interested. If you prefer the surprise
factor of 'knowing as little as possible about who you're going to meet', you can also
just write me via E-mail, of course.
LEIPZIG, GERMANY
Contact: Gunther Forderung, notavailable[at]riseup[dot]net
Time: Tuesday, October 4, 6:00 PM
Location: In the Lene-Voigt-Park, in the secluded area opposite of the swings
Coordinates: 9F3J8CM2+PF
Event link(s): LessWrong
TÜBINGEN, GERMANY
Contact: Emma, emma[dot]tuebingen[at]gmail[dot]com
Time: Sunday, October 23, 6:00 PM
Location: The ACX/SSC meetup and dinner (with vegan options) will be on October
23rd at the Annette Kade dormitory (Mohlstraße 44). If you'd like to attend, please
write me an email, and I'll send you an invitation to our WhatsApp group.
Coordinates: 8FWFG3H5+XR
Event link(s): LessWrong
Notes: Please email me to get my phone number. If a lot of people are out of town for
the holidays and can't come we could meet on, say, October 1st. I would like to know
how many people to expect.
ATHENS, GREECE
Contact: Spyros, spyros[dot]dovas[at]gmail[dot]com
Time: Monday, September 5, 7:00 PM
Location: On the plaza in front of the National Library
Coordinates: 8G95WMQR+WRP
Event link(s): LessWrong, Meetup.com
Group info: We have organized 2 events so far, fall and spring, we just sit around and
discuss. We have a Whatsapp group that hasn't picked up momentum yet.
Notes: Please RSVP on LessWrong or Meetup.com
BUDAPEST, HUNGARY
Contact: Tim Underwood, timunderwood9[at]gmail[dot]com, WhatsApp 19513120591
Time: Sunday, September 11, 2:00 PM
Location: Champs Sziget bar on Margit Sziget, near the front. I'll have a big
hardcover copy in Hungarian of a book by Richard Dawkins.
Coordinates: 8FVXG2CW+2H
Event link(s): LessWrong
Group info: We've been meeting in Budapest for two years now, with our ﬁrst
meeting being the 2020 ACX meetups everywhere. We meet about once a month, and
usually we have two articles that are suggested reading that we discuss.
CORK, IRELAND
Contact: Mikey, Godojhana[at]gmail[dot]com
Time: Thursday, September 29, 6:00 PM
Location: If sunny: The Lough. If not, then the game arcade on the parade
Coordinates: 9C3HVGQ7+JQ

Event link(s): LessWrong
DUBLIN, IRELAND
Contact: Lucius, lucius[at]bushnaq[dot]de, LessWrong proﬁle
Time: Sunday, October 2, 12:30 PM
Location: Clement & Pekoe, William Street South, Dublin 2. We'll be sitting inside,
and there'll be a sign with ACX written on it on the table
Coordinates: 9C5M8PRP+JV
Event link(s): LessWrong
Group info: LessWrong
FOLIGNO, ITALY
Contact: Mauro, orﬁno[at]yandex[dot]com, LW proﬁle, Telegram
Time: Saturday, September 24, 5:00 PM
Location: Parco dei Canapé, at the open air cafe, ask the barista
Coordinates: 8FJJXP22+HC
Event link(s): LessWrong
Notes: Please RSVP on LessWrong so I know how much food to get. No kids please.
MILANO, ITALY
Contact: Raﬀaele, raﬀa[dot]mauro[at]gmail[dot]com
Time: Friday, September 16, 6:30 PM
Location: Viale Luigi Majno, 18, 20129 Milano MI - Primo Ventures / T8P, IInd ﬂoor.
Coordinates: 8FQFF6C4+9C
Event link(s): LessWrong
Group info: We meet once per month. The group started in May 2022.
Notes: Please RSVP by email by the 1st of September
PADOVA, ITALY
Contact: Carlo, carlo[dot]martinucci[at]gmail[dot]com
Time: Saturday, October 1, 3:30 PM
Location: Prato della Valle, fountain in the middle. I'll be carrying a sign with ACX
MEETUP on it :)
Coordinates: 8FQH9VXG+9J
Event link(s): LessWrong
Notes: We'll probably ﬁnd a bar to have a hot chocolate or tea or something :)
PISA, ITALY
Contact: Raﬀaele, raﬀaelesalvia[at]alice[dot]it
Time: Saturday, October 22, 5:00 PM
Location: We will meet in Piazza dei Cavalieri, near the steps of Palazzo della
Carovana
Coordinates: 8FMGPC92+R44
Event link(s): LessWrong
ROMA, ITALY
Contact: Grigorio, greghero12[at]gmail[dot]com, Facebook, +393920366026
Time: Saturday, October 8, 6:00 PM
Location: We'll be around Gardenie metro station, at the benches, and I will be
wearing a red shirt and sitting on top of the station to be seen
Coordinates: 8FHJVHP9+8F
Event link(s): LessWrong
Group info: We meet around 20-25 times a year but it is asymmetrical, focused in
summer, Christmas and Easter. We discuss opinions, engage in circling, play games

where we spot logical fallacies and biases by attacking our members ideological
weakpoints and formalize some debating stances. Occasionally we ﬁnd the willpower
to devote meetups in steelmanning and understanding the outgroup (roughly 4-5
times a year)
Notes: If you are into ACX enough to see this post, I believe we have enough common
ground to be worth meeting each other. Aren't you curious who else is within this
niche community in Rome? Come on, take a leap of faith. P.S. Would be nice if you
sent me a message in WhatsApp with your name and probability of attendance, but I
love walk-ins just ﬁne. No space limit after all ;-)
RIGA, LATVIA
Contact: Andis, cerulean[dot]lemniscate[at]protonmail[dot]com
Time: Saturday, September 17, 4:00 PM
Location: Bastejkalns (on top of the hill)
Coordinates: 9G86X426+Q5Q
Event link(s): LessWrong
AMSTERDAM, NETHERLANDS
Contact: Pierre, pierreavdb[at]gmail[dot]com
Time: Saturday, September 10, 3:00 PM
Location: Kanarie Klub (Bellamyplein 51, 1053 AT Amsterdam)
Coordinates: 9F469V89+W4
Event link(s): LessWrong
Group info: The rationality community is growing in the Netherlands, and we're now
planning on having monthly meetups! Join the Rationality NL Discord server.
Notes: Please RSVP on LessWrong so I can plan a diﬀerent venue if needed
DELFT, NETHERLANDS
Contact: Pierre Bongrand, bongrand[dot]pierre[at]gmail[dot]com, 0033620644013
(Whatsapp/Telegram/Signal)
Time: Thursday, September 22, 6:30 PM
Location: Delftse Hout Beach, on the grass, in the center of the beach, I will be
wearing a red T-shirt and carrying a sign with ACX MEETUP on it.
Coordinates: 9F4629FG+66
Event link(s): LessWrong
HATTEM, NETHERLANDS
Contact: Shoshannah, shos[dot]rationality[at]gmail[dot]com, Discord: Dark#0849
Time: Saturday, October 8, 2:00 PM
Location: Lijsterbeslaan 6, Hattem
Coordinates: 9F48F378+PR
Event link(s): LessWrong, Facebook event
Group info: We support and coordinate groups across the country, including
everything from social meetups to structured events and applied rationality. The
intention is to connect all Dutch rationalists and rationalists in the Netherlands. We
also discuss rationality topics online and coordinate events on our Discord server.
Notes: Feel free to bring kids. Ours will be there :) Also, please park 't Heem if you are
coming by car. It's a 2 minute walk to our house.
HELMOND, NETHERLANDS
Contact: Rutger, silvery[dot]swift[at]protonmail[dot]com
Time: Saturday, September 17, 3:00 PM
Location: De Motte (On top of the hill). Nearest road is Palladio.
Coordinates: 9F37FMC5+VR

Event link(s): LessWrong
THE HAGUE, NETHERLANDS
Contact: Kristof Redei, acxmeetup[at]kristof[dot]me
Time: Wednesday, September 14, 6:00 PM
Location: Paleistuin, Prinsessewal, 2513 EE Den Haag, Netherlands. We'll have a
picnic blanket with an ACX sign on the large central ﬁeld, somewhere near the
playground.
Coordinates: 9F4638J3+GP
Event link(s): LessWrong, Facebook event
Notes: Please RSVP on Facebook if possible! All ages/species welcome. If it's not
outdoor weather, we'll go to The Bookstor Cafe next door as a backup.
OSLO, NORWAY
Contact: Hans Andreas & Jonas, acxoslomeetup[at]gmail[dot]com
Time: Saturday, September 17, 1:00 PM
Location: Look for the sign of Moloch at Café Billabong - Bogstadveien 53B 0366 Oslo
Coordinates: 9FFGWPH7+QP
Event link(s): LessWrong, Meetup.com
Group info: We're hoping to do at least a quarterly meetup, but we'll base it on the
turnout and enthusiasm of this event.
Notes: The cafe has historically been accepting of guests' not ordering--please don't
let ﬁnancial reasons keep you away!
GDAŃSK, POLAND
Contact: Frank, frankastralcodexten[at]gmail[dot]com, Discord: frhrpr#1663
Time: Saturday, August 27, 3:00 PM
Location: Next to Park Kuźniczki, opposite the train station, on the circular benches
around the water pump; I will be wearing a red armband
Coordinates: 9F6W9JJ4+JW
Event link(s): LessWrong
KRAKÓW, POLAND
Contact: Mateusz Bagiński, bagginsmatthew[at]gmail[dot]com
Time: Saturday, September 17, 2:30 PM
Location: Celna 6/9, the oﬃce of the Optimum Pareto Foundation
Coordinates: 9F2X2WVX+V2
Event link(s): LessWrong, Facebook event
Group info: We meet every month, here is our Facebook group.
LUBLIN, POLAND
Contact: Piotr, piotrekzlublina[at]gmail[dot]com
Time: Saturday, September 17, 5:00 PM
Location: Między Słowami cafe, Rybna 4, Lublin
Coordinates: 9G346HX8+FX
Event link(s): LessWrong
POZNAŃ, POLAND
Contact: Ofelia Kerr, ofel[dot]kerr[at]gmail[dot]com, Discord: ofelia#0001
Time: Saturday, October 8, 6:00 PM
Location: Van Gogh Pub, Żydowska 12, 61-761. I'll most likely be on the ground ﬂoor
and I'll have an ACX sign.
Coordinates: 9F4RCW5P+X3F
Event link(s): LessWrong

WARSAW, POLAND
Contact: Michał, rationalwarsaw[at]gmail[dot]com
Time: Sunday, September 4, 6:00 PM
Location: Południk Zero, Wilcza 25
Coordinates: 9G4362G8+2V
Event link(s): LessWrong, Meetup.com
Group info: The community of Warsaw LessWrong/SSC/ACX/etc. readers is active for
over 8 years now. We're trying to organise regular monthly meetups. You can join
our Facebook group or Meetup.com.
LISBOA, PORTUGAL
Contact: Luís Campos, luis[dot]ﬁlipe[dot]lcampos[at]gmail[dot]com
Time: Saturday, September 10, 3:00 PM
Location: Jardim Amália Rodrigues, close to Linha d'Água cafe, in the top of a hill,
below a bunch of trees
Coordinates: 8CCGPRJW+V8
Event link(s): LessWrong
Group info: We've been meeting every month for around 1 year. Get in contact if you
want to participate in the WhatsApp group. :)
BUCHAREST, ROMANIA
Contact: Tony, skyrimtracer[at]gmail[dot]com
Time: Sunday, October 16, 3:00 PM
Location: Plaza România Mall, Bd. Timișoara 26 - food court
Coordinates: 8GP8C2HM+9X
Event link(s): LessWrong
Notes: Please RSVP by email
CLUJ-NAPOCA, ROMANIA
Contact: Marius Pop, pop[dot]marius[at]gmail[dot]com
Time: Saturday, September 3, 11:00 AM
Location: Deva Host, Strada Deva 1-7
Coordinates: 8GR5QH8F+MW
Event link(s): LessWrong
BELGRADE, SERBIA
Contact: Ivica Bogosavljevic, ibogosavljevic[at]gmail[dot]com, Viber +381 65 3473
433
Time: Monday, September 12, 6:00 PM
Location: Pool Cafe on Prve pruge
Coordinates: 8GP2RCP7+G7
Event link(s): LessWrong
Notes: Please RSVP on my Viber number, so I know how big the room we need.
BRATISLAVA, SLOVAKIA
Contact: Viliam, viliam[at]bur[dot]sk
Time: Saturday, September 10, 3:00 PM
Location: Medická záhrada, by the fountain
Coordinates: 8FWV44X9+XW8
Event link(s): LessWrong
Notes: I will post an announcement on LessWrong later. In case of rain, a new
meeting place nearby will be announced there.

LJUBLJANA, SLOVENIA
Contact: Demjan Vester, demjan[dot]vester[at]gmail[dot]com
Time: Wednesday, September 14, 6:00 PM
Location: Probably Lili Novy bar, near modern gallery and park Tivoli
Coordinates: 8FRP3F3X+6V
Event link(s): LessWrong, Meetup.com
Group info: We meet about 0.7 times a month.
Notes: Please RSVP because last time we just barely got a place big enough.
BARCELONA, SPAIN
Contact: Alfonso, alfonso[dot]martinez[at]upf[dot]edu, WhatsApp +34693846738
Time: Sunday, October 2, 5:30 PM
Location: Parc de la Ciutadella, by the Lion Catcher statue; I'll have an ACX sign
Coordinates: 8FH495QP+96
Event link(s): LessWrong
Notes: The idea is to sit on the grass; bring a foulard along for your comfort, or a
foldable chair if preferred. Don't worry about the language: English, Spanish, Catalan,
we'll ﬁnd a way.
MADRID, SPAIN
Contact: Jaime, jaimesevillamolina[at]gmail[dot]com
Time: Saturday, September 10, 5:00 PM
Location: Teatro de títeres del Parque del Retiro. We'll be on the stands with an ACX
sign
Coordinates: 8CGRC897+F8C
Event link(s): LessWrong
Group info: We are an EA / rationality group, we've been active for around 5 years
but have less in-person activity since the pandemic started. We have a WhatsApp
group and a channel in the Spanish-speaking EA Slack.
SEVILLA, SPAIN
Contact: Edu, edur[dot]acx[at]gmail[dot]com
Time: Saturday, September 10, 8:00 PM
Location: Parque de María Luisa. I'll be on the grass behind the Museum of Popular
Arts and Traditions. I'll be the guy next to an ACX sign, a white wooden chair, and a
cardboard ukulele with a tiny cardboard hat on it.
Coordinates: 8C9P92F6+3RG
Event link(s): LessWrong
GOTHENBURG, SWEDEN
Contact: Joacim, joacimj[at]gmail[dot]com
Time: Saturday, September 24, 3:00 PM
Location: Condeco Fredsgatan. I'll have a stack of three books on my table.
Coordinates: 9F9HPX4C+39G
Event link(s): LessWrong, Facebook event
STOCKHOLM, SWEDEN
Contact: Sal, niktonick[at]gmail[dot]com, Telegram
Time: Sunday, September 25, 3:00 PM
Location: Humlegården, Karlavägen. We will meet near blue gazebo, I will have 'ACX
meetup' sign.
Coordinates: 9FFW83RF+3M5
Group info: Facebook group

BERN, SWITZERLAND
Contact: Daniel, dd14214+acx[at]gmail[dot]com
Time: Sunday, October 2, 4:00 PM
Location: Grosse Schanze, at the statue in front of the main uni building, heading to
the Pittaria if it's cold or raining
Coordinates: 8FR9XC2Q+4G
Event link(s): LessWrong
GENEVA, SWITZERLAND
Contact: Eric, eric[dot]c[dot]p[dot]meier[at]gmail[dot]com
Time: Sunday, September 11, 4:00 PM
Location: Park de la Grange, just towards the lake below Villa de la grange
Coordinates: 8FR86548+J4
Event link(s): LessWrong
Group info: We have a small persistent group who has tried to meet up once a month
since last years Meetup.
Notes: Feel free to bring other people you think would be interested!
ZURICH, SWITZERLAND
Contact: MB, acxzurich[at]proton[dot]me
Time: Saturday, September 24, 3:00 PM
Location: TBD
Event link(s): LessWrong
ISTANBUL, TURKEY
Contact: J, jinai[dot]jyap[at]gmail[dot]com
Time: Sunday, September 25, 4:00 PM
Location: The House Cafe in Ortaköy. I am a young Asian woman and imagine I'll be
easy to spot, but will also try to bring a sign with ACX MEETUP on it.
Coordinates: 8GHF22XG+23P
Event link(s): LessWrong, Partiful
Group info: I do not live here; I am just digital nomading for an indeﬁnite amount of
time and would like to meet anyone who's here!
Notes: Please RSVP via the Partiful link (you can RSVP as a Maybe)!
BIRMINGHAM, UK
Contact: Thomas Read, thomas[dot]read[dot]acx[at]gmail[dot]com
Time: Saturday, September 3, 1:00 PM
Location: We'll be at The Wellington, 37 Bennetts Hill, on the roof terrace if possible.
I'll wear an orange shirt and have a sign saying ACX on the table.
Coordinates: 9C4WF3JX+7Q
Event link(s): LessWrong
Notes: It's only a few minutes walk from the stations, so hopefully people can join
from all over the West Midlands!
BRIGHTON, UK
Contact: Alan Enright, alanenright[at]protonmail[dot]com
Time: Saturday, September 10, 11:00 AM
Location: We'll be at the Alcampo Lounge on London Road—we will try and get a
table on the raised area in front of you and to the left as you come in but will also
have a little ACX sign.
Coordinates: 9C2XRVM6+3X
Event link(s): LessWrong, Meetup.com

BRISTOL, UK
Contact: Nick Lowry, bristoleﬀectivealtruism[at]gmail[dot]com
Time: Saturday, September 24, 2:00 PM
Location: We'll be meeting at entrance closet to Tesco Express in the Galleries,
Bristol City Centre
Coordinates: 9C3VFC45+RJM
Event link(s): LessWrong, Facebook event, Meetup.com
Group info: Meet twice monthly for socials, more regular 'productive' meetups. Been
active for 3+ years, please message for WhatsApp group
CAMBRIDGE, UK
Contact: Hamish Todd, hamish[dot]todd1[at]gmail[dot]com
Time: Saturday, September 17, 2:00 PM
Location: Bath House Pub, UPSTAIRS!! I will have a copy of Peter Singer's The Most
Good You Can Do
Coordinates: 9F426439+J9
Event link(s): LessWrong, Facebook event
Group info: We meet on the third Saturday of every month. The group has been
around almost a year and is well-attended!
Notes: My phone/WhatsApp number is +44 0730 *** 3550, where the *** are
replaced by the serial number of the Boeing plane whose ﬁrst ﬂight was on September
2, 1998. Email me to get on the mailing list for future events if you'd like that :)
CARDIFF, WALES
Contact: AF, strmnova[at]gmail[dot]com
Time: Friday, September 16, 5:00 PM
Location: Little Man Coﬀee (note new location!)
Coordinates: 9C3RFRHH+W2
Event link(s): LessWrong
EDINBURGH, SCOTLAND, UK
Contact: Sam, acxedinburgh[at]gmail[dot]com
Time: Saturday, September 24, 2:00 PM
Location: Pleasance Cafe. Go through the arch and the door to the cafe is on your left
Coordinates: 9C7RWRW9+M8
Group info: ~Monthly meetups, often in Pleasance Cafe but have experimented with
other locations. Email me to join the mailing list & WhatsApp group.
LANCASTER, UK
Contact: Gruﬀydd Gozali, gruﬀyddgozali[at]gmail[dot]com
Time: Saturday, October 15, 3:00 PM
Location: Lancaster University Library, will be on the ground ﬂoor by the tree wearing
an EA shirt.
Coordinates: 9C6V2657+WJR
Event link(s): LessWrong
LINCOLN, UK
Contact: Tobias, tobias[dot]showan[at]yahoo[dot]co[dot]uk
Time: Saturday, September 10, 2:00 PM
Location: Nosey Parker pub, I'll bring a little paper ACX sign.
Coordinates: 9C5X6C9R+XJ
Event link(s): LessWrong
LONDON, UK

Contact: Edward Saperia, edsaperia[at]gmail[dot]com
Time: Sunday, September 25, 2:00 PM
Location: Newspeak House
Coordinates: 9C3XGWGH+3F7
Event link(s): LessWrong, Facebook event, Meetup.com, Eventbrite
Group info: You can join our mailing list or our Meetup.com group
MANCHESTER, UK
Contact: Matthew Gibson, melkartmtg[at]hotmail[dot]com
Time: Sunday, September 18, 11:00 AM
Location: Sackville Gardens, Alan Turing Memorial
Coordinates: 9C5VFQG7+MH
Event link(s): LessWrong
NEWCASTLE UPON TYNE, UK
Contact: Joshua William, iamjoshwilliam[at]icloud[dot]com, Telegram
Time: Saturday, September 3, 12:30 PM
Location: Trinity Square, High Street Gosforth. You can get the bus to Gosforth from
the city center just outside the famous 'Tyneside Cinema' (bus number: 30, 31, or 35
at Monument Pilgrim Street bus stop), or you can take a walk if you want to get your
'steps' in (if you'd like to do the latter, send me an email and I'll send you the
directions), which takes ~60-min.
Coordinates: 9C7W294H+5V
Event link(s): LessWrong
Group info: To my knowledge, there isn't an ACX meet up in this city, or region of the
UK, though if there is a demand for a reoccurrence, I'd be happy to keep facilitating
such. I'd also happily formulate a WhatsApp group if theres interest, after the meet up.
Notes: We have a deli, '1901 cafe', on the square, which we can grab an immediate
bite to eat at [so save some hunger if you'd like to do that]. There's a safe [and lovely]
park with some benches just by the way, which, if the weather is nice, we can sit at
after a bite to eat, or, otherwise, we can remain in the cafe.
OXFORD, UK
Contact: Sam, ssc[at]sambrown[dot]eu, There's a Signal group people can join :)
contact Sam for info
Time: Wednesday, October 19, 6:30 PM
Location: The Star, Rectory Road, Oxford. We'll be in the beer garden round the back,
with a sign 🙂
Coordinates: 9C3WPQX6+QP9
Event link(s): LessWrong, Facebook event, Meetup.com
Group info: We run socials every months, and applied rationality workshops from
time to time!
Notes: Please RSVP on any of the platforms (or email) for free pizza
PENRYN, CORNWALL, UK
Contact: mini t, tminns[at]btinternet[dot]com
Time: Saturday, August 27, 3:00 PM
Location: glasney playing ﬁeld and valley
Coordinates: 9C2P5V8V+P9
Event link(s): LessWrong
Notes: I don't mind rescheduling, or organizing another event, not many people are
likely to turn up this far out of the way.

LATIN AMERICA
SÃO PAULO, BRAZIL
Contact: Simon, sjclawrence[at]gmail[dot]com
Time: Saturday, September 10, 2:00 PM
Location: Ibirapuera Park in Praca do Porquinho. I will be wearing a white t-shirt, be
very tall and have a sign.
Coordinates: 588MC85Q+6X
Event link(s): LessWrong
BOGOTÁ, COLOMBIA
Contact: Dan P, shorty[dot]george[dot]productions[at]gmail[dot]com
Time: Sunday, September 18, 4:00 PM
Location: Illy Cafe, Kr 15 with Park Virrey. Sign will say ACX
Coordinates: 67P7MWFW+3F7
Event link(s): LessWrong
MEDELLÍN, COLOMBIA
Contact: HP, hp-med-acx[at]proton[dot]me
Time: Sunday, September 18, 5:00 PM
Location: Hija Mia Nomada
Coordinates: 67R66C7G+8V
Event link(s): LessWrong
MÉRIDA, MEXICO
Contact: Mati Roy, mathieu[dot]roy[dot]37[at]gmail[dot]com, Facebook
Time: Sunday, August 28, 5:00 PM
Location: Parque Gardenia, C. 65-A, Residencial Floresta, 97309 Mérida, Yuc.
Coordinates: 76HG2C7X+8F
Event link(s): LessWrong, Facebook event
Group info: Facebook group
Notes: Please let me know if you'll be coming.
MEXICO CITY, MEXICO
Contact: Calcifer, fagarrido[at]gmail[dot]com, Discord: Francisco (Mexico City)#0227
Time: Saturday, September 10, 4:00 PM
Location: Comedor de los Milagros. I'll be wearing a green shirt and will carry a
'ACX/CDMX Meetup' sign.
Coordinates: 76F2CR6P+37
Event link(s): LessWrong
Group info: We are a rather new group. We've been meeting sporadically since April,
and we recently settled on a formal twice-per-month frequency. We have a WhatsApp
group which we use mostly for coordination purposes. Send me an email if you want
in.
Notes: If possible, RSVP on Less Wrong to get a sense of how many people to expect.
Feel free to come if you haven't RSVP'd, though!
PUNTA DEL ESTE, URUGUAY
Contact: Manuel, acx[at]maraoz[dot]com
Time: Saturday, September 24, 5:00 PM
Location: Borneo Coﬀee, patio del fondo. Ruta 10, 20001 La Barra, Departamento de
Maldonado, Uruguay

Coordinates: 48Q734PQ+58
Event link(s): LessWrong
UNITED STATES
HUNTSVILLE, AL
Contact: Mike, mjhouse[at]protonmail[dot]com
Time: Saturday, September 3, 3:00 PM
Location: Barnes & Noble - 300 The Bridge St #100, Huntsville, AL 35806. I'll be in
the cafe with a sign that says ACX MEETUP on it.
Coordinates: 866MP88H+53
Event link(s): LessWrong
Notes: Barnes & Noble has an area for little kids. If you want to bring a service
animal, that's probably ﬁne, but I doubt they allow pets.
PHOENIX, AZ
Contact: Ben Morin, benjamin[dot]j[dot]morin[at]gmail[dot]com
Time: Saturday, October 15, 1:00 PM
Location: Thirsty Lion Pub in Tempe. I will have a table with an ACX sign.
Coordinates: 8559FVVQ+6C
Event link(s): LessWrong
Group info: This will be our 5th meetup (started during the meetups everywhere last
year).
Notes: Please email if interested to be added to the email list, even if you can't make
this event
BELMONT, CA
Contact: Moshe Z., belmont-acx[at]devskillup[dot]com
Time: Sunday, September 4, 2:00 PM
Location: Twin Pines Park, Picnic Tables. The table will have some sign saying 'ACX
Meetup' on it.
Coordinates: 849VGP8C+RRG
Event link(s): LessWrong
Group info: You can join the mailing list here.
BERKELEY, CA
Contact: Scott
Time: Sunday, September 18, 1:00 PM
Location: Rose Garden Inn, a rationalist event space at 2740 Telegraph Ave. Come in
through the front gate on Telegraph.
Coordinates: 849VVP5R+X7V
Event link(s): LessWrong
Group info: The Bay rationality community has a mailing list, a Discord server, and
a Facebook group. There are dinner meetups every Thursday at 7 PM in the East Bay,
and occasional meetups in SF and South Bay.
FILLMORE, CA
Contact: Ryan, wiserd[at]gmail[dot]com, Discord: Wiserd#0906
Time: Saturday, October 1st, 6:00 PM
Location: It's my house. There are a bunch of plants on the porch and garbage bins in
the driveway.
Coordinates: 856393VX+VQ

Event link(s): LessWrong
Notes: Please RSVP to my email or Discord. Kids and dogs are welcome in the back
yard. Full vaccinations (on the honor system) and masks required.
GRASS VALLEY, CA
Contact: Max Harms, raeliﬁn[at]gmail[dot]com
Time: Saturday, September 10, 2:00 PM
Location: Condon Park by the prospector statue. In the case of rain we'll change the
location to a residence, so RSVP to get updated!
Coordinates: 84FW6W8H+C5
Event link(s): LessWrong
IRVINE, CA
Contact: Nick C, cohenskijanuary1[at]mail[dot]com
Time: Saturday, October 1, 2:00 PM
Location: University Town Center
Coordinates: 8554M526+7H
Event link(s): LessWrong
Group info: We meet once a month at the same location.
LOS ANGELES, CA
Contact: Vishal Prasad (koreindian), vprasadcs[at]gmail[dot]com, Contact me on
Discord. I am "Vishal" on the server.
Time: Saturday, October 8, 6:30 PM
Location: 11841 Wagner St., Culver City, CA 90039
Coordinates: 8553XHWM+GP
Event link(s): LessWrong
Group info: We meet weekly every Wednesday. We have been around for over 8
years. We discuss articles, watch movies, lift weights. We have a Discord server,
a LessWrong group, and a website!
Notes: Please RSVP on LessWrong so I know how much food to get.
NEWPORT BEACH, CA
Contact: Michael M, michaelmichalchik[at]gmail[dot]com
Time: Saturday, August 27, 2:00 PM
Location: Picnic tables next to 1900 Port Carlow community clubhouse. The park is
verdant and pleasant and easy to access. Free street parking nearby. In case of bad
weather, we have a couple of near by places to relocate to.
Coordinates: 8554J48R+WCX
Event link(s): LessWrong, Facebook event
Group info: We will meet most Saturdays at 2pm until whenever. There will be short
suggested readings and question most weeks to spur conversation, but they are
optional. Each week we will ask if people have had something happen recently that
surprised them or changed the way they looked at the world. Something that should
or did update their priors. Participation is optional.
Notes: Its a public park with tables and BBQ's so you can bring food and well behaved
pets. We may regularly go on casual walks in the surrounding area.
SAN DIEGO, CA
Contact: Julius, julius[dot]simonelli[at]gmail[dot]com
Time: Sunday, October 9, 3:00 PM
Location: We will meet up in Bird Park. I will be wearing a red shirt.
Coordinates: 8544PVQ8+Q7
Event link(s): LessWrong, Meetup.com

Group info: Join our Discord server
SAN FRANCISCO, CA
Contact: Derek Pankaew, derekpankaew[at]gmail[dot]com
Time: Sunday, September 18, 11:00 AM
Location: We'll between in the Panhandle, between Ashbury and Masonic, with a
'ACX' sign.
Coordinates: 849VQHC3+V8
Event link(s): LessWrong
SAN JOSE, CA
Contact: David Friedman, ddfr[at]daviddfriedman[dot]com
Time: Saturday, September 17, 2:00 PM
Location: 3806 Williams Rd, San Jose, CA 95117
Coordinates: 849W825J+6P
Event link(s): LessWrong
Group info: Before Covid we hosted every month or two. No structure, just
conversation and food. We feed everyone who is still there at dinner time. We have
done it once or twice since Covid. I have an email list of interested people.
Notes: Kids are welcome. Please RSVP to my email so I will have a rough count of
how many we are feeding.
SAN MARCOS, CA
Contact: Eric F., EricF14159[at]gmail[dot]com
Time: Sunday, September 25, 2:00 PM
Location: Hollandia Park Soccer Field. At the tables near the top parking lot.
Coordinates: 85544VW4+RV
Event link(s): LessWrong
BOULDER, CO
Contact: Josh Sacks, josh[dot]sacks+acx[at]gmail[dot]com
Time: Sunday, October 16, 3:00 PM
Location: 9191 Tahoe Ln, Boulder, CO 80301
Coordinates: 85GP2V96+JQ
Event link(s): LessWrong
Notes: Please RSVP on LessWrong so we know ~ how many people to expect!
CARBONDALE, CO
Contact: Nick, naj[at]njarboe[dot]com
Time: Saturday, September 3, 1:00 PM
Location: Sopris Park - Center covered picnic tables - blue shirt with ACX sign on
table
Coordinates: 85FJ9QXP+QMF
Event link(s): LessWrong
DENVER, CO
Contact: Ian Philips, iansphilips[at]gmail[dot]com, Discord: palebone#2796
Time: Sunday, October 2, 11:00 AM
Location: We'll be in the backyard patio of St. Mark's Coﬀee House. I'll wear a white
shirt with (my brothers') baby faces on it and have a brown hat on.
Coordinates: 85FQP2VP+9R
Event link(s): LessWrong
Group info: We meet typically 4 times a year.

LAKEWOOD, CO
Contact: Steven Zuber, stevenjzuber[at]gmail[dot]com
Time: Wednesday, October 5, 7:00 PM
Location: We meet in the clubhouse located in this townhome community: 8769 W
Cornell Ave Lakewood, CO 80227
Coordinates: 85FPMW64+MW
Event link(s): LessWrong, Meetup.com
Group info: We meet the ﬁrst Wednesday of every month. Informal, casual
atmosphere with occasional presentations by people.
Notes: Check the Meetup page or Facebook group for updates.
FAIRFIELD, CT
Contact: Justin Barclay, barclay[dot]justin[at]gmail[dot]com
Time: Saturday, September 10, 10:00 AM
Location: South Pine Creek Beach. I'll set up near the lifeguard stand.
Coordinates: 87H84PCH+CM
Event link(s): LessWrong
MANCHESTER, CT
Contact: Mike, park-mike[at]outlook[dot]com
Time: Saturday, September 17, 5:00 PM
Location: Near ﬂagpole on top of hill
Coordinates: 87H9QFFH+J7
Event link(s): LessWrong
NEW HAVEN, CT
Contact: RM, acx[dot]meetup[dot]nhv[at]gmail[dot]com
Time: Sunday, September 18, 12:30 PM
Location: Cross Campus (Yale University), New Haven, CT 06511. We'll be on the
grass on the northern half of Cross Campus, closest to Sterling Memorial Library. I'll be
wearing an orange shirt.
Coordinates: 87H9836C+8VG
Event link(s): LessWrong
Notes: Feel free to bring friends! The vibe will be welcoming and relaxed, and you can
stay for any amount of time. Please email me if you're thinking about coming so I can
get the right number of Insomnia cookies!
WASHINGTON, DC
Contact: John Bennett, WashingtonDCAstralCodexTen[at]gmail[dot]com
Time: Saturday, September 17, 6:00 PM
Location: Froggy Bottom Pub: 2021 K Street NW, Washington, D.C. 20006
Coordinates: 87C4WX33+3J
Event link(s): LessWrong, Facebook event
Group info: The Washington DC ACX/SSC group has been active since the ﬁrst
Meetups Everywhere in 2017. We have Monthly Socials downtown, hikes, board game
days, and other cultural events. We're looking to spin up more rationality Dojo-type
events with nearby groups in the coming months.
Notes: We've rented out the Froggy Bottom Pub for the night, dinner and soft drinks
will be provided. Alcohol available for purchase if desired, but no purchases are
required. Metered street parking on nearby blocks is free after 6:30. Closest Metros
are Farragut West and Farragut North.
CAPE CORAL / FORT MYERS, FL
Contact: Shawn Spilman, shawn[dot]spilman[at]outlook[dot]com, 508 655 8123

Time: Sunday, October 2, 1:00 PM
Location: 929 SW 54th Ln, Cape Coral, FL 33914
Coordinates: 76RWH224+44
Event link(s): LessWrong
Notes: RSVP via email. I can be ﬂexible about the date.
GULF BREEZE / PENSACOLA, FL
Contact: Christian, christian[dot]h[dot]williams[at]gmail[dot]com
Time: Wednesday, October 12, 7:30 PM
Location: The Bridge Bar - 33 Gulf Breeze Pkwy A, Gulf Breeze, FL 32561
Coordinates: 862J9RCF+G6
Event link(s): LessWrong
Notes: Please RSVP by emailing me. Thanks! If I don't hear from anyone, I won't be
there. I work for Metaculus, but promise not to talk your ear oﬀ about forecasting.
(Unless you want it talked oﬀ.)
MIAMI, FL
Contact: Eric Magro, eric135033[at]gmail[dot]com, Discord: eric135#4943
Time: Sunday, September 11, 5:00 PM
Location: Buckminster Fuller Fly's Eye Dome 140 NE 39th St #001, Miami, FL 33137 -
---- Look for a paper sign on a table that says ACX MEETUP west of the dome.
Coordinates: 76QXRR65+V2
Event link(s): LessWrong
Group info: Miami ACX started in 2017. Our oﬃcial meetup happens monthly in
either Miami or Broward. There are activities happening on a weekly basis from Miami
to Palm Beach. We have a Facebook group, Discord server, and Meetup.com group.
ORLANDO, FL
Contact: Noah Topper, noah[dot]topper[at]gmail[dot]com
Time: Friday, September 16, 7:00 PM
Location: 4000 Central Florida Blvd, Orlando, FL. We'll be meeting up at UCF's
pavilion near Garages A and I. I'll have a pretty ACX Meetup sign.
Coordinates: 76WWJQ2X+82
Event link(s): LessWrong
Group info: We try to meet up once a month, so far they've just been casual social
meetups with natural discussions of rationality topics. Here's our Discord link :)
Notes: RSVPs on LessWrong would be greatly appreciated. :)
TALLAHASSEE, FL
Contact: JF, jf19o[at]fsu[dot]edu
Time: Monday, August 29, 2:00 PM
Location: Landis, FSU. I will be wearing a black shirt
Coordinates: 862QCPR3+PX
Event link(s): LessWrong
ATHENS, GA
Contact: Dallon, knox[dot]dallon[dot]a[at]gmail[dot]com, Discord: leonard#4208
Time: Saturday, October 15, 3:00 PM
Location: Hendershots on Prince Avenue
Coordinates: 865RXJ68+2W
Event link(s): LessWrong
Notes: I might bring some board games
ATLANTA, GA

Contact: Steve French, steve[at]digitaltoolfactory[dot]net
Time: Saturday, September 17, 2:00 PM
Location: Bold Monk Brewing - 1737 Ellsworth Industrial Blvd NW suite d-1 · Atlanta,
GA (upstairs - look for the ACX Atlanta sign)
Coordinates: 865QRH2F+V8
Event link(s): LessWrong, Meetup.com
Group info: We've been in existence for four years - we have a dedicated crew and a
very active Slack group
Notes: Please RSVP on LessWrong or Meetup.com
HONOLULU, HI
Contact: Matt Popovich, mattpopovich[at]outlook[dot]com
Time: Saturday, September 3, 4:00 PM
Location: We'll meet at Magic Island at Ala Moana Beach Park, 1201 Ala Moana Blvd,
Honolulu, HI 96814. From the parking lot, walk along the left side of the peninsula out
toward Magic Island Lagoon. We're usually near the end of the peninsula, somewhere
around the bathroom building. Look for the large 'ACX' sign.
Coordinates: 73H475M3+JP
Event link(s): LessWrong, Meetup.com
Group info: Honolulu Rationality hosts discussion meetups about twice a month in
Ala Moana Beach Park. Check us out on our website
BOISE, ID
Contact: Julia and John, jae[dot]miomu[at]gmail[dot]com
Time: Friday, October 7, 6:00 PM
Location: Old Timer's Shelter in Ann Morrison Park. I will have an ACX sign.
Coordinates: 85M5JQ6P+96
Event link(s): LessWrong
Notes: Please RSVP and feel free to bring kids.
CHAMPAIGN-URBANA, IL
Contact: Ben, cu[dot]acx[dot]meetups[at]gmail[dot]com
Time: Friday, September 9, 7:00 PM
Location: Siebel Center for Computer Science, Room 4403
Coordinates: 86GH4Q7G+H8F
Event link(s): LessWrong
Group info: Discord server
Notes: RSVPs are appreciated but not at all required. You can RSVP by email or by
pinging me in the Discord server. Suggested entrance is the East side of the building
(see Coordinates) - we'll try to make sure at least that door is unlocked, but if it isn't
then ping us on email or Discord.
CHICAGO, IL
Contact: Todd, info[at]chicagorationality[dot]com, https://chicagorationality.com/
Time: Sunday, September 18, 1:00 PM
Location: Grant Park - North side of Balbo between the tracks and Columbus
Coordinates: 86HJV9FH+84
Event link(s): LessWrong
Group info: Chicago Rationality does a monthly discussion meetup (typically the ﬁrst
Saturday of the month) and a monthly social meetup (typically the third weekend of
the month)
Notes: Sign up for our email list to be notiﬁed of future meetups
EVANSTON, IL

Contact: Uzair, uzairq93[at]gmail[dot]com
Time: Saturday, October 1, 7:00 PM
Location: 626 Church Street, Evanston IL 60201
Coordinates: 86JJ28X9+5WQ
Event link(s): LessWrong
Notes: The venue is a pub but it's really more of a restaurant, big long tables
available so space should be ﬁne and non drinkers shouldn't feel too out of place.
BLOOMINGTON, IN
Contact: Avery, acxbloomington[at]fastmail[dot]com
Time: Sunday, October 16, 2:00 PM
Location: Switchyard Park. Will be at one of the tables near the Rogers Street parking
lot. I will bring a cardboard sign that says "ACX".
Coordinates: 86FM4FX6+4Q
Event link(s): LessWrong
Group info: We met last year for Meetups Everywhere and it was fun! Here's a link
to our Discord.
Notes: You can RSVP via Discord or email, but you are encouraged to show up even if
you did not RSVP!
WEST LAFAYETTE, IN
Contact: NR, mapreader4[at]gmail[dot]com
Time: Saturday, September 17, 1:00 PM
Location: 1275 1st Street, West Lafayette, IN 47906. We'll be in the south of the
Earhart Hall lobby (not the dining court) near the piano, and I will be wearing a green
shirt and carrying a sign with ACX MEETUP on it.
Coordinates: 86GMC3GG+728
Event link(s): LessWrong
LEXINGTON, KY
Contact: Nathan, nwculley[at]gmail[dot]com
Time: Saturday, September 3, 7:00 PM
Location: Blue Stallion Brewing. 610 W. 3rd St., Lexington, KY 40508. We will have a
sign indicating we are the ACX meetup.
Coordinates: 86CQ3F4X+VF
Event link(s): LessWrong
Group info: We meet 1-2 times a month to talk about ACX, books, memes, etc., often
over drinks and board games.
NEW ORLEANS, LA
Contact: Blake, blake[at]philosophers[dot]group
Time: Sunday, September 4, 11:11 AM
Location: Petite Clouet Cafe. Look for the group with an iPad that has a People's Pint
sticker.
Coordinates: 76XFXX73+8R
Event link(s): LessWrong
Group info: Website
Notes: Hybrid in-person and online, video link sent weekly. Email for the link.
BOSTON, MA
Contact: Robi Rahman, robirahman94[at]gmail[dot]com, 7039818526
Time: Saturday, September 10, 5:00 PM
Location: Boston Common, at the Parkman Bandstand gazebo
Coordinates: 87JC9W3M+PR

Event link(s): LessWrong, Facebook event
Group info: Mailing list, Facebook group, Meetup.com
Notes: We'll be providing food at the meetup, and giving out free books related to
ACX, rationality, and eﬀective altruism. Email the hosts if you'd like a particular book
or you have any dietary restrictions. Our group is also doing a tour of the JFK
Presidential Library on September 9, you're welcome to join!
NORTHAMPTON, MA
Contact: Alex, alex[at]alexliebowitz[dot]com
Time: Friday, September 9, 6:00 PM
Location: The Deck, 125A Pleasant St., Northampton MA 01096. The oﬃcial address
is bizarre and inaccurate; it's the outdoor dining part of a group of bars & restaurants
in a former rail station... a whole block away from Pleasant St. The simplest way to get
to The Deck is to enter The Platform, one of the other restaurants, by its street
entrance around 36 Strong Ave., here (make sure to look at street view). Go inside and
ask them to show you to The Deck. We'll have a sign.
Coordinates: 87J9899F+H7H
Event link(s): LessWrong, Facebook event
Group info: We started in the 2018 Meetups Everywhere and is still going strong. We
aim to meet about once every two weeks. At most meetups we get about 5-7 people
out of a rotation of 15-20; Meetups Everywhere and other special events tend to bring
in a few more than usual. We're a totally social meetup with no 'format' or suggested
readings. Although it's not rare for us to touch on ACX articles and related topics, the
conversation varies wildly, and you are welcome even if you're the most occasional
ACX reader.
Notes: We have a (not very active) Discord where you can DM me or post on a public
channel. I'm most responsive by email. There is a small chance we'll have to change
the location to somewhere else in Northampton. Please check the Less Wrong or
Facebook posts on or after August 26 to get the ﬁnal word on location.
BALTIMORE, MD
Contact: Rivka, rivka[at]adrusi[dot]com
Time: Sunday, September 11, 7:00 PM
Location: UMBC outside of the Performing Arts and Humanities Building, on the north
side. I will have a sign that says ACX meetup. Parking is free on the weekends. Edit:
Rain is forecasted; if it's raining, we will be inside of the Performing Arts building, on
the ground ﬂoor just inside the entrance.
Coordinates: 87F5774P+53
Event link(s): LessWrong
Group info: We meet Sundays at 7pm — half are in person and half are virtual.
Notes: There will be pizza and drinks
DETROIT, MI
Contact: Matt Arnold, matt[dot]mattarn[at]gmail[dot]com
Time: Tuesday, September 20, 7:00 PM
Location: Tenacity Craft, 8517 2nd Ave, Detroit, MI 48202
Coordinates: 86JR9WG9+R6
Event link(s): LessWrong
MINNEAPOLIS, MN
Contact: Timothy, tmbond[at]gmail[dot]com
Time: Saturday, September 10, 1:00 PM
Location: Meet at the picnic tables near the southeast corner of Powderhorn Park -
the ones by the parking lot. I will be wearing a green Google t-shirt and have a sign

that says ACX.
Coordinates: 86P8WPRW+76
Event link(s): LessWrong
Notes: I will bring some snacks (but not a full lunch, so eat before or bring something
if you'll be that hungry). Please RSVP on LessWrong.
KANSAS CITY, MO
Contact: Alex, alex[dot]hedtke[at]gmail[dot]com
Time: Friday, September 16, 6:30 PM
Location: We will be in the courtyard above Whole Foods (which is also an apartment
complex). You can enter through the apartment lobby, located on Oak Street. We will
have runners shepherding people from the entrance up to the courtyard.
Coordinates: 86F72CM8+RR
Event link(s): LessWrong, Meetup.com
SAINT LOUIS, MO
Contact: JohnBuridan, littlejohnburidan[at]gmail[dot]com
Time: Saturday, October 8, 1:00 PM
Location: Lily Pond Shelter, Tower Grove Park, St. Louis
Coordinates: 86CFJP4R+XV
Event link(s): LessWrong
Notes: BYOB
WEST PLAINS, MO
Contact: Liam, liamhession[at]gmail[dot]com
Time: Saturday, September 17, 12:00 PM
Location: 10/40 Coﬀee, 24 Court Square, West Plains, MO
Coordinates: 868CP4HW+CV
Event link(s): LessWrong
Notes: Hoping to get anyone from around the Ozark region
DURHAM, NC
Contact: Will Jarvis, willdjarvis[at]gmail[dot]com
Time: Thursday, September 8, 7:30 PM
Location: Ponysaurus Brewing Company, 219 Hood St, Durham
Coordinates: 8773X4Q3+QW
Event link(s): LessWrong
Group info: We meet weekly! We also have a Discord
LAKEWOOD, NJ
Contact: Ben L, mywebdev3[at]gmail[dot]com
Time: Saturday, October 29, 8:30 PM
Location: TBD
Event link(s): LessWrong
MORRISTOWN, NJ
Contact: Matt, matt[dot]brooks[at]impactmarkets[dot]io, Discord: Matt B#0216
Time: Saturday, October 1, 2:00 PM
Location: 10 N Park Pl, Morristown, NJ 07960 (at the center of the Morristown Green)
Coordinates: 87G7QGW9+RJ
Event link(s): LessWrong
Group info: This is the ﬁrst meetup, come be a founding member of the Northern NJ
ACX/EA/LW group!

PRINCETON, NJ
Contact: Danny K, dskumpf[at]gmail[dot]com
Time: Saturday, October 1, 3:00 PM
Location: Palmer Square, Princeton, NJ 08540. On the green right outside The Bent
Spoon and Rojo's Roastary, near the big tree. I'll have some sort of ACX Meetup sign!
Coordinates: 87G7982Q+2CP
Event link(s): LessWrong
LAS VEGAS, NV
Contact: Jonathan Ray, ray[dot]jonathan[dot]w[at]gmail[dot]com
Time: Sunday, September 11, 11:45 AM
Location: At El Segundo Sol restaurant with giant ACX MEETUP signs
Coordinates: 85864RHJ+3H
Event link(s): LessWrong, Facebook event
Group info: We meet regularly and mostly just socialize. We have a new Discord
server.
RENO, NV
Contact: Steven, stevenl451[at]gmail[dot]com, Discord: Steeven#7407
Time: Friday, September 2, 5:30 PM
Location: We'll be in Crissie Caughlin Park, near the tables and the swing set
Coordinates: 85F2G46W+FG
Event link(s): LessWrong
Notes: Feel free to bring kids/dogs and please RSVP on LessWrong if you are going
BUFFALO, NY
Contact: George Herold, ggherold[at]gmail[dot]com
Time: Sunday, September 11, 1:00 PM
Location: 932 Welch Rd. Java Center, NY 14082
Coordinates: 87J3W467+8P
Notes: Last-minute location change!
LONG ISLAND, NY
Contact: Gabe, gabeaweil[at]gmail[dot]com
Time: Thursday, October 27, 7:00 PM
Location: Whales Tale in Northport
Coordinates: 87G8VJRW+99
Event link(s): LessWrong
NEW YORK CITY, NY
Contact: Jasmine, jasminermj[at]gmail[dot]com
Time: Sunday, September 11, 4:00 PM
Location: Pavillion @ Rockefeller Park, Warren St / River Terrace
Coordinates: 87G7PX9M+4J3
Event link(s): LessWrong
Group info: OBNYC has a Discord and a Google Group; the Google Group is the main
mailing list we use for events
NEWBURGH, NY
Contact: Pedro David Bonilla, proportionatetoevidence[at]gmail[dot]com, Cell
8452001681
Time: Saturday, September 24, 10:00 AM
Location: Perkins Restaurant & Bakery, 1421 NY-300, Newburgh, NY 12550
Coordinates: 87H7GWCH+GF

Event link(s): LessWrong
ROCHESTER, NY
Contact: Skivverus, skivverus[at]gmail[dot]com, Discord: Skivverus#5915
Time: Saturday, October 8, 1:00 PM
Location: 4870 Culver Road; will be wearing a polo shirt, jeans, and glasses, and may
or may not have ﬁgured out a sign due to just getting back from honeymoon. Look for
a pair of parrots, one white, one green with a yellow/orange head.
Coordinates: 87M46FM6+Q5P
Event link(s): LessWrong
Notes: Venue very near amusement park; non-bathroom, non-parking amenities are
therefore available but not free. Plan accordingly. Not particularly attached to speciﬁc
location named, just happen to live reasonably close to there; alternative suggestions
acceptable. Canadian visitors also welcome should your logistics permit; airport
transportation available. RSVP via Discord preferred, but email will also work.
CLEVELAND, OH
Contact: Jack Zhang, LukeZhao9[at]protonmail[dot]com
Time: Saturday, September 24, 1:00 PM
Location: Picnic tables at Wade Oval (university circle)
Coordinates: 86HWG96Q+GC5
Event link(s): LessWrong
COLUMBUS, OH
Contact: Daniel, daniel[dot]m[dot]adamiak[at]gmail[dot]com
Time: Saturday, September 17, 3:00 PM
Location: Jeﬀrey Park - Clinton Shelter. I will be wearing a red shirt.
Coordinates: 86FVX3C3+QF
Event link(s): LessWrong
Group info: We meet once a month. We discuss EA, AI and other two letter
initialisms. Occasionally we go for walks in local grottos and nature trails.
Notes: Email me if you want to be added to the mailing list to receive any updates or
future invites. RSVPing is appreciated.
TOLEDO, OH
Contact: Scout, scout[dot]sivar[at]gmail[dot]com
Time: Saturday, September 10, 12:00 PM
Location: Black Kite Coﬀee
Coordinates: 86HRMCCV+9R
Event link(s): LessWrong
OKLAHOMA CITY, OK
Contact: bean, battleshipbean[at]gmail[dot]com
Time: Sunday, October 9, 1:00 PM
Location: Edmond Public Library/Shannon Miller Park. I will be wearing a hat that says
USS Iowa on it.
Coordinates: 8674MG3C+MW
Event link(s): LessWrong
Group info: Had four people last year and a good time, moved to Edmond because a
lot of us are up here.
ALBANY, OR
Contact: Kenan (he/him), kbitikofer[at]gmail[dot]com
Time: Saturday, October 1, 2:00 PM

Location: Bowman Park, Albany, Oregon. In or near the shelter. I will wear a bright
red shirt and carry a sign with ACX MEETUP on it.
Coordinates: 84PRJWR7+XC6
Event link(s): LessWrong
CORVALLIS, OR
Contact: Ethan Ashkie, ethanashkie[at]gmail[dot]com
Time: Wednesday, September 7, 6:00 PM
Location: Common Fields, in the reserved outdoor seating near the entrance
Coordinates: 84PRHP5P+VQ
Event link(s): LessWrong
EUGENE, OR
Contact: Ben Smith, benjsmith[at]gmail[dot]com
Time: Wednesday, August 31, 7:00 PM
Location: The Barn Light, 924 Willamette St, Eugene 97401
Coordinates: 84PR2WX4+VV
Event link(s): LessWrong
Notes: Please RSVP on LessWrong so I know how much pizza to get, but if you forget,
don't worry about it, we want you to come along anyway
PORTLAND, OR
Contact: Sam F Celarek, support[at]pearcommunity[dot]com, 513-432-3310, Discord:
Sam Celarek#2845
Time: Friday, September 9, 5:00 PM
Location: 205 NW 4th Ave
Coordinates: 84QVG8FG+V4
Event link(s): LessWrong, Meetup.com
Group info: Portland Eﬀective Altruism and Rationality is very active. We have book
clubs, bi-weekly AI safety meet-ups, bi-weekly topical meet-ups, bi-weekly socials, and
have an active Discord.
Notes: We would prefer you RSVP on Meetup.com a week beforehand so that we can
get the right amount of food!
HARRISBURG, PA
Contact: Phil, acxharrisburg[at]gmail[dot]com
Time: Saturday, September 24, 2:00 PM
Location: Ever Grain Brewing Co, 4444 Carlisle Pike, Camp Hill, PA 17011 - We will be
sitting at one of the picnic tables outside with an ACX MEETUP sign
Coordinates: 87G562QQ+8P
Event link(s): LessWrong
Group info: Small monthly meetup group based out of Harrisburg - celebrating 1 year
of actuality! You can see more of our events on LessWrong.
INDIANA, PA
Contact: Eric, ericindianapa[at]gmail[dot]com, 717-256-2717
Time: Saturday, September 24, 11:00 AM
Location: Caﬀè Amadeus in downtown Indiana, PA. I will have a sign with 'ACX
Meetup' on one of the tables.
Coordinates: 87G2JRFX+48
Event link(s): LessWrong
Notes: Please RSVP via email or text message so I know how many to expect.
PHILADELPHIA, PA

Contact: Wes and Diana, rationalphilly[at]gmail[dot]com
Time: Thursday, September 22, 6:30 PM
Location: The Philadelphia Ethical Society, 1906 Rittenhouse Square. The meeting
room is in the basement, look for the signs.
Coordinates: 87F6WRXG+FQ
Event link(s): LessWrong
Group info: We tend to meet in downtown Philly on the last Thursday of the month.
We're aiming to make the Ethical Society our new steady location. We have many
links: Discord, Google Calendar, Facebook, Meetup, Google Group
Notes: We'll be ordering food from a local restaurant, so no need to eat ﬁrst. BYOB
PITTSBURGH, PA
Contact: Justin, pghacx[at]gmail[dot]com
Time: Saturday, September 24, 2:00 PM
Location: Westinghouse Shelter @ Schenley Park (W Circuit Rd near Schenley Dr). We
have the outdoor shelter reserved, so light rain shouldn't be a problem, but in the
event of extreme weather, we may relocate indoors (our default 'contingency indoor
location' is Crazy Mocha Coﬀee on 2100 Murray Ave in Squirrel Hill).
Coordinates: 87G2C3Q4+773
Event link(s): LessWrong
Group info: We meet monthly-ish for general discussion and chit-chat, email me if
you'd like to be notiﬁed of future meetups.
STATE COLLEGE, PA
Contact: John Slow, auk480[at]psu[dot]edu
Time: Thursday, September 8, 5:00 PM
Location: Old Main. I will be carrying an ACX meetup sign.
Coordinates: 87G4Q4WP+HV
Event link(s): LessWrong
SAN JUAN, PUERTO RICO
Contact: Dan Gelfarb, danielgelfarb[at]gmail[dot]com
Time: Saturday, September 10, 1:00 PM
Location: Lote 23, back corner under the tents. I will be wearing a blue shirt with a
sign that says ACX meetup on it.
Coordinates: 77CMCWVM+W32
Event link(s): LessWrong
PROVIDENCE, RI
Contact: James Bailey, feanor1600[at]gmail[dot]com
Time: Saturday, September 17, 4:00 PM
Location: Prospect Terrace park, to the right of the Roger Williams statue
Coordinates: 87HCRHJV+24
Event link(s): LessWrong
SIOUX FALLS, SD
Contact: S. C., villainsplus[at]protonmail[dot]com
Time: Sunday, October 2, 5:00 PM
Location: 410 E 26th St, Sioux Falls, SD 57105 - the pavillion on the west side of
McKennan Park, or the tables just south of it if I can't book it. I'll be the guy with the
grill.
Coordinates: 86M5G7JH+W57
Event link(s): LessWrong

MEMPHIS, TN
Contact: Michael, michael[at]postlibertarian[dot]com
Time: Monday, September 5, 1:00 PM
Location: French Truck Coﬀee at Crosstown Concourse, Central Atrium 1350
Concourse Ave, Memphis, TN 38104. We will be at one of the many tables near French
Truck Coﬀee and I will have a sign that says ACX MEETUP.
Coordinates: 867F5X2P+QHC
Event link(s): LessWrong
Group info: We meet about every month or so. We've been around since 2019 but
only regularly since mid 2021 due to the pandemic. We have a Discord server.
NASHVILLE, TN
Contact: Ellen, enwiegand[at]gmail[dot]com
Time: Saturday, October 1, 11:00 AM
Location: OneCity Nashville (8 City Blvd, Nashville, TN 37209), next to the volleyball
courts. I'll have a pink ballcap that says SPINSTER on it.
Coordinates: 868M552H+XW
Event link(s): LessWrong
AUSTIN, TX
Contact: Silas Barta, sbarta[at]gmail[dot]com
Time: Saturday, October 8, 12:00 PM
Location: 4001 N Lamar, Austin Texas, park by Central Market near stone tables and
tents
Coordinates: 86248746+8C
Event link(s): LessWrong
Group info: Austin LessWrong has a weekly focused discussion, a weekly social
mixer, a weekly online book club, and a monthly movie night. Been around since
2011.
Notes: Location may change as we are talking to other venues
BRYAN/COLLEGE STATION, TX
Contact: Kenny, easwaran[at]gmail[dot]com
Time: Friday, September 9, 5:00 PM
Location: Back patio of Torchy's Tacos at Texas and New Main. I'll have a yellow
umbrella and pinkish/purple hair
Coordinates: JMFC+4J
Event link(s): LessWrong
DALLAS, TX
Contact: Ethan Morse, ethan[dot]morse97[at]gmail[dot]com, Discord:
ethanmorse#5255
Time: Sunday, September 11, 12:00 PM
Location: Union, 3705 Cedar Springs Rd, Dallas, TX 75219. We'll be in the upstairs
conference room.
Coordinates: 8645R55R+9M9
Event link(s): LessWrong
Notes: Please RSVP on LessWrong so I know how much food to get
HOUSTON, TX
Contact: Eric Magro, eric135033[at]gmail[dot]com
Time: Sunday, September 18, 4:00 PM
Location: Empire Cafe, 1732 Westheimer Rd, Houston, TX 77098 ---- Look for a table
with an ACX MEETUP sign.

Coordinates: 76X6PHVW+5H
Event link(s): LessWrong
Group info: There are meetups every week. We have a Discord and a Facebook
group.
WACO, TX
Contact: Mike, BaylorACX[at]gmail[dot]com
Time: Saturday, October 1, 1:00 PM
Location: Cameron Park, picnic tables next to Jacob's Ladder
Coordinates: 8634HVG2+V9
Event link(s): LessWrong
Notes: Please email me if you're thinking about attending! Would love to start an ACX
community here :)
SALT LAKE CITY, UT
Contact: Ross Richey (aka Jeremiah), wearenotsaved[at]gmail[dot]com
Time: Saturday, October 8, 3:00 PM
Location: Liberty Park near the ChargePoint stations
Coordinates: 85GCP4WF+VJ
Event link(s): LessWrong
Group info: We meet every other month, we do book clubs and movie nights as well.
Notes: Will be outdoors. If the weather looks bad, email event organizer to check on
location.
CHARLOTTESVILLE, VA
Contact: RL, eﬀectivealtruismatuva[at]gmail[dot]com
Time: Sunday, September 4, 5:00 PM
Location: 12 Rotunda Drive Charlottesville, VA 22903 - We'll meet at the picnic tables
across the street from The Virginian. There will be an ACX sign.
Coordinates: 87C32FPX+3H4
Event link(s): LessWrong
LYNCHBURG, VA
Contact: Craig, craigbdaniel[at]gmail[dot]com
Time: Saturday, September 17, 4:00 PM
Location: Three Roads Brewing - I will be wearing a purple t-shirt and will place an
""ACX"" card on the table
Coordinates: 8792CV65+5G
NORFOLK, VA
Contact: Willa, walambert[at]pm[dot]me
Time: Sunday, September 18, 4:00 PM
Location: Pagoda & Oriental Garden, 265 W Tazewell St, Norfolk, VA 23510. I will be
wearing a bright green shirt, will have a large green & yellow hat on, and will have a
sign with ACX Meetup on it.
Coordinates: 8785RPX4+W3
Event link(s): LessWrong, Facebook event
Group info: Hi! Virginia Rationalists was co-founded in Norfolk VA earlier this year by
Willa & Yitzi with the goal of growing a thriving ACX / LW / EA community in our city &
the state of Virginia. We meet every week at Fair Grounds cafe on Wednesday
evenings from 5-7:30pm Eastern Time. We have a Discord server and a Twitter.
RESTON, VA
Contact: James, jrbalch333[at]gmail[dot]com

Time: Saturday, September 24, 1:30 PM
Location: The matchbox at 1900 Reston Station Blvd, Reston, VA 20190 on the 1st
ﬂoor of the giant Google building. I'll be holding a copy of Sapiens.
Coordinates: 87C4WMX6+9X
Event link(s): LessWrong
Notes: Email me to be added to the WhatsApp group
RICHMOND, VA
Contact: Cedar, cedar[dot]ren+acxmeetup[at]gmail[dot]com, @Cedar at this Discord
server
Time: Saturday, October 1, 2:30 PM
Location: Richmond Public Libraries, West End Branch 5420 Patterson Ave,
Richmond, VA 23226
Coordinates: 8794HFHQ+3G
Event link(s): LessWrong
Notes: Please RSVP on LessWrong & optionally reach out to me on Discord to
introduce yourself!
BURLINGTON, VT
Contact: Forrest, lucidobservor[at]gmail[dot]com
Time: Saturday, September 10, 2:00 PM
Location: Battery Park, at the benches in the south-western corner of the park, near
the cannons facing the lake. I will have an 'ACX Meetup' sign.
Coordinates: 87P8FQJH+8P
Event link(s): LessWrong
BELLINGHAM, WA
Contact: Alex, bellinghamrationalish[at]gmail[dot]com
Time: Thursday, September 29, 5:30 PM
Location: Lake Padden Park, at one of the tables near the lake by the dog park. If it's
rainy, we'll meet in one of the two covered gazebo areas just north (right, if you're
facing the lake) of the planned spot. If the forecast looks really bad (e.g. very cold), I'll
post an indoor location to the Meetup.com page at least three days in advance.
Coordinates: 84WVMHX3+GM
Event link(s): LessWrong, Meetup.com
Group info: Bellingham Rationalish discusses (in good faith!) topics in and around
rationality. We usually meet the evening of the last Wednesday of each month. Our
ﬁrst meeting was a 2021 ACX Everywhere meetup.
Notes: Please RSVP on Meetup so I have an idea how many people to expect. Kids,
animals, food, beverages, etc. are all welcome.
SEATTLE, WA
Contact: Nikita Sokolsky, sokolx[at]gmail[dot]com
Time: Sunday, October 9, 5:00 PM
Location: Optimism Brewing (1158 Broadway, Seattle)
Coordinates: 84VVJM7H+4Q
Event link(s): LessWrong, Facebook event, Meetup.com
Notes: Please RSVP on LessWrong (or FB/Meetup) for planning purposes
MADISON, WI
Contact: Mary Wang, mmwang[at]wisc[dot]edu
Time: Saturday, September 10, 1:00 PM
Location: 1022 High St. Blue house with red porches. If weather permits, we'll be in
my large backyard, which has more seating now than last year. If rain, come in the

side door. There will be air puriﬁers and open windows. Masks optional. Look for a sign
at the end of the driveway that says ACX/SSC Meetup.
Coordinates: 86MG3H3X+XW
Event link(s): LessWrong, Facebook event
Group info: We have met fortnightly in the past, but quit last year when it got too
cold to meet outside. We typically have shared a meal, sat around my kitchen table
and talked. Have held a Solstice celebration.

Announcing the Introduction to ML
Safety course
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
TLDR
We're announcing a new course designed to introduce students with a background in
machine learning to the most relevant concepts in empirical ML-based AI safety. The
course is available publicly here.
Background
AI safety is a small but rapidly growing ﬁeld, and both younger and more experienced
researchers are interested in contributing. However, interest in the ﬁeld is not enough:
researchers are unlikely to make much progress until they understand existing work,
which is very diﬃcult if they are simply presented with a list of posts and papers to
read. As such, there is a need for curated AI safety curricula that can get new
potential researchers up to speed.
Richard Ngo's AGI Safety Fundamentals ﬁlled a huge hole in AI safety education,
giving hundreds of people a better understanding of the landscape. In our view, it is
the best resource for anyone looking for a conceptual overview of AI safety.
However, until now there has been no course that aims to introduce students to
empirical, machine learning-based AI safety research, which we believe is a crucial
part of the ﬁeld. There has also been no course that is designed as a university course
usually is, complete with lectures, readings, and assignments; this makes it more
likely that it could be taught at a university. Lastly, and perhaps most importantly,
most existing resources assume that the reader has higher-than-average openness to
AI x-risk. If we are to onboard more machine learning researchers, this should not to
be taken for granted.
In this post, we present a new, publicly-available course that Dan Hendrycks has been
working on for the last eight months: Introduction to ML Safety. The course is a project
of the Center for AI Safety.
Introduction to ML Safety
Philosophy
The purpose of Introduction to ML Safety is to introduce people familiar with machine
learning and deep learning to the latest directions in empirical ML safety research and
explain existential risk considerations. Our hope is that the course can serve as the
default for ML researchers interested in doing work relevant to AI safety, as well as

undergraduates who are interested in beginning research in empirical ML safety. The
course could also potentially be taught at universities by faculty interested in teaching
it.
The course contains research areas that many reasonable people concerned about AI
x-risk think are valuable, though we exclude those that don't (yet) have an empirical
ML component, as they aren't really in scope for the course. Most of the areas in the
course are also covered in Open Problems in AI X-Risk.
The course is still very much in beta, and we will make improvements over the coming
year. Part of the improvements will be based on feedback from students in the ML
Safety Scholars summer program.
Content
The course is divided into seven sections, covered below. Each section has lectures,
readings, assignments, and (in progress) course notes. Below we present descriptions
of each lecture, as well as a link to the YouTube video. The slides, assignments, and
notes can be found on the course website.
Background
The background section introduces the course and also gives an overview of deep
learning concepts that are relevant to AI safety work. It includes the following lectures:
Introduction: This provides motivation for studying ML safety, with an overview
of each of the areas below as well as potential existential hazards.
Deep Learning Review: This lecture covers some important deep learning
content that is good to review before moving on to the main lectures.
This section includes a written assignment and a programming assignment designed
to help students review deep learning concepts.
Hazard Analysis
In Complex Systems for AI Safety, we discussed the systems view of safety. It's
unclear to what extent AI safety is like particular other safety problems, like making
cars, planes, or software programs safer. The systems view of safety provides general
abstract safety lessons that have been applicable across many diﬀerent industries.
Many of these industries, such as information security and the defense community,
must contend with powerful adversarial actors, not unlike AI safety. The systems view
of safety thus provides a good starting point for thinking about AI safety. The hazard
analysis section of the course discusses foundational systems safety concepts and
applies them to AI safety. It includes the following lectures:
Risk Decomposition: Rather than abstractly focusing on "risk," it is often
useful to decompose risk into more understandable components: hazards,
exposure to hazards, and vulnerability to hazards.
Accident Models: In this lecture, we consider many diﬀerent accident models
which have proved useful in safety analysis, and explore their relevance to AI
safety.

Black Swans: Safety requires being conscious of extreme, unprecedented
events, often referred to as black swans. This lecture discusses the implications
of such events and the long tailed distributions they are drawn from.
This section includes a written assignment where students test their knowledge of the
section.
Robustness
In Open Problems in AI X-Risk, we covered the relevance of robustness to AI
safety. Robustness focuses on ensuring models behave acceptably when exposed to
abnormal, unforeseen, unusual, highly impactful, or adversarial events. We expect
such events will be encountered frequently by future AI systems. This section includes
the following lectures:
Adversarial Robustness: Adversarial robustness focuses on ensuring that
models behave well in worst-case scenarios. Since we care about building
models (e.g. reward models) that do not collapse under optimization pressure,
studying adversarial robustness may be useful. We cover the most common
methods for adversarial robustness research as well as some directions which
may be most relevant to AI safety. For example, we discuss certiﬁed robustness,
which provides precise mathematical guarantees for how real-world large neural
networks will behave in new situations (this line of work could potentially give us
guarantees that a model will not behave undesirably or take a treacherous turn
in certain situations).
Black Swan Robustness: Models may fail to work or behave catastrophically
when placed in new distributions and especially when exposed to black swan
events. This lecture discusses how to increase robustness to such events, ideally
before they ever occur.
This section includes a written assignment where students test their knowledge of the
section, and a programming assignment where students implement various methods
in adversarial robustness.
Monitoring
We also covered monitoring in Open Problems in AI X-Risk, which we deﬁne as
research that reduces exposure to hazards as much as possible and allows their
identiﬁcation before they grow. The course covers monitoring in more depth, and
includes the following lectures:
Anomaly Detection: This lecture covers the use of ML to detect anomalous
situations. Good anomaly detectors could be used to detect both failures of AI
systems and situations where we might expect an AI system to behave
maliciously or in novel unforeseen ways.
Interpretable Uncertainty: Humans and other systems need to understand
when they can rely on an AI system, and when they can override it. Otherwise,
they may take catastrophic actions or fail to take necessary actions. This lecture
covers ways to measure and improve methods of uncertainty communication.
Transparency: Understanding the inner workings of models could allow us to
more easily detect and prevent failures, such as deception. It could also enhance
the ability for AI systems to cooperate and monitor each other. This lecture
covers existing research into transparency and possible future directions.

Trojans: Neural networks trained on poisoned data can act as "Trojan horse
models," behaving normally except when exposed to particular inputs, where
they can behave as the attacker wants them to. Trojan detection research aims
to develop methods to detect Trojaned models before they pose a threat. We are
interested in this area mainly because it may produce methods for detecting
treacherous turns.
Detecting Emergent Behavior: Behavior not explicitly programmed into a
model may arise for instrumental reasons. One example of this is proxy gaming,
where a model does something unexpected in response to an imperfect reward
function. This lecture covers methods for detecting proxy gaming, using ideas
from anomaly detection.
This section includes a written assignment where students test their knowledge of the
section, and two programming assignments: one focused on anomaly detection, and
the other on Trojan detection.
Alignment
We also cover alignment, which has varying deﬁnitions but we deﬁne as reducing
inherent model hazards: hazards that result from models (explicitly or operationally)
pursuing the wrong goals. The course covers the following areas, which are also
covered in Open Problems in AI X-Risk.
Honest Models: If models can be made to always be honest, signiﬁcant
progress would be made in alignment. This lecture covers the distinction
between honesty (whether a model's outputs reﬂect its internal representations)
and truthfulness (whether a model outputs true information), and why some
models are currently dishonest.
Power Aversion [forthcoming]: This refers to research designed to reduce the
power-seeking tendencies of AI systems. Since empirical ML research on this
topic is currently very limited, this lecture will be released in the fall.
Machine Ethics: It may be useful to have models that have a good
understanding of diﬀerent normative ethical theories, in order to constrain and
shape the behavior of other agents. Such models will need to be robust to a
wide range of unusual real-life situations, and so will have to go beyond
simplistic assessments and trolley problems. An example of this is trying to build
a reliable automated moral parliament. This lecture covers current methods that
aim to make progress in this area.
This section includes a written assignment where students test their knowledge of the
section, and a programming assignment where students use transparency tools to
identify inconsistencies with language models trained to model ethics.
Systemic Safety
In addition to directly reducing hazards from AI systems, there are several ways that
AI can be used to make the world better equipped to handle the development of AI by
improving sociotechnical factors like decision making ability and safety culture. This
section covers a few of such areas, which are also covered in Open Problems in AI X-
Risk.
ML for Improved Decision-Making: This lecture discusses how AI might be
used to produce better institutional decision making, which may be necessary to

handle the rapidly-changing world that AI will likely induce. For instance, it
covers ways in which AI could be used to improve forecasting.
ML for Cyberdefense: Some have argued that cybersecurity is potentially
quite important for the overall ecosystem to prevent the proliferation of AI
technology among nefarious or reckless actors. Misaligned AI systems may also
project themselves through cyberattacks. ML systems can be used to reduce the
risk of such attacks. This lecture covers current methods in ML for cyberdefense.
Cooperative AI: Cooperative AI is currently being studied as a way of reducing
the risk of catastrophic conﬂicts between AI systems. In a world with multiple AI
systems, alignment of single systems may not be enough to produce good
outcomes. This lecture covers ways in which AI systems could be made to better
cooperate.
Additional Existential Risk Discussion
As is typical for a topics course, the last section covers the broader importance of the
concepts covered earlier: namely, existential risk and possible existential hazards. We
also cover strategies or tractably reducing existential risk, following Pragmatic AI
Safety and X-Risk Analysis For AI Research.
X-Risk Overview: This lecture gives several broad arguments for why AI may
pose an existential risk.
Possible Existential Hazards: This lecture covers speciﬁc ways in which AI
could potentially cause an existential catastrophe, such as weaponization, proxy
gaming, treacherous turn, deceptive alignment, value lock-in, persuasive AI. We
err on the side of including more failure modes rather than less in this lecture. A
fuller description of the failure modes can be found in X-Risk Analysis for AI 
Research.
Safety-Capabilities Balance: When researching AI safety, it is important to
make diﬀerential progress in safety rather than advance safety as a
consequence of advancing capabilities. As such, it is useful to think about having
minimal capabilities externalities. This lecture covers how to do so.
Risks from Human-AI Coevolution [forthcoming]: This lecture will be based
on forthcoming research and will also cover ideas such as mesa-optimization.
Review and Conclusion: This lecture concludes the course and discusses how
to practically carry out the research learned in the earlier parts of the course.
This section includes a ﬁnal reﬂection assignment where students review the course
and notably encourages students to evaluate AI safety arguments for themselves.
Next Steps
All course content is available online, so anyone can work through it on their own. The
course is currently being trialed by the students in ML Safety Scholars, who are
providing valuable feedback.
We are interested in running additional formal versions of this course in the future. If
you have the operational capacity to run this course virtually, or are interested in
running it at your university, please let us know!
If you notice bugs in the lectures and/or assignments, you can message any of us or
email info@centerforaisafety.org.

Acknowledgements
Dan Hendrycks would like to thank Oliver Zhang, Rui Wang, Jason Ding, Steven Basart,
Thomas Woodside, Nathaniel Li, and Joshua Clymer for helping with the design of the
course, and the students in ML Safety Scholars for testing the course.

Rant on Problem Factorization for
Alignment
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
This post is the second in what is likely to become a series of uncharitable rants about
alignment proposals (previously: Godzilla Strategies). In general, these posts are intended to
convey my underlying intuitions. They are not intended to convey my all-things-considered,
reﬂectively-endorsed opinions. In particular, my all-things-considered reﬂectively-endorsed
opinions are usually more kind. But I think it is valuable to make the underlying, not-
particularly-kind intuitions publicly-visible, so people can debate underlying generators
directly. I apologize in advance to all the people I insult in the process.
With that in mind, let's talk about problem factorization (a.k.a. task decomposition).
HCH
It all started with HCH, a.k.a. The Inﬁnite Bureaucracy.
The idea of The Inﬁnite Bureaucracy is that a human (or, in practice, human-mimicking AI) is
given a problem. They only have a small amount of time to think about it and research it, but
they can delegate subproblems to their underlings. The underlings likewise each have only a
small amount of time, but can further delegate to their underlings, and so on down the
inﬁnite tree. So long as the humans near the top of the tree can "factorize the problem" into
small, manageable pieces, the underlings should be able to get it done. (In practice, this
would be implemented by training a question-answerer AI which can pass subquestions to
copies of itself.)
At this point the ghost of George Orwell chimes in, not to say anything in particular, but just
to scream. The ghost has a point: how on earth does an inﬁnite bureaucracy seem like
anything besides a terrible idea?
"Well," says a proponent of the Inﬁnite Bureaucracy, "unlike in a real bureaucracy, all the
humans in the inﬁnite bureaucracy are actually just trying to help you, rather than e.g.
engaging in departmental politics." So, ok, apparently this person has not met a lot of real-
life bureaucrats. The large majority are decent people who are honestly trying to help. It is
true that departmental politics are a big issue in bureaucracies, but those selection pressures
apply regardless of the peoples' intentions. And also, man, it sure does seem
like Coordination is a Scarce Resource and Interfaces are a Scarce Resource and scarcity of
those sorts of things sure would make bureaucracies incompetent in basically the ways
bureacracies are incompetent in practice.
Debate and Other Successors
So, ok, maybe The Inﬁnite Bureaucracy is not the right human institution to mimic. What
institution can use humans to produce accurate and sensible answers to questions, robustly
and reliably? Oh, I know! How about the Extremely Long Jury Trial? Y'know, because juries
are, in practice, known for their extremely high reliability in producing accurate and sensible
judgements!

The Magic 8 Ball, known for being about as reliable as a Jury Trial.
"Well," says the imaginary proponent, "unlike in a real Jury Trial, in the Extremely Long Jury
Trial, the lawyers are both superintelligent and the arguments are so long that no human
could ever possibility check them all the way through; the lawyers instead read each other's
arguments and then try to point the Jury at the particular places where the holes are in the
opponent's argument without going through the whole thing end-to-end."
I rest my case.
Anyway, HCH and debate have since been followed by various other successors, which
improve on their predecessors mostly by adding more boxes and arrows and loops and
sometimes even multiple colors of arrows to the diagram describing the setup. Presumably
the strategy is to make it complicated enough that it no longer obviously corresponds to
some strategy which already fails in practice, and then we can bury our heads in the sand
and pretend that We Just Don't Know whether it will work and therefore maybe it will work.
(Reminder: in general I don't reﬂectively endorse everything in this post; it's accurately
conveying my underlying intuitions, not my all-things-considered judgement. That last bit in
particular was probably overly harsh.)
The Ought Experiment

I have a hypothesis about problem factorization research. My guess is that, to kids fresh out
of the ivory tower with minimal work experience at actual companies, it seems totally
plausible that humans can factorize problems well. After all, we manufacture all sorts of
things on production lines, right? Ask someone who's worked in a non-academia cognitive
job for a while (like e.g. a tech company), at a company with more than a dozen people, and
they'll be like "lolwut obviously humans don't factorize problems well, have you ever seen an
actual company?". I'd love to test this theory, please give feedback in the comments about
your own work experience and thoughts on problem factorization.
Anyway, for someone either totally ignorant of the giant onslaught of evidence provided by
day-to-day economic reality, or trying to ignore the giant onslaught of evidence in order to
avoid their hopes being crushed, it apparently seems like We Just Don't Know whether
humans can factorize cognitive problems well. Sort of like We Just Don't Know whether a
covid test works until after the FDA ﬁnishes its trials, even after the test has been approved
in the EU ok that's a little too harsh even for this post.
So Ought went out and tested it experimentally. (Which, sarcasm aside, was a great thing to
do.)
The experiment setup: a group of people are given a Project Euler problem. The ﬁrst person
receives the problem, has ﬁve minutes to work on it, and records their thoughts in a google
doc. The doc is then passed to the next person, who works on it for ﬁve minutes recording
their thoughts in the doc, and so on down the line. (Note: I'm not sure it was 5 minutes
exactly, but something like that.) As long as the humans are able to factor the problem into
5-minute-size chunks without too much overhead, they should be able to eﬃciently solve it
this way.
So what actually happened?
The story I got from a participant is: it sucked. The google doc was mostly useless, you'd
spend ﬁve minutes just trying to catch up and summarize, people constantly repeated work,
and progress was mostly not cumulative. Then, eventually, one person would just ignore the
google doc and manage to solve the whole problem in ﬁve minutes. (This was, supposedly,
usually the same person.) So, in short, the humans utterly failed to factor the problems well,
exactly as one would (very strongly) expect from seeing real-world companies in action.
This story basically matches the oﬃcial write-up of the results.
So Ought said "Oops" and moved on to greener pastures lol no, last I heard Ought is still
trying to ﬁgure out if better interface design and some ML integration can make problem
factorization work. Which, to their credit, would be insanely valuable if they could do it.
That said, I originally heard about HCH and the then-upcoming Ought experiment from Paul
Christiano in the summer of 2019. It was immediately very obvious to me that HCH was
hopeless (for basically the reasons discussed here); at the time I asked Paul "So when the
Ought experiments inevitably fail completely, what's the fallback plan?". And he basically
said "back to more foundational research". And to Paul's credit, three years and an Ought
experiment later, he's now basically moved on to more foundational research.
Sandwiching
About a year ago, Cotra proposed a diﬀerent class of problem factorization experiments:
"sandwiching". We start with some ML model which has lots of knowledge from many
diﬀerent ﬁelds, like GPT-n. We also have a human who has a domain-speciﬁc problem to
solve (like e.g. a coding problem, or a translation to another language) but lacks the relevant
domain knowledge (e.g. coding skills, or language ﬂuency). The problem, roughly speaking,
is to get the ML model and the human to work as a team, and produce an outcome at-least-

as-good as a human expert in the domain. In other words, we want to factorize the "expert
knowledge" and the "having a use-case" parts of the problem.
(The actual sandwiching experiment proposal adds some pieces which I claim aren't
particularly relevant to the point here.)
I love this as an experiment idea. It really nicely captures the core kind of factorization
needed for factorization-based alignment to work. But Cotra makes one claim I don't buy:
that We Just Don't Know how such experiments will turn out, or how hard sandwiching will be
for cognitive problems in general. I claim that the results are very predictable, because
things very much like this already happen all the time in practice.
For instance: consider a lawyer and a business owner putting together a contract. The
business owner has a rough intuitive idea of what they want, but lacks expertise on
contracts/law. The lawyer has lots of knowledge about contracts/law, but doesn't know what
the business owner wants. The business owner is like our non-expert humans; the lawyer is
like GPT.
In this analogy, the analogue of an expert human would be a business owner who is also an
expert in contracts/law. The analogue of the "sandwich problem" would be to get the lawyer
+ non-expert business-owner to come up with a contract as good as the expert business-
owner would. This sort of problem has been around for centuries, and I don't think we have a
good solution in practice; I'd expect the expert business-owner to usually come up with a
much better contract.
This sort of problem comes up all the time in real-world businesses. We could just as easily
consider a product designer at a tech startup (who knows what they want but little about
coding), an engineer (who knows lots about coding but doesn't understand what the
designer wants), versus a product designer who's also a ﬂuent coder and familiar with the
code base. I've experienced this one ﬁrst-hand; the expert product designer is way better.
Or, consider a well-intentioned mortgage salesman, who wants to get their customer the
best mortgage for them, and the customer who understands the speciﬁcs of their own life
but knows nothing about mortgages. Will they end up with as good a mortgage as a
customer who has expertise in mortgages themselves? Probably not. (I've seen this one ﬁrst-
hand too.)
There's tons of real-life sandwiching problems, and tons of economic incentive to solve them,
yet we do not have good general-purpose solutions.
The Next Generation
Back in 2019, I heard Paul's HCH proposal, heard about the Ought experiment, and
concluded that this bad idea was already on track to self-correct via experimental feedback.
Those are the best kind of bad ideas. I wrote up some of the relevant underlying principles
(Coordination as a Scarce Resource and Interfaces as a Scarce Resource), but mostly waited
for the problem to solve itself. And I think that mostly worked... for Paul.
But meanwhile, over the past year or so, the ﬁeld has seen a massive inﬂux of bright-eyed
new alignment researchers fresh out of college/grad school, with minimal work experience in
industry. And of course most of them don't read through most of the enormous, undistilled,
and very poorly indexed corpus of failed attempts from the past ten years. (And it probably
doesn't help that a plurality come through the AGI Safety Fundamentals course, which last
time I checked had a whole section on problem factorization but, to my knowledge, didn't
even mention the Ought experiment or the massive pile of close real-world economic
analogues. It does include two papers which got ok results by picking easy-to-decompose
tasks and hard-coding the decompositions.) So we have a perfect recipe for people who will
see problem factorization and think "oh, hey, that could maybe work!".

If we're lucky, hopefully some of the onslaught of bright-eyed new researchers will attempt
their own experiments (like e.g. sandwiching) and manage to self-correct, but at this point
new researchers are pouring in faster than any experiments are likely to proceed, so
probably the number of people pursuing this particular dead end will go up over time.

The Core of the Alignment Problem
is...
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Produced As Part Of The SERI ML Alignment Theory Scholars Program 2022 Under John
Wentworth
Introduction
When trying to tackle a hard problem, a generally eﬀective opening tactic is to Hold Oﬀ
On Proposing Solutions: to fully discuss a problem and the diﬀerent facets and aspects
of it. This is intended to prevent you from anchoring to a particular pet solution and (if
you're lucky) to gather enough evidence that you can see what a Real Solution would
look like. We wanted to directly tackle the hardest part of the alignment problem, and
make progress towards a Real Solution, so when we had to choose a project for SERI
MATS, we began by arguing in a Google doc about what the core problem is. This post
is a cleaned-up version of that doc.
The Technical Alignment Problem
The overall problem of alignment is the problem of, for an Artiﬁcial General Intelligence
with potentially superhuman capabilities, making sure that the AGI does not use these
capabilities to do things that humanity would not want. There are many reasons that
this may happen such as instrumental convergent goals or orthogonality.
Layout
In each section below we make a diﬀerent case for what the "core of the alignment
problem" is. It's possible we misused some terminology when naming each section.
The document is laid out as follows: We have two supra-framings on alignment: Outer
Alignment and Inner Alignment. Each of these is then broken down further into
subproblems. Some of these speciﬁc problems are quite broad, and cut through both
Outer and Inner alignment, we've tried to put problems in the sections we think ﬁts
best (and when neither ﬁts best, collected them in an Other category) though
reasonable people may disagree with our classiﬁcations. In each section, we've laid out
some cruxes, which are statements that support that frame on the core of the
alignment problem. These cruxes are not necessary or suﬃcient conditions for a
problem to be central. 
Frames on outer alignment

The core of the alignment problem is being able to precisely specify what we value, so
that we can train an AGI on this, deploy it, and have it do things we actually want it to
do. The hardest part of this is being mathematically precise about 'what we value', so
that it is robust to optimization pressure.
The Pointers Problem 
The hardest part of this problem is being able to point robustly at anything in the
world at all (c.f. Diamond Maximizer). We currently have no way to robustly specify
even simple, crisply deﬁned tasks, and if we want an AI to be able to do something like
'maximize human values in the Universe,' the ﬁrst hurdle we need to overcome is
having a way to point at something that doesn't break in calamitous ways oﬀ-
distribution and under optimization pressure. Once we can actually point at something,
the hope is that this will enable us to point the AGI at some goal that we are actually
okay with applying superhuman levels of optimization power on.
There are diﬀerent levels at which people try to tackle the pointers problem: some
tackle it on the level of trying to write down a utility function that is provably resilient
to large optimization pressure, and some tackle it on the level of trying to prove things
about how systems must represent data in general (e.g. selection theorems).
Cruxes (around whether this is the highest priority problem to work on)
This problem being tractable relies on some form of the Natural Abstractions
Hypothesis.
There is, ultimately, going to end up being a thing like "Human Values," that
can be pointed to and holds up under strong optimization pressure.
We are suﬃciently confused about 'pointing to things in the real world' that we
could not reliably train a diamond maximizer right now, if that were our goal. 
Distribution Shift
The hardest part of alignment is getting the AGI to generalize the values we give it to
new and diﬀerent environments. We can only ever test the AGI's behavior on a limited
number of samples, and these samples cannot cover every situation the AGI will
encounter once deployed. This means we need to ﬁnd a way to obtain guarantees that
the AGI will generalize these concepts when out-of-distribution in the way that we'd
want, and well enough to be robust to intense optimization pressure (from the vast
capabilities of the AGI). 
If we are learning a value function then this problem falls under outer alignment,
because the distribution shift breaks the value function. On the other hand, if you are
training an RL agent, this becomes more of an inner alignment problem.
Cruxes:
Creating an AGI necessarily induces distribution shift because:
 The real world changes from train to test time.
 The agent becomes more intelligent at test time, which is itself a
distribution shift. 
Understanding inductive biases well enough to get guarantees on generalization
is tractable.

We will be able to obtain bounds even for deep and fundamental distribution
shifts.
Corrigibility
The best way to solve this problem is to specify a utility function that, for the most
part, avoids instrumentally convergent goals (power seeking, preventing being turned
oﬀ). This will allow us to make an AGI that is deferential to humans, so that we
can safely perform a pivotal act, and hopefully buy us enough time to solve the
alignment problem more robustly.
Cruxes:
Corrigibility is further along the easier/safer Pareto-frontier than Coherent
Extrapolated Volition of Humanity.
Corrigibility is a concept that is more "natural" than human values.
Goodharting
Encoding what we actually want in a loss function is, in fact, too hard and will not be
solved, so we will ultimately end up training the AGI on a proxy. But proxies for what we
value are generally correlated with what we actually value, until you start optimizing on
the proxy somewhat unreasonably. An AGI will apply this unreasonable optimization
pressure, since it has no reason to 'understand what we mean, rather than what we
said.' 
Cruxes:
The inner values of an AGI will be a proxy for our values. In other words, it will not
be a True Name for what we care about, when we apply optimization pressure, it
will perform worse and not better as measured by our true preferences.
We can get a soft-optimization proposal that works to solve this problem (instead
of having the AGI hard-optimize something safe).
It is either impossible, or too hard, to specify the correct loss function and so we
will end up using a proxy.
General Cruxes for Outer Alignment
Getting the AGI to do what we want it to do (when we learn how to specify that) is
at least one of:
Not as hard.
Going to be solved anyway by making suﬃcient progress on these
problems.
Solvable through architectural restrictions.
The best way to make progress on alignment is to write down a utility function for
an AI that:
Generalizes
Is robust to large optimization pressure
Speciﬁes precisely what we want

Frames on inner alignment
The core of the alignment problem is ﬁguring out how to induce inner values into an
AGI from an outer training loop. If we cannot do this, then an AGI might end up
optimizing for something that corresponded to the outer objective on the training set
but generalizes poorly.
Mesa-Optimizers
The hardest part of this problem is avoiding the instantiation of malign learned
optimizers within the AGI. These arise when training on the base reward function does
not in fact cause the AGI to learn to optimize for that reward function, but instead to
optimize for some mesa-objective that obtains good performance in the training
environment.
One key insight for why this is the core of the alignment problem is human intelligence
being a mesa-optimizer induced by evolution. Evolution found intelligence as a good
method for performing well on the 'training set' of the ancestral environment, but now
that human intelligence has moved to a new environment where we have things like
contraception, the mesa objective of having sex has decoupled from the base objective
of maximizing inclusive genetic ﬁtness, and we pursue the former, and do not much
care about the latter.
Some ways that mesa-optimization can happen: 
There is a learned inner optimizer, e.g. in a language model, that values things in
the outside world, and so outputs things to hijack the output of the LM. 
You train an RL agent to accomplish a task, e.g. pick strawberries, but there is a
distribution shift from training to test time, and though the goal aligned on the
training distribution, the actual inner goal, e.g. take red things and put them in a
shiny metal basket extrapolates oﬀ-distribution to pulling oﬀ someone's nose and
throwing it at a lamppost
You prompt an LLM to accomplish a task, e.g. be a twitter bot that spreads
memes. You've now instantiated something that is optimizing the real world. The
LLM's outer objective was just text prediction, but via prompting, we've induced a
totally diﬀerent mesa-objective. 

Some people think that this doesn't count because the optimizer is still
optimizing the outer objective of text autocompletion. 
Cruxes:
We will not be able to prevent the instantiation of learned optimizers through
architectural adaptations.
Gradient descent selects for compressed and generalizable strategies, and
optimization/search capabilities meet both of these requirements. See
also: Conditions for Mesa-Optimization.
Ontology Identiﬁcation
The hardest part of this problem is being able to translate from a human ontology to an
AGI's ontology. An AGI is likely to use a diﬀerent ontology from us, potentially radically
diﬀerent, and also to learn new ontologies once deployed. Translating from a human
ontology to an AGI's ontology is going to be hard enough, but we also need translation
mechanisms that are robust to (potentially very large) ontology shifts.
We think that a lot of the paths to impact for interpretability research are as solutions
to this problem. Whether it is: using interpretability to obtain guarantees that an AGI
has learned precisely the concepts that we want, using interpretability tools in training
to incentivize the AGI to learn the concepts we want, using interpretability to improve
the eﬃciency/eﬀectiveness of human feedback on AGI models during training, or other
more detailed interpretability agendas; most of interpretability's impact comes down to
its use in ontology identiﬁcation.
Cruxes:
Natural Abstraction Hypothesis will make this problem tractable, or even if NAH
ends up holding only very weakly, then with enough work, we can overcome that.
If working on circuits-style ontology identiﬁcation, then at least one of:
AGI will look a lot like modern systems.
We will get good information about how to interpret future systems
(although they may look very diﬀerent) by working on modern systems.
Wireheading
We will end up using reward-circuitry mechanisms to train an AGI to optimize for what
we want, however when we do this, it may instead learn to optimize only for
maximizing its reward-circuitry. The hardest part of inner alignment is preventing these
'reward-optimizers' from developing, since as these get more capable, they will start
pretending to be aligned to the outer-objective in the training process, and then once
deployed and given access to their reward-circuitry, they will wirehead. This entails the
AGI maximizing its reward circuitry, and then taking actions to maintain that high
reward value, including eliminating threats that may try to turn it oﬀ, or modify it in
any way (i.e. us).
Cruxes:
Reward is not not the optimization target.

Deception 
When an agent interacts with the world, there are two possible ways the agent makes
mistakes: 
Its values were not aligned with the outer objective, and so it does something
intentionally wrong,
Its world model was incorrect, so it makes an accidental mistake.
Thus, the training process of an AGI will improve its values or its world model, and
since it eventually gets diminishing marginal returns from both of these, both the world
model and the values must improve together. Therefore, it is very likely that the agent
will have a suﬃciently good world model to understand that it is in a training
loop before it has fully aligned inner values.
This means we are likely to see deception: the agent will act aligned to the outer
objective while it's in the training phase, in order to get humans to deploy it in the real
world where it can pursue its true objectives. If we can see this misalignment then
hitting it with gradient descent should be able to ﬁx it, but deception gets around this,
so the core problem in alignment is avoiding deceptive alignment during training.
Cruxes:
Deception becomes a natural thing for an AGI to learn at high-enough capabilities
levels.
Detecting deception will be very hard at high-enough capabilities levels.
Cruxes for Inner Alignment
What matters is not so much the explicit outer utility function that we train the
AGI on, but instead the values that the training process instantiates in the AGI.
These values actually exist, and we're not just anthropomorphizing.
The agent will learn to model the training process as a whole before it learns to
value the utility function we are training it on.
Other Frames
Non-agentic AI/Oracle AI 
The core problem in alignment is to ﬁgure out how to make an AI that does not act like
an agent (and avoids malign subagents), and get this AI to solve the alignment
problem (or perform a pivotal act). This tries to avoid the problem of corrigibility, by
developing AIs that aren't (generally capable) optimizers (and hence won't stop you
from turning them oﬀ).
Cruxes:
A non-agentic AI can be intelligent enough to do something pivotal, e.g. writing
the alignment textbook from the future. 
Training an LLM using methods like SSL is accurately described as learning a
distribution over text completions, and then conditioning on the prompt. 

You can simulate an optimizer without being an optimizer yourself. 
The Sharp Left Turn 
The core of alignment is the speciﬁc distribution shift that happens at general
intelligence: the sharp left turn  — the AI goes from being able to only do narrow tasks
similar to what it is trained on, to having general capabilities that allow it to succeed on
diﬀerent tasks. 
Capabilities generalize by default: in a broad range of environments, there are
feedback loops that incentivize the agent to be capable. When operating in the real
world, you can keep on getting information about how well you are doing, according to
your current utility function.
However, you can't keep on getting information about how "good" your utility function
is. But there is nothing like this for alignment, nothing pushing the agent towards "what
we meant" in situations far from the training distribution. In a separate utility function
model, this problem appears when the utility function doesn't generalize well, and in a
direct policy selection model, this problem appears in the policy selector. 
Cruxes:
There will be a large, sudden distribution shift from below human level to far
superhuman level.
There will be no way to keep the AGI on-distribution for its training data.
Capabilities generalize faster than alignment.
For a given RL training environment, "strategies" are more overdetermined
than "goals".
Conclusion
The frame we think gets at the core problem best is (drumroll please) distribution shift:
robustly pointing to the right goal/concepts when OOD or under extreme optimization
pressure. This frame gives us a good sense of why mesa-optimizers are bad, ﬁts well
with the sharp left turn framing, and explains why ontology identiﬁcation is important.
Even though this is what we landed on, it should not be the main takeaway -- the real
post was the framings we made along the way. 
 

