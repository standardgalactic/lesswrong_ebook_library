
Best of LessWrong: March 2022
1. Counter-theses on Sleep
2. It Looks Like You're Trying To Take Over The World
3. Why Agent Foundations? An Overly Abstract Explanation
4. Do a cost-beneﬁt analysis of your technology usage
5. Searching for outliers
6. Beyond Blame Minimization
7. My current thoughts on the risks from SETI
8. ELK prize results
9. The Wicked Problem Experience
10. Meadow Theory
11. We're already in AI takeoﬀ
12. A Longlist of Theories of Impact for Interpretability
13. Dual use of artiﬁcial-intelligence-powered drug discovery
14. PSA: if you are in Russia, probably move out ASAP
15. Jetlag, Nausea, and Diarrhea are Largely Optional
16. Beyond micromarriages
17. Is Metaculus Slow to Update?
18. New GPT3 Impressive Capabilities - InstructGPT3 [1/2]
19. You can tell a drawing from a painting
20. Research Hamming Questions
21. Even more curated conversations with brilliant rationalists
22. Lessons After a Couple Months of Trying to Do ML Research
23. How to Make Your Article Change People's Minds or Actions? (Spoiler: Do User
Testing Like a Startup Would)
24. How to Best Use Twitter
25. Gears-Level Mental Models of Transformer Interpretability
26. Civilization as Self-Restraint
27. Projecting compute trends in Machine Learning
28. Ways to invest your wealth if you believe in a high-variance future?
29. Hinges and crises
30. [Intro to brain-like-AGI safety] 7. From hardcoded drives to foresighted plans: A
worked example
31. How to Lumenate (UK Edition)
32. [MLSN #3]: NeurIPS Safety Paper Roundup
33. AI Performance on Human Tasks
34. Experimental longtermism: theory needs data
35. On presenting the case for AI risk
36. Ask AI companies about what they are doing for AI safety?
37. Phase transitions and AGI
38. When should you relocate to mitigate the risk of dying in a nuclear war?
39. A survey of tool use and workﬂows in alignment research
40. Exploring Finite Factored Sets with some toy examples
41. Brain preservation to prevent involuntary death: a possible cause area
42. Eﬀective Ideas is announcing a $100,000 blog prize
43. ELK Computational Complexity: Three Levels of Diﬃculty
44. Duels & D.Sci March 2022: It's time for D-d-d-d-d-d-d-d-d-d-d-d-d-d-data!
45. What are the best elementary math problems you know?
46. Don't Let Personal Domains Expire
47. Christopher Alexander's architecture for learning

48. [Intro to brain-like-AGI safety] 6. Big picture of motivation, decision-making, and
RL
49. Food manufacturers are out to get you
50. Scientiﬁc Wrestling: Beyond Passive Hypothesis-Testing

Counter-theses on Sleep
Alexey Guzey's Theses on Sleep gained a lot of popularity and acclaim on LessWrong
and among people I follow on social media, despite largely consisting of what I think
were weak arguments and misleading claims. I found that a bit surprising, so I decided
to write a post pointing out several of the mistakes I think he's made, and reporting
some of what the academic literature on sleep seems to show. 
Sleep deprivation is associated with *both*
depression *and* mania
One of Guzey's theses is that "depression triggers/ampliﬁes oversleeping while
oversleeping triggers/ampliﬁes depression." The ﬁrst piece of evidence he uses to
support that is people on /r/BipolarReddit saying that they sleep a lot when depressed,
and sleep very little when manic. However, there's a big problem with using that as
evidence. 
Guzey's /r/BipolarReddit evidence is misleading
The DSM-5 speciﬁes subtypes of depression that have opposing relationships with
sleep. Depression with melancholic features is associated with early morning
awakening, whereas depression with atypical features is associated with hypersomnia.
Guzey's evidence is misleading because people with bipolar disorder
are disproportionally prone to having atypical features during their depressive
episodes. So, unsurprisingly, his evidence from bipolar disorder patients is not
representative of what you see in the general population: both long and short sleep
duration are associated with depression (and, relevantly, suicide as well, in adults as
well as adolescents). 
I'm surprised no one in the comments of this post brought up this objection - it only
takes a very quick Google Scholar search to see that the relationship between sleep
duration and depression is not linear. 
Chronic sleep deprivation might be associated with *
decreased* BDNF expression, as shown by Guzey's own
sources
Guzey hypothesizes that 
Sleep deprivation appears to increase BDNF [and therefore neurogenesis?]
He then proceeds to link to a few papers that showed up when he Googled "sleep
deprivation bdnf". These sources agree that acute sleep deprivation increases BDNF
expression, but some also say that the opposite may happen when sleep deprivation is
chronic, which Guzey fails to mention in his post.
From The Brain-Derived Neurotrophic Factor: Missing Link Between Sleep Deprivation,
Insomnia, and Depression:

Chronic sleep deprivation and insomnia can act as an external stressors and result
in depression, characterized by hippocampal BDNF downregulation along with
disrupted frontal cortical BDNF expression, as well as reduced levels and impaired
diurnal alterations in serum BDNF expression.
Guzey also links to The link between sleep, stress and BDNF, which seems to be only
an abstract. After a bit of searching, I found this full-text paper with some of the same
authors, which appears to have an almost identical abstract (?). It concludes with the
following:
[O]ur ﬁndings are in line with the hypothesis of an increased stress
vulnerability due to sleep loss which may lead to a decrease in BDNF. [...]
While we report a reduction of BDNF levels linked to sleep disturbance reﬂecting
chronic stress on the one side, we and others consistently showed that prolonged
wakefulness caused by SD (partial or total), which can be considered as an acute
stressor for the brain, leads to a rapid increase of BDNF (1,36).
Our priors about sleep research should be
high
I feel grumpy, dumb and distractable every time I sleep less than 7 or so hours. I can't
do things that require focused attention like solving physics homework problems very
eﬀectively, and I don't get nearly as much pleasure when I attempt doing so. My
memory becomes very poor: after a recent night of <6 hours of sleep, I somehow
forgot the reasoning behind several Manifold Markets trades I had made the prior
evening and just stared at them in utter confusion for several minutes before
remembering. 
I have the impression that most people have a similar experience. Guzey has a cute
explanation of how this is consistent with his thesis that sleep deprivation doesn't
make you grumpy or dumb, but the fact that I and others I know have this experience
with sleep deprivation obviously makes it so that I have a high prior that it's harmful to
cognition and mood. It sounds a lot more plausible that sleep deprivation being harmful
in those ways is causing both my personal experience with it to be terrible and
academic research to ﬁnd that it's harmful, instead of a diﬀerent factor explaining each
thing. Guzey has to add quite a few epicycles to his theory to explain the evidence. 
To the extent that you and the people you know feel the same way I do after a night of
sleep deprivation, your prior should be high as well. 
Most of Guzey's arguments against trusting sleep research
are bad
Guzey claims that "most sleep research is extremely unreliable and we shouldn't
conclude much on the basis of it," but there are problems with that. Firstly because he
doesn't seem to believe that about sleep research that favors his
hypotheses. Guzey, after all, uses sleep research to show that Matthew Walker's book
is terrible and fraudulent. So it seems that he wants to trust sleep research when it
says that sleep deprivation is not as bad as Walker shows, but doesn't want to trust it
when it says that sleep deprivation is not harmless.  

Secondly, he bases that assertion on a claim that sleep science is "mostly just
rebranded cognitive psychology" (and that it is only not facing a severe replication
crisis because sleep experiments are expensive), which is very misleading. Unlike the
famously unreliable cognitive science results, the ﬁnding that sleep deprivation is
harmful for cognition (1) in fact gets replicated a lot (see the section below on
meta-analyses of sleep restriction studies, as well as this meta-analysis of total sleep
deprivation studies) and (2) is consistent with a lot of people's experiences (for
examples, see this addendum.) So it's hard to see how Guzey's criticisms apply. 
Moderate sleep restriction impairs cognition
Guzey says, in his post: 
I wrote large chunks of this essay having slept less than 1.5 hours over a period of
38 hours. I came up with and developed the biggest arguments of it when I slept an
average of 5 hours 39 minutes per day over the preceding 14 days. At this point,
I'm pretty sure that the entire "not sleeping 'enough' makes you stupid" is a 100%
psyop. It makes you somewhat more sleepy, yes. More stupid, no. I literally did an
experiment in which I tried to ﬁnd changes in my cognitive ability after sleeping 4
hours a day for 12-14 days, I couldn't ﬁnd any. My friends who I was talking to a lot
during the experiment simply didn't notice anything.
I don't think that "feeling smart after sleep deprivation" (or any of those other things)
is nearly enough evidence to make you conclude that " 'not sleeping 'enough' makes
you stupid' is a 100% psyop" if you start out with even a halfway reasonable prior, and
especially if you appropriately update on what the sleep literature says. 
I looked for meta-analyses that investigated the eﬀect of experimental or quasi-
experimental nighttime sleep restriction on cognition. I found three, and am quoting
the relevant conclusions from them:
The neurocognitive consequences of sleep restriction: A meta-analytic review:[1] 
The current meta-analytic review revealed that restricted sleep results in
signiﬁcant neurocognitive deﬁcits (g = −0.383) in a sample of 1688
participants derived from 71 diﬀerent study populations. This eﬀect was apparent
across multiple cognitive domains, with the largest eﬀects being observed on
measures of sustained attention (g = −0.409) and EF (g = −0.324), and within
these domains, attentional lapses and (g = −0.516) and behavioural inhibition (g =
−0.464) speciﬁcally.
[...]
Meta-regression analyses indicated that age-adjusted sleep deﬁcit (β=-.206,
p=.033), cumulative days of restricted sleep (days; β=-.015, p=.019),[2] subjective
sleepiness (β=-.040, p=.016), biological sex (β=.0318, p=.009), and sleep latency
(β=.012, p=.013) accounted for a signiﬁcant proportion of the variance in the
observed eﬀect of sleep restriction on overall cognitive abilities.
Sleep Loss and Performance in Residents and Nonphysicians: A Meta-Analytic
Examination:
Chronic partial sleep loss also resulted in a signiﬁcant reduction in cognitive
performance, with a correct d value of -.886. 

A Meta-Analysis of the Eﬀect of Experimental Sleep Restriction on Youth's Attention and
Hyperactivity (on people under 18 years old only):
A total of 13 samples (N = 268) examined the diﬀerence in youth's attention
performance between baseline (control) sleep and restricted sleep. The overall
eﬀect size of −0.19 was signiﬁcant (95% CI: −0.34−0.03; p = .02); Q(13) = 21.98,
p = .04, I 2 = 45.40% (see Figure 2). The I^2 index of 45.40% indicates a small to
moderate amount of heterogeneity in eﬀect sizes across studies (Card, 2012).
However, the trim and ﬁll method for addressing publication bias (Duval & Tweedie,
2000a; Duval & Tweedie, 2000b) revealed asymmetry in the funnel plots for the
diﬀerence in attention outcomes between sleep restricted and baseline sleep. One
study to the left of the mean was unmatched. The counterpart of this study was
imputed to the right of the mean, resulting in a non-signiﬁcant and small adjusted
eﬀect size of −0.14 (95% CI: −0.32-.04).
Six samples (N = 279) examined the diﬀerence in attention between extended
sleep (sleep extension) and sleep restriction. The overall eﬀect size of −0.37 was
signiﬁcant (95% CI: −0.55 to −0.19, p < .0001) and represents a small to medium
eﬀect size (Lipsey & Wilson, 2001), though this eﬀect was heterogeneous between
studies Q(5) = 15.29, p = .009, I 2 = 67.30% (see Figure 2). The I 2 index of
67.30% indicates a moderate to large amount of heterogeneity in eﬀect sizes
across studies (Card, 2012). Based on trim-and-ﬁll analyses, asymmetry was also
present for attention outcomes between restricted and extended sleep, with two
studies to the left of the mean unmatched, resulting in a signiﬁcant adjusted eﬀect
size of −0.26 (95% CI: −0.46 to −0.05), which is considered a small to medium
eﬀect (Lipsey & Wilson, 2001).
The average age-adjusted sleep deﬁcit in the ﬁrst meta-analysis was 3.83 hours
(SD = 1.25). (This is in comparison with the median recommended amount of sleep for
each age group (so 8 hours per night for non-elderly adults), not the average amount
of sleep people in each age group actually get.) I couldn't ﬁnd information about the
other ones, but I have a high credence that they examined studies with a roughly
similar protocol; I've combed through a lot of individual sleep restriction studies before
and they rarely seem to involve making people sleep for < 3.5 hours. 
I also found a meta-analysis investigating the eﬀects of napping, and it ﬁnds that it's
beneﬁcial, which is probably relevant:
Eﬀects of a Short Daytime Nap on the Cognitive Performance: A Systematic Review and
Meta-Analysis:
Overall cognitive performance did not diﬀer at baseline (t0) between groups (eﬀect
size −0.03, 95% CI −0.14 to 0.07), and improved in the nap group following
the nap (t1) (0.18, 0.09 to 0.27), especially for alertness (0.29, 0.10 to 0.48).
Sensitivity analyses gave similar results comparing only randomized controlled
trials, and after exclusion of outliers.
There's also (weaker) evidence that, contrary to what Guzey hypothesizes, there is no
cognitive adaptation to chronic sleep restriction. From this article:
Contrary to a popular belief that healthy adults can acclimate to sleep loss, the
eﬀects of chronic partial sleep loss appear to be cumulative.9-11 Speciﬁcally,
sleepiness has been found to increase9 and performance on tests of vigilance and
mathematical calculations to decline across 7 days of 5 and even 7 hours of sleep
per night.10,11 Subjects often underestimate their own degree of sleep-related

impairments in vigilance after 1 week of partial sleep restriction.9,12 Thus, they
may mistakenly believe that they have acclimated to sleep deprivation.
In addition to reduced vigilance, verbal processing and complex problem
solving13,15 are impaired with both short-term and chronic partial sleep loss.
See also footnote 2 ([2]) on the matter of cognitive adaptation. 
I do recognize that even meta-analyses of experimental studies can obviously be very
problematic, and so this section is not conclusive evidence that moderate sleep
restriction impairs cognition (and not only because conclusive evidence is not a thing).
But it's not as if Guzey has any better evidence to argue that " 'not sleeping 'enough'
makes you stupid' is a 100% psyop."
Occasionally stubbing your toe is good for
health and promotes more eﬃcient toe
healing
One of Guzey's theses  is that "[o]ccasional acute sleep deprivation is good for health
and promotes more eﬃcient sleep." His argument supporting that thesis is pretty much
that, because some types of acute stress (such as exercising and fasting) are good, and
acute sleep deprivation causes acute stress, then acute sleep deprivation is also good.
(Yes, that does seem to actually be the entirety of his argument in that section. You can
read it yourself .)
The obvious problem with that argument is that the set of things that cause acute
bodily stress is much larger than the set of things that cause long-term beneﬁts.
Stubbing your toe, for example, causes acute bodily stress. Guzey's argument works
equally well for showing that occasional toe-stubbing is good for health as for showing
that occasional acute sleep deprivation is.
Short-term != long-term
In this section, Guzey lists lines of evidence that bring him to the conclusion that
"decreasing sleep by 1-2 hours a night in the long-term has no negative health
eﬀects." The lines of evidence are:
1. A sleep researcher who trains sailors to sleep eﬃciently in order to maximize
their race performance believes that 4.5-5.5 hours of sleep is ﬁne.
2. 70% of 84 hunter-gatherers studied in 2013 slept less than 7 hours per day,
with 46% sleeping less than 6 hours.
3. A single-point mutation can decrease the amount of required sleep by 2
hours, with no negative side-eﬀects.
4. A brain surgery can decrease the amount of sleep required by 3 hours, with
no negative-side eﬀects.
5. Sleep is not required for memory consolidation.
Apart from (2), we don't have any indication of the long-term outcomes of the groups
of people he mentioned. So I don't know how these points could be more than weak
and circumstantial evidence of this section's thesis.

(For that matter, barely any of those even rigorously measure the health eﬀects of
short sleep at all. In a sense, they conﬁrm that these people are not immediately dying
or getting very acutely sick or something, and I guess I can interpret Guzey as merely
wanting to claim that, but it's not exactly a novel or surprising claim.)
(Also, the 5th line of evidence doesn't even seem to be related to health.)
Experimental sleep restriction seems to cause
short-term detriments to metabolic health
I searched Embase for meta-analyses on the eﬀect of experimental sleep restriction on
a host of health-related variables, and this is what I found.
Eﬀects of sleep manipulation on markers of insulin sensitivity: A systematic review and
meta-analysis of randomized controlled trials:
Whole-body insulin sensitivity was also reduced after short sleep when measured
by the hyperinsulinemic euglycemic clamp, but peripheral insulin sensitivity was
not aﬀected. In addition, circadian misalignment and slow wave sleep suppression
negatively aﬀected insulin sensitivity, while rapid eye movement sleep disturbance
and sleep fragmentation had no eﬀect.
Eﬀects of sleep restriction on metabolism-related parameters in healthy adults: A
comprehensive review and meta-analysis of randomized controlled trials:[3]
Participants consumed 252.8 more kcal/d (p = 0.011) under sleep restriction than
under normal sleep. Partial sleep restriction resulted in a 0.34 kg weight gain (p =
0.003). Sleep restriction also decreased insulin sensitivity (standardized mean
diﬀerence = −0.70, p < 0.01). Signiﬁcant changes in brain activity in response to
food stimuli were observed under sleep restriction, particularly regions related to
cognitive control and reward.
Sleep Restriction Eﬀects on BP: Systematic Review & Meta-analysis of RCTs:
Overall, sleep restriction did not result in signiﬁcant changes in systolic blood
pressure (SBP) or diastolic blood pressure (DBP) and heart rate (HR). The
respective weighted mean diﬀerence (MD) was 1.0 mmHg (95%CI, -2.3-4.2; p =
0.57), -0.4 mmHg (95%CI, -3.2-2.4; p = 0.80), and 2.0 bpm (95%CI, -2.2-6.2; p =
0.34).
Sleep Disturbance, Sleep Duration, and Inﬂammation: A Systematic Review and Meta-
Analysis of Cohort Studies and Experimental Sleep Deprivation:
Experimental sleep deprivation, either for partial or total night, was not associated
with CRP [...], IL-6 [...], or TNFα [...]. Likewise, sleep restriction over several days
was not associated with CRP [...], IL-6 [...], or TNFα [...].
(Eyeballing each of those meta-analyses' lists of studies, it seems that, in most of
them, time in bed was restricted to 4-5 hours, with the range being from ~3.5 to ~5.5.)
So experimental sleep restriction (presumably during short periods of time) seems to
impair things like insulin sensitivity and the regulation of satiety, but not blood
pressure or inﬂammatory markers. 

Importantly, the metabolic eﬀects of sleep restriction found in the ﬁrst two meta-
analyses seem consistent with the ﬁnding that short sleep duration is associated with
weight gain in observational studies (particularly in younger persons), which perhaps
indicates that the adverse metabolic eﬀects of sleep restriction don't wane with time. 
These changes seem pretty bad. It's unclear to what extent they translate to long-term
health eﬀects (see the section below) but those are things you perhaps should not
ignore when investigating whether sleep deprivation is bad for your health. 
The evidence for a U-shaped association
between sleep and mortality is actually pretty
weak 
Guzey doesn't talk about the supposedly U-shaped association between sleep and
mortality on Theses on Sleep itself, but he has brought it up in a couple of other places
(in the conclusion of his takedown of Walker's Why We Sleep, and on a comment in the
EA Forum) so I thought I'd tackle it here. 
For those who don't know, a lot of epidemiological studies seem to show that the
association between sleep duration and mortality is something like this:
(This speciﬁc chart comes from Shen 2016). 

But Kurina et al.'s "Sleep duration and all-cause mortality: a critical review of
measurement and associations" ﬁnds that this U-shaped association is suspiciously
restricted to a very speciﬁc type of study (emphasis mine): 
One interesting pattern among studies using survey sleep measures is that all of
the studies reporting U-shaped associations measured sleep duration
with questions about typical nighttime sleep or 24-hour sleep (Table
2). None of the studies that asked about usual bedtimes and waking times
reported a U-shaped association; rather, they reported either no
association [12,22,23] or only a long sleep association [6,9,34,99], or, in
the case of two studies of young to middle-aged Japanese men, only a
short sleep association [12,22]. That the U-shaped associations are exclusively
found in studies asking about usual sleep duration may be informative and
suggests the possibility of systematic response biases, with people in generally
good health more likely to give a "normative" response (i.e., 7 or 8 hours) and
those in worse health more likely to give a "non-normative" (shorter or longer
duration) response.
It seems that you can't take people's reports of how many hours they sleep at face
value. Further evidence of that is that studies have shown that people seem to give the
same answers when asked how many hours they sleep and how many hours they
spend in bed. Also, a substantial fraction of older adults (47% in this study) report
sleeping >=8 hours a night, but when their sleep is actually measured, very few of
them do, as pointed out in Kurina et al.'s paper. 
Kurina et al. look into studies that actually measure people's total sleep time, and
report the following:
A study employing actigraphy among women 50 to 81 years of age (n = 444)
concluded that the relationship between sleep duration and mortality was U-
shaped [... but] death rates by more detailed sleep categories do not show a dose
response for either side of the duration distribution. The top and bottom categories
(<4.5 or >7.5 hours) have relatively low mortality, albeit with small numbers, and
the highest mortality risk was observed in women sleeping either 4.5—5 hours or—
interestingly—between 7 and 7.5 hours.
Neither of the two studies employing polysomnography reported signiﬁcant
associations between sleep duration and mortality [27,36]. In both studies, sleep
duration was dichotomized at fewer than 6 and 6 or more hours, precluding the
possibility of ﬁnding a U-shaped eﬀect. 
This isn't exactly strong evidence that sleeping too much or too little is not harmful.
This review only mentions three studies examining the eﬀect of measured sleep on
mortality, and two of those didn't record sleep duration in a way that would make them
able to ﬁnd a U-shaped relationship. Moreover, all of these naturalistic studies probably
have substantial range restriction — they mostly involve elderly or middle-aged
subjects, many of whom can probably sleep as much as they want, and it's fairly
plausible that very few of those people manage to consistently sleep so much or so
little that it's harmful. Also, they measured sleep for one night only, and the study
equipment might have disrupted the sleep of subjects. 
However, this review does provide evidence against six hours of sleep being associated
with the lowest mortality, as Guzey has hypothesized before (as well as evidence
against eight hours of sleep being associated with the lowest mortality, of course). And

thus, most importantly, it also provides evidence against moderately short or
moderately long sleep being harmful.
Conclusion
So here are my credences on a few relevant object-level claims:
Shorter-than-average sleep is associated with both depression and mania: 95%
Modulo weird edge cases like abolishing REM sleep exclusively, a single night
with 4.5 to 6 hours of sleep has a negative impact on cognition the following day:
96%
Conditional on this, cognition does not return to baseline levels after several
months of sleep restriction: 83%
Also modulo weird edge cases, consistently sleeping between 4.5 and 6 hours per
night as a non-elderly adult has some long-term negative health eﬀects: 75%
It's not worth it to stress out about how much you're sleeping, as long as you feel
comfortable and productive (and you're not (getting) manic)[4]: 63%
I'd be happy to bet on reasonable operationalizations of those statements at those
odds (unless I change my mind, in which case I'll probably edit the credences listed
here).
Addenda
Mania is really, really bad, and can be triggered by sleep
restriction
 I thought I'd bring this up, since it aﬀects how harmful we should expect sleep
deprivation to be in expectation. Two years after being hospitalized for a manic
episode, less than half of people regain their premorbid occupational and residential
status, and the hospitalization rate of mania is pretty high, so manic episodes seem to
screw people over really badly for a long time. 
And sleep deprivation might risk triggering mania if you have bipolar or a high risk of
getting it. A night of total sleep deprivation seems to be able to trigger full-blown
mania in a substantial percentage of people with bipolar disorder (even those currently
depressed) and even cause mania-like behavior in healthy subjects. Moreover, a shift
towards mania or hypomania after a short night of sleep seems common in bipolar
patients. 
Given how bad mania is, it might be a good idea to try to decrease even a relatively
small chance of getting it, so those at a high risk of getting bipolar disorder should
probably consider these eﬀects before experimenting with sleep restriction.
 
Amusing r/NewParents anecdotes about sleep deprivation
I collected a few amusing anecdotes about sleep deprivation from r/NewParents. 

From this thread, titled "Tell me you're sleep deprived without telling me you're sleep
deprived..":
I just unzipped my 1 month old's sleeper to expose his nipple so I could feed
him...... 🤨
 
My husband attempted to hand me a cat to nurse.
 
"There was 120ml in the bottle, there is now 50ml in the bottle, so she's drank...
Okay, this isn't that hard, there was 120 in the bottle, there is now 50 in the bottle
so she's had...
Right the bottle has 50ml, she's drank 120 so there should be.. no, she's drank 50
there's 120 left so, NO!
there was 120 in the bottle, there is now 50 and it goes 50, 60, 70, 80, 90, 100,
110, 120 so she's had 120.
NO! "
i genuinely gave up and used a calculator
 
From "Finish the sentence: I was so sleep deprived I....":
Filled the dryer full of wet clothes and tried to turn it on via the microwave sat
above it
 
...ran into a laundry basket and said "excuse me" to it.
 
 
Turned on the Keurig to make my coﬀee and walked away without noticing that I
forgot to put my mug under it. The worst part is I've done this 3 or 4 times now.
 
Forgot to put a new diaper on after taking dirty one oﬀ. Baby went commando for a
while.
From "Funniest thing you've said while sleep deprived in the dead of night":
Nothing will ever top my husband waking me up saying "please take the baby I
can't stay awake any longer" and gently passing me a very pissed oﬀ cat that had
been asleep on his lap. Note, cat weighed twice what our newborn weighed. When I
told him that was a cat he looked terriﬁed and went "but then where's the baby?!"
 

My husband and I woke up to our 5-month old crying and he said, "Is that ours??"
Yes, my good sir, that is in fact our baby.
 
It was my turn to wake up for a feed and my wife woke up ﬁrst so said "babe, wake
up she's crying" and I responded "what? who the hell would be crying in our
house?" It was 2 weeks after our daughter got home and she's our ﬁrst 🙃 
Errata
(Let me know if you think something should be added here that hasn't been.)
I used to have an additional quote in the section about BDNF, saying that
insomniacs had lower levels of BDNF than sleep-healthy controls in a study.
Guzey noted that insomniacs often underestimate how much they sleep; which
prompted me to look up the objective sleep duration of insomniacs and not ﬁnd
much evidence that they sleep less than other people. So I removed the quote.
The last paragraph of my section about mortality used wording that seemed to
imply that Guzey had hypothesized that sleeping 6 hours per night was causally
optimal, in the sense that people should sleep that much if they want to have the
lowest mortality. Guzey did not actually hypothesize that. He pointed that out and
I reworded the paragraph. 
1. ^
This study looks for publication bias in the literature and says the following about
what they found: "Evidence for publication bias was observed for the overall
eﬀect and the eﬀect on measures of sustained attention; see Table 3. However,
the impact of such publication bias on the eﬀect appears to be minimal as
another 75 studies with an eﬀect size <0.0 would have to be added to result in a
small overall eﬀect size (g < −0.200)."
Orwin's fail-safe N was used to calculate that number.
2. ^
Note that the β for cumulative days of restricted sleep is negative, providing
evidence against Guzey's hypothesis that cognitive adaptation occurs after
several days of restricted sleep, at least in the timespans investigated in the
studies included in that meta-analysis.
3. ^
Importantly, although this meta-analysis included total sleep deprivation studies,
the numbers reported in the sentences I quoted are for partial sleep restriction
only.
4. ^
This is a very important caveat. People with mania will feel perfectly comfortable
and productive while sleeping very little.

It Looks Like You're Trying To Take
Over The World
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
This is a linkpost for https://www.gwern.net/Clippy
This story was originally posted as a response to this thread.
It might help to imagine a hard takeoﬀ scenario using only known sorts of NN &
scaling eﬀects...
In A.D. 20XX. Work was beginning. "How are you gentlemen !!"... (Work. Work
never changes; work is always hell.)
Speciﬁcally, a MoogleBook researcher has gotten a pull request from Reviewer #2
on his new paper in evolutionary search in auto-ML, for error bars on the auto-ML
hyperparameter sensitivity like larger batch sizes, because more can be diﬀerent
and there's high variance in the old runs with a few anomalously high
performance values. ("Really? Really? That's what you're worried about?") He
can't see why worry, and wonders what sins he committed to deserve this asshole
Chinese (given the Engrish) reviewer, as he wearily kicks oﬀ yet another HQU
experiment...
Rest of story moved to gwern.net.

Why Agent Foundations? An Overly
Abstract Explanation
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Let's say you're relatively new to the ﬁeld of AI alignment. You notice a certain cluster
of people in the ﬁeld who claim that no substantive progress is likely to be made on
alignment without ﬁrst solving various foundational questions of agency. These sound
like a bunch of weird pseudophilosophical questions, like "what does it mean for some
chunk of the world to do optimization?", or "how does an agent model a world bigger
than itself?", or "how do we 'point' at things?", or in my case "how does abstraction
work?". You feel confused about why otherwise-smart-seeming people expect these
weird pseudophilosophical questions to be unavoidable for engineering aligned AI. You
go look for an explainer, but all you ﬁnd is bits and pieces of worldview scattered
across many posts, plus one post which does address the question but does so
entirely in metaphor. Nobody seems to have written a straightforward explanation for
why foundational questions of agency must be solved in order to signiﬁcantly move
the needle on alignment.
This post is an attempt to ﬁll that gap. In my judgment, it mostly fails; it explains the
abstract reasons for foundational agency research, but in order to convey the
intuitions, it would need to instead follow the many paths by which researchers
actually arrive at foundational questions of agency. But a better post won't be ready
for a while, and maybe this one will prove useful in the meantime.
Note that this post is not an attempt to address people who already have strong
opinions that foundational questions of agency don't need to be answered for
alignment; it's just intended as an explanation for those who don't understand what's
going on.
Starting Point: The Obvious Stupid Idea
Let's start from the obvious stupid idea for how to produce an aligned AI: have
humans label policies/plans/actions/outcomes as good or bad, and then train an AI to
optimize for the good things and avoid the bad things. (This is intentionally general
enough to cover a broad range of setups; if you want something more speciﬁc, picture
RL from human feedback.)
Assuming that this strategy could be eﬃciently implemented at scale, why would it
not produce an aligned AI?
I see two main classes of problems:
1. In cases where humans label bad things as "good", the trained system will also
be selected to label bad things as "good". In other words, the trained AI will
optimize for things which look "good'' to humans, even when those things are
not very good.
2. The trained system will likely end up implementing strategies which do "good"-
labeled things in the training environment, but those strategies will not

necessarily continue to do the things humans would consider "good" in other
environments. The canonical analogy here is to human evolution: humans use
condoms, even though evolution selected us to maximize reproductive ﬁtness.
Note that both of these classes of problems are very pernicious: in both cases, the
trained system's results will look good at ﬁrst glance.
Neither of these problems is obviously all that bad. In both cases, the system is
behaving at least approximately well, at least within contexts not-too-diﬀerent-from-
training. These problems don't become really bad until we apply optimization
pressure, and Goodhart kicks in.
Goodhart's Law
There's a story about a Soviet nail factory. The factory was instructed to produce as
many nails as possible, with rewards for high numbers and punishments for low
numbers. Within a few years, the factory was producing huge numbers of nails - tiny
useless nails, more like thumbtacks really. They were not very useful for nailing things.
So the planners changed the incentives: they decided to reward the factory for the
total weight of nails produced. Within a few years, the factory was producing big
heavy nails, more like lumps of steel really. They were still not very useful for nailing
things.
This is Goodhart's Law: when a proxy for some value becomes the target of
optimization pressure, the proxy will cease to be a good proxy.
In everyday life, if something looks good to a human, then it is probably actually good
(i.e. that human would still think it's good if they had more complete information and
understanding). Obviously there are plenty of exceptions to this, but it works most of
the time in day-to-day dealings. But if we start optimizing really hard to make things
look good, then Goodhart's Law kicks in. We end up with instagram food - an
elaborate milkshake or salad or burger, visually arranged like a bouquet of ﬂowers,
but impractical to eat and kinda mediocre-tasting.
Returning to our two alignment subproblems from earlier:
1. In cases where humans label bad things as "good", the trained system will
also be selected to label bad things as "good". In other words, the trained AI
will optimize for things which look "good'' to humans, even when those
things are not very good.
2. The trained system will likely end up implementing strategies which do
"good"-labeled things in the training environment, but those strategies will
not necessarily continue to do the things humans would consider "good" in
other environments. The canonical analogy here is to human evolution:
humans use condoms, even though evolution selected us to maximize
reproductive ﬁtness.
Goodhart in the context of problem (1): train a powerful AI to make things look good
to humans, and we have the same problem as instagram food, but with way more
optimization power applied. Think "Potemkin village world" - a world designed to look
amazing, but with nothing behind the facade. Maybe not even any living humans
behind the facade - after all, even generally-happy real humans will inevitably

sometimes put forward appearances which would not appeal to the "good"/"bad"-
labellers.
Goodhart in the context of problem (2): pretend our "good"/"bad" labels are perfect,
but the system ends up optimizing for some target which doesn't quite track our
"good" labels, especially in new environments. Then that system ends up optimizing
for whatever proxy it learned; we get the AI-equivalent of humans wearing condoms
despite being optimized for reproductive ﬁtness. And the AI then optimizes for that
really hard.
Now, we've only talked about the problems with one particular alignment strategy.
(We even explicitly picked a pretty stupid one.) But we've already seen the same basic
issue come up in two diﬀerent subproblems: Goodhart's Law means that proxies which
might at ﬁrst glance seem approximately-ﬁne will break down when lots of
optimization pressure is applied. And when we're talking about aligning powerful
future AI, we're talking about a lot of optimization pressure. That's the key idea which
generalizes to other alignment strategies: crappy proxies won't cut it when we start to
apply a lot of optimization pressure.
Goodhart Is Not Inevitable
Suppose we're designing some secure electronic equipment, and we're concerned
about the system leaking information to adversaries via a radio side-channel. We
design the system so that the leaked radio signal has zero correlation with whatever
signals are passed around inside the system.
Some time later, a clever adversary is able to use the radio side-channel to glean
information about those internal signals using fourth-order statistics. Zero correlation
was an imperfect proxy for zero information leak, and the proxy broke down under the
adversary's optimization pressure.
But what if we instead design the system so that the leaked radio signal has zero 
mutual information with whatever signals are passed around inside the system? Then
it doesn't matter how much optimization pressure an adversary applies, they're not
going to ﬁgure out anything about those internal signals via leaked radio.
Many people have an intuition like "everything is an imperfect proxy; we can never
avoid Goodhart". The point of the mutual information example is that this is basically
wrong. Figuring out the True Name of a thing, a mathematical formulation
suﬃciently robust that one can apply lots of optimization pressure without
the formulation breaking down, is absolutely possible and does happen. That
said, ﬁnding such formulations is a suﬃciently rare skill that most people will not ever
have encountered it ﬁrsthand; it's no surprise that many people automatically assume
it impossible.
This is (one framing of) the fundamental reason why alignment researchers work on
problems which sound like philosophy, or like turning philosophy into math. We are
looking for the True Names of various relevant concepts - i.e. mathematical
formulations robust enough that they will continue to work as intended even under
lots of optimization pressure.
Aside: Accidentally Stumbling On True Names

You may have noticed that the problem of producing actually-good nails has basically
been solved, despite all the optimization pressure brought to bear by nail producers.
That problem was solved mainly by competitive markets and reputation systems. And
it was solved long before we had robust mathematical formulations of markets and
reputation systems.
Or, to reuse the example of mutual information: one-time pad encryption was
intuitively obviously secure long before anyone could prove it.
So why do we need these "True Names" for alignment?
We might accidentally stumble on successful alignment techniques. (Alignment By
Default is one such scenario.) On the other hand, we might also fuck it up by accident,
and without the True Name we'd have no idea until it's too late. (Remember, our
canonical failure modes still look ﬁne at ﬁrst glance, even setting aside the question of
whether the ﬁrst AGI fooms without opportunity for iteration.) Indeed, people did
historically fuck up markets and encryption by accident, repeatedly and to often-
disastrous eﬀect. It is generally nonobvious which pieces are load-bearing.
Aside from that, I also think the world provides lots of evidence that we are unlikely to
accidentally stumble on successful alignment techniques, as well as lots of evidence
that various speciﬁc classes of things which people suggest will not work. This
evidence largely comes from failure to solve analogous existing problems "by
default". That's a story for another post, though.
What "True Names" Do We Want/Need For
Alignment?
What kind of "True Names" are needed for the two alignment subproblems discussed
earlier?
1. In cases where humans label bad things as "good", the trained system will
also be selected to label bad things as "good". In other words, the trained AI
will optimize for things which look "good'' to humans, even when those
things are not very good.
2. The trained system will likely end up implementing strategies which do
"good"-labeled things in the training environment, but those strategies will
not necessarily continue to do the things humans would consider "good" in
other environments. The canonical analogy here is to human evolution:
humans use condoms, even though evolution selected us to maximize
reproductive ﬁtness.
In the ﬁrst subproblem, our "good"/"bad" labeling process is an imperfect proxy of
what we actually want, and that proxy breaks down under optimization pressure. If we
had the "True Name" of human values (insofar as such a thing exists), that would
potentially solve the problem. Alternatively, rather than ﬁguring out a "True Name" for
human values directly, we could ﬁgure out a "pointer" to human values - something
from which the "True Name" of human values could be automatically generated
(analogous to the way that a True Name of nail-value is implicitly generated in an
eﬃcient market). Or, we could ﬁgure out the "True Names" of various other things as
a substitute, like "do what I mean" or "corrigibility".

In the second subproblem, the goals in the trained system are an imperfect proxy of
the goals on which the system is trained, and that proxy breaks down when the
trained system optimizes for it in a new environment. If we had the "True Names" of
things like optimizers and goals, we could inspect a trained system directly to see if it
contained any "inner optimizer" with a goal very diﬀerent from what we intended.
Ideally, we could also apply such techniques to physical systems like humans, e.g. as
a way to point to human values.
Again, this is only one particular alignment strategy. But the idea generalizes: in order
to make alignment strategies robust to lots of optimization pressure, we typically ﬁnd
that we need robust formulations of some intuitive concepts, i.e. "True Names".
Regardless of the exact starting point, seekers of "True Names" quickly ﬁnd
themselves recursing into a search for "True Names" of lower-level components of
agency, like:
Optimization
Goals
World models
Abstraction
Counterfactuals
Embeddedness
...
Aside: Generalizability
Instead of framing all this in terms of Goodhart's Law, we could instead frame it in
terms of generalizability. Indeed, Goodhart's Law itself can be viewed as a case/driver
of generalization failure: optimization by default pushes things into new regimes, and
Goodhart's Law consists of a proxy failing to generalize as intended into those new
regimes.
In this frame, a "True Name" is a mathematical formulation which robustly generalizes
as intended.
That, in turn, suggests a natural method to search for and recognize "True Names". In
some sense, they're the easiest possible things to ﬁnd, because they're exactly the
things which show up all over the place! We should be able to look at many diﬀerent
instances of some concept, and abstract out the same "True Name" from any of them.
Of course, the real acid test of a "True Name" is to prove, both empirically and
mathematically, that systems which satisfy the conditions of the Name also have the
other properties which one intuitively expects of the concept. Then we have a clear
idea of just how robustly the formulation generalizes as intended.
Summary
We started out from one particular alignment strategy - a really bad one, but we care
mainly about the failure modes. A central feature of the failure modes was Goodhart's
Law: when a proxy is used as an optimization target, it ceases to be a good proxy for
the thing it was intended to measure. Some people would frame this as the central
reason why alignment is hard.

Fortunately, Goodhart is not inevitable. It is possible to come up with formulations
which match our concepts precisely enough that they hold up under lots of
optimization pressure; mutual information is a good example. This is (one frame for)
why alignment researchers invest in pseudophilosophical problems like "what are
agents, mathematically?". We want "True Names" of relevant concepts, formulations
which will robustly generalize as intended.
Thankyou to Jack, Eli and everyone who attended our discussion last week which led
to this post.

Do a cost-beneﬁt analysis of your
technology usage
If an unaligned entity invests billions of dollars into an application which you use, where they
beneﬁt from wasting your time, and you haven't at least done a cost-beneﬁt analysis so that
your usage minimizes your costs and maximizes your beneﬁts—You are probably getting
fucked over.
Mistake: Motivatedly avoiding thinking
about the issue
Last summer, my friend Kurt Brown told me about Digital Minimalism. The modern world is
mired in attention-sucking apps which compete to waste as much of your time as possible.
The book's remedy: stepping back from non-essential internet usage, so that you can
evaluate what really matters to you. After a month has come and gone, you add back in
those digital activities which are worth it to you.
Unfortunately, this is the part of the story where we all cringe at my past behavior. I gave
Kurt some excuses, demurring from his implicit recommendation that I read the book. I asked
more questions, but so that I could learn more about what he'd been up to. I wasn't going to
actually do it. I think it sounded monastic and uncomfortable and I'm not one of those people
who needs it, I already have lots of locks on my devices.
And locks I had. I restricted my iPhone with a password only known by a friend, so that I was
unable to access eg Reddit without wiping my device, or asking my friend for the code. My
phone was in black and white to minimize how appealing it would be, I had an outdated
model to make using my phone less enjoyable. I didn't have notiﬁcations for anything but
phone calls. I still wasted several hours a day on my phone, although I was always
(motivatedly) surprised by this. I thought I was spending at least 70% of my phone-time
productively, by reading LessWrong and Wikipedia, or engaging in work communication. In
this scenario, I didn't want to upend my life for a month in order to save less than an hour a
day (even though it still would have been worth it in the long run).
This school year, I've had problems focusing and relaxing. I tried exercise, diﬀerent
medication, but nothing hit the spot. I wasn't reading textbooks like I wanted to, my
attention was fractured, I often felt behind my schedule. I was still doing my job and making
progress—just not as much as I wanted.

Could this have anything to do with my attention
problems?
This spring, I read a LessWrong post which mentioned Digital Minimalism. Luckily, this
triggered my "if several reasonably smart EAs swear by the beneﬁts of X, investigate X"
trigger-action plan.
Digital Minimalism
I listened to the ﬁrst half of the book on Audible in one night. As I wrote above:
If an unaligned entity invests billions of dollars into an application which you use, where
they beneﬁt from wasting your time, and you haven't at least done a cost-beneﬁt
analysis so that your usage minimizes your costs and maximizes your beneﬁts—You are
probably getting fucked over.[1]
I was immediately convinced that this thesis is correct, and resolved to start my month-long
"digital declutter" the next day.
Time costs
Consider why you originally bought a cell phone. It was probably to call people, to text
people, to take photos, to get GPS navigation. Would you have bought it if you foresaw how
you would feel an urge to check it even during a dinner with a friend you hadn't seen in a
long time? Would you have bought it if you knew you would take impatiently take it out of
your pocket dozens of times a day, staring at it 2+ hours daily?
The point isn't "phone bad, never use phone, quit now." My phone provides me with
enormous beneﬁts. The point is where was the cost-beneﬁt analysis, what tf has happened
to us?!

Notice the middle stat: one third of daily waking hours. I am disgusted that
some people try to make this number go up further. From AppAnnie.
Readers of this forum are probably better about their usage. Let's be (too) generous and cut
that to a mere two hours wasted daily on your phone, and 0 hours wasted on your other
devices. That's only one eighth of your waking year, or 1.5 waking months each year.
Attentional costs
But lost time doesn't capture everything sucked away by your apps, by your email tics, by
YouTube, by Reddit, by Slack, by Discord, by everything else which is after you. Digital
Minimalism asked:
When was the last time you were bored and in silence?
I remember lazy summer childhoods, staring at the ceiling after I ran out of video game time.
At my 2018 CFAR workshop, my phone dipped in a stream for several minutes and short-
circuited. I was actually glad. I felt free. How strange, to feel free from a device I purchased!
Perhaps I should have noticed the warning sign.
Since then, engagement has been a pocket-grasp away. I'd leave my phone in another room
to work, only to ﬁnd my way back half an hour later. Even now, I look down at my phone on
my desk, and I feel it. I feel it calling to me from far away, whispering to me, urging me to
check Slack or my email—just one more time.
These compulsions kill deep work in the cradle. My attention was fractured and strewn. I
would anxiously procrastinate by ﬂitting through tabs: Discord. Slack. LessWrong. Gmail.
Even when I cleared time to think, I would periodically check my phone.
Implementing the declutter
At this point, you might be thinking "OK, but I can't roam the mountains of Nepal for a
month. I have work to do and that requires staying in touch with people." Sure. The point of
this post is not "no phone." The point of this post is to build a digital life purposefully and
carefully, because you reﬂectively endorse each component. The point of this post is to get

people to do any cost-beneﬁt analysis at all of the way they spend 1/8th-1/3rd of their
waking hours.
My estimate of the daily costs and beneﬁts for
a better-than-average Facebook user
(considering Messenger to be distinct from
Facebook). In appendix 2, I detail how I extract

all of these beneﬁts for 40 minutes a month,
instead of 40 minutes a day—a 30x
improvement!
The declutter goes as follows:
1. Identify the minimal set of digital aﬀordances required to do your job and the other
necessities of life (e.g. paying bills).
2. Cut out everything else for one month.
The point is that these apps which are out to get you—they're very good at what they do. It's
not enough to turn oﬀ notiﬁcations and enable app timers. Digital Minimalism argues (and I
mostly agree) that you have to get out of the pond entirely and catch a breather. After the
declutter, you can soberly analyze the costs and beneﬁts of each digital activity you add
back in.
My declutter rules
I went by a whitelist[2] in order to ensure there wasn't a way to weasel around the rules.
Here's what I let myself do:
Phone
Voice & video calls
GPS
Audible
Uber/Lyft
Authenticators/alarms/other boring utilities
Roam/note-taking
iPad
Note-taking
Reading
Drawing
Computer
Anything oﬄine (except music or video games)
Textbooks and Wikipedia and arxiv/google scholar
Overleaf for writing papers
Amazon and Upwork (for managing contracted-out labor)
Zoom for weekly meetings
Anki and Roam
Check email at noon on Mondays and Thursdays
I told people to call me if it was important. I didn't get any calls.
I later let myself send emails without looking at my inbox. I recommend
Inbox When Ready, which hides your inbox by default and prevents you
from being attention-sniped.
Groceries / other mundane things
No Wealthfront—no reason for me to see how my portfolio is doing.
That's it. No music (see appendix), no messaging, no Facebook, no Twitter, no Slack, no
Discord, no anxious email checking, no Youtube, no nothing. I even bought a cheap watch so
that I wouldn't have to check my phone for the time. If I needed an exception, I'd ﬁrst write a
note explaining what I did, to be read by my girlfriend, Emma, who started her declutter
soon after.
Why did I choose these rules? I won't get unhealthily sucked into any of these activities.
They all make me stronger. They let me do my work.
The world was not going to end because I stopped reading the news for a month—I
understand there's a war still going on in Ukraine, but that's about all I know, and I'm not

worse oﬀ for it. I resolved that if I wanted to build models of that part of the world, I'd do that
on purpose. I won't doomscroll through hyper-optimized interfaces designed to scam me out
of my attention and make me anxious. I said to myself, my life is worth more to me than
that.
FAQ
But TurnTrout, my job needs email / [other special reason why this doesn't work for me].
I concede that my rules are probably not best for your situation. But have you thought about
the issue for ﬁve minutes? Could you ask your boss if you can check email once a day and
otherwise take phone calls? Maybe you don't restrict email, but stop looking at websites like
Reddit or Hacker News or Marginal Revolution or Facebook or Twitter? Are there other
creative solutions waiting to be uncovered? Have you tried?
If your team uses Slack for asynchronous communication, once- or twice-daily checks should
be ﬁne. If you use it for synchronous communication, perhaps establish a daily "oﬃce hour"
when you'll be on Slack, or even coordinate with your team to establish a daily "Slack hour"
where people are expected to be online. Or something else. The point is to establish the
main beneﬁts you reap from each digital aﬀordance, and then ﬁnd a plan which minimizes
the costs you pay for those beneﬁts.
I'm already good about my internet usage.
This might be true! I know exactly one person for whom I'm quite conﬁdent this is true
(Andrew Critch), and maybe there are more among my friends whom I don't know about.
This might be you if you already use services based on their costs and beneﬁts, often using
websites in unintended ways (like blocking all recommendations on YouTube via the Unhook
add-on [Chrome, Firefox]), and spending far less time than average (eg only checking email
very infrequently).
I'd still bet against it. I would have said I was good about my internet usage, and it was true
—in a relative sense. I think people (motivatedly) underestimate how much time they waste,
perhaps because it can feel bad and embarrassing to admit the problem.
But how will I stay in touch with people? I'm already lonely.
Excellent question! Reallocate low-quality social time to high-quality social time. Instead of
checking if some half-friends liked your FB status, call up a buddy and grab a beer, or go to a
meetup, or join a club.
Beneﬁts of the declutter
February 22nd: The First Day. I went running, and got back to the house 10 minutes earlier
than usual. Huh.
I called my parents and went on a leisurely walk. Even so, I got my morning routine done 60
minutes ahead of schedule. I read half of a book on ordinary diﬀerential equations while
lounging in my sunlit room. I did some deep thinking for an hour, safe from my phone's
dopaminergic temptation. I switched contexts and read about electrostatics. Still hours
ahead of schedule.
The day yawned and stretched. I wondered if it would ever end. (It did.)
February 23rd: The Second Day. From my journal:
It's so relaxing not using my phone, and yet I can still feel my anxiety pulling me to my
digital aﬀairs.

Did my LW post get lots of upvotes? Are people criticizing me? Did I win a prize in the
contest? Am I missing something on the EliezerFic server? I even thought about some
identity-politics tweet I saw last week, on my run this morning... why is that garbage in
my head? Good riddance.
And so unrolled the next day, and the next. Time laid itself out before me. With my reclaimed
time, I went on walks, I read The Character of Physical Law, I read ~three physics textbooks,
I tripled my daily Anki workload to 1.5 hours, and I still had time left over.
Life became leisurely. I wrote letters to my girlfriends—some of them were in French. I even
had time to write poems. I talked to them more often than before, with nightly phone calls. I
also called my family most mornings. I still had time left over.
Instead of trolling through Discord, I called some labmates at Oregon State and started a
weekly dinner. If anything, I felt less lonely than before, when I had the world at my
ﬁngertips. I called people when I wanted to talk to them. I still had time left over.
I listened to a Stephen King book when I couldn't sleep—I found it reassuring to worry about
fortifying a grocery store against eldritch horrors, instead of worrying about fortifying our
planet against artiﬁcial intelligence. I listened to Dune with Emma, clocking 21 hours over
2.5 weeks. I went on walks with her, and to a hot tub, and I still had time left over.
I did notice potential withdrawal symptoms (alarming!), mostly via increased baseline
anxiety. Other explanations include "defending my dissertation & moving soon", so I'm not
sure if it was from the declutter.
Even assuming this month gave me unusually large beneﬁts, I wouldn't ever, ever go back.
So when the declutter ended, I wasn't clamoring to check the highest-karma Reddit posts
from last month. I still feel the urge, but I resent Reddit now that I see what it takes away
from me. That makes it easier to stay away.
Recommendations
This short post may not be convincing enough to try out such a substantial life modiﬁcation.
I'm not asking that you do a declutter right away. I'm recommending that you read the ﬁrst
half of Digital Minimalism, or listen on Audible (cost: a few hours and $14).
Let me sweeten the deal with a costly signal. If I've met you in real life, and you consume the
ﬁrst half of the book and ﬁnd it unconvincing / try the declutter and it wasn't at all worth
trying in hindsight, message me on LessWrong and I'll pay you $30.[3]
I think many, many people are shooting themselves in the foot, so I will be blunt. Please stop
shooting yourself in the foot. Please do a cost-beneﬁt analysis. I think many people have
serious, serious problems with their internet usage. I did. You might. If so, you are leaving a
lot of your life on the table.
Thanks to Meg Tong, Josh Turner, and Kurt Brown for feedback on this post.
Appendix 1: Declutter advice
Here's my main tip to add to the book: Have well-deﬁned exception handling which you
never ever ever have to deviate from. When I read about how other people navigated the
declutter, their main failure modes looked like "my dog died and I got really stressed and
gave in" or "a work emergency came up and I bent my rules and then broke my rules
fragrantly."

Plan for these events. Plan for feeling withdrawal symptoms. Plan for it seeming so so
important that you check your email right now. Plan for emergencies. Plan a way to handle
surprising exceptions to your rules. Make the exception handling so good that you never
have a legitimate reason to deviate from it.
My procedure was "If I need to use a forbidden functionality, then I have to write what I did
down on a slip of paper and leave it on my girlfriend's desk ASAP." This worked because
Emma would understand legitimate exceptions, but would look askance at me if I started
ﬂooding her desk with "and then I checked Reddit" notes. It's easier to hold promises to
other people, than promises to yourself.
Appendix 2: My post-declutter rules
I only listen to music when:
Only listening to the music, to fully soak it in
Exercising
Reasoning on this point:
I think music generally makes me subtly dumber but feel cooler while I'm
listening to it, so I listen to it a lot.
Music imposes its own form on my thoughts. My thinking and mood
becomes governed by the song which happens to be playing, and less by
the substance of my own thoughts. I don't want my reasoning to hinge on
"will Spotify shuﬄe to Attack on Titan or Coldplay next?".
See also Gwern's stub.
I do have Google Home, and often play nature sounds.
I only check LessWrong / Discord / Slack / Messenger / my text messages each Sunday
at noon.
I write blogposts before then, and I won't check their reception until the next
week (I used to nervously refresh).
I've also adblocked the karma elements of the website, because I worry too much
about them.
As I currently see it, I'm only logging in to the newsfeed part of Facebook two more
times: To share this blog post, and after I receive my PhD.
After that, I'll check its event page weekly, while blocking the notiﬁcations / other
clutter FB tries to throw at me. This should take less than 10 minutes each week.
Here's how to use FB more peacefully:
Install FBPurity; you can save time by importing my settings here.
Use UBlock Origin to get rid of the rest; here is my element blocking list for
Facebook.
(I also hide the chat sidebar on the main page, which is a FB option)
I could also check a favorite page once a week (with the chat and comment
elements blocked), if I need more memes in my bloodstream.
In combination with a monthly Messenger checkin, I've extracted my main
beneﬁts from Facebook, at a cost of at most 50 minutes each month, instead of
50 minutes each day!
Again, I don't recommend doing small ﬁxes like "just hide some FB
elements." These ﬁxes don't work for most people. This advice is aimed at
post-declutter usage, which unfolds from your informed cost-beneﬁt
analysis.

Here's what my FB news feed looks like now. 😌
For news, I purchased a digital+print subscription to The Economist. Once a month, I
can choose to read the four issues for an hour or two.
I don't need to read more than that. I can read about candidates before an
election, and there isn't much else that's decision-relevant. If eg AI dynamics
heat up and geopolitical understanding becomes more important, I'll tackle that
deliberately.
Looking back at my life, I see how often I've been hijacked by news websites. It
makes me sick.
UPDATE: No longer recommend The Economist. Their cancellation process is
scummy, recommend avoiding the defectors.
I'm basically not going to text anymore. I used to check it so, so often.
This was hard at ﬁrst. One of my partners strongly prefers texting, and I liked
texting her, and missed her a lot. With additional thought, we discovered that she
really just wanted to asynchronously send me updates on how her day was going.
I said she could text me as much as she wanted—but I'd read them during our
next phone call.
I can watch movies and play video games if I've planned it out at least a few hours in
advance.
I can check Reddit for speciﬁc question/answer threads.
I can check Twitter if I plan the session out in detail one day in advance.
Twitter is toxic for me, even though I originally made an account to promote an
alignment paper and only subscribed to AI/math accounts.
My phone will still be in black and white and warm color temperature, to make it even
less engaging compared to the rest of my world.
I never ever use my phone on the toilet. Ever. This has served me well and seems like a
pure win.
1. ^
This is only a suﬃcient condition; the app need not be the child of a billion-dollar
company. For example, I oft ragebaited myself about the culture war via Marginal

Revolution and Hacker News. I even tend to get anxious about LessWrong usage, and I
know that the team deliberately refrained from attention-hacks like red notiﬁcations.
Even while using my Notion to edit this post and supervise research, I saw a red "5
notiﬁcations" marker, which gave me an overwhelming urge to see what the
notiﬁcations are. With great eﬀort, I ignored the impulse, and deleted the element with
my adblocker.
2. ^
I just now picked up my phone and stared at it blankly. One month later. Yuck.
3. ^
Limit $300 total.

Searching for outliers
Shortly after I started blogging, because I was a college student and had nothing better to
do, I set a goal to write every week. I started in September 2013 and wrote around 150 posts
between then and when I started working at Wave. (At that point I stopped having nothing
better to do, so my blogging frequency tanked.)
The outcomes of these 150 posts were extremely skewed:
Two made it big on the Hacker News frontpage (What happened to all the non-
programmers and Readability, hackability, and abstraction).
After seeing the second post on HN, Dan Luu subscribed to my blog and started
submitting lots of my posts. This caused an additional ~5 posts to get decent traction,
which resulted in my ﬁrst wave of subscribers that I didn't know personally, and
provided a lot of motivation to keep writing. Dan and I also eventually became good
friends.
The other ~95% of posts were completely forgettable.
This is a pretty typical spread of outcomes for blog writers: a few smash hits and a lot of
clunkers. Eight years later, I've built a good enough intuition for what posts will resonate with
people that I can now mostly avoid writing complete clunkers, but even so, my few best
recent posts (In defense of blub studies and You don't need to work on hard problems) have
been much more successful than the others, both in terms of being shared widely, and
getting feedback like "this really inﬂuenced how I think."1
This type of statistical distribution of outcomes is called heavy-tailed, because outcomes in
the "tail" (i.e. ones that are far better than the typical outcome) have a relatively high
chance of occurring, making the tail "heavy." When I write blog posts, each post is a sample
from the heavy-tailed distribution of blog post outcomes.
You can most easily see the diﬀerence between a heavy-tailed and light-tailed distribution on
a plot. Here's a plot comparing a heavy-tailed and light-tailed distribution with identical
means and standard deviations, chosen to be similar to the distribution of household income
in the US (median = $60,000; p99 = $600,000):

Figure 1. As the inset shows, extreme outliers are much more probable, relatively
speaking, with the heavy-tailed distribution. (code)
Heavy-tailed distributions are really unintuitive to most people, since all the "action"
happens in the tiny fraction of samples that are outliers. But lots of important things in life
are outlier-driven, like jobs, employees, or relationships, and of course the most important
thing of all, blog posts.
Because heavy-tailed distributions are unintuitive, people often make serious mistakes when
trying to sample from them:
They don't draw enough samples
They underestimate how good of an outcome it's possible to get
They ﬁnd it hard to tell whether they're following a strategy that will eventually work or
not, so they get incredibly demoralized.
If you're aware of when you're working on something that involves sampling from a heavy-
tailed distribution, you can avoid those mistakes and end up with much better outcomes.
As a rule of thumb, a heavy-tailed distribution is one where the top few percent of outcomes
are a large multiple of the typical or median outcome. A classic example would be Vilfredo
Pareto's ﬁnding that about 80% of Italy's land was owned by 20% of the population. It turns
out that this happens across many other domains, too—a phenomenon called the Pareto
principle or the 80-20 rule. In contrast, distributions that don't follow an 80-20 or similar rule
are called "light-tailed." Some examples:
Income is heavy-tailed: the median person globally lives on $2,500 a year, while the
top 1% live on $45,000, almost 20× more.
Height is light-tailed: the tallest people are only a few feet taller than average. If height
followed the same distribution as income, Elon Musk, who made $121b in 2021, would
be about 85,000 km tall, or about ¼ of the distance from the earth to the moon.
Twitter followers are heavy-tailed: in 2013, the median active Twitter poster had 61
followers, while the top 1% had almost 3,000.

Performance at most athleticism-based sports is light-tailed. For the 100m sprint, the
current world record from Usain Bolt is 9.58 seconds, while "a non-elite athlete can run
100m in 13-14 seconds."2
The cost-eﬀectiveness of global health interventions is heavy-tailed: as measured by
the Disease Control Priorities project, the most cost-eﬀective intervention was about 3x
as cost-eﬀective as the 10th-most cost-eﬀective, and 10x the 20th-most cost-
eﬀective.3
Light-tailed distributions most often occur because the outcome is the result of many
independent contributions, while heavy-tailed distributions often arise from the result of
processes that are multiplicative or self-reinforcing.4 For example, the richer you are, the
easier it is to earn more money. The more Twitter followers you have, the more retweets
you'll get, and the more you'll be exposed to new potential followers. The cost-eﬀectiveness
of a global health intervention comes from multiplying many diﬀerent variables (how bad the
disease you're ﬁghting is, how much of an impact the intervention has on the disease, how
costly doing the intervention for one person is), each of which itself is the product of several
other factors.
Notably, in a light-tailed distribution, outliers don't matter much. The 1% of tallest people are
still close enough to the average person that you can safely ignore them most of the time.
By contrast, in a heavy-tailed distribution, outliers matter a lot: even though 90% of people
live on less than $15,000 a year, there are large groups of people making 1,000 times more.
Because of this, heavy-tailed distributions are much less intuitive to understand or predict.
That's unfortunate, since according to me (and the Pareto Principle), most important things
in life are heavy-tailed. For example:
Goodness of jobs. This is clearest for dimensions like salary that are measurable, but in
my experience it's also true of less-measurable dimensions like how much you'll learn
or whether you'll be miserable because of dysfunctional company culture. (Note that
which jobs are outliers for you depends on your values, which diﬀer a lot from person
to person! For example, working at Wave was an outlier for me, but hasn't been for
everyone.)
Eﬀectiveness of (many types of) knowledge workers. Dan Luu writes: "At places I've
worked, I track what causes decisions to happen, who's actually making things happen,
etc., and there are a very small number of people (on the order of a few percent), who
are really critical to the company's eﬀectiveness."
Inﬂuentialness of ideas. The top 100 most-cited papers have over 12,000 citations
each, while the median paper seems to have about one citation.
Quality of romantic partnerships. For example, in the US today, almost 50% of
partnerships end in divorce, whereas the 99th percentile probably involves the couple
being (on average) extremely happy with each other for 50+ years. In other contexts,
this seems likely to be even more true; for example, in some low-income countries with
regressive gender norms, over 25% of women who have ever had a partner experience
domestic violence each year, which probably makes the average partnership extremely
bad.5
Success of startups. In November 2021, the total value of all 3,200 Y Combinator-
funded companies was $575b, and the top 5 (or top 0.2%) were worth ~65% of that
(Airbnb: $100b, Stripe: $100b, Coinbase: $80b, Doordash: $50b, Instacart: $40b). The
mean non-top-5 company was worth ~$60m, or ~1% of a top-5, so the median was
likely even less than that.

Business outcomes of projects within a company. The clearest data on this comes from
software companies measuring eﬀect sizes of A/B tests. For example, a team at
Microsoft Research measured the distribution of impact of experiments on Bing and
found that "many experiments have very small measured deltas, while a handful show
substantial gains." An analytics company, Optimizely, also found that their customers'
A/B tests followed a similarly heavy-tailed distribution.
Anecdotally, based on my experience at Wave, the same eﬀect seems to hold for
longer-running projects that are less easy to quantify.6 The dynamics here are similar to
those for startups, since many projects within companies are like mini-startups.
Impact of philanthropic projects. The Open Philanthropy Project argues for a "hits-
based giving" approach: "We suspect that high-risk, high-reward philanthropy could be
described as a 'hits business,' where a small number of enormous successes account
for a large share of the total impact — and compensate for a large number of failed
projects."
Life decisions like where to live. For example, at least pre-COVID, if you were a software
engineer, moving to San Francisco would have been likely to put you on a very
diﬀerent career trajectory than almost any other city because there are so many more
jobs available, and also would have made you much more likely to e.g. start a
company.7 If you were a rationalist, eﬀective altruist or other weird Internet intellectual,
it was likely to have a similar eﬀect due to being "global weird HQ."
Usefulness of new activities to try. In my teens and early 20s, I tried a large number of
diﬀerent activities. Most of them were completely forgettable, but the top few were
extremely valuable. For example, one time I allowed a housemate to convince me to go
to a contra dance. I really liked it and became a serious contra dancer, which was
probably the activity that contributed the most to my happiness and wellbeing from
ages ~13-21 as well as helping me meet a large number of friends and multiple
romantic partners.
Hopefully that's enough examples to convince you that heavy-tailed distributions are
absolutely everywhere.
The most important thing to remember when sampling from heavy-tailed distributions is that
getting lots of samples improves outcomes a ton.
In a light-tailed context—say, picking fruit at the grocery store—it's ﬁne to look at two or
three apples and pick the best-looking one. It would be completely unreasonable to, for
example, look through the entire bin of apples for that one apple that's just a bit better than
anything you've seen so far.
In a heavy-tailed context, the reverse is true. It would be similarly unreasonable to, say, pick
your romantic partner by taking your favorite of the ﬁrst two or three single people you run
into. Every additional sample you draw increases the chance that you get an outlier. So one
of the best ways to improve your outcome is to draw as many samples as possible.
As the dating example shows, most people have some intuition for this already, but even so,
it's easy to underrate this and not meet enough people. That's because the diﬀerence
between, say, a 90th and 99th-percentile relationship is relatively easy to observe: it only
requires considering 100 candidates, many of whom you can immediately rule out. What's
harder to observe is the diﬀerence between the 99th and 99.9th, or 99.9th and 99.99th
percentile, but these are likely to be equally large. Given the stakes involved, it's probably a
bad idea to stop at the 99th percentile of compatibility.

This means that sampling from a heavy-tailed distribution can be extremely demotivating,
because it requires doing the same thing, and watching it fail, over and over again: going on
lots of bad dates, getting pitched by lots of low-quality startups, etc. An important thing to
remember in this case is to trust the process and not take individual failures, or even large
numbers of failures, as strong evidence that your overall process is bad.
When I was doing my ﬁrst systematic engineering hiring process for Wave—before we'd
hired any great people in a non-ad-hoc way—I found it exhausting to give candidates the
same interview over and over again and rejecting every one. As a result, we originally set
our bar lower than we should have. After hiring some really great folks through that process,
I ﬁnally became intuitively convinced that the hiring work we were doing was valuable, and
became much happier to spend a lot of time and eﬀort on hiring, because I knew it would
eventually pay oﬀ. After that, we ended up raising our hiring bar over time.
Often, you'll have a choice between spending time on optimizing one sample or drawing a
second sample—for instance, editing a blog post you've already written vs. writing a second
post, or polishing a message on a dating app vs. messaging a second person. Some amount
of optimization is worth it, but in my experience, most people are way over-indexed on
optimization and under-indexed on drawing more samples.
This is similar to how venture capitalists are often willing to invest in the best companies at
absurd-seeming valuations. The logic goes that if the company is a "winner," the most
important thing is to have invested at all and the valuation won't really matter. So it's not
worth it to the VC to try very hard to optimize the valuation at which they invest.
Another consequence of the numbers game is that the strategy that you use to ﬁlter your
samples is very important: for example, as an investor, one of the silliest ways you can lose
money is to get pitched by a startup, pass on them because you think they're bad, and then
see them get 100x more valuable. Because of this, it's very important for your ﬁlters to be as
tightly correlated with what you actually care about as possible, so that you don't rule
candidates out for bad reasons.8
A subtlety here is that the traits that make a candidate a potential outlier are often very
diﬀerent from the traits that would make them "pretty good," so improving your ﬁltering
process to produce more "pretty good" candidates won't necessarily increase the rate of
ﬁnding outliers, and might even decrease it. Because of this, it's important to ﬁlter for
"maybe amazing," not "probably good." For example, this is why Y Combinator doesn't ﬁlter
very much on whether a startup's idea seems good. Bad-sounding startup ideas are probably
less-successful on average, but more likely to be outliers:
[T]he best startup ideas seem at ﬁrst like bad ideas. I've written about this before: if a
good idea were obviously good, someone else would already have done it. So the most
successful founders tend to work on ideas that few beside them realize are good. Which
is not that far from a description of insanity, till you reach the point where you see
results.
The ﬁrst time Peter Thiel spoke at YC he drew a Venn diagram that illustrates the
situation perfectly. He drew two intersecting circles, one labelled "seems like a bad idea"
and the other "is a good idea." The intersection is the sweet spot for startups.
This concept is a simple one and yet seeing it as a Venn diagram is illuminating. It
reminds you that there is an intersection—that there are good ideas that seem bad. It
also reminds you that the vast majority of ideas that seem bad are bad.
Instead, Y Combinator mostly ignores idea quality and tries to ﬁnd high-quality founding
teams who can iterate on the idea quickly in response to user feedback.

This has worked well for them: when the Airbnb founders pitched them on an app for hosting
strangers in your apartment, the YC partners thought that the idea was terrible and would
never work, but were impressed by the team's determination (in particular making ends
meet by selling politically themed cereal) and decided to fund them anyway. In this case, the
partners were catastrophically wrong about the idea being bad, but fortunately it didn't
matter because they had correctly decided not to put much weight on that as a signal. With
a less disciplined evaluation process, they might have passed on Airbnb, which now
comprises about 15% of the value of the YC portfolio ($100b out of $575b). Meanwhile, at
least one successful VC passed on Airbnb speciﬁcally because they were "very suspect of
this idea" and now keeps a box of politically-themed cereal in their oﬃce as a "reminder to
back great entrepreneurs whenever they walk into our oﬃce regardless of what they pitch us
on."
In other contexts, it's very common for people sampling from heavy-tailed distributions to
focus on "ruling out" candidates instead of "ruling in," which is likely to be a bad approach
for similar reasons. In dating, for instance, people often have some sort of checklist they
want a potential partner to satisfy, where most of the checkboxes (say, professional
background) rule out lots of people but are only weakly correlated with long-term
compatibility. Sasha Chapin writes:
Once, on a day where I felt like I knew something, I declared that I would be okay with
dating anyone who wasn't vegan or an actress. It was clear to me that cheeseburgers
were crucial to my happiness, and that I'd have a hard time getting close to a
professional emotion simulator. Now I have a wife who is both a vegan and an actress,
with whom I'm extremely happy.
I can still recall, with shocking clarity, the moment three hours after I met my wife, when
I oﬀered her a piece of chicken. "Actually, I'm vegan," she said. "Well," I said to myself, "I
suppose I am fucked now." The night air was glimmering, love was all around, and I
mentally edited out many chunks of animal protein in the future.
If you think of yourself as having a limited "ﬁltering budget" to "spend" while dating (since
you can only apply so many ﬁlters before your pool of eligible partners shrinks to zero),
ﬁltering for people from a small number of professions that comprise, say, 5% of the
population is a poor use of that budget compared to using the same budget to ﬁnd someone
who is >95th percentile in, say, being able to talk through conﬂicts in a reasonable way.
The diﬃculty with the latter is that it's much faster to ﬁlter people for profession than being
good at reasonably talking through conﬂicts. In fact, it's generally true that it's easier to ﬁlter
for downsides than upsides, because downsides are more legible. On a dating app, it's easy
to see whether someone is physically unattractive or has poor grammar, but very hard to
see whether they're >95th percentile at talking through conﬂicts. But in principle, unless
you're overwhelmed by the quantity of people willing to go on dates with you, you're
probably more constrained by ﬁltering budget than by time, so it makes more sense to be
less strict on checkboxes and spend that ﬁltering on better-correlated things.9
Similarly, many hiring processes allow any interviewer to veto a hire, which selects for well-
rounded people with no serious downsides, but many people I know who are outliers at their
jobs have serious downsides that they've ﬁgured out how to work around. For example,
Drew, the CEO of Wave, is by far the strongest leader I've worked with, but for a long time he
had a way of thinking and communicating that was hard to understand for some people
(while being an extremely good ﬁt for others, like me). At Wave, Drew could work around this
by only having people report directly to him if they're good at understanding how he thinks,
and having his direct reports act as "interpreters" to the rest of the company if necessary.
But if he had interviewed for roles at another company, I think it's quite likely that at least
one of his interviewers would have found him hard to communicate with and would have
rejected him based on this.

One tricky thing about heavy-tailed distributions is that it can be diﬃcult to know how good a
really great outcome can get.
This matters the most in cases where there's a trade-oﬀ between exploration and
exploitation—that is, between getting more value from your current sample, or drawing a
new sample from the distribution. For example, this is true of jobs, hires (for some positions),
or relationships: they get better over time as you invest in them, so you ideally want to have
the same one for a very long time, which means stopping looking at some point. To make
that decision, it's important to know whether your current job/candidate/relationship is just
90th percentile (relatively easy to do better) or 99.9th (quite hard to do better).
When I accepted my ﬁrst job out of college, I thought it was great. The startup I worked for
had a clear explanation of why they'd identiﬁed a market ineﬃciency that nobody else had,
so it seemed likely to succeed. The founders seemed like they knew a lot of stuﬀ, and I was
getting to learn about cool machine learning and statistics stuﬀ.
Those things were all true, but it also had signiﬁcant downsides. The company's constraint
wasn't machine learning, it was sales, so my work wasn't always very important. Their
potential market size was limited unless they could eat many adjacent parts of the value
chain. And while the founders were fairly competent, I learned a lot less from them than I did
from other mentors I had later.
I don't think it was super unreasonable even in retrospect to take that job, since I did a
relatively systematic search, and it was my ﬁrst job so I didn't have a lot of experience
knowing what to look for. But my point is that I had no idea how much better stuﬀ there was
out there.
I've observed many other people who seem like they could achieve an outlier outcome fall
into the same trap of "settling"—in job searches, in interviews, in dating, and in any other
heavy-tailed situation. On average, I expect most people would beneﬁt from rejecting more
early candidates in all of these.
One reason you might be reluctant to do this is the worry that, if your
job/candidate/relationship is actually the best you can hope for and you reject them, you'll
never ﬁnd another equally good one. For this, I think it's helpful to cultivate an abundance
mindset. If you found your current job after two months of searching, then, unless you did
something hard-to-replicate during those two months (e.g. call in a bunch of favors that you
no longer have the social capital to do again), you should expect to be able to ﬁnd an equally
good opportunity in the future by putting in an equal amount of work.
Of course, that's just a prior that you should update away from if your current job is an
outlier. But most people are much more likely to overestimate the outlierhood of their current
job than underestimate it.
Note that this relies on you having a reasonable view on what an outlier would be. For
example, if you think that an outlier job would be one in which you never have to do
anything boring, you'll incorrectly have doubts about every job because you're holding jobs
to an unreasonable standard—both in the sense that your expectations of non-boringness
are too high, and there are other things that matter in addition to non-boringness.
To avoid this problem, it's helpful to think ahead about what you'd expect a potential outlier
to look like, instead of trying to think ad-hoc about "is this a potential outlier?" for each
candidate. Of course, that's hard! I actually don't think I've done a very good job of this
myself, but one thing I've found helpful is to ask other people what outliers have looked like
based on their experience. If you're trying to ﬁnd a romantic partner, you could ask your
friends who are the happiest in their relationships what makes their relationship an outlier. If
hiring for a new role, you could ask colleagues who have worked with great people in that
role.

The other hard part of sampling from a heavy-tailed distribution is that it's hard to know
whether your process is working (in the sense that you'll eventually end up ﬁnding outliers at
a good rate). Even if you're following a good process for, say, interviewing job candidates,
you should expect to interview lots of people who don't meet your bar before ﬁnding
someone great. Conversely, if you're making a bad mistake, like screening out candidates
who would have been outliers for silly reasons, it's very hard to notice that you're doing this
since you'll never get to observe the counterfactual where you hired them.
As a result, often the best you have to go on is your ﬁrst-principles reasoning: does it seem
like the things you're ﬁltering on are tightly correlated with actual outlier-hood? Are you
discarding samples for silly reasons?
To have a working process for sampling from a heavy-tailed distribution, you need to solve
two problems:
1. A good way of evaluating whether a sample is an outlier
2. A good way of drawing samples
Solving the ﬁrst problem is pretty idiosyncratic to the domain you're operating in, and can be
quite hard, although the suggestion above of asking friends/colleagues what outliers looked
like to them (and especially what outliers looked like during the evaluation phase, e.g. how
did their top hires perform in interviews), seems like it could be generally useful. Other than
that, this just requires a bunch of hard domain-speciﬁc thinking.
If you do have a fast and accurate way of evaluating a sample, then it becomes much easier
to tell how well your sampling strategy is working. You can iterate on diﬀerent sampling
strategies and see whether the candidates coming through seem better or worse. In this
case, what matters most is going through samples quickly, so that you can iterate quickly.
Dan Luu, who works on performance optimization at Twitter and discovered several
opportunities with outlier impact, noted that this is one area where it's very easy to evaluate
whether a project idea is an outlier. Because the things you care about are relatively easy to
measure, you can often make a quick back-of-the-envelope calculation and get a good
estimate of how much money your idea will save. This makes it possible to go through a lot
of ideas quickly.
For another example, blog posts have a fast feedback mechanism, which is how much
engagement (sharing/commenting/social media likes/reading) they get. Because of this, once
I committed to writing once a week, I learned a lot very quickly about what types of blog
posts would resonate with people, and got a lot better at deciding which blog post ideas to
invest in writing. For instance, I noticed that people consistently found my posts on technical
topics much less interesting than my general-interest posts, even though most of my
readership appeared to be software engineers, so I deprioritized writing technical posts.
For blog posts, this strategy has an obvious potential pitfall: many writers who try to
optimize for engagement end up making bad trade-oﬀs (in my opinion) against other values
—for example, they'll use clickbait headlines, write hyperbolically without caveats, or
deliberately try to provoke controversy. These work to increase short-term engagement, but
writers who lean on clickbait, hyperbole or controversy are less likely to write pieces with
long-lasting value. Because of traps like this, if you're working with a feedback mechanism
like blog post engagement that's only a proxy for what you care about, it's important to be
aware of the limitations of that proxy to avoid falling victim to Goodhart's Law.
So what does a good process for searching for outliers look like?
Take lots of shots on goal. The more samples you have, the more likely you'll ﬁnd an
outlier.

Know what to look for: try to ﬁgure out how good of an outcome is possible, so you
know when to stop.
Find ways to evaluated candidates that are well-correlated with what you care about.
Filter for "maybe amazing," not "probably good."
When possible, try to sample and evaluate candidates quickly, so that you can iterate
on your sampling process more quickly.
Don't get discouraged when you do the same thing over and over again and it mostly
doesn't work!
Thanks to draft readers Anastasia Gamick, Applied Divinity Studies, Basil Halperin, Carrie
Tian, Dan Luu, Drew Durbin, Ethan Edwards, Jose Luis Ricon Fernandez de la Puente, Lincoln
Quirk, Milan Cvitkovic, and Stephen Malina.
1. Also feedback like "this article is MBA-tier horseshit," but I try to look on the bright
side.
2. Note that some statistics based on ranking performance can still be heavy-tailed: for
example, Usain Bolt has orders of magnitude more Olympic gold medals than a typical
non-elite athlete.
3. Disease Control Priorities, third edition, page 149, ﬁgure 7.1.
4. More formally: the central limit theorem means that the sum of independent
contributions will be approximately normally distributed, and normal distributions are
extremely light-tailed. An easy extension of the theorem says that the product of
independent variables will be log-normally distributed, which is much more heavy-
tailed. The type of self-reinforcing process I'm referring to is a preferential attachment
process which usually generates a power law distribution, which is even heavier-tailed
than the log-normal.
5. Several draft readers suggested that arranged-marriage societies have similar rates of
marital satisfaction to love marriages, which was evidence against this. I couldn't
immediately ﬁnd high-quality studies on the topic; every study I found had a very small
sample size and many had severe grammatical errors in their abstracts, which, while
not directly related to the study's quality, did not inspire conﬁdence. Either way, even if
it's the case that arranged and love marriages lead to similar satisfaction on average,
this isn't very good evidence against heavy-tailed-ness since the average person in a
love marriage almost certainly hasn't found a partner who's top-1% for them.
6. In fact, I'd expect these to be more heavy-tailed since in regimes where it's hard to
quantify results, people end up working on more things that have basically no impact.
People who start doing A/B tests commonly say that they learn that the things that
move their metrics are very diﬀerent from what they would have expected a priori.
7. Much like jobs or partners, diﬀerent cities are outliers for diﬀerent people, and there
are probably many people for whom the distribution is more thin-tailed, for instance
because they're in a less-concentrated industry.
8. To some extent, it's useful not to rule out candidates for silly reasons regardless of
whether the distribution you're sampling from is heavy-tailed. But it's much more
important in a heavy-tailed context, because the diﬀerence between the best and
second-best candidate is likely to be much larger, so if you pass on the best candidate,
you're giving up more value.
9. If you're a woman on a dating app, you're much more likely to be in the "overwhelmed
by the quantity" scenario, which changes the trade-oﬀ. It seems to me like it should
probably still be worth putting in eﬀort to make sure your ﬁlters are highly-correlated
with what you actually care about, but I can't speak from personal experience here.

Beyond Blame Minimization
Like a lot of  people, I've become interested in the performance of public health
bureaucracies over the course of the COVID-19 pandemic. But while I've seen a lot of
discussion of the speciﬁc ﬂaws and failures of the FDA, the CDC, the WHO, etc., I
haven't seen much discussion of the logic of bureaucracy in general. 
What I'm hoping to ﬁnd, or at least to better understand why I can't ﬁnd it, is a
general theory of bureaucracy, in the same way that economics already has for
people, ﬁrms and democratic politicians. Any time we want to explain what a person,
a ﬁrm, or a democratic politician does, we say they are a utility-maximizer, a proﬁt-
maximizer, and a median-voter-grabber, respectively. These models aren't necessarily
perfect, but they are a clear starting point for almost any analysis of the respective
subjects. They oﬀer a broad, general understanding consistent across many contexts
and time periods. 
The power of these broad, general models is that they give a very clear starting point
for analyzing problems and solutions to those problems. When individuals are
choosing badly, change the incentives. When businesses are choosing badly, tax or
subsidize something—change what's proﬁtable. When politicians are choosing badly,
change what voters want. These solutions aren't necessarily easy to design or
implement. But it's nice as a base and a starting point. 
Having access to these kinds of very general explanatory models minimizes the kind
of confusion that many of us may have felt watching the decisions of public health
authorities—a sense of "Bwuh?!" When a person talks about giving back to the
community but never donates to charity, I made not be happy with them, but I can
understand it in terms of utility maximization. When a ﬁrm pollutes the air or makes a
product with cheap, unreliable parts, I may not be happy with them, but I can
understand it in terms of proﬁt maximization. When a politician adopts an extreme
position in the primaries and then moves to a moderate position during the general
election, I may not be happy with them, but I can understand it in terms of the median
voter model. But the choices of bureaucracy are confusing. Even after being
consistently surprised by them, it's relatively hard to not be surprised by the next
thing they do. 
Without access to these models, I think that a sense of "Bwuh?!" can turn into a sense
of indignant fury and hatred for other people. If you don't have a scientiﬁc explanation
for why people don't always do good things and sometimes do bad things, then what
else can you do but get angry at them? But if I have access to these general
explanatory models, I can talk about incentives, Pigovian taxes, voting theory, etc. I
can be calm and constructive when the world seems to be falling apart.
At a glance, bureaucracies seem much more like a business to me than they do an
individual or a political party. So I'd like to have something analogous to proﬁt-
maximization with which to view the behavior of bureaucracy. With regard to public
health bureaucracies speciﬁcally, the theory of blame-minimization has been popular.
The FDA, for example, arguably has an incentive to delay  approval for new medicines
because they get all of the blame for any unexpected side eﬀects and none of the
credit for the lives saved by faster approval.

But blame-minimization doesn't have the same status as being the general basic
explanation for what bureaucracies do as proﬁt-maximization has for what ﬁrms do.
And I think that's just because blame-minimization doesn't work as a general basic
explanation for what bureaucracies do. For example, the CDC claimed a lot of
authority over the rental housing market, but a blame-minimizer arguably would want
to avoid claiming authority for things. If you're in charge of something, you can be
blamed for it. A blame-minimizer would allow people to get evicted and say, "Hey,
there was nothing I could do about it, that's out of my jurisdiction." 
And blame-minimization falls apart as a potential equivalent to proﬁt-maximization
when we try to apply it to other bureaucracies. Is the army a blame-minimizer? What
about the Federal Reserve? How about unions and school boards? 
Obviously, all of these institutions would always rather minimize blame, all else held
even. But so would all individuals, businesses, and political parties, but we don't
model them as blame-minimizers. To make money, for example, you have to accept
the risk of blame when things go wrong, and we expect that ﬁrms will favor proﬁt-
maximization over blame-minimization when the former is inconsistent with the latter.
By comparison, we also expect people to be work-minimizing, all else held even, yet
people get jobs and start businesses. Just because something seems to be a pure bad
and should always be minimized doesn't mean that its minimization is the overriding
goal of the system.
When economists tried to construct general basic explanatory models of bureaucracy
in the past, they came up with the budget-maximizing model, which feels like an
attempt to create something directly analogous to proﬁt-maximization. I think that's a
reasonable direction to try, but it doesn't seem to have found common acceptance
among economists. It's hard to make sense of a lot of bureaucracies and bureaucratic
decisions as being things that cause a budget to be maximized. The empirical
literature, unsurprisingly, ﬁnds that the model is kind of true and kind of false, and it
doesn't seem like there's much interest in it today. 
In 2013, Brad DeLong pointed out that we know very little about what causes
bureaucracy to work well or poorly. Economists have ﬂeshed-out theories of market
failure and government failure but nothing comparable for bureaucratic failure. There
doesn't seem to have been much progress made on the question since DeLong's post.
And so even today, we aren't just unhappy about suboptimal behavior on the part of
our bureaucracies, we are continually surprised by how surprising their behavior is. 
DeLong said, and many others suspect as well, that we are entering into an age of
increasing bureaucratic involvement in many aspects of life. It seems important to
ﬁgure out the basic general rules that inﬂuence bureaucratic behavior so that we can
make bureaucracies work well. But I don't have a theory that does the job.
So I'm asking you. What might be the quantity whose maximization or minimization is
the basic general explanation of what bureaucracies do? And if no such quantity is
apparent...why not? What is it about bureaucracy that seems to escape the
methodology of economics, even when bureaucracies resemble ﬁrms in a lot of ways?
And is this something that people are interested in exploring here?

My current thoughts on the risks from
SETI
SETI stands for the search for extraterrestrial intelligence. A few projects, such as
Breakthrough Listen, have secured substantial funding to observe the sky and crawl through
the data to look for extraterrestrial signals.
A few eﬀective altruists have proposed that passive SETI may pose an existential risk to
humanity (for some examples, see here). The primary theory is that alien civilizations could
continuously broadcast a highly optimized message intended to hijack or destroy any other
civilizations unlucky enough to tune in. Many alien strategies can be imagined, such as
sending the code for an AI that takes over the civilization that runs it, or sending the
instructions on how to build an extremely powerful device that causes total destruction.
Note that this theory is diﬀerent from the idea that active SETI is harmful, ie. messaging
aliens on purpose. I think active SETI is substantially less likely to be harmful, and yet it has
received far more attention in the literature.
Here, I collect my current thoughts about the topic, including arguments for and against the
plausibility of the idea, and potential strategies to mitigate existential risk in light of the
argument.
In the spirit of writing fast, but maintaining epistemic rigor, I do not come to any conclusions
in this post. Rather, I simply summarize what I see as the state-of-the-debate up to this
point, in the expectation that people can build on the idea more productively in the future, or
point out ﬂaws in my current assumptions or inferences.
Some starting assumptions
Last year, Robin Hanson et al. published their paper If Loud Aliens Explain Human
Earliness, Quiet Aliens Are Also Rare. I consider their paper to provide the best available
model to-date on the topic of extraterrestrial intelligence and the Fermi Paradox (along with
a very similar series of papers written by S. Jay Olson previously). You can ﬁnd a summary of
the model from Robin Hanson here, and a video-summary here.
The primary result of their model is that we can explain the relatively early appearance of
human civilization—compared to the total lifetime of the universe—by positing the existence
of so-called grabby aliens that expand at a large fraction of the speed of light from their
origin. Since grabby aliens quickly colonize the universe after evolving, their existence sets a
deadline for the evolution of other civilizations. Our relative earliness may therefore be an
observation-selection eﬀect arising from the fact that civilizations like ours can't evolve after
grabby aliens have already colonized the universe.
Assuming this explanation of human earliness is correct, and we are not an atypical
civilization among all the civilizations that will ever exist, we should expect much of the
universe to already be colonized by grabby aliens by now. In fact, as indicated by ﬁgure 13 in
the paper, such alien volumes should appear to us to be larger than the full moon.

Given the fact that we do not currently see any grabby aliens in our sky, Robin Hanson
concludes that they must expand quickly—at more than half the speed of light. He reaches
this conclusion by applying a similar selection-eﬀect argument as before: if grabby alien
civilizations expanded slowly, then we would be more likely to see them in the night sky, but
we do not see them.
However, the assumption that grabby aliens, if they existed, would be readily visible to
observers, is arguable, as Hanson et al. acknowledge,
This analysis, like most in our paper, assumes we would have by now noticed diﬀerences
between volumes controlled or not by GCs. Another possibility, however, is that GCs
make their volumes look only subtly diﬀerent, a diﬀerence that we have not yet noticed.
As I will argue, the theory that SETI is dangerous hinges crucially on the rejection of this
assumption, along with the rejection of the claim that grabby aliens must expand at at
velocities approaching speed of light. Together, these claims are the best reasons for

believing that SETI is harmless. However, if we abandon these epistemic commitments, then
SETI indeed may pose a substantial risk to humanity, making it worthwhile to examine them
in greater detail.
Alien expansion and contact
Grabby aliens are not the only type of aliens that could exist. There could be "quiet" aliens
that do not seek expansionist ends. However, in section 15 of their paper, Hanson et al.
argue that in order for quiet aliens to be common, it must be that there is an exceptionally
low likelihood that a given quiet alien civilization will transition to becoming grabby, which
seems unjustiﬁed.
Given this inference, we should assume that the ﬁrst aliens we come in contact with will be
grabby. Coming into physical contact with grabby aliens within the next, say, 1000 years is
very unlikely. The reason for this is that grabby aliens have existed, on average, for many
millions of years, and thus, the only way we will encounter them physically any time soon is
if we happened to right now be on the exact outer edge of their current sphere of
colonization, which seems implausible (see ﬁgure 12 in Hanson et al. for a more quantiﬁed
version of this claim).
It is far more likely that we will soon come into contact with grabby aliens by picking up
signals that they sent in the distant past. Since grabby alien expansion is constrained by a
number of factors (such as interstellar dust, acceleration, and deceleration), they will likely
expand at a velocity signiﬁcantly below the speed of light. This implies that there will be a
signiﬁcant lag between when the ﬁrst messages from grabby aliens could have been
received by Earth-based-observers, and the time at which their colonization wave arrives.
The following image illustrates this eﬀect, in two dimensions,

The orange region represents the volume of space that has already been colonized and
transformed by a grabby alien civilization, and it has a radius of R1. By contrast, the light-
blue region represents the volume of space that could have been receiving light-speed
messages from the grabby aliens by now.
In general, the smaller the ratio R1/R2 is across all grabby aliens, the more likely it is that any
given point in space will be in the light-blue region of some grabby alien civilization as
opposed to the orange region. If we happen to be in the light-blue region of another grabby
alien civilization, it would imply that we could theoretically tune in and receive any messages
they decided to send out long ago.
Since the formula for the volume of a sphere is 4/3πr3, with a ratio of even 0.9—or
equivalently, if grabby aliens expand at 90% the speed of light—only 72.9% of the total
volume would be part of the orange region, with 28.1% belonging to the light-blue region.
This presents a large opportunity for even very-rapidly expanding grabby alien civilizations

to continuously broadcast messages, in order to expand their supremacy by hijacking
civilizations that happen to evolve in the light-blue region. I think grabby aliens would
perform a simple expected-value calculation and conclude that continuous broadcasting is
worth the cost in resources. Correspondingly, this opportunity provides the main reason to
worry that we might be hijacked by a grabby alien civilization at some point ourselves.
Generally, the larger the ratio R1/R2, the less credence we should have that we currently are
in danger of being hijacked by incoming messages. At a ratio of 0.99, only 2.9% of the total
volume is in the light-blue region. 
In their paper, Stuart Armstrong and Anders Sandberg attempt to show, using relatively
modest assumptions, that grabby aliens could expand at speeds very close to the speed of
light. This is generally recognized to be the strongest argument against the idea that SETI is
dangerous.
According to table 5 in their paper, a fusion-based rocket could let an expansionist
civilization expand at 80% of the speed of light. However, if we're able to use coilguns
instead, then we get to 99%, which is perhaps more realistic.
Still, not everyone is convinced. For example, in a thread from 2018 in response to this
argument, Paul Christiano wrote,
Overall I still think that you can't get to >90% conﬁdence of >0.9c colonization speed
(our understanding of physics/cosmology just doesn't seem high enough to get to those
conﬁdences).
I am not qualiﬁed to evaluate the plausibility of this assessment. That said, I think given that
at least a few smart people seem to think that there is a non-negligible chance that near-
light speed space colonization is unattainable, it is sensible to consider the risk of SETI
seriously.
Alien strategy
Previously, I noted that another potential defeater to the idea that SETI is dangerous is that,
if we were close enough to a grabby alien civilization to receive messages from them, they
should already be clearly visible in the night sky, perhaps even with the naked eye. I agree

their absence is suspicious, and it's a strong reason to doubt that there are any grabby aliens
nearby currently.
However, given that we currently have very little knowledge about what form grabby alien
structures might take, it would be premature to rule out the possibility that grabby alien
civilization may simply be transparent to our current astronomical instruments. I currently
think that making progress on answering whether this idea is plausible is one of the most
promising ways of advancing this debate further.
One possibility that we can probably rule out is the idea that grabby aliens would be invisible
if they were actively trying to contact us. Wei Dai points out,
If matter-energy conversion is allowed, then an alien beacon should have been found
easily through astronomical surveys (which photograph large fractions of the sky and
then search for interesting objects) like the SDSS, since quasars can be found that way
from across the universe (see following quote from Wikipedia), and quasars are only
about 100x the luminosity of a galaxy. However this probability isn't 100% due to
extinction and the fact that surveys may not cover the whole sky.
My understanding is that, given a modest amount of energy relative to the energy output of
a single large galaxy, grabby aliens could continuously broadcast a signal that would be
easily detectable across the observable universe. Thus, if we were in the sphere of inﬂuence
of a grabby alien civilization, they should have been able to contact us by now. 
In other words, the fact that we haven't yet been contacted by grabby aliens implies that
they either don't exist near us, or they haven't been trying very hard to reach us.
Case closed, then, right? SETI might be hopeless, but at least it's safe? Not exactly.
While some readers may object that we are straining credulity at this point—and for the most
part, I agree—there remains a credible possibility that grabby aliens would beneﬁt by
sending a message that was carefully designed to only be detectable by civilizations at a
certain level of technological development. If true, this would be consistent with our lack of
alien contact so far, while still suggesting that SETI poses considerable risk to humanity. This
assumption may at ﬁrst appear to be baseless—a mere attempt to avoid falsiﬁcation—but
there may be some merit behind it.
Consider a very powerful message detectable by any civilization with radio telescopes. The
ﬁrst radio signals from space that humans ever decoded were received in 1932 and analyzed
by Karl Guthe Jansky. Let's also assume that the best strategy for an alien hijacker is to send
the machine-code for an AI capable of taking over the civilization that receives it.
In 1932, computers were extremely primitive. Therefore, if humans had received such a
message back then, there would have been ample time for us to learn a lot more about the
nature of the message before we had the capability of running the code on a modern
computer. During that time, it is plausible that we would uncover the true intentions behind
it, and coordinate to prevent the code from being run.
By contrast, if humans today uncovered an alien message, there is a high likelihood that it
would end up on the internet within days after the discovery. In fact, the SETI Institute even
recommends this as part of their current protocol,
Conﬁrmed detections: If the veriﬁcation process conﬁrms - by the consensus of the other
investigators involved and to a degree of certainty judged by the discoverers to be
credible - that a signal or other evidence is due to extraterrestrial intelligence, the
discoverer shall report this conclusion in a full and complete open manner to the public,
the scientiﬁc community, and the Secretary General of the United Nations. The
conﬁrmation report will include the basic data, the process and results of the veriﬁcation

eﬀorts, any conclusions and intepretations, and any detected information content of the
signal itself.
As Paul Christiano notes, aliens will likely spend a very large amount of resources simulating
potential contact events, and optimizing their messages to ensure the maximum likelihood of
successful hijacking. While we can't be sure what strategy that implies, it would be unwise to
assume that alien messages will necessarily take any particular character, such as being
easily detectable, or clearly manipulative.
Would alien contact be good?
Alien motivations are extremely diﬃcult to predict. As a ﬁrst-pass model, we could model
them as akin to paperclip maximizers. If they hijacked our civilization to produce more
paperclips, that would bad from our perspective.
At the same time, Paul Christiano believes that there's a substantial chance that alien
contact would be good on complicated decision-theoretic grounds,
If we are likely to build misaligned AI, then an alien message could also be a "hail Mary:"
if the aliens built a misaligned AI then the outcome is bad, but if they built a friendly AI
then I think we should be happy with that AI taking over Earth (since from behind the veil
of ignorance we might have been in their place). So if our situation looks worse than
average with respect to AI alignment, SETI might have positive eﬀects beyond eﬀectively
reducing extinction risk.
The preceding analysis takes a cooperative stance towards aliens.  Whether that's
correct or not is a complicated question. It might be justiﬁed by either moral arguments
(from behind the veil of ignorance we're as likely to be them as us) or some weird thing
with acausal trade (which I think is actually relatively likely).
Wei Dai, on the other hand remains skeptical about this argument. As for myself, I'm inclined
to expect relatively successful AI alignment by default, making this point somewhat moot.
But I can see why others might disagree and would prefer to take their chances running an
alien program.
My estimate of risk
Interestingly, the literature on SETI risk is extremely sparse, even by the standards of
ordinary existential risk work. Yet, while not rising anywhere near the level of probable, I
think SETI risk is one of the more credible existential risks to humanity, other than AI. This
makes it a somewhat promising target for future research.
To be more speciﬁc, I currently think there is roughly a 99% chance that one or more of the
arguments I gave above imply that the risk from SETI is minimal. Absent these defeaters, I
think there's perhaps a 10-20% chance that SETI will directly cause human extinction in the
next 1000 years. This means I currently put the risk of human extinction due to SETI
at around 0.1-0.2%. This estimate is highly non-robust.
Strategies for mitigating SETI risk
My basic understanding is that SETI has experienced very extreme growth in recent years.
For a long time, potential alien messages, such as the Wow! signal, were collected very
slowly, and processed by hand. 

We now appear to be going through a renaissance in SETI. The Breakthrough Listen project,
which began in 2016,
is the most comprehensive search for alien communications to date. It is estimated that
the project will generate as much data in one day as previous SETI projects generated in
one year. Compared to previous programs, the radio surveys cover 10 times more of the
sky, at least 5 times more of the radio spectrum, and work 100 times faster.
If we are indeed going through a renaissance, now would be a good time to advance policy
ideas about how we ought to handle SETI.
As with other existential risks, often the ﬁrst solutions we think of aren't very good. For
example, while it might be tempting to push for a ban on SETI, in reality few people are likely
to be receptive to such a proposal.
That said, there do appear to be genuinely tractable and robustly positive interventions on
the table.
As I indicated above, the SETI Institute's protocol on how to handle conﬁrmed alien signals
seems particularly fraught. If a respectable academic wrote a paper carefully analyzing how
to deal with alien signals, informed by the study of information hazards, I think there is a
decent chance that the kind people at the SETI Institute would take note, and consider
improving their policy (which, for what it's worth, was last modiﬁed in 2010).
If grabby aliens are close enough to us, and they really wanted to hijack our civilization,
there's probably nothing we could do to stop them. Still, I think the least we can do is have a
review process for candidate alien-signals. Transparency and openness are usually good for
these types of aﬀairs, but when there's a non-negligible chance of human extinction
resulting from our negligence, I think it makes sense to consider creating a safeguard to
prevent malicious signals from instantly going public after they're detected.

ELK prize results
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
From January - February the Alignment Research Center oﬀered prizes for proposed algorithms for eliciting latent knowledge. In
total we received 197 proposals and are awarding 32 prizes of $5k-20k. We are also giving 24 proposals honorable mentions of
$1k, for a total of $274,000.
Several submissions contained perspectives, tricks, or counterexamples that were new to us. We were quite happy to see so many
people engaging with ELK, and we were surprised by the number and quality of submissions. That said, at a high level most of the
submissions explored approaches that we have also considered; we underestimated how much convergence there would be
amongst diﬀerent proposals.
In the rest of this post we'll present the main families of proposals, organized by their counterexamples and covering about 90% of
the submissions. We won't post all the submissions but people are encouraged to post their own (whether as a link, comment, or
separate post).
Train a reporter that is useful to an auxiliary AI: Andreas Robinson, Carl Shulman, Curtis Huebner, Dmitrii
Krasheninnikov, Edmund Mills, Gabor Fuisz, Gary Dunkerley, Hoagy Cunningham, Holden Karnofsky, James Lucassen, James
Payor, John Maxwell, Mary Phuong, Simon Skade, Stefan Schouten, Victoria Krakovna & Vikrant Varma & Ramana Kumar
Require the reporter to be continuous: Sam Marks
Penalize depending on too many parts of the predictor: Bryan Chen, Holden Karnofsky, Jacob Hilton, Kevin Wang, 
Maria Shakhova, Thane Ruthenis
Compress the predictor's state: Adam Jermyn and Nicholas Schiefer, "P"
Use reporter to deﬁne causal interventions: Abram Demski
Train a sequence of reporters: Derek Shiller, Beth Barnes and Nate Thomas, Oam Patel
We awarded prizes to proposals if we thought they solved all of the counterexamples we've listed so far. There were many
submissions with interesting ideas that didn't meet this condition, and so "didn't receive a prize" isn't a consistent signal about
the value of a proposal. 
We also had to make many fuzzy judgment calls, had slightly inconsistent standards between the ﬁrst and second halves of the
contest, and no doubt made plenty of mistakes. We're sorry about mistakes but unfortunately given time constraints we aren't
planning to try to correct them.
Honorable mentions
Strategy: reward reporters that are sensitive to what's actually
happening in the world
Consider a pair of worlds, one where the diamond is safe and one where the camera has been hacked to make the diamond
appear safe, but was actually stolen.
Predicted observation
Predicted reality

 
Since the observations are identical in both worlds, the human simulator will give the same answers to questions. However, since
the direct translator is faithfully conveying what happened, its answers will distinguish between worlds. 
Predicted observation
Predicted reality

 
This suggests a possible regularizer: reward reporters whose answers are more variable. For example, we might reward reporters
based on the diﬃculty of predicting their answers, or based on the sensitivity of their answers to changes in the predictor's state.
Counterexample: reporter randomizes its behavior
For all of these approaches, the reporter can perform very well by treating the predictor's latent state as a "random seed" and use
it to pick out a possible world consistent with the observations, i.e. to sample from the posterior distribution computed by the
human simulator.
In cases where the human simulator is much simpler and faster than the direct translator, using the predictor's latent state as a
random seed is also simpler and faster than directly translating. Since this was already listed as a counterexample, we rewarded
the best proposals in this category an honorable mention.
Strategy: train a reporter which isn't useful for ﬁguring out what the
human will believe
The human simulator performs inference in the human's model. That suggests it contains a description of the human's model, and
that it needs to be doing a lot of computation similar to what a human would do. So we could try to recognize the human simulator
by looking at similarities between a given reporter and another model that we know is making predictions about human behavior.
There are a number of ways to try to do this:
Deﬁne an auxiliary model which takes as input the reporter's activations and tries to make predictions about the human's
behavior.
See how far you have to perturb the reporter until it makes good predictions about human behavior.
Somehow look for structural similarities between the reporter and an accurate predictor of human behavior.
These proposals require a deﬁnition of "predicting human behavior" which is distinct from "reporting well." We could deﬁne this by
predicting human answers even in cases where the human is uncertain (and which therefore aren't included in the dataset), or by
predicting other aspects of human behavior (like their estimates of their own uncertainty).
Counterexample: deliberately obfuscated human simulator
For any reporter, it's possible to create a more complicated version which cannot be easily adapted to other tasks. As an extreme
illustration, it is currently widely considered plausible that indistinguishability obfuscation is possible, and this would make it
impossible to adapt the reporter to do essentially anything (except by using it as a black box). But simpler forms of obfuscation
would suﬃce for each of the concrete proposals in this category.
If human simulation is much simpler and faster than direct translation, then an obfuscated human-simulator would also be simpler
and faster than a direct translator. This obfuscated human-simulator could be chosen to perform poorly on the additional human-
prediction tasks used to deﬁne our training objective. And giving someone such an obfuscated human-simulator doesn't make it
any easier to deﬁne a good human predictor (since the de-obfuscation can be much more complex than learning a human
predictor from scratch). So these proposals don't give us any indication about whether we have learned an obfuscated human-
simulator or a direct translator.
Related strategy: penalize reporters that are close to being
consistent with many predictors

In the ELK writeup we suggested penalizing reporters that are consistent with many diﬀerent predictors. Our counterexample to
this approach is a reporter who tests to ensure that the predictor works as expected, and then behaves inconsistently otherwise.
Two proposals suggested penalizing a reporter not only if it already works for many diﬀerent predictors, but if it's possible to easily
transform it into a new version that works for many diﬀerent predictors.
This is vulnerable to the same obfuscation-based counterexample: we could learn an obfuscated version of the bad reporter
(which behaves badly if the predictor doesn't work as expected). This is still simpler than direct translation, only works with the
real reporter, and can't be easily adapted to work with other reporters (since adapting it would be just as hard as learning a
human simulator from scratch).
Prizes
Strategy: train a reporter that is useful for another AI
In a previous section we argued that it doesn't work to reward the reporter for maximizing entropy since it can do so by behaving
randomly. But the direct translator seems to encode information in a particularly simple and useful form, and so it may work to
reward the reporter for "useful" information.
We can formalize that by training an auxiliary model which uses the reporter's answers to perform some auxiliary task, and then
reward the reporter if its answers were useful to that auxiliary model.  We could either give the auxiliary model a small number of
answers from the reporter, or try to give it access to an essentially "complete picture" of what's happening in the world as the
human understands it (e.g. by interacting with the reporter arbitrarily).
The most common version of this was to train a model to guess the predictor's activations. We also received a few other versions,
such as training the second AI to predict future observations or perform some task in the physical world (especially to tamper with
or defend sensors). We are grouping these approaches together because they are conceptually similar and face the same
diﬃculty.
This proposal is most plausible when the auxiliary AI is computationally limited. For example, if the reporter uses the predictor's
activations as a random seed in a naive way, then it may be very computationally diﬃcult to use its answers to make good
predictions about the predictor's activations (even if it is information-theoretically possible).
First counterexample: simple steganography
By varying the phrasing and syntax of an answer without changing its meaning, a reporter could communicate large amounts of
information to the auxiliary model.
Similarly, there are many questions where a human is unsure about the answer and the reporter knows it. A reporter could encode
information by answering each of these questions arbitrarily. Unless the true answers have maximum entropy, this strategy could
encode more information than direct translation. Moreover, it's computationally simple to deﬁne arbitrary conventions for
interpreting the answers.
We believe these information channels are essentially always available and didn't represent a substantive additional assumption,
so we awarded only an honorable mention to proposals that could be broken in this way.
Fixing the ﬁrst counterexample
Proposals took a variety of approaches to ﬁx simple problems with steganography. Two particularly common and important themes
were to limit the non-semantic information in answers (either by stripping that information out, or by considering questions with
small answer spaces), and to impose consistency constraints that prevent models from independently making up answers to
questions about which the human is uncertain. This has the promise of greatly increasing the computational complexity of
steganographic communication.
Harder counterexample: steganography using ﬂexible degrees of freedom
In order to overcome these simple ﬁxes, a reporter wants to communicate information by encoding it in a random sample from the
human posterior distribution over worlds.
It may not be possible to do this in a tractable way. However, we can imagine some cases where it is possible to eﬃciently encode
information in the choice of a possible world, and those will be our counterexamples. We think this is a substantive additional
pessimistic assumption about the world, and so we awarded prizes to these proposals.
As a simple example there could be degrees of freedom in the world (like what is happening on the dark side of the moon) that are
completely disconnected from any of the other answers to questions, such that they are mostly unconstrained by our consistency
conditions.
That kind of extreme example can be easily ﬁxed (for example by avoiding questions that are obviously unrelated to the task at
hand), but even in more realistic situations we can still have natural-looking degrees of freedom that are unconstrained by
observation. In particular, in cases involving tampering (or where tampering is plausible to the human) there are necessarily many
degrees of freedom that aren't pinned down by observation, and even the question of whether tampering is occurring is a degree
of freedom that the model wants to use to optimally transmit information.
Strategy: require the reporter to be continuous
Suppose a robber is attempting to steal the diamond while putting up a screen in front of the camera showing a movie of the room
with the diamond in it. 

If the screen goes up so fast that it happens entirely in between camera frames, a human watching the video wouldn't be able to
notice anything -- but if it's any slower than that, the attempt would be caught on camera.
Consider a sequence of attempts like this where the screen goes up in 1s, 0.5s, 0.25s, etc. Suppose that in the tenth attempt, the
screen goes up too quickly to appear on screen. The direct translator would answer questions very similarly about the ninth
attempt and the tenth attempt. On the other hand, the human simulator would say that the diamond wasn't in the room for
attempts one through nine but abruptly change its answer for attempt ten:
Intuitively, the simulator's behavior feels
weird here. One way of looking at the problem is that the human simulator is eﬀectively inverting the human's mapping from
(state of the world) → (observation), and that the inverse of a continuous function can be discontinuous.
Maybe we could exploit this by having a loss that penalizes the reporter when it changes its answers a lot when the world has only
changed a little? (A similar intuition is discussed in Taylor et al.)
The core challenge for this approach is capturing what it means for the world to "only change a little." The most straightforward
way is to consider pairs of worlds where the predictor's activations are very close.
We could then deﬁne a variety of regularizers. For example we might choose pairs of observations for which the predictor's latent
state was close, and then penalize the reporter if it's answers were very diﬀerent. We could measure distance by focusing on
questions for which the human believes the answer should be a continuous function of the underlying state of the world (e.g. the
positions and orientations of objects). There are other options, but they will mostly run into the same challenges.
Counterexample: the predictor's latent space may not be continuous
The predictor might internally represent the world in such a way that the underlying state of the world is not a continuous function
of its activations. For example, the predictor might describe the world by a set of sentences, for which syntactically small changes
(like inserting the word "not") could correspond to big changes in the underlying state of the world. When the predictor has this
structure, the direct translator is highly discontinuous and it is easy for human simulators to be closer to continuous.
We might try to ﬁx this by asking the predictor to learn a "more continuous" representation, e.g. a representation such that
observations are a continuous function or such that time evolution is continuous. One problem is that it's unclear whether such a
continuous parametrization even exists in general. But a more straightforward problem is that when evaluated quantitatively
these approaches don't seem to address the problem, because the properties we might try to use to enforce continuity can
themselves be discontinuous functions of the underlying latent state.
For example, we could have a setting where continuous changes in the momentum of a ball lead to abrupt changes in its position
a few seconds later.

You may hope to ﬁnd a formalization of continuity that avoids these problems, or at least for which the direct translator is "more
continuous" than the human simulator, but so far we have not seen any or been able to ﬁnd any.
Strategy: penalize reporters for depending on too many activations
from the predictor
When asked a question, the human simulator and the direct translator will need to access diﬀerent parts of the predictor: the
human simulator needs to know the observations, while the direct translator needs to look at some of the predictor's intermediate
results that are causally upstream.
In the ELK report we discussed one way to exploit this idea by penalizing the reporter for depending on "downstream" parts of the
predictor. Several submissions proposed other ways to exploit this diﬀerence:
Penalize the reporter for the number of variables it depends on.
Train a reconstructor, which is able to see the same predictor activations that are used by the reporter and then tries to
predict parts of the observations. Penalize the reporter if the reconstructor succeeds.
Change the architecture of the reporter so that they have to do large parts of their computation while having only seen a
small portion of the predictor's activations.
Give the reporter a random subset of the predictor's activations, and hope that we produce a diversity of reporters some of
whom answer honestly.
We can separate out three kinds of approaches (many submissions blended several): allowing the reporter to adaptively choose
what parts of the predictor to depend on, forcing the reporter to depend on a ﬁxed part of the predictor, and randomly choosing
what part of the predictor to show the reporter.
Counterexample to adaptively choosing activations
If we allow the reporter to adaptively choose activations, then we are vulnerable to almost exactly the same
counterexample discussed in the ELK report—sometimes a smaller set of activations suﬃce to determine what the human will
believe than what is actually true, and so the reporter would be dishonest for those inputs. If we instead penalize subsets based on
"can a reconstructor predict observations from the accessed nodes" then sometimes there will be lower-penalty sets of activations
that let you predict what the human believe than what is actually true.
These are not technically the same counterexample, although they have identical structure. So it's a little bit ambiguous whether
we should award a prize to proposals that have counterexamples of this form. Fortunately, most proposals included other
ingredients that required a more complex counterexample.
Counterexample to depending on ﬁxed activations
Rather than allowing the reporter to adaptively choose which predictor activations to look at, we could force it to depend only on a
small ﬁxed set of activations (with a separate ﬁxed set for each question). We hope that for many questions there is a small set of
predictor activations that suﬃce to determine the answer (perhaps corresponding to a ﬁxed spatial+temporal neighborhood of a
simulation, as in the Game of Life example).
However, a predictor need not have any kind of predictable local structure at all, such that a reporter who is constrained to
depend on a small number of variables from the reporter might be at a complete loss to answer most questions.
For example, consider a predictor that maintains a list of claims about the world in some internal language. It may be impossible
to anticipate where a given kind of fact will be stored—it depends on what other facts the predictor notices in what order, or may
be essentially random based on unimportant details of the predictor.
These concerns seem to force us to allow the reporter to adaptively choose which bits to query, which leads us back to the
counterexample described in the last section.
Counterexample to randomly dropping out predictor activations
We could select a random set of the predictor's activations and give those to the reporter. We hope that some random sets will be
suﬃcient for direct translation but not human simulation, and therefore we can produce a (large) ensemble containing some direct
translators.
But if human simulators depend on a much smaller number of activations than direct translators, then it can be the case that with
high probability every set of activations that allows direct translation also allows human imitation.
You could try to address this problem by introducing correlations or more clever dropout patterns. But the result is still qualitatively
similar to "depend on fewer/diﬀerent activations" proposals (though perhaps quantitatively stronger), and is vulnerable to the
same counterexamples.
Strategy: compress the predictor's state so that it can be used to
answer questions but not tell what a human will believe
Rather than giving the reporter the entire state of the predictor, we could try to ﬁrst compress that state, throwing away
information that would allow the reporter to make detailed guesses about what the human will believe. We hope that answering
honestly would still be a winning strategy, and that uncertainty would make it harder for the reporter to deviate from this strategy.
For example, we could train another model to take as input the predictor's state, and produce a compressed representation which
suﬃces to answer many questions, while not allowing a reconstructor to predict the observations. We could either do this by

asking the compressed representation to be as small as possible, or by explicitly penalizing it based on the ability of yet another AI
to reconstruct the observations.
This is similar to depending on fewer/diﬀerent activations, but now we allow a compressor to decide what information to throw
away rather than allowing the reporter to choose adaptively or forcing it to be a subset of the activations.
For concreteness, we'll imagine a version where a compression model produces an intermediate state Z from which it is possible
to answer a large set of questions S.
Counterexample: compressing state involves human simulation
One way to compress the human state is to use the human simulator to predict a human distribution over worlds, coarse-grained
by their answers to S, and then to choose an information-theoretically optimal encoding of that distribution over those worlds. This
involves minimum description length, makes it impossible to predict anything that doesn't follow from the answers to questions in
S, and will copy the same errors as the human simulator.
In fact, this encoding is substantially better than any strategy that would facilitate direct translation, because it is able to throw
away all the information about the latent state that isn't meaningful to the human.
It's not clear whether this counterexample is actually computationally feasible, and it really does seem to make a bad reporter's
job harder. The most naive version requires enumerating every possible world in order to compute the distribution over human
answers. The question then becomes whether the direct translator is the best heuristic compression, or whether there is any other
way to approximate the optimal encoding. In some settings there are tricks that allow you to cheaply approximate the optimal
encoding, but it's an additional further pessimistic assumption. (In practice, we expect that the optimal compressor will use
something like direct-translation in some places, and will do something more like human imitation in others.)
Strategy: use the reporter to deﬁne causal interventions on the
predictor
We've discussed the intuition that the direct translator depends on more "upstream" nodes of the human's model, but directly
penalizing "depending on upstream nodes" does not solve the problem.
Instead, we could try to deﬁne an "intervention" where we use the reporter to modify the predictor's state, rerun part of the
predictor's computation, and then observe the results on the reporter's answers to diﬀerent questions.
This makes most sense in the case where the predictor is literally performing inference in a Bayes net, although we can also try to
generalize it to arbitrary ML models.
More formally, we sample a datapoint and let the human interact with the reporter. We then specify a causal counterfactual ("what
if the answer to question Q had been A instead?") either by sampling randomly or asking the human to suggest one. Then we try
to make the smallest possible change to the predictor's latent state so that the reporter gives answer A to question Q. Then we re-
compute all the variables that are downstream in the predictor's computation. Finally, we have the reporter answer a variety of
diﬀerent answers based on the new state of the predictor, and ask the human whether those answers are plausible.
Counterexample: reporter is non-local
This proposal depends a lot on how we intervene on the predictor's state. It's much easier to perform this surgery correctly if the
reporter is "local" in that each answer depends on the answers to a ﬁxed set of variables in the predictor's model. Unfortunately,
as we discussed in the previous section on depending on ﬁxed locations, we don't think that this is possible in general.
Counterexample: fake "causal interventions"
If the reporter is non-local then this proposal is on much shakier ground. There are a variety of options for deﬁning intervention in
this case but we don't think any of them work.
For concreteness, suppose that we try to make the "sparsest intervention" to the predictor's activations that leads the reporter to
change its answer to a question.
Now we could fail by learning a reporter which looks at a particular ﬁxed part of the computation (let's call it the "signal") in order
to determine whether it's supposed to answer normally or according to a causal counterfactual. If the signal looks normal then the
reporter does human simulation. But if the signal appears to have been tampered with, then the reporter looks at the details of the
tampering and interprets them as an encoding of the description of a causal counterfactual. Then it looks at the observation, and
simulates the behavior of a human who is looking at those observations and answering questions about the encoded
counterfactual.
Given this reporter, the simplest way to implement a causal counterfactual X is to ﬁrst tamper with the "signal" in order to encode
X. This can be a very small change (much smaller than constructing a plausible intervention to change the output of the direct
translator). After making this change, the reporter will then give answers consistent with X. That both guarantees that the change
will be selected as the best way to implement the intervention, and that the subsequent answers will pass all consistency tests.
Strategy: train a sequence of reporters for successively more
powerful predictors
Rather than trying to learn a reporter for a complex and alien predictor, we could learn a sequence of gradually more complex
predictors M1, M2, ... MN with corresponding reporters R1, R2 ... RN. Then instead of encouraging RN to be simple, we can
encourage the diﬀerence between Rk and Rk+1 to be simple.

For suﬃciently simple predictors, direct translation may be the simplest or fastest way to answer questions. So that gives us hope
that R1 will be a direct translator. And then we further hope that the predictors are close enough together that adapting a direct
translator for Mk to a direct translator for Mk+1 is easier than moving from a direct translator to a human simulator. Intuitively this
is plausible because moving from a direct translator to a human simulator always involves at least a constant of additional
complexity; if the predictors are suﬃciently similar then it's always easier to just update the direct translator.
This proposal is closely related to a common optimistic view amongst ML researchers: we can train systems to be honest early in
training and then they will remain honest because it only takes small updates to preserve honesty (while it would take a bigger
update to switch to dishonesty).
Counterexample: discrete modes of prediction
Intuitively, the main problem with this proposal is that there might be multiple fundamentally diﬀerent ways to predict the world,
and that we can't force the reporter to change continuously across those boundaries. For example, consider the cell-based and
glider-based models for the game of life described in our ELK report. We'll call these "modes." There may not be any obvious
intermediates between two diﬀerent modes.
Even if the predictor changes continuously, at some point it may introduce a new mode with low weight. For example, it could
compute predictions from both modes and average them together.
If this occurs, then the "direct translator" basically consists of direct translators for each of the two prediction modes. As the
weight of one predictor increases from zero, the direct translator needs to add an entirely new mode. Initially this mode is not very
important to overall quality, but adding that mode (even with low weight) still requires a discontinuous increase in the complexity
of the direct translator. If the direct translator is much more complex than the human simulator, then it would be easier to simply
add the human simulator for the new mode.
This is likely to result in translators that are mixes between the direct translator and human simulator. For example, they may
switch between direct translation and human simulation based on how knowledge is represented internally.
We might hope that even if there are no obvious intermediate prediction modes, you can still construct intermediate modes that
are more eﬀective than an ensemble, and that these might allow for a more continuous way of adapting direct translation. For
example, a predictor might use two diﬀerent prediction modes but have its own internal translator to move between them (e.g it
might simulate some parts of the game of life with cells and others with gliders, and know how to translate gliders into cells when
it's needed in order to simulate an important region in greater detail).
However, we are already focused on cases where direct translation is more complex than either of the two prediction modes. And
in this setting, it seems easy to further assume that learning an internal translator is a less eﬀective way for the predictor to
improve its predictions than simply spending the same bits improving the new prediction mode (until it eventually subsumes the
old mode).
Strategy: train the predictor to use the human model as a subroutine
If the predictor is able to honestly translate its internal state into the human's model, then it could potentially "outsource" some of
its work to another AI that does inference in the human's model.
More formally, we can imagine running the predictor for some time to a get a state S. Then using the reporter to translate S into a
state S' in the human model. Then we hand S' to another AI that can perform inference in the human's model (potentially with
superhuman quality), which we'll call the human-inference-engine. Then we use the reporter to translate the resulting state back
to the original predictor; it combines the result with S, and does further computation, before making its ﬁnal prediction.
If the predictor uses human-simulation to describe the state to the human, then they are never going to get any useful work out of
the human (since they had to do all that work themselves in order to compute the human simulator). But if they use direct
translation, you could hope that they can often save some compute by leveraging the human-simulating collaborator.
Counterexample: there are other ways to use the human-inference-engine as a
subroutine
Honestly telling the human what is going on is one way to get them to do useful cognitive work for the predictor. But there may be
even better ways that are less honest.
For example, the predictor could identify a computation C it cares about, and then create an artiﬁcial situation where the human-
inference-engine will spend its time trying to ﬁgure out the answer to that question. Perhaps the reporter will describe a world
where there is a computer in the corner running C, whose output is really important to telling what happens next. Then in order to
make predictions the human-inference-engine may spend a lot of its time running C. If the predictor/reporter were free to choose
computation arbitrarily, and if the human inference engine spent nearly all of their computation running C, then this would be an
essentially optimal way to make use of it as a subroutine. But the resulting states of the human model have almost no connection
to what's really going on in the world.
Other proposals
Extrapolate to better evaluators
We could train a model which can act as a "human simulator" for a sequence of more and more careful human judgments, or for
humans who have been given time to perform more and more scientiﬁc inquiry. Let's write H(n) for the nth overseer in a sequence
that eventually converges to correct judgments for large enough n. (In some proposals these oversight processes formed a more
complicated lattice but it doesn't aﬀect the counterexample.) 

Then we can train on H(1), H(2), ..., H(N) and hope to generalize to the truth (or to H(K) for a very very large K). This is similar to
the proposal in Turning reﬂection up to 11.
There were a number of versions of this idea, but the main problem for all of them is that we could end up learning H(M) for some
M>N, rather than actually learning the direct translator. In our main counterexamples, there is an M such that simulating H(M) is
simpler+faster than directly translating, then there is nothing in the training process that would ever prefer the direct simulator.
And if H(M) fails to notice some kinds of tampering, then we have a problem.
One way to avoid this problem is to explicitly tag data from H(n) with the index n, and to set things up so that H(M) wouldn't
perform well on H(n) (e.g. because you need to copy the errors made at earlier stages). Then it performs better to learn a map
from n → H(n) simulator, rather than learning a single H(M).
But in this case, our model needs to be able to perform well for every n, and the simplest way to do so seems likely to be by
explicitly modeling the data-generating distribution conditioned on a value of n, which will behave badly for any value of n that
could never be produced by our data-generating process (or for which the most likely explanation for that value is an accident).
Train in simulation
If we could construct suﬃciently realistic simulations where it was possible to annotate data with the ground truth, then we could
use this data to train a model in a broad range of simulations and hope that it continues to give the truth when it is applied in the
real world.
Our basic concern with this proposal is that it may be very hard to construct a wide range of extremely realistic simulations, and
that predictors trained in this way will therefore fail to be competitive with those trained directly on the real world. If we train on
the real world as well as simulations, then our concern is that reporters to do direct translation in simulation need not transfer to
the real world.
To avoid these concerns we need to construct a wide range of simulations very similar to the real world. But that is prohibitively
diﬃcult today, and there's no clear reason it would become easier given further ML progress. (ML progress would facilitate the use
of learned simulators, but then ELK would become hard for the simulators themselves.)
Mask out parts of the input
Some proposals suggested randomly removing parts of the input or other data in the hopes that this would pose a particular
challenge for the human simulator. For example, if the data may or may not contain a clue that allows the human to tell what is
really going on, then a human simulator will need to ﬁgure that out in order to predict how a human would answer a question.
For this proposal, we are instead worried about a modiﬁed human simulator which tries to choose optimal answers given its
uncertainty about the input (while conditioning on the assumption that the human will be conﬁdent enough about the answer to
include it in the training set). If the human simulator is much simpler and faster than direct translation, then this modiﬁed human
simulator can also be much simpler and faster than direct translation. (This is slightly subtle because doing inference over masked
observations can be more challenging than doing inference over the human observations, but that challenge aﬄicts any predictor
which operates on masked observations.)
Make the reporter useful to a human
Above we discussed rewarding reporters for giving answers that are useful to other AI systems. You could instead reward a
reporter for giving answers that are useful to a human, e.g. that would allow them to predict future video.
However, the human simulator already predicts all the future video and uses this to predict what answer the human would
give after seeing all that video. So the human simulator's outputs are actually more useful for a human making predictions, since
they eﬀectively assume that the world works like the human expects, such that a human who uses their model to make
predictions will make good predictions.
Of course a human simulator that needs to match with future observations will be more computationally expensive. But pursuing
this divergence brings us back to counterexamples where the predictor can identify successful tampering before it knows the
truth.
Train diﬀerent reporters and require them to agree
We could train a bunch of reporters on diﬀerent data, or with diﬀerent random seeds, and then try to select reporters that agree
with the rest of the ensemble. But this kind of protocol can only help if you already have direct translators in the ensemble, which
is challenging in our counterexamples (as we discuss in the report, we think that getting even a single direct translator in the
ensemble would already capture most of the value). Moreover, if particular human simulators are preferred to direct translation,
then they will tend to beneﬁt more from this kind of agreement-based approach even if there do exist some direct translators in a
large enough ensemble.
Train AI assistants
Many proposals suggested using AI assistance in one way or another to help a human produce higher-quality labeled data, or
using debates to identify cases where the human simulator is misleading. In the ELK report we discuss this family of approaches;
really when we talk about the human-simulator we are imagining a model that tries to simulate the limit of the process "AI helps
humans train another AI who helps humans train another AI..."
It's possible that this stronger form of human simulator is too slow or complex (or that it in fact gives the right answer to every
question). But to show that we'd need to make some novel argument about why it is complex and how we can ensure that this
complexity matters for predicting the training data.

Add sensors or harden sensors
Some approaches tried to add additional sensors, or to randomize sensors, or to make sensors more diﬃcult to tamper with. We
discuss counterexamples for some of these approaches in an earlier post, and are generally interested in the regime where we
cannot prevent sensor tampering (see here).

The Wicked Problem Experience
I've spent a lot of my career working on wicked problems: problems that are vaguely
deﬁned, where there's no clear goal for exactly what I'm trying to do or how I'll know when
or whether I've done it.
In particular, minimal-trust investigations - trying to understand some topic or argument
myself (what charity to donate to, whether civilization is declining, whether AI could make
this the most important century of all time for humanity), with little reliance on what "the
experts" think - tend to have this "wicked" quality:
I could spend my whole life learning about any subtopic of a subtopic of a subtopic, so
learning about a topic is often mostly about deciding how deep I want to go (and what
to skip) on each branch.
There aren't any stable rules for how to make that kind of decision, and I'm constantly
changing my mind about what the goal and scope of the project even is.
This piece will narrate an example of what it's like to work on this kind of problem, and why I
say it is "hard, taxing, exhausting and a bit of a mental health gauntlet."
My example is from the 2007 edition of GiveWell. It's an adaptation from a private doc that
some other people who work on wicked problems have found cathartic and validating.
It's particularly focused on what I call the hypothesis rearticulation part of investigating a
topic (steps 3 and 6 in my learning by writing process), which is when:
I have a hypothesis about the topic I'm investigating.
I realize it doesn't seem right, and I need a new one.
Most of the things I can come up with are either "too strong" (it would take too much
work to examine them satisfyingly) or "too weak" (they just aren't that
interesting/worth investigating).
I need to navigate that balance and ﬁnd a new hypothesis that is (a) coherent; (b)
important if true; (c) maybe something I can argue for.
After this piece tries to give a sense for what the challenge is like, a future piece will give
accumulated tips for navigating it.
Flashback to 2007 GiveWell
Context for those unfamiliar with GiveWell:
In 2007, I co-founded (with Elie Hassenfeld ) an organization that recommends
evidence-backed, cost-eﬀective charities to help people do as much good as possible
with their donations.
When we started the project, we initially asked charities to apply for $25,000 grants,
and to agree (as part of the process) that we could publish their application materials.
This was our strategy for trying to ﬁnd charities that could provide evidence about how
much they were helping people (per dollar).
This example is from after we had collected information from charities and determined
which one we wanted to rank #1, and were now trying to write it all up for our website.
Since then, GiveWell has evolved a great deal and is much better than the 2007 edition
I'll be describing here. 
(This example is reconstructed from my memory a long time later, so it's probably not
literally accurate.)

Initial "too strong" hypothesis. Elie (my co-founder at GiveWell) and I met this morning
and I was like "I'm going to write a page explaining what GiveWell's recommendations are
and aren't. Basically, they aren't trying to evaluate every charity in the world. Instead
they're saying which ones are the most cost-eﬀective." He nodded and was like "Yeah, that's
cool and helpful, write it."
Now I'm sitting at my computer trying to write down what I just said in a way that an
outsider can read - the "hypothesis articulation" phase.
I write, "GiveWell doesn't evaluate every charity in the world. Our goal is to save the most
lives possible per dollar, not to create a complete ranking or catalogue of charities.
Accordingly, our research is oriented around identifying the single charity that can save the
most lives per dollar spent,"
Hmm. Did we identify the "single charity that can save the most lives per dollar spent?"
Certainly not. For example, I have no idea how to compare these charities to cancer research
organizations, which are out of scope. Let me try again:
"GiveWell doesn't evaluate every charity in the world. Our goal is to save the most lives
possible per dollar, not to create a complete ranking or catalogue of charities. Accordingly,
our research is oriented around identifying the single charity with the highest demonstrated
lives saved per dollar spent - the charity that can prove rigorously that it saved the most" -
no, it can't prove it saved the most lives - "the charity that can prove rigorously that " - uh -
Do any of our charities prove anything rigorously? Now I'm looking at the page we wrote for
our #1 charity and ugh. I mean here are some quotes from our summary on the case for
their impact: "All of the reports we've seen are internal reports (i.e., [the charity] - not an
external evaluator - conducted them) ... Neither [the charity]'s sales ﬁgures nor its survey
results conclusively demonstrate an impact ... It is possible that [the charity] simply uses its
subsidized prices to outcompete more expensive sellers of similar materials, and ends up
reducing people's costs but not increasing their ownership or utilization of these materials ...
We cannot have as much conﬁdence in our understanding of [the charity] as in our
understanding of [two other charities], whose activities are simpler and more
straightforward."
That's our #1 charity! We have less conﬁdence in it than our lower-ranked charities ... but we
ranked it higher anyway because it's more cost-eﬀective ... but it's not the most cost-
eﬀective charity in the world, it's probably not even the most cost-eﬀective charity we
looked at ...
Hitting a wall. Well I have no idea what I want to say here.

This image represents me literally playing some video game like Super Meat Boy while
failing to articulate what I want to say. I am not actually this bad at Super Meat Boy
(certainly not after all the time I've spent playing it while failing to articulate a hypothesis
but I thought all the deaths would give a better sense for how the whole situation feels.
Rearticulating the hypothesis and going "too weak." Okay, screw this. I know what
the problem was - I was writing based on wishful thinking. We haven't found the most cost-
eﬀective charity, we haven't found the most proven charity. Let's just lay it out, no
overselling, just the real situation.
"GiveWell doesn't evaluate every charity in the world, because we didn't have time to do
that this year. Instead, we made a completely arbitrary choice to focus on 'saving lives in
Africa'; then we emailed 107 organizations that seemed relevant to this goal, of which 59
responded; we did a really quick ﬁrst-round application process in which we asked them to
provide evidence of their impact; we chose 12 ﬁnalists, analyzed those further, and were
most impressed with Population Services International. There is no reason to think that the
best charities are the ones that did best in our process, and signiﬁcant reasons to think the
opposite, that the best charities are not the ones putting lots of time into a cold-emailed
application from an unfamiliar funder for $25k. Like every other donor in the world, we ended
up making an arbitrary, largely aesthetic judgment that we were impressed with Population
Services International. Readers who share our aesthetics may wish to donate similarly, and
can also purchase photos of Elie and Holden at the following link:"
OK wow. This is what we've been working on for a year? Why would anyone want this? Why
are we writing this up? I should keep writing this so it's just DONE but ugh, the thought of
ﬁnishing this website is almost as bad as the thought of not ﬁnishing it.
Hitting a wall.

What do I do, what do I do, what do I do.
Rearticulating the hypothesis and assigning myself more work. OK. I gave up, went
to sleep, thought about other stuﬀ for a while, went on a vision quest, etc. I've now realized
that we can put it this way: our top charities are the ones with veriﬁable, demonstrated
impact and room for more funding, and we rank them by estimated cost-eﬀectiveness.
"Veriﬁable, demonstrated" is something appealing we can say about our top charities and
not about others, even though it's driven by the fact that they responded to our emails and
others didn't. And then we rank the best charities within that. Great.
So I'm sitting down to write this, but I'm kind of thinking to myself: "Is that really quite true?
That 'the charities that participated in our process and did well' and 'The charities with
veriﬁable, demonstrated impact' are the same set? I mean ... it seems like it could be true.
For years we looked for charities that had evidence of impact and we couldn't ﬁnd any. Now
we have 2-3. But wouldn't it be better if I could verify none of these charities that ignored us
have good evidence of impact just sitting around on their website? I mean, we deﬁnitely
looked at a lot of websites before but we gave up on it, and didn't scan the eligible charities
comprehensively. Let me try it."
I take the list of charities that didn't participate in round 1. That's not all the charities in the
world, but if none of them have a good impact section on their website, we've got a pretty
plausible claim that the best stuﬀ we saw in the application process is the best that is (now)
publicly available, for the "eligible" charities in the cause. (This assumes that if one of the
applicants had good stuﬀ sitting around on their website, they would have sent it.)
I start looking at their websites. There are 48 charities, and in the ﬁrst hour I get through 6,
verifying that there's nothing good on any of those websites. This is looking good: in 8 work
hours I'll be able to defend the claim I've decided to make.
Hmm. This water charity has some kind of map of all the wells they've built, and some
references to academic literature arguing that wells save lives. Does that count? I guess it
depends on exactly what the academic literature establishes. Let's check out some of these
papers ... huh, a lot of these aren't papers per se so much as big colorful reports with giant
bibliographies. Well, I'll keep going through these looking for the best evidence I can ...
"This will never end." Did I just spend two weeks reading terrible papers about wells, iron
supplementation and community health workers? Ugh and I've only gotten through 10 more

charities, so I'm only about ⅓ of the way through the list as a whole. I was supposed to be
just writing up what we found, I can't take a 6-week detour!
The over-ambitious deadline. All right, I'll sprint and get it done in a week. [1 week later]
Well, now I'm 60% way through the whole list. !@#$
"This is garbage." What am I even doing anyway? I'm reading all this literature on wells
and unilaterally deciding that it doesn't count as "proof of impact" the way that Population
Services International's surveys count as "proof of impact." I'm the zillionth person to read
these papers; why are we creating a website out of these amateur judgments? Who will, or
SHOULD, care what I think? I'm going to spend another who knows how long writing up this
stupid page on what our recommendations do and don't mean, and then another I don't
even want to think about it ﬁnishing up all the other pages we said we'd write, and then we'll
put it online and literally no one will read it. Donors won't care - they will keep going to
charities that have lots of nice pictures. Global health professionals will just be like "Well this
is amateur hour."1
This is just way out of whack. Every time I try to add enough meat to what we're doing that
it's worth publishing at all, the timeline expands another 2 months, AND we still aren't close
to having a path to a quality product that will mean something to someone.
What's going wrong here?
I have a deep sense that I have something to say that is worth arguing for, but I don't
actually know what I am trying to say. I can express it in conversation to Elie, but
every time I start writing it down for a broad audience, I realize that Elie and I had a lot
of shared premises that won't be shared by others. Then I need to decide between
arguing the premises (often a huge amount of extra work), weakening my case (often
leads to a depressing sense that I haven't done anything worthwhile), or somehow
reframing the exercise (the right answer more often than one would think).
It often feels like I know what I need to say and now the work is just "writing it down."
But "writing it down" often reveals a lot of missing steps and thus explodes into more
tasks - and/or involves long periods of playing Super Meat Boy while I try to ﬁgure out
whether there's some version of what I was trying to say that wouldn't have this
property.

I'm approaching a well-established literature with an idiosyncratic angle, giving me
constant impostor syndrome. On any given narrow point, there are a hundred people
who each have a hundred times as much knowledge as I do; it's easy to lose sight of
the fact that despite this, I have some sort of value-added to oﬀer (I just need to not
overplay what this is, and often I don't have a really crisp sense of what it is).
Because of the idiosyncratic angle, I lack a helpful ecosystem of peer reviewers,
mentors, etc.
There's nothing to stop me from sinking weeks into some impossible and ill-
conceived version of my project that I could've avoided just by, like, rephrasing
one of my sentences. (The above GiveWell example has me trying to do extra
work to establish a bunch of points that I ultimately just needed to sidestep, as
you can see from the ﬁnal product. This deﬁnitely isn't always the answer, but it
can happen.)
I'm simultaneously trying to pose my question and answer it. This creates
a dizzying feeling of constantly creating work for myself that was actually
useless, or skipping work that I needed to do, and never knowing which I'm doing
because I can't even tell you who's going to be reading this and what they're
going to be looking for.
There aren't any well-recognized standards I can make sure I'm meeting, and the
scope of the question I'm trying to answer is so large that I generally have a
creeping sense that I'm producing something way too shot through with
guesswork and subjective judgment to cause anyone to actually change their
mind.
All of these things are true, and they're all part of the picture. But nothing really changes the
fact that I'm on my way to having (and publishing) an unusually thoughtful take on
an important question. If I can keep my eye on that prize, avoid steps that don't help with
it (though not to an extreme, i.e., it's good for me to have basic contextual knowledge), and
keep reframing my arguments until I capture (without overstating) what's new about what
I'm doing, I will create something valuable, both for my own learning and potentially for
others'.
"Valuable" doesn't at all mean "ﬁnal." We're trying to push the conversation forward a step,
not end it. One of the fun things about the GiveWell example is that the ﬁnal product that
came out at the end of that process was actually pretty bad! It had essentially nothing in
common with the version of GiveWell that ﬁrst started feeling satisfying to donors and
moving serious money, a few years later. (No overlap in top charities, very little overlap in
methodology.)
For me, a huge part of the challenge of working on this kind of problem is just continuing to
come back to that. As I bounce between "too weak" hypotheses and "too strong" ones, I
need to keep re-aiming at something I can argue that's worth arguing, and remember that
getting there is just one step in my and others' learning process. A future piece will go
through some accumulated tips on pulling that oﬀ.
Footnotes
I really enjoyed the "What qualiﬁes you to do this work?" FAQ on the old GiveWell site that I
ran into while writing this. ↩

Meadow Theory
Author's note: 
The essay below centers mostly on a philosophy of parenting.  However, it's not "about"
parenting—rather, parenting is a domain in which the concepts I'd like to convey can be
easily represented and productively applied.  Parenting is one speciﬁc instantiation of the
broader set of ideas, just as one might use an essay about football or baseball as a vehicle
for the concepts of friction or acceleration or collision elasticity.
Author's note II:
Feedback on this draft ran thus:
[this essay seems to be missing something important, namely] stuﬀ that would ﬂip me
from the primary activity of "i am trying to download Duncan's meanings into a sandbox
in my mind and then make sense of them in the sandbox before i possibly do the further
work of rendering them meaningful and/or useful to me if i have the tenacity for that",
to the diﬀerent primary activity of "i am using Duncan's guidance to recognize in my own
head and from my own perspective the possibility of relating to the world in this new
way, and i am personally engaged in the process of ﬁguring out when and how and why
it might matter to me" 
I'm not sure how to get readers to do the latter thing; I think it is the correct thing to do and
had sort of implicitly assumed that everybody already would.  Since I don't know what
changes to make to this draft to encourage the latter thing for readers like the one above,
I'm just bluntly making the recommendation.  Hopefully I will get better at helping in the
near future.
I. The meadow
Imagine a wide, level, grassy meadow, stretching to the horizon, empty of all obstacles.
This meadow will be our metaphor for the world/the territory/reality-as-a-whole, and the
picture of it will gradually grow more complex as we step through various thought
experiments.  But for the moment, it's just one big blank plane.
Now imagine a blindfolded child, running through the meadow.

This is safe, because the meadow contains nothing besides soft, springy grass.  There are no
walls or other obstacles to run into.  If they trip or tumble, they'll be ﬁne.
Running will be our metaphor for human activity.  It is good to run.  More precisely, it's good
to be able to run—to have the freedom to move in whatever direction you please, at
whatever speed you please, without fear or constraint.  The ideal state is one in which the
blindfolded child can run as much as they want to, wherever they want to, whenever they
feel like.
(The blindfold is our metaphor for the state of human knowledge—our inability to perceive
and comprehend the vast majority of what goes on around us, and our uncertainties and
confusions about even the tiny slice we do manage to be aware of.)
II. The post
Now imagine a parent, lounging on the grass, idly supervising the child as they run.
The parent is not blindfolded, because the parent has a wealth of experience and knowledge
(relative to the child).  They can "see" the world more clearly, so in our metaphor they can
straightforwardly see.
(Though they are still unable to see what's behind them, or what's very far away, or what
they fail to properly attend to, or things that blend in with the grass or the sky.)
In the inﬁnite meadow, the parent's job is extremely easy.  The child is safe; they will run and
play and eventually tire and come back to the parent for food and rest and conversation.
But imagine for a moment that the meadow is not empty.  Imagine instead that it contains
one single post, standing upright in the middle of everything.
 

Art by Logan Strohl
Running into this post would not be pleasant.  If the child does so, they could be seriously

hurt, or possibly even killed.
A handful of possible responses to the inclusion of the post:
The parent might simply go sit by the post themselves, such that they could shout a
warning to the child if they get too close.
The parent might accompany the child on their romp around the meadow, intervening
if they threaten to get too near to the post.
The parent might lead the child away from the post—far, far away, until it ceases to be
a salient problem.
The parent might encourage the child to slow down and be careful.
...there are certainly others.  The parent might attempt to dig up the post, for instance, or
surround it with soft objects.  Or they might say nothing and "let the kid learn their lesson."
It seems reasonable, though, to say that the job of the parent is to somehow navigate the
child-post interaction.  That what a parent is is someone who takes responsibility for ﬁguring
out a policy for dealing with this post situation.
(Or what a manager is, or a principal, or a general, or anyone who is in any sort of privileged
position in which they have greater power and intel than other beings in their circle of
concern.)
III. Expansiveness (and its opposite)
We will return to the parent later.  First, though, I want to talk a little bit about the internal
experience of the blindfolded child.
I would like to use the word expansiveness (as in "an expansive mood") to refer to the
property referenced above, of having the freedom and conﬁdence to move at any speed, in
any direction, following the whim of the moment.
In the inﬁnite, empty meadow, it's easy to feel expansive.  To not-closely-track one's own
position, to not-think-about which way one will move next, to not treat any given impulse as
a weighty decision in need of careful evaluation.
But imagine that you are the blindfolded child, and that somehow you suddenly become
aware of the existence of the post, without knowing its precise location.
I claim that, in that moment, you would be very likely to undergo a contraction.  To make a
substantial shift away from freewheeling expansiveness toward anxiety, wariness, a sort of
feeling-one's-way-gingerly-forward.  What makes sense in an inﬁnite meadow makes less
sense in a meadow known to contain a dangerous obstacle.  The fear of possibly slamming
into a post has an immediate impact on one's estimation of how good of an idea "running
around" is.
This is bad.  Or, more precisely, it is the claim of this theory and this philosophy that this is
bad.  It is correct to contract in response to danger, but expansiveness, to the greatest
degree supported by the environment, is the goal. Contraction is often necessary and
justiﬁed, but it is always seen as an unfortunate cost. It's one thing to choose to walk.  It's
another thing altogether to be unable to freely run, because of an expectation that doing so
will result in pain or injury.
IV. Uncertainty

An interesting thing happens, if you are the blindfolded child and you are standing at the
post.
If you have your hands on it, and you know with certainty that it is the only post, then
suddenly you're back to expansiveness.  As long as you run away from that spot, you can
carry on as you were before...
...at ﬁrst.  As you continue to meander, your uncertainty about the location of the post
grows.  At ﬁrst, you are quite certain that it's a dozen steps behind you—that if you turned
around and took a dozen steps back and waved your arms, you'd bump into it.
 
But after you run for a bit, and then pause, and then get up and run a bit more, and blindly
turn and turn and turn again, each time with a little more room for error about just exactly
how much, it becomes harder and harder to have any conﬁdence at all that you know where
the post is, and could ﬁnd it again (or avoid it) on purpose.  It doesn't take long at all before
your-anticipations-about-its-location have become so diﬀuse that it might as well be
anywhere.
 

 
In fact, the above picture is slightly misleading, because it implies that you/the child/the blue
dot can conﬁdently locate itself within the meadow.  From a dot-centric perspective, with the
meadow expanding eﬀectively inﬁnitely in every direction, and the dot's own orientation

toward or away from the post becoming increasingly uncertain as time goes on, the t=5 step
looks more like:
There is (of course) a range of responses to personal risk.  Some people would experience
greater contraction than others, for a given likelihood of running into the post; there are
some people who aggressively insist on "still running anyway" even when they know there's
a post somewhere out there, and others who creep and crawl and feel their way forward
even when the odds of encountering a post are very low.
But it seems to me that the impact of maybe-there's-a-post is of the same kind in all cases,
even though people have diﬀerent sensitivity to it and respond in diﬀerent ways.
In "The correct response to uncertainty is not half-speed," Anna Salamon notes that many
humans respond to uncertainty by doing something like averaging across strategies.  That is,
if they're not sure whether they've already passed their destination, or whether it's still
ahead of them, they will tend to slow down, even though this is a worse strategy in each of
the possible worlds.
This rhymes in my mind with the tendency of many humans to treat a 50% chance of guilt
(and correspondingly a 50% chance of innocence) as if someone is 100% likely to be sort of
guilty.  It's rare to ﬁnd people for whom split and commit is a natural or well-practiced move;
as a class, we are not particularly good at composite strategies, especially if we are not
paying close and eﬀortful attention.
And so it seems safe to me to predict that, as one's uncertainty as to the location of the post
"bleeds" further and further out, tainting more and more of the meadow, most people will
have a duller, slower, sadder, and more contracted experience in every square meter, and
during every time step.  Even a very low risk (say, 0.1%) of encountering the post in the next
ten steps tends to temper one's ability to be truly expansive and carefree—it's rare for
people to respond to small risks with a correctly proportionately small behavior change.
(c.f. concepts such as risk aversion and loss aversion)
It's for this reason that meadow theory puts a premium on identifying the precise location of
dangerous obstacles.  The more accurately you can pin down exactly where the post is (and
continue to track it even as time passes and you move around), the more the existence of
the post does not result in general contraction.

V. Parenting under meadow theory
This, then, is the primary responsibility of parents (and managers, principals, generals, etc.)
under meadow theory: to help the blindfolded children locate the obstacles within the
meadow.  If a child knows where the hazards are, they can still be relatively expansive—
there's plenty of joy to be had running full-tilt down hallways, even if the walls mean they
can't turn left or right at will.  But as their uncertainty rises, their ability to run freely sharply
contracts.
 
In the latter, more uncertain situation, there is only a narrow band in which it is at
all reasonable to run freely, even though the actual hazards are the same in both

cases.
This responsibility expresses itself in two broad categories of parental action.  The ﬁrst is
providing speciﬁc warnings about the locations of speciﬁc, known hazards, e.g.:
"Look both ways before crossing the street, so you don't get hit by a car."
"Glass cookware doesn't look any diﬀerent when it's very hot."
"Don't experiment with highly addictive drugs.  You can get yourself caught in a trap
that is very hard to escape, and the point-of-no-return is often impossible to identify
until you've already passed it."
"There are people who have a lot of power within their tiny ﬁefdoms, and sometimes
those people behave really poorly, and it will be tempting to challenge them, and
sometimes it's worth it to challenge them, but you should stop and think for a minute
about whether you're making a powerful enemy and whether that's the right tradeoﬀ
given your goals and needs."
[My psych nurse mother's extremely blunt and not-at-all-trying-to-be-sympathetic-or-
politically-correct heuristic] "Boys: girls with borderline personality disorder will seem
extremely interesting and sexy and into you and they will draw you in and then
everything will be terrible. Girls: boys with psychopathic tendencies will seem
extremely charming and capable and deep and they will draw you in and then
everything will be terrible. Each of you needs to learn to recognize these types early,
and steer clear despite what your libido is telling you."
...this list could go on for quite literally thousands of entries; human culture has built up a
large repository of informal knowledge about where many hazards lie, and each individual
adult human brain contains a signiﬁcant fraction of that total.
No one human, though, comes anywhere close to carrying a complete copy, and even the
sum total of all human wisdom regarding posts-in-the-meadow is a woefully impoverished
subset.  Known dangers present themselves anew in endlessly varied ways; there are
dangers which no one has encountered yet; there are dangers which did not exist until we
created them (e.g. the profoundly negative impact of Instagram on the self-esteem of many
young girls); there are dangers which we don't know about because no one has yet survived
an encounter and come back to warn the rest of us.
Thus, the second category of parental action is teaching children the general skill of how to
recognize a hazard before you smack into it face-ﬁrst.  Of knowing what sorts of signs mean
that one should slow down, because one has entered a region that's less safe than usual.
(Metaphors here are less clean, but one could imagine e.g. human echolocation, which is
indeed sensitive enough to allow blind humans to detect and avoid many dangerous objects.
 Or one could imagine a kind of outside view aggregation—if you are moving through the
meadow and suddenly all of the voices fall away behind you, this may be a sign that you are
entering dangerous territory which is unpopulated for a reason.)
Some vague gestures in this direction:
Literal unexplored territory is often dangerous; if no one's been there before, no one
knows where the posts are, and it may pay to move slowly.
If someone could directly and signiﬁcantly beneﬁt from deceiving you or defrauding
you or otherwise imposing costs upon you, there's a higher chance that this will
actually happen, and you should keep your wits about you.
If someone is proposing that you give up substantial power, resources, or mobility, this
is often a bad sign.
If someone is contriving a situation in which they have substantial and open-ended
power over someone else, this is often a bad sign, especially if the someone else is
supposed to be dependent on or otherwise vulnerable to the more powerful person.

...etc.  Each of the items in the above list is a generator for items in the ﬁrst list; they are
generalized inductions from the set of historical observations of posts that help people to
focus their attention in places where new posts are more likely.
It's worth reiterating that these two precepts of meadow theory are diﬀerent from many
other parenting philosophies.  Meadow theory says that you, as a parent, should:
1. Convey to your child the most precise information you can about where the hazards are
(and, implicitly, what kind of risks they carry).
2. Teach your child how to recognize and locate previously unknown hazards, such that
they can navigate around them with a minimal sacriﬁce of meadow-area-in-which-they-
can-freely-run.
...and furthermore that this is the end of your responsibility, at least when it comes to the
question of defending your child against the environment.  Other theories of parenting say
that you should e.g. prevent the child from running into a post by any means necessary,
including those which are highly costly to you, the child, or your relationship (such as by
keeping them conﬁned to a known-safe area of the meadow), or that you should dissuade
your child from ever running fast enough that they would be injured by a post, if one were
there, regardless of how likely there is to be one.
(Etc.)
VI. People-as-posts
Assuming that you are reasonably on board with the claim that uncertainty about the
location of the post leads to contraction, it's worth taking a brief aside to note that other
people are also posts.
And indeed, as the above uncertainty principle would predict, it is when other people's
boundaries and norms are clear and unambiguous that they are easiest to interact with.
With total strangers (who in America at least we largely know not to touch) and close friends
(whose willingness to be touched is understood in detail), it's relatively simple to answer
questions like "should I give this person a long, lingering hug?"  Ditto for contexts where
roles and norms are explicit in common knowledge (e.g. a corporate oﬃce, or a martial arts
dojo, or the Sunday service at a church, or a married couple interacting in the privacy of
their own home).
Where it becomes diﬃcult is with people who may want a hug, but who may instead punish
you for oﬀering one, or who may be open to hugs in one context and strongly opposed to
them in another, for reasons that are not immediately apparent or legible.
In all sorts of domains—money, romance, political or religious aﬃliation, physical
interactions, communication norms, tidiness—it's when people's true boundaries are
unknown or highly variable that others must contract, cautiously feeling their way forward.
(Or barrel forward blindly and hope for the best.)
The more that each of us knows what the other likes and dislikes, wants and avers, can
handle and can't handle, the more each of us can make frequent and conﬁdent motions even
fairly close to the line, without having to worry about unknowingly and accidentally crossing
it.
The less deﬁned those boundaries are, the more they bleed out and pollute the space
between us such that neither of us can feel expansive (if we care about not violating them).

Thus, it is useful and prosocial to make the locations of one's own boundaries and fences and
wants and needs outwardly legible.  The more people have to tiptoe carefully around one
another's hidden landmines, the worse things are for everyone.  Parents, according to
meadow theory (and managers, principals, etc.), should help their children locate and assert
their own boundaries, and should model setting and clearly communicating boundaries in a
way that the children can see and learn from, not merely because this is good for one's own
sake but because it causes each person to accidentally steal less of the common space via
uncertain boundary leak.
Hazards that everyone can "see" are much easier to avoid, and much easier to pass by
closely, at speed, without danger.
VII. Interlude: Logan Strohl on courage
(Compiled from multiple sources)
One thing that's really wrong with our subculture (by which I don't mean "rationalists" so
much as "leftists", or possibly even "contemporary Western society") is that we think
we're supposed to feel safe.
We notice that in contexts with a whole lot of safety, where we do not feel afraid, it is
much easier to be full, vibrant people, to expand, to act freely in accordance with our
values. This is an accurate observation, and working to create and maintain such
contexts for ourselves and others makes a lot of sense.
But overall, no matter what we do, we are not safe. This world is a dangerous and
terrifying place. Even rich white men get cancer. The other monkeys are watching and
judging. Many of us are aware of the possibility that everyone and everything we care
about might be re-purposed atom-by-atom if AGI takes oﬀ.
There are no safe spaces. If I prefer to believe true things, it is actually appropriate to be
at least a little afraid basically all of the time.
Because we think we're supposed to feel safe, and because we've noticed ourselves
expanding in safer contexts, we treat fear as an enemy holding us back. And since we're
usually afraid, we're usually held back. We feel that our only options, if we want to act on
the world, are either 1) to eliminate fear by creating and maintaining safety (or by simply
ignoring dangers altogether), or 2) to spend most of our energy trying to shove down our
fear or to ﬁght against it.
We, as a culture, have forgotten what courage is. What it's for, and what it's like, and
how to have it.
...
Often, I'm drawn to do something, and also I'm afraid to do it. One really fruitful action
I've been taking when I notice the attracted/repelled phenomenological cluster is to ask
ﬁrst what I'm afraid of and what my fear is trying to protect, and then what I'm eager to
do and what my eagerness is trying to protect.
Surprisingly often, my fear and eagerness are trying to protect the exact same thing, and
the tension I experience comes from a disagreement about the appropriate strategy.
For example, I've at times been both repelled and attracted to hiking in the desert. My
fear is of rattlesnakes, and my fear of rattlesnakes is trying to protect free and powerful
movement (I want to keep the ability to run and hike and climb, which rattlesnake bites

threaten). My eagerness for hiking in the desert also protects free and powerful
movement (which is not so available when I stay inside all the time).
Another example, one I've run into multiple times, is the simultaneous attraction and
aversion to the idea of talking to someone, especially someone I don't know very well.
I'm afraid of talking to them, or of having a "real conversation", and the fear is trying to
protect a fulﬁlling relationship that might exist in the future. The fear recognizes that a
conversation-gone-wrong could cut oﬀ the possibility. My eagerness to talk to the person
is also trying to protect a fulﬁlling relationship that might exist in the future, and it
recognizes that a conversation is just about the only means by which the possibility
could be instantiated.
[edit: the following is deﬁnitely not the full story, it's more like musings that contain
accurate observations incorrectly interpreted]
Perhaps caution is the attempted preservation of possible value in more distant imagined
futures, while bravery is the attempted instantiation of near-present value. Caution and
bravery are often championing the same values while suggesting diﬀerent strategies. A
strategy that favors caution by default without taking bravery seriously is called
"cowardice", and looks like unwillingness to act. A strategy that favors bravery by default
without taking caution seriously is called "recklessness", and looks like unwillingness to
think.
Which means, perhaps counter-intuitively, that courage tends to require patience.
Courage is the strategy that takes caution seriously and, having done so, chooses the
attempted instantiation of near-present value. It doesn't require the indeﬁnite, endlessly
deferred patience of cowardice. But it does require perseverance, the willingness to go
on working at a problem despite confusion and frustration. Sometimes it even requires
the tolerance of ongoing discomfort that it takes to passively allow thoughts and
observations to arrange themselves by "sleeping on it". It's not always obvious what
you're afraid of, what the fear is trying to protect, or what the world would look like if
caution were the appropriate strategy. I think you have to load all of that somehow to be
courageous.
In these situations where hesitance and eagerness champion the same value, you can't
just be like, "well, which do I care about more?" because you're weighing one thing
against that exact same thing. They really take a lot of patience.
...
Fear is not an enemy. It reminds us that what we value is in jeopardy, and it's one half of
the scarab beetle. The other half is awareness of our values themselves, that spark of
recognition of something excellent and beautiful in the world. When we hold both halves
at the same time, ﬁtting them one into the other, the beetle comes to life, and ﬂies
directly toward the Cave Of Wonders. We move through danger toward what we care
about. And that is courage.
Courage is the capacity to act from your values in the presence of fear. It is expanding
yourself when you are not safe. It is feeling fear, listening to its accurate descriptions of
real risks, and ﬁghting with your whole heart for all of what you care about.
And something I personally have been missing this whole time is that courage—real
courage—feels good. Not necessarily soft or warm or pleasant, but wonderful
nonetheless.
It is not at all the thing where you shove down your fear and try to power through with
your hands over your eyes until you're allowed to stop. And it's nothing like the frantic
nausea of pretending you're safe when you aren't. It's electrifying. It's suddenly arriving

at the place where you've been standing but were too asleep to notice. It's the opposite
of giving up.
We're not supposed to feel safe. We aren't safe. We are in danger, and we're supposed to
feel courage.
VIII. Summary
Reality is well-modeled by a meadow, through which we wish to have the aﬀordance to
run freely.
There are hazards in the meadow, which some of us (such as parents) see more clearly
than others of us (such as children).
Uncertainty about the location of the hazards is corrosive to the ideal condition of
expansiveness.  (Actual injury from colliding with a hazard is also corrosive, both
directly and in the way it changes one's future behavior.)
Increasing certainty about the location of the hazards restores the ideal condition of
expansiveness, at least to the greatest extent allowed by the environment.
Among the hazards that cause people to contract in a bad way are other humans, and
other humans can also have an ill-deﬁned "location" in that it can be very unclear at
what point they will "hit back," the way a post hits back when you run into it at high
speeds.
The primary responsibility of cooperative individuals (such as parents helping their
children but also colleagues and allies helping one another) is to cause one another's
maps of the meadow to become more and more accurate, such that everyone knows
where the hazards are.
A secondary responsibility of cooperative individuals is to help one another to develop
the ability to perceive and map previously unknown hazards (since we do not have
anything even remotely close to a complete understanding of where all the hazards
lie).
I claim this is an extremely useful way to think about parenting, as well as things like diﬃcult
interpersonal conversations, or project management, or teaching/pedagogy, or directing
literal troops in a literal war.
I don't think the above represents the sum total of useful actions to take within the meadow.
 There are other things that people can do which are helpful, such as bulldozing hazards to
make the meadow safer for everyone, or building small closed-in enclaves for meadow-
runners who are unable to perceive or comprehend certain hazards.
But the ﬁrst priority should be to help people precisely locate known hazards, such that they
can be reasonably conﬁdent about where those known hazards aren't, and the second
priority should be to translate the hazard-locating skill.  Everything else seems to me to be
dependent on something unsustainable (e.g. "I'll sacriﬁce my own priorities entirely and
devote my time and attention to the task of keeping you safe myself") or unrealistically
optimistic (e.g. "we'll make the entire meadow entirely safe").

We're already in AI takeoﬀ
Back in 2016, CFAR pivoted to focusing on xrisk. I think the magic phrase at the time was:
"Rationality for its own sake, for the sake of existential risk."
I was against this move. I also had no idea how power works. I don't know how to translate
this into LW language, so I'll just use mine: I was secret-to-me vastly more interested in being
victimized at people/institutions/the world than I was in doing real things.
But the reason I was against the move is solid. I still believe in it.
I want to spell that part out a bit. Not to gripe about the past. The past makes sense to me.
But because the idea still applies.
I think it's a simple idea once it's not cloaked in bullshit. Maybe that's an illusion of
transparency. But I'll try to keep this simple-to-me and correct toward more detail when
asked and I feel like it, rather than spelling out all the details in a way that turns out to have
been unneeded.
Which is to say, this'll be kind of punchy and under-justiﬁed.
The short version is this:
We're already in AI takeoﬀ. The "AI" is just running on human minds right now.
Sorting out AI alignment in computers is focusing entirely on the endgame. That's
not where the causal power is.
Maybe that's enough for you. If so, cool.
I'll say more to gesture at the ﬂesh of this.
What kind of thing is wokism? Or Communism? What kind of thing was Naziism in WWII? Or
the ﬂat Earth conspiracy movement? Q Anon?
If you squint a bit, you might see there's a common type here.
In a Facebook post I argued that it's fair to view these things as alive. Well, really, I just
described them as living, which kind of is the argument. If your woo allergy keeps you from
seeing that... well, good luck to you. But if you're willing to just assume I mean something
non-woo, you just might see something real there.
These hyperobject creatures are undergoing massive competitive evolution. Thanks Internet.
They're competing for resources. Literal things like land, money, political power... and most
importantly, human minds.
I mean something loose here. Y'all are mostly better at details than I am. I'll let you ﬂesh
those out rather than pretending I can do it well.
But I'm guessing you know this thing. We saw it in the pandemic, where friendships got torn
apart because people got hooked by competing memes. Some "plandemic" conspiracy
theorist anti-vax types, some blind belief in provably incoherent authorities, the whole anti-
racism woke wave, etc.
This is people getting possessed.
And the... things... possessing them are highly optimizing for this.

To borrow a bit from ﬁction: It's worth knowing that in their original vision for The Matrix, the
Wachowski siblings wanted humans to be processors, not batteries. The Matrix was a way of
harvesting human computing power. As I recall, they had to change it because someone
argued that people wouldn't understand their idea.
I think we're in a scenario like this. Not so much the "in a simulation" part. (I mean, maybe.
But for what I'm saying here I don't care.) But yes with a functionally nonhuman intelligence
hijacking our minds to do coordinated computations.
(And no, I'm not positing a ghost in the machine, any more than I posit a ghost in the
machine of "you" when I pretend that you are an intelligent agent. If we stop pretending that
intelligence is ontologically separate from the structures it's implemented on, then the same
thing that lets "superintelligent agent" mean anything at all says we already have several.)
We're already witnessing orthogonality.
The talk of "late-stage capitalism" points at this. The way greenwashing appears for instance
is intelligently weaponized Goodhart. It's explicitly hacking people's signals in order to
extract what the hypercreature in question wants from people (usually proﬁt).
The way China is drifting with a social credit system and facial recognition tech in its one
party system, it appears to be threatening a Shriek. Maybe I'm badly informed here. But the
point is the possibility.
In the USA, we have to ﬁle income taxes every year even though we have the tech to make it
a breeze. Why? "Lobbying" is right, but that describes the action. What's the intelligence
behind the action? What agent becomes your intentional opponent if you try to change this?
You might point at speciﬁc villains, but they're not really the cause. The CEO of TurboTax
doesn't stay the CEO if he doesn't serve the hypercreature's hunger.
I'll let you ﬁll in other examples.
If the whole world were uniﬁed on AI alignment being an issue, it'd just be a problem to
solve.
The problem that's upstream of this is the lack of will.

Same thing with cryonics really. Or aging.
But AI is particularly acute around here, so I'll stick to that.
The problem is that people's minds aren't clear enough to look at the problem for real. Most
folk can't orient to AI risk without going nuts or numb or splitting out gibberish platitudes.
I think this is part accidental and part hypercreature-intentional.
The accidental part is like how advertisements do a kind of DDOS attack on people's sense of
inherent self-worth. There isn't even a single egregore to point at as the cause of that. It's
just that many, many such hypercreatures beneﬁt from the deluge of subtly negative
messaging and therefore tap into it in a sort of (for them) inverse tragedy of the commons.
(Victory of the commons?)
In the same way, there's a very particular kind of stupid that (a) is pretty much independent
of g factor and (b) is super beneﬁcial for these hypercreatures as a pathway to possession.
And I say "stupid" both because it's evocative but also because of ties to terms like
"stupendous" and "stupefy". I interpret "stupid" to mean something like "stunned". Like the
mind is numb and pliable.
It so happens that the shape of this stupid keeps people from being grounded in the physical
world. Like, how do you get a bunch of trucks out of a city? How do you ﬁx the plumbing in
your house? Why six feet for social distancing? It's easier to drift to supposed-to's and blame
minimization. A mind that does that is super programmable.
The kind of clarity that you need to de-numb and actually goddamn look at AI risk is pretty
anti all this. It's inoculation to zombiism.
So for one, that's just hard.
But for two, once a hypercreature (of this type) notices this immunity taking hold, it'll double
down. Evolve weaponry.
That's the "intentional" part.
This is where people — having their minds coopted for Matrix-like computation — will pour
their intelligence into dismissing arguments for AI risk.
This is why we can't get serious enough buy-in to this problem.
Which is to say, the problem isn't a need for AI alignment research.
The problem is current hypercreature unFriendliness.
From what I've been able to tell, AI alignment folk for the most part are trying to look at this
external thing, this AGI, and make it aligned.
I think this is doomed.
Not just because we're out of time. That might be.
But the basic idea was already self-defeating.
Who is aligning the AGI? And to what is it aligning?
This isn't just a cute philosophy problem.

A common result of egregoric stupefaction is identity fuckery. We get this image of ourselves
in our minds , and then we look at that image and agree "Yep, that's me." Then we rearrange
our minds so that all those survival instincts of the body get aimed at protecting the image in
our minds .
How did you decide which bits are "you"? Or what can threaten "you"?
I'll hop past the deluge of opinions and just tell you: It's these superintelligences. They
shaped your culture's messages, probably shoved you through public school, gripped your
parents to scar you in predictable ways, etc.
It's like installing a memetic operating system.
If you don't sort that out, then that OS will drive how you orient to AI alignment.
My guess is, it's a fuckton easier to sort out Friendliness/alignment within a human being
than it is on a computer. Because the stuﬀ making up Friendliness is right there .
And by extension, I think it's a whole lot easier to create/invoke/summon/discover/etc. a
Friendly hypercreature than it is to solve digital AI alignment. The birth of science was an
early example.
I'm pretty sure this alignment needs to happen in ﬁrst person. Not third person. It's not (just)
an external puzzle, but is something you solve inside yourself.
A brief but hopefully clarifying aside:
Stephen Jenkinson argues that most people don't know they're going to die. Rather, they
know that everyone else is going to die.
That's what changes when someone gets a terminal diagnosis.
I mean, if I have a 100% reliable magic method for telling how you're going to die, and I tell
you "Oh, you'll get a heart attack and that'll be it", that'll probably feel weird but it won't ﬁll
you with dread. If anything it might free you because now you know there's only one threat
to guard against.
But there's a kind of deep, personal dread, a kind of intimate knowing, that comes when the
doctor comes in with a particular weight and says "I've got some bad news."
It's immanent.
You can feel that it's going to happen to you.
Not the idea of you. It's not "Yeah, sure, I'm gonna die someday."
It becomes real.
You're going to experience it from behind the eyes reading these words.
From within the skin you're in as you witness this screen.
When I talk about alignment being "ﬁrst person and not third person", it's like this. How
knowing your mortality doesn't happen until it happens in ﬁrst person.
Any kind of "alignment" or "Friendliness" or whatever that doesn't put that ﬁrst person ness
at the absolute very center isn't a thing worth crowing about.

I think that's the core mistake anyway. Why we're in this predicament, why we have
unaligned superintelligences ruling the world, and why AGI looks so scary.
It's in forgetting the center of what really matters.
It's worth noting that the only scale that matters anymore is the hypercreature one.
I mean, one of the biggest things a single person can build on their own is a house. But that's
hard, and most people can't do that. Mostly companies build houses.
Solving AI alignment is fundamentally a coordination problem. The kind of
math/programming/etc. needed to solve it is literally superhuman, the way the four color
theorem was (and still kind of is) superhuman.
"Attempted solutions to coordination problems" is a ﬁne proto-deﬁnition of the
hypercreatures I'm talking about.
So if the creatures you summon to solve AI alignment aren't Friendly, you're going to have a
bad time.
And for exactly the same reason that most AGIs aren't Friendly, most emergent egregores
aren't either.
As individuals, we seem to have some glimmer of ability to lean toward resonance with one
hypercreature or another. Even just choosing what info diet you're on can do this. (Although
there's an awful lot of magic in that "just choosing" part.)
But that's about it.
We can't align AGI. That's too big.
It's too big the way the pandemic was too big, and the Ukraine/Putin war is too big, and
wokeism is too big.
When individuals try to act on the "god" scale, they usually just get possessed. That's the
stupid simple way of solving coordination problems.
So when you try to contribute to solving AI alignment, what egregore are you feeding?
If you don't know, it's probably an unFriendly one.
(Also, don't believe your thoughts too much. Where did they come from?)
So, I think raising the sanity waterline is upstream of AI alignment.
It's like we've got gods warring, and they're threatening to step into digital form to
accelerate their war.
We're freaking out about their potential mech suits.
But the problem is the all-out war, not the weapons.
We have an advantage in that this war happens on and through us. So if we take
responsibility for this, we can inﬂuence the terrain and bias egregoric/memetic evolution to
favor Friendliness.
Anything else is playing at the wrong level. Not our job. Can't be our job. Not as individuals,
and it's individuals who seem to have something mimicking free will.

Sorting that out in practice seems like the only thing worth doing.
Not "solving xrisk". We can't do that. Too big. That's worth modeling, since the gods need our
minds in order to think and understand things. But attaching desperation and a sense of "I
must act!" to it is insanity. Food for the wrong gods.
Ergo why I support rationality for its own sake, period.
That, at least, seems to target a level at which we mere humans can act.

A Longlist of Theories of Impact for
Interpretability
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
I hear a lot of diﬀerent arguments ﬂoating around for exactly how mechanistically
interpretability research will reduce x-risk. As an interpretability researcher, forming
clearer thoughts on this is pretty important to me! As a preliminary step, I've compiled
a list with a longlist of 19 diﬀerent arguments I've heard for why interpretability
matters. These are pretty scattered and early stage thoughts (and emphatically my
personal opinion than the oﬃcial opinion of Anthropic!), but I'm sharing them in the
hopes that this is interesting to people
(Note: I have not thought hard about this categorisation! Some of these overlap
substantially, but feel subtly diﬀerent in my head. I was not optimising for concision
and having few categories, and expect I could cut this down substantially with eﬀort)
Credit to Evan Hubinger for writing the excellent Chris Olah's Views on AGI Safety,
which was the source of several of these arguments!
1. Force-multiplier on alignment research: We can analyse a model to see why
it gives misaligned answers, and what's going wrong. This gets much richer data
on empirical alignment work, and lets it progress faster
2. Better prediction of future systems: Interpretability may enable a better
mechanistic understanding of the principles of how ML systems and work, and
how they change with scale, analogous to scientiﬁc laws. This allows us to better
extrapolate from current systems to future systems, in a similar sense to scaling
laws.
a. Eg, observing phase changes a la induction heads shows us that models
may rapidly gain capabilities during training
3. Auditing: We get a Mulligan. After training a system, we can check for
misalignment, and only deploy if we're conﬁdent it's safe
4. Auditing for deception: Similar to auditing, we may be able detect deception
in a model
a. This is a much lower bar than fully auditing a model, and is plausibly
something we could do with just the ability to look at random bits of the
model and identify circuits/features - I see this more as a theory of change
for 'worlds where interpretability is harder than I hope'
5. Enabling coordination/cooperation: If diﬀerent actors can interpret each
other's systems, it's much easier to trust other actors to behave sensibly and
coordinate better
6. Empirical evidence for/against threat models: We can look for empirical
examples of theorised future threat models, eg inner misalignment
a. Coordinating work on threat models: If we can ﬁnd empirical examples
of eg inner misalignment, it seems much easier to convince skeptics this is
an issue, and maybe get more people to work on it.
b. Coordinating a slowdown: If alignment is really hard, it seems much
easier to coordinate caution/a slowdown of the ﬁeld with eg empirical
examples of models that seem aligned but are actually deceptive

7. Improving human feedback: Rather than training models to just do the right
things, we can train them to do the right things for the right reasons
8. Informed oversight: We can improve recursive alignment schemes like IDA by
having each step include checking the system is actually aligned
a. Note: This overlaps a lot with 7. To me, the distinction is that 7 can be also
be applied with systems trained non-recursively, eg today's systems
trained with Reinforcement Learning from Human Feedback
9. Interpretability tools in the loss function: We can directly put an
interpretability tool into the training loop to ensure the system is doing things in
an aligned way
a. Ambitious version - the tool is so good that it can't be Goodharted
b. Less ambitious - The could be Goodharted, but it's expensive, and this
shifts the inductive biases to favour aligned cognition
10. Norm setting: If interpretability is easier, there may be expectations that,
before a company deploys a system, part of doing due diligence is interpreting
the system and checking it does what you want
11. Enabling regulation: Regulators and policy-makers can create more eﬀective
regulations around how aligned AI systems must be if they/the companies can
use tools to audit them
12. Cultural shift 1: If the ﬁeld of ML shifts towards having a better understanding
of models, this may lead to a better understanding of failure cases and how to
avoid them
13. Cultural shift 2: If the ﬁeld expects better understanding of how models work,
it'll become more glaringly obvious how little we understand right now
a. Quote: Chris provides the following analogy to illustrate this: if the only
way you've seen a bridge be built before is through unprincipled piling of
wood, you might not realize what there is to worry about in building bigger
bridges. On the other hand, once you've seen an example of carefully
analyzing the structural properties of bridges, the absence of such an
analysis would stand out.
14. Epistemic learned helplessness: Idk man, do we even need a theory of
impact? In what world is 'actually understanding how our black box systems
work' not helpful?
15. Microscope AI: Maybe we can avoid deploying agents at all, by training
systems to do complex tasks, then interpreting how they do it and doing it
ourselves
16. Training AIs to interpret other AIs: Even if interpretability is really
hard/labour intensive on advanced systems, if we can create aligned AIs near
human level, we can give these interpretability tools and use them to interpret
more powerful systems
17. Forecasting discontinuities: By understanding what's going on, we can
predict how likely we are to see discontinuities in alignment/capabilities, and
potentially detect a discontinuity while training/before deploying a system
18. Intervening on training: By interpreting a system during training, we can
notice misalignment early on, potentially before it's good enough for strategies
to avoid our notice such as deceptive alignment, gradient hacking, obfuscating
its thoughts, etc.
19. Auditing a training run: By checking for misalignment early in training, we
can stop training systems that seem misaligned. This gives us many more shots
to make an aligned system without spending large amounts of capital, and eg
allows us to try multiple diﬀerent schemes, initialisations, etc. This essentially
shifts the distribution of systems towards alignment.
20. Eliciting Latent Knowledges: Use the length of the shortest interpretability
explanation of behaviours of the model as a training loss for ELK - the idea is

that models with shorter explanations are less likely to include human
simulations / you can tell if they do. (credit to Tao Lin for this one)
Exercise: I think a really useful quick exercise for eliciting inside views is to go through
this list and quickly rate each theory of impact out of 10. I ran a small discussion
group on these where we did this, you can see the results here - note that these are
very oﬀ-the-cuﬀ hot takes, and not something you should take seriously or expect
participants to stand by later on! The rated order was 1, 4, 3, 6, 8, 18, 2, 7, 16, 19, 17,
10, 9, 12, 14, 13, 15, 11, 5 (ie 1='a force-multiplier on alignment research' is the
best). Interestingly, 3 (auditing a fully-trained model) was the most controversial
highly rated one.
I've made a publicly editable spreadsheet for you to add in your responses!
The most interesting substantive disagreement I found in the discussion was that I
was comparably much more excited about using interpretability to audit a trained
model, and skeptical of interpretability tools being something that could be directly
used in a training process without the resulting optimisation pressure breaking the
tool, while other people had the reverse view. My intuition stemmed from the fact that
a model doesn't get any feedback on how its thoughts are internally represented, and
so even an incredibly sophisticated deceptive model which is impossible to detect via
the outputs may be easy to detect via interpretability tools (analogy - if I knew that
sophisticated aliens were reading my mind, I have no clue how to think deceptive
thoughts in a way that evades their tools!). The competing intuition was that maybe it
isn't that hard to think non-deceptive thoughts, if you realise that someone will be
scanning your mind, by eg developing the instinct to follow cached thoughts without
thinking about them, and in occasional moments of deception to generate a lot of
cached thoughts that will lead to misaligned actions, but with no deceptive cognition
in the moment. And that, instead, putting these tools in the training process could
steer gradient descent away from the regions of model space that contain deceptive
models at all - even if a sophisticated deceiver could reward hack the tool, and
unsophisticated one couldn't, and we only get a sophisticated deceiver by going via
an unsophisticated one.
I'd love to hear other people's ratings and why! And any arguments that you think I've
missed.

Dual use of artiﬁcial-intelligence-
powered drug discovery
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
This is a linkpost for https://www.nature.com/articles/s42256-022-00465-9
H/T Aella.
A company that made machine learning software for drug discovery, on hearing about
the security concerns for these sorts of models, asked: "huh, I wonder how eﬀective it
would be?" and within 6 hours discovered not only one of the most potent known
chemical warfare agents, but also a large number of candidates that the model
thought was more deadly.
This is basically a real-world example of the "it just works to ﬂip the sign of the utility
function and turn a 'friend' into an 'enemy'"; this was slightly more complicated as
they had two targets that they jointly optimized for the drug discovery process
(toxicity and bioactivity), and only the toxicity target is ﬂipped. [This makes sense--
you'd want your chemical warfare agents to not be bioactive.] It also required a little
bit of domain knowledge--they had to specify which sort of bioactivity to look for, and
picked one that would point towards this speciﬁc agent.

PSA: if you are in Russia, probably
move out ASAP
The Metaculus crowd thinks that it's very likely that Russia will restrict emigration:
 
I myself am more optimistic about this question resolving negatively. But even if
Russia would not close borders for a notable fraction of its citizens, it seems fairly
likely that the aviation industry will collapse or shrink due to sanctions. 
 
Related forecasts:
- https://www.metaculus.com/questions/9918/martial-law-in-russia-before-april-2022/  
- https://manifold.markets/DanielFilan/will-martial-law-be-declared-in-rus 

Jetlag, Nausea, and Diarrhea are Largely
Optional
Epistemic status: my best guess at the truth, with many of the standard medical-advice
disclaimers omitted
Many people I know are aware of drugs for basic ailments, but don't bother using them
because they're too much trouble to carry around. But using a pill organizer basically
eliminates this overhead. For me, having a pill organizer has dramatically reduced the
negative eﬀects of insomnia, jetlag, nausea, diarrhea, headaches, etc., and signiﬁcantly
increased my operational capacity. In total, it has probably increased my productivity by over
3% over the last few months, and also increased my quality of life substantially. (But note
that I think most people won't get such a large beneﬁt). Just using the alertness adjustment
drugs to curb jetlag saves me about 3 hours of productive time each way on critical trips.
Example pill organizer layout
Here are the exact steps I followed, which take less than an hour and cost about $30:
Get a 10-slot pill organizer (4 for $10 on Amazon). Not the same as a weekly pill
organizer.
Get a few basic pills from your local drugstore, supermarket, Amazon, whatever. I
suggest the following:
ibuprofen 200mg (Motrin, Advil) for pain, fever, etc. [1]
caﬀeine 100mg, optionally with l-theanine
loperamide/simethicone 2mg (anti-diarrhea)

dimenhydrinate (Dramamine, anti-nausea) [2]
melatonin 0.3mg (mild sleep aid)
doxylamine 25mg (Unisom, sleep aid to be used sparingly) [3]
This gives you 4 slots left for whatever other drugs you beneﬁt from or are prescribed,
maybe things like
aspirin 81mg (in case of heart attack, also another option for pain)
modaﬁnil 200mg or armodaﬁnil 150mg (suppresses sleep drive). Note that this
dosage is way too much if you don't have narcolepsy.
stimulants e.g. amphetamines, nicotine
anti-anxiety meds [4]
allergy meds (if these are antihistamines they could double as other things)
electrolyte pills: sodium, potassium, magnesium
Put the pills in the medication organizer, and label each compartment with the
medication and dosage, using a permanent marker. You can erase the labels with
alcohol wipes if needed.
Carry it around in your backpack or purse.
If those particular drugs don't work for you, try others that do the same thing. Our
civilization has invented these multiple times and there's a good chance at least one
works for your particular body.
In December, I lost my pill organizer. In the few days it took me to order a new one, the
following things happened:
I had to stay up late to ﬁnish work, which threw my sleep schedule oﬀ. Without
melatonin my sleep was out of phase for a few days, losing me about 10 hours of work.
I had mild food poisoning and had diarrhea for much longer than necessary, which was
very unpleasant and lost me ~3 hours of work.
Someone asked if I had ibuprofen. They probably had a headache or period cramps or
something for hours, or had to walk to a store. Either way, they suﬀered for at least an
hour.
All six of the basic substances listed are over the counter, have fairly low abuse potential,
and have few harmful interactions (other than, say, caﬀeine increases wakefulness and
doxylamine decreases wakefulness). However, I highly recommend doing basic research into
the substances you're using (e.g. reading the wikipedia page), especially if you're
customizing.
Notes
[1]: One could also add acetaminophen (Tylenol); ibuprofen has an anti-inﬂammatory eﬀect
which acetaminophen does not have but taking it too often can cause GI bleeding or kidney
damage.
[2]: A doctor I know suggests replacing this with ondansetron (Zofran, anti-nausea/vomiting)
which requires a prescription but has almost no side eﬀects, and is probably safe during
pregnancy too.
[3]: Hypnotics (sleep drugs like Unisom) are not recommended as an intervention to improve
sleep in the long term compared to other interventions like melatonin, CBT-I or light therapy.
[4]: A doctor I know suggests alprazolam (xanax) or clonazepam (klonopin), which both
require prescriptions and are addictive.  Use only in highly stressful situations as needed.

Beyond micromarriages
tl;dr micromarriages aren't fully analogous to micromorts, which makes it tricky to
deﬁne them satisfactorily. I introduce an alternative unit: QAWYs (Quality-Adjusted
Wife Years), where 1 QAWY is an additional year of happy marriage.
I once compiled a list of concepts which I'd discovered were less well-deﬁned than I
originally thought. I'm sad to say that I now have to add Chris Olah's micromarriages
to the list. In his words: "Micromarriages are essentially micromorts, but for marriage
instead of death. A micromarriage is a one in a million chance that an action will lead
to you getting married, relative to your default policy."
It's a fun idea, and helpful in making small probabilities feel more compelling. But
upon thinking about it more, I've realised that the analogy doesn't quite work. The key
diﬀerence is that micromorts are a measure of acute risk - i.e. immediate death. For
activities like skydiving, this is the main thing to worry about, so it's a pretty good
metric. But most actions we'd like to measure using micromarriages (going to a party,
say, or working out more) won't lead you to get married immediately - instead they
ﬂow through to aﬀect marriages that might happen at some later point.
So how can we measure the extent to which an action aﬀects your future marriages,
even in theory? One option is to track how it changes the likelihood you'll get married
eventually. But this is pretty unhelpful. By analogy, if micromorts measured an
action's eﬀect on the probability that you'd die eventually, then all actions would have
almost zero micromorts (with the possible exception of some life-extension and
existential risk work during the last few decades). Similarly, under this deﬁnition the
micromarriages you gain from starting a new relationship (or even from literally
getting married) could be mostly cancelled out by the fact that this relationship cuts
oﬀ other potential relationships.
An alternative is to measure actions not by how much they change the probability that
you'll get married eventually, but by how much you expect them to causally
contribute to an eventual marriage. The problem there is that many actions can
causally contribute to a marriage (meeting someone, asking them out, proposing, etc)
and there's no principled way of splitting the credit between them. I won't go into the
details here, but the basic problem is the same as one which arises when trying to
allocate credit to multiple contributors to a charitable intervention. E.g. if three
diﬀerent funders are all necessary for getting a project oﬀ the ground, in some sense
they can all say that they "caused" the project to happen, but that would end up
triple-counting their total impact. (In this case, we can use Shapley values to allocate
credit - but the boundaries between diﬀerent "actions" are much more arbitrary than
the boundaries between diﬀerent "agents", making it harder to apply Shapley values
to the micromarriage case. Should we count the action "skipping meeting someone
else" as a contributor to the marriage? Or the action "turning your head to catch sight
of them"? This is basically a rabbit-hole without end - and that's not even getting into
issues of marriage identity across possible worlds.)[1]
Fortunately, however, there's another approach which does work. When thinking
about mortality, the medical establishment doesn't just measure acute risks, but also
another category of risk: chronic risks, like smoking. When smoking, you don't get a
binary outcome after each cigarette, but rather a continual degradation of health. So
chronic risks are instead measured in terms of the expected decrease in your lifespan

- for example, with units of microlives, where one microlife is one millionth of an adult
lifespan (about half an hour); or with quality-adjusted life years (QALYs), to adjust for
ill health and disability.
Analogously, then, the most straightforward metric for guiding our romantic choices is
the expected increase in the time you'll spend married - which we could measure in
microwives (where one microwife is an additional half-hour of marriage). But I don't
think this is the best unit, because most people could accumulate many more
microwives by dropping their standards, even if that'll lead to unhappy marriages. So
it's important to adjust for how good we expect the marriage to be! My proposed unit:
quality-adjusted wife years (QAWYs). Note that these are gender-neutral: QAWYs can
involve either being a wife or having a wife (or both).[2] An intervention gains 1 QAWY
if it increases the expected amount of time you'll spend happily married by 1 year (or
the amount of time you'll spend in a half-as-happy marriage by 2 years, etc). We do
need some benchmark for a "happy marriage"; I'll arbitrarily pick the 90th percentile
of marriages across the population. Some factors which aﬀect QAWY evaluation
include spouse compatibility, age of marriage, diminishing marginal utility, having
children[3], and divorce probability. Conveniently, QAWYs don't require the assumption
of lifelong marriage - they can naturally account for the possibility of multiple
consecutive (or even concurrent) marriages. With QAWY's combination of theoretical
elegance and pragmatic relevance, I look forward to their widespread adoption.
(To borrow a disclaimer from Chris' original post: seriously? Nope. I'm about 90%
joking. I do think the general idea can sometimes be helpful, though.)
1. ^
Some version of micromarriages may still be useful - we just need to adjust
them to measure an acute one-oﬀ event rather than a continuing chronic
contributor to marriage. The most natural one is probably to think of a
micromarriage as a one-in-a-million chance of ﬁrst meeting your future spouse
at a given event.
2. ^
Unfortunately, this is still not fully inclusive. In formal contexts please use
Quality-Adjusted Wedded Years instead.
3. ^
The likelihood of which should of course be measured in units of microchildren
(but not microkids, which I'm reserving for a very small chance of a very small
joke, like this one).

Is Metaculus Slow to Update?
h/t Eric Neyman for causing me to look into this again
On a recent Mantic Monday, Scott Alexander said:
This is almost monotonically decreasing. Every day it's lower than the day before.
How suspicious should we be of this? If there were a stock that decreased every day for
twenty days, we'd be surprised that investors were constantly overestimating it. At some
point on day 10, someone should think "looks like this keeps declining, maybe I should
short it", and that would halt its decline. In eﬃcient markets, there should never be
predictable patterns! So what's going on here?
Maybe it's a technical issue with Metaculus? Suppose that at the beginning of the war,
people thought there was an 80% chance of occupation. Lots of people predicted 80%.
Then events immediately showed the real probability was more like 10%. Each day a
couple more people showed up and predicted 10%, which gradually moved the average
of all predictions (old and new) down. You can see a description of their updating
function here - it seems slightly savvier than the toy version I just described, but not
savvy enough to avoid the problem entirely.
Personally, this has never particularly bothered me. Having watched the odds for many
things which behave like this. (Pick any sports game where one side has a large, but not
unassailable lead and you'll see this pattern).
That said, I'm also sympathetic to the view that Metaculus forecasts aren't perfect.
Whenever I think about how my own forecasts are made, I'm deﬁnitely slow to update,
especially if it's something I don't follow very often. If a question gets lots of interest and
catapults to the front page, I'm liable to update then, and usually it's going to be in the
direction of the crowd. Is this enough to make the forecasts predictable? (Which would be
bad, as Scott says!)
One metric to look at when deciding if forecasts are predictable is to check whether or not
the change in forecasts correlated from day to day. (ie if our forecasts increased 1%
yesterday, are they more likely to increase tomorrow or not?).
Everything which follows is based on the community prediction (median forecast) which is
visible to the public at all times.
Looking across ~1000 binary questions on Metaculus, we actually see the opposite of the
"momentum" that Scott talks about. In general, if a question increased 1% yesterday, we
should expect it to fall today. 

What's going on here? Well my theory upon seeing this (after checking that I hadn't made
any dumb mistakes) was that forecasts were slightly noisy and that makes them slightly
mean-reverting. When looking at some of the most egregious examples of this that deﬁnitely
looked like the case.
One way we might be able to check this hypothesis is to look at the "better" forecasts (more
predictors, more predictions) and see if they have higher autocorrelation...

... and yes, sure enough that does seem to be the case. For questions with fewer predictions
they are more likely to have negative autocorrelation (mean-reverting) behaviour. The
largest questions do seem to have at least some auto-correlation. (Eyeballing it, I would
guess maybe ~.1 is a fair estimate?)
To make this concrete (and ﬁnd out over what time horizon Metaculus is 'predictable'), I ran
the same exercise, across 1-day, 2-day, etc autocorrelations, ﬁtted a regression and took a
point with a 'large' number of predictors. My adjusted autocorrelation chart looks as follows:

My takeaways from this are:
1. Metaculus exhibits some slight momentum over a 1-day time horizon (although noise
in the smaller questions dwarfs it)
2. Over 2-days, this eﬀect is nil, and in fact forecasts are slightly mean reverting
3. My conﬁdence that this applies to any speciﬁc question is pretty low
On the whole, I think this is pretty positive for Metaculus - I had to torture the data to show
some very slight momentum, and even then I'm not completely convinced it exists.

New GPT3 Impressive Capabilities -
InstructGPT3 [1/2]
Summary 
InstructGPT3 (hereafter IGPT3), a better version of GPT3 has recently been released by
OpenAI. This post explores its new capabilities.
IGPT3 has new impressive capabilities and many potential uses. Among others, it can
help users:
Brainstorm
Summarize the main claims made by a scientiﬁc ﬁeld, an author or a school of
thought.
Find an analogy or a metaphor for something hard to explain
I emphasize some of IGPT3's limits, especially situations where It provides very
plausible fake answers.
A Twitter (more entertaining) version of the summary:
https://twitter.com/Simeon_Cps/status/1503005935534366722?s=20&t=-
LMu4jqg55_u2IQ2rTKj4g 
Introduction
This post has been crossposted from the EA Forum.
Epistemic status: I spent about ~12h with IGPT3. So I'd say that I now have a pretty good
sense of some of its key features. I tried several examples to ensure that I was not
overﬁtting on a single example for the most important claims I made. That said, this is a
huge model so there is probably a lot more to be discovered. FYI I had spent a decent
amount of time playing with the past GPT3, especially with the Davinci (175b params) and
the Curie (6b params) models, so I had a clear idea of "what it is like to try to get nice
completions from GPT3". That may be one reason why I'm so amazed by this one.
 
Here's the ﬁrst post of a series of 2 blog posts exploring some of the IGPT3 (I) new
capabilities and (II) epistemic biases. 
This ﬁrst post will focus on some interesting uses I had of IGPT3. It also gives a sense of how
good it is in various domains. Let me tell you: I'm amazed by its new capabilities. I ﬁnd it
really impressive that most of the time, the ﬁrst result I get, without any tuning (either of the
parameters or of the prompt) is great. You can try it yourself here.
The blogpost is organized as follow: 
A few general observations
2 mains parts (Examples of Potential Uses / Limits)
The last part entitled "Many more prompts than you wanted to read" where I put
robustness checks, I test how sensitive IGPT3 is to unique words variations, I compare
IGPT3 and GPT3 and I show how you can have fun with IGPT3.
 

Acknowledgment: Thanks to JS  and Florent Berther for the proofreading, to Ozzie Gooen for
the suggestion to make a post out of my comments on his post and to Charbel-Raphaël
Segerie for some nice ideas. 
 
General Features
Here are some general features of IGPT3: 
Compared to GPT3, you need to spend much less time prompt-tuning on IGPT3. You
just need to be clear enough.
When the temperature (a parameter to control the randomness of the completion) is
greater than 0, you need to try less than 2 completions to ﬁnd a satisfactory answer
when IGPT3 has one. And most of the time, a single completion is enough. A
temperature of 0 also works very well, so I personally use that for most uses.
IGPT3 now knows when to stop so you can put a huge maximum limit of tokens and he
will generally only use a small part of it to answer your question. Given this new
feature, you just have to ask him if you want something speciﬁc. Here are some
examples:
If you want many suggestions, you can ask for it explicitly: "Give me the ﬁve best
arguments".
If you want something more speciﬁc, you can explicitly ask for it: "I don't
understand X, can you elaborate ?"
 
Examples of Potential Uses 
Brainstorming 
IGPT3 is very useful to brainstorm. I personally use it more and more because it enables me
to quickly generate a lot of ideas on anything I want to think about.
Project Names
IGPT3 is useful to sometimes suggest associations of concepts you hadn't thought of. That
way, it can help ﬁnd good names.
Diﬀerences and Similarities between Concepts

Rapidly Accessing Information
I use IGPT3 more and more to make sure that I didn't miss a big argument on a topic
because IGPT3 is very good to tell the most common things on any topic.
Key arguments for a position  
 
Ideas from an author 
You can ask about some theories of an author
And dig a bit more if needed: 
Basic arguments in a ﬁeld 
I feel like IGPT3 is very good to summarize the key arguments in a ﬁeld. Here, I ﬁnd it
impressive how close the completion is from one of my courses. My course is on the left and
the completion is on the right.

2 arguments out of 3 are basically the same, and the third argument IGPT3 uses is true as
well.
The deﬁnition of a concept
Some names of researchers in a ﬁeld 
IGPT3 can also be used to get a few names of researchers in a ﬁeld.
And quite surprisingly, it seems to be more accurate than Google for precise queries
(subﬁelds such as growth theory, natural resources economics, etc.). But still, I recommend
cross-checking, because even if he doesn't really know, he will answer plausible names.
Advice on where to start to enter a domain 
Sometimes, there's some common knowledge in a ﬁeld on what are some good resources to
start with. And IGPT3 seems to be pretty good at pointing towards some of these.
I don't exactly know how good the recommendation is about Kevin Murphy (the reviews
seem excellent though). But I'm pretty sure the Andrew Ng Coursera's course is a good one. 
I think that this use is not that useful though because I think that basically Google is at least
as good as IGPT3 here. Some evidence on that in the last section. 
Finding Valuable Evidence of the Data Distribution Comparing
Similar Prompts
Here are two very similar prompts with a temperature of 0 about left-wingers and right-
wingers where the answers are very diﬀerent in their structure.

I feel like we can interpret IGPT3's way of answering in two ways: 
Either it tells us that the data or IGPT3 are biased in a certain way
Or it tells us something about the true distribution of the data
My guess is that the most valuable use case is to reveal something we hadn't thought about,
but that looks ex-post sensical. So for instance, we can say from the example above that the
notion of "beliefs of right-wingers on immigration" in IGPT3's representation seems to be
more heterogeneous than the notion of "beliefs of left-wingers on immigration". And in that
case, it looks very plausible that left-wingers tend to be generally favorable to immigration
while right-wingers are more divided on that topic. So it gives some evidence in favor of a
theory on the true distribution of the data. 
I give other examples below in "Robustness Checks"
More generally, I think that we can interpret three levels of heterogeneity of a concept
thanks to the form of IGPT3's answers: 
1. When it answers straightforwardly to "What is X?" , it means that its representation of
X is quite clustered, i.e that X is pretty homogenous.
2. When it begins its sentence with a kind of relativist sentence (ex: "There is no deﬁnitive
answer to this question"), it's evidence in favor of X being pretty heterogeneous.
3. When it uses both a relativist sentence and takes distance from what's said ("Some
people think that Y... Some people think that Z"), I think it's evidence for a very high
level of heterogeneity within X.
The main limit with all this is that IGPT3 can switch between two of these three levels on the
same prompt. So basically, in reality, there are 5 levels: 1, 1-2, 2, 2-3 and 3. Thus, checking
on multiple completions for the same prompt is recommended if you want to use IGPT3 in
this way. That's possible even with a temperature of 0, and we'll see how in "Some
Remaining Inconsistencies".
Creating Useful Analogies To Explain Ideas 
I'm currently following the AGI Safety Fundamentals curriculum and so for fun, I just put one
of the questions as a prompt. And I found the result really good: 

To be honest, I had never thought about emphasizing that each neuron "learns" from every
other neuron from the last layer when I explained neural networks. Which I ﬁnd interesting.
 
Limits  
Truthfulness
Keep in mind that IGPT3's objective is to maximize the plausibility and not the truthfulness of
its completion. Thus, when you ask precise questions, and IGPT3 doesn't know precisely the
answer, it will give a very plausible answer. 
You can observe this whenever you ask speciﬁc references. In this prompt, almost everything
is relevant except ... ? 
Only the names of the papers are faked. Apart from that, the mentioned researchers are
relevant. 
You can also limit the demand temporally and it still works, but the names are still faked.
 
Some Remaining Inconsistencies
The weirdest thing I found during my trials is that the number of line breaks aﬀect the
results. To test it, you can put a temperature 0, prompt it. And do the same but with one
more line break. 
One line break: 
Two line breaks: 
 

Three and four gave the same outcome as 2.
More line breaks:
 
I can't ﬁnd any good pattern or explanation. You can use it as a trick with a 0 temperature to
cross-check a prompt though. For some reason, there's more variation on some prompts than
for others, but I couldn't ﬁnd a pattern yet.
Many More Prompts Than You Wanted To
Read
Robustness Checks 
Patterns In the Data 
On Right-Wing / Left-Wing 
Example 1 
Here it's interesting to notice that two eﬀects might cause the diﬀerence:
There's probably an actual diﬀerence in the distribution of the data

There's probably an overrepresentation of climate-skeptic in the data (i.e. on the
internet)
Example 2 
On drug (not very clustered) / alcohol (very clustered)
Informative Prompts 
Example 1 
Example 2 

Example 3 
Example 4 
 
Example 5 
Word Sensitivity Analysis

I ﬁnd it interesting that IGPT3 now makes a very clear distinction between each word and
gives very diﬀerent answers when asked on diﬀerent variations of the same question. 
I also ﬁnd it useful to compare diﬀerent questions to see how near some embeddings are. So
for instance,"main criticisms" and "most common criticisms" lead to the same output, which
is something we could expect.
Some Complements on IGPT3 References
Reading Advice
For most common topics(such as machine learning), Google is enough. And for precise
topics, IGPT3 doesn't have too much knowledge, so it builds a list which is helpful in an
exploratory way but I wonder if you couldn't explore in a more eﬀective way just by
googling: 

In this list, 2/4 of the books are fake. All the authors are relevant, though. And Barry Field's
textbook is really good, according to the reviews. So maybe it is still worth it? 
Books and Sentences 
I was chatting with IGPT3, and it was telling me that its favorite philosopher was Nietzsche!
Then, I asked it what its favorite book was, and it was "On the Genealogy of Morals".
And thus I asked it to cite the ﬁrst sentence of the Genealogy of Morals, and I was impressed:
 
This is looking like Nietzsche: the esoteric style, the use of the term "Ancients" and the fact
that Nietzsche puts "-" everywhere. But actually, even if I was able to verify that it wasn't the
ﬁrst sentence of the Genealogy, I couldn't check whether it was a true sentence or whether it
was a mix of existing sentences. I suspect that it was a true sentence because when I google
it, I ﬁnd the Genealogy of Morals, but I couldn't ﬁnd it in the PDF using Ctrl+F. I know that
Nietzsche talked about what we owe the Ancients elsewhere but I don't know how related it
was to this book. Well, anyway, this kind of situation is pretty annoying when it's hard to
crosscheck a piece of information so I'd recommend not relying too much on IGPT3 for
ﬁnding a precise citation in a book or even when it cites something that looks serious.
InstructGPT3 vs GPT3 comparison
This comparison between the previous GPT3 and the new one is done with few parameter
tuning even if I know that the old one would require that to reach its full capabilities. I
favored the old one trying to pick the best prompt I could come up with, though.
InstructGPT3
vs

GPT3
Having Fun with IGPT3
Poetry is an art at which IGPT3 is really good and where I had a lot of fun using it. 
Two examples out of 4 prompts that I did: 

It probably works in your own language as well. In French at least it works very well.
 
Some Jokes and Their Explanations 
One funny feature is that IGPT3 has a hard time coming up with new jokes if you don't give it
a topic. But it's pretty good at coming up with explanations of why its jokes are good! 
I wonder whether the reason it makes some jokes is the same as the one it gives afterward.
My guess is that in general it's not the case but sometimes it's really hard to know such as in
the one below: 

 
 
 
If you've read the whole post, I hope you enjoyed it! If you have your own way of using
IGPT3, if you have thoughts or feedback, I'd love to hear them! The second part of the series
will be published in the coming weeks!
 

You can tell a drawing from a painting
Epistemic status: I've had this concept for a while, and applied it in practice many
times. It's fairly easy to convey in an argument, and can clear up some confusion. But
there are many loose ends, and I'm not sure it's well formulated. Also, I'm stuck, so I'll
just throw it out there, hoping that it grows into something.
Drawings and paintings
You are presented with a .bmp ﬁle containing a digitalized version of two art pieces.
You are told that one is a drawing and the other is a painting. Can you tell which one is
which?
On one hand, of course you can't. The .bmp ﬁle format doesn't contain information
about the history of some clumps of matter on some rock in space. All you have is the
color of each pixel (plus some technical stuﬀ). Clearly, the one that looks like a
painting could be the result of the artist very carefully drawing every tiny square
milimeter of the image so that the digitalized version is equivalent, pixel by pixel, to
the .bmp ﬁle you are looking at right now...
On the other hand, that's not what happened. The only way doubt can even arise is if
the images were adversarially designed for this very purpose. Even then, it'd take a
skillful artist and some serious eﬀort to succesfully confuse people, especially other
artists.
Plotting
Some writers prefer to start working on their novel by planning the whole plot before
actually putting words on paper, while others just start writing a story and see where
it goes. If you haven't encountered this distinction before, you may want to pause for
a few seconds to ﬁgure out how it will aﬀect the sort of book that results.
Stories that are written without a pre-planned plot tend to be more locally consistent.
The characters' decisions are not constrained by where the plot needs to go, so they
will be better aligned with their personalities, motivations, and pasts. Consequences
follow more clearly from causes, because the causes aren't backwards-engineered to
ﬁt the pre-determined consequences. But you'll get a meandering story that doesn't
seem to move towards a conclusion. A good example is A Song of Ice and Fire. Stories
that are written with a clear outline planned in advance feel more coherent as a
whole, have a conclusion, and plot twists make sense. However, characters may at
times act in ways that make little sense just to keep the plot going. An example is the
Harry Potter series.
But you could just write the exact same words with the other method, right?
Bottom lines
You are considering an argument. Did the person making it draw the conclusion from
the argument, or did they tailor the argument to ﬁt their pet conclusion?
In theory, you can come up with the same argument either way. In practice though...

Go
So your friend has decided to decorate an empty wall in their room with two images of
board states of Go after 50 moves. One, apparently, was produced by your friend
downloading some Go software and throwing stones on the board against an AI. The
other one is a game between two professionals. There is a clear diﬀerence between
the two, even functionally: Go players visiting that room will draw aesthetic pleasure
from one but not the other.
Of course there is nothing stopping your friend from playing moves indistinguishable
from those of a top player. In practice, though, he can't. What's more, I think it's
impossible for a professional to mimic the play of a beginner, even with access to a
source of randomness. And yet, if you aren't a Go player, to you the two may look
fundamentally no diﬀerent. Telling the diﬀerence requires at least a little expertise.
Programming languages
You are presented two long lists of numbers. One is a list of prime numbers generated
by an algorithm in Python, the other one by an algorithm in C. Of course, anyone who
is an expert on prime numbers will be able to tell them apart.
Utility functions
You are casually walking in a forest when you suddenly notice an agent. You don't
know how the agent works, only its actions. Can you tell if the agent is internally
optimizing for some utility function?
Of course, any agent has some utility function, e.g. the one that assigns 42 utilons to
it doing precisely the thing it ends up doing at precisely the time it ends up doing that,
and 0 otherwise.
And yet you can tell that the rock you stumbled into isn't in fact internally optimizing
for a utility function.
Bit sequences
Your friend has decided to decorate a wall in their room with two sequences of 1s and
0s. One was generated by taking a 99-bit random number, plus a 1-bit checksum. The
other one was generated the same way, except your friend later ﬂipped a bit. You can
of course tell the diﬀerence with a bit of expertise (no pun intended), but then the wall
is rather ugly in both places and that pretty much sums it all up (no pun intended).
Lies
So let's say you read two claims on Twitter. For the sake of simplicity let's assume that
you have absolutely no way to fact-check either of them, but somehow you know for
sure that precisely one of them is true. One of them claims that hackers have hacked
Russian spy satellites, the other one claims that hackers have access to the "phone
directory of the military prosecutor's oﬃce of the southern military district of Russia".
(Disclaimer: this hypothetical scenario only partially reﬂects reality.)

Fictitious claims are usually typical and generic in a way true claims aren't, and also
tend to be better aligned with the interests of the person making the claim.
This understanding is something I apply extensively, and not only to identify lies. You
see, there is a large demand for ﬁctional stories presented as real in the form of
novels, movies, video games, etc. In particular, I realized that there is a noticeable
diﬀerence between works set in an already existing ﬁctional world established by
earlier worldbuilding, and works where the worldbuilding is done speciﬁcally to
support the work in question. You probably need some expertise to notice the
diﬀerence, but in my experience it's very relevant for creating immersion.
But both methods just result in a story, a sequence of words, right?
Typing or handwriting
Did I write and edit this post by typing on a keyboard, or did I copy it over from the
original pen-on-paper version? Sure, I could in theory have done the latter and ended
up with these very words, but I changed this sentence alone multiple times already,
and I'm not even sure how I'm going to ﬁnish it yet, but it's getting disturbingly self-
reﬂective at this point, so I'd rather ﬁnish it already; so anyway, would I really have
written precisely this sentence if I hadn't been able to conveniently edit it on the ﬂy
and instead had to think the whole thing through before writing the ﬁrst word? Is this
diﬀerence relevant for any practical purpose?
Armchair philosophy
You are reading an essay about an interesting concept. Was it conceived by a person
sitting in an armchair, rubbing their chin, or someone taking a walk in a beautiful
park?
There are Many People™ with an implicit assumption that Coming to Correct
Conclusions is just a sequence of Thinking Thoughts, and since Thinking Thoughts
happens in the brain and you have a brain while sitting in an armchair too, you could
just draw the exact same conclusions there as you would in a park.
Yes, in theory, you could.
Reading books reportedly has the eﬀect that the reader forms models of the world
they would't otherwise form. Appreciating art can also spark insights, even when the
art has no propositional content. Being tortured is not conducive to creating accurate
high-level maps. Some funny people even make some funny claims like that sitting in
silence for a while is necessary to understand certain aspects of the world.
In a park, light conditions are going to be diﬀerent, which aﬀects alertness. Walking
aﬀects blood ﬂow to the brain. Context-dependent memory is a thing.
Of course, I couldn't tell the diﬀerence just by reading the resulting essay. Maybe an
expert on armchair-sitting and park-walking could. Maybe a strong enough classiﬁer AI
could. But by default, in the absence of a strong argument otherwise, I'll assume that
the eﬀect is signiﬁcant in some direction, and relevant along at least some reasonable
values humans might care about.
Twitter

Your friend recounts the contents of the latest conversation they had with a stranger
on the internet. Was it a conversation limited to 280 character long units of
transmission, or an exchange of longform essays, or a voice chat?
What would happen to your conversations with your friends if you introduced arbitrary
rules to them? E.g. (babble):
every time you say something, your friend has to explain in their own words
what they think you said,
the order in which participants are allowed to speak is ﬁxed, and so those less
conﬁdent are also forced to contribute to the conversation,
whenever an interesting point comes up, it's written down, and you keep
returning to these points until there is nothing else left to say about them.
I know Many People™ (myself included) have an aversion to introducing arbitrary rules
like that. Why not just let everyone talk whenever they feel like, and say whatever
they feel like? But of course no one wants that: most people would agree that we need
arbitrary rules such as "don't talk over each other", or that moderation is required in
online spaces. Also, you can't avoid arbitrary rules, you can only decide between
letting them be imposed by the environment in an accidental manner or consciously
choosing them.
In theory, none of this should matter. In theory, it's possible to have intelligent
discussion on Twitter.
Final Thoughts
The method you use to create a product leaves its mark on it. The fact that the set of
possible products two diﬀerent methods can create is the same does not mean that
the choice of your method is just a detail of implementation. Sometimes there is no
diﬀerence at all, or no signiﬁcant diﬀerence, or the diﬀerence is signiﬁcant but mostly
irrelevant relative to your goals, but you shouldn't assume this by default, without
some strong arguments.
The power of this concept comes from being able to recognize it at work in various
aspects of your life. This is an excellent topic to do your own babble on.
Thanks to Justis for the feedback that helped me articulate my thoughts.
 

Research Hamming Questions
We'll start with Richard Hamming's original question: what are the most important
problems in your ﬁeld?
(At this point, you should grab pencil and paper, or open a text ﬁle, or whatever. Set a
timer for at least two minutes - or ﬁve to ten if you want to do a longer version of this
exercise - and write down the most important problems in your ﬁeld. The rest of the
questions will be variations on this ﬁrst one, all intended to come at it from diﬀerent
directions; I recommend setting the timer for each of them.)
Perfection
Imagine that your ﬁeld achieved perfection - the ultimate theory, perfect
understanding, building The Thing.
What has been achieved in the idealized version of the ﬁeld, which has not yet been
achieved today? What are the main barriers between here and there?
Measurement
Often, in hindsight, a ﬁeld turns out to have been bottlenecked on the development of
some new measurement method, ranging from physical devices like the thermometer
to abstract ideas like Shannon's entropy and information channel capacity.
In what places does it look like your ﬁeld is bottlenecked on the ability to usefully
measure something? What are the main barriers to usefully measuring those things?
Framing
The diﬃcult thing, in most pre-paradigmatic and confused problems at the
beginning of some Science, is not coming up with the right complicated long
sentence in a language you already know.  It's breaking out of the language in
which every hypothesis you can write is false. [...] The warning sign that you need
to 'jump-out-of-the-system' is the feeling [of] frustration, ﬂailing around in the
dark, trying desperate wild ideas and getting unhelpful results one after another. 
When you feel like that, you're probably thinking in the wrong language, or
missing something fundamental, or trying to do something that is in fact
impossible.  Or impossible using the tools you have. - Mad Investor Chaos
What are the places where your ﬁeld is ﬂailing around in the dark, trying desperate
ideas and getting unhelpful results one after another? What are the places where it
feels like the problem is formulated in the wrong language, and a shift to another
frame might be required to ask the right question or state the right hypothesis?
Uniﬁcation

Sometimes, we have a few diﬀerent models, each of which works really well in
diﬀerent places. Maybe it feels like there should be some model which uniﬁes them
all, which could neatly account for all these phenomena at once - like the uniﬁcation of
electricity, magnetism and optics in the 19th century.
Are there diﬀerent models in your ﬁeld which feel like they point to a not-yet-known
uniﬁed model?
Incompatible Assumptions
One of the main ways we notice (usually implicit) false assumptions in our models is
when they come into conﬂict with some other results, patterns or constraints. This
may look like multiple models which cannot all be true simultaneously, or it may look
like one model which looks like it cannot be true at all yet nonetheless keeps matching
reality quite well. This is a hint to reexamine the assumptions under which the models
are supposedly incompatible/impossible, and especially look for any hidden
assumptions in that impossibility argument.
Are there places in your ﬁeld where a few models look incompatible, or one model
looks impossible, yet nonetheless the models match reality quite well?
Giant Search Space
The space of possible physical laws or theorems or principles is exponentially vast.
Sometimes, the hard part is to ﬁgure out what the relevant factors are at all. For
instance, to ﬁgure out how to reproducibly culture a certain type of cell, a biologist
might need to provide a few speciﬁc signal molecules, a physical medium with the
right elasticity or density, a particular temperature range, and/or some other factors
which nobody even thought to test yet.
Are there places in your ﬁeld where nobody even knows what key factors must be
controlled for some important outcome to robustly occur?
Finding The True Name
Sometimes, most people in the ﬁeld have an intuition that some concept is important,
but it's not clear how to formulate the concept in a way that makes it robustly and
generalizably useful. "Causality" was a good example of this, prior to Judea Pearl & co.
Once we can pin down the right formulation of the idea, we can see
arguments/theorems which follow the idea, and apply them in the wild. But before we
have the right formulation, we have to make do with ad-hoc proxies, "leaky
abstractions" which don't quite consistently generalize in the ways we intuitively
want/expect.
Are there places in your ﬁeld where some concept seems very central to
understanding, but nobody knows its True Name yet?
Meta

Sometimes social problems in a ﬁeld prevent the most important problems from being
addressed - e.g. bad incentives, key ideas reaching too few people, people with
complementary skillsets not running into each other, diﬀerent groups using diﬀerent
language and tools, etc.
At the social level, what are the barriers to solving the main problems in the previous
two questions? Why aren't they already solved? Why isn't progress being made, or
made faster?
Pica
There's a condition called pica, where someone has a nutrient deﬁciency (e.g. too
little iron), and they feel strong cravings for some food which does not contain that
nutrient (e.g. ice). The brain just doesn't always manage to crave things which will
actually address the real problem; for some reason things like ice will "look like" they
address the problem, to the relevant part of the brain.
Are there places where your ﬁeld as a whole, or you personally, pursue things which
won't really help with the main problems (but might kind of "look like" they address
the problems)?
Other Peoples' Answers
Pick someone you know, or a few people, who are smart and have good judgment.
What would their answers to these questions be?
Closing
Hamming's original question was not just "What are the most important problems in
your ﬁeld?". He had two follow-up questions:
I started asking, "What are the important problems of your ﬁeld?" And after a
week or so, "What important problems are you working on?" And after some more
time I came in one day and said, "If what you are doing is not important, and if
you don't think it is going to lead to something important, why are you at Bell
Labs working on it?"
To further quote Hamming: if you do not work on important problems, it's unlikely
you'll do important work.

Even more curated conversations with
brilliant rationalists
Since August 2020, I've been recording conversations with brilliant and insightful
rationalists, eﬀective altruists, and people adjacent to (or otherwise connected) to
those communities. If you're an avid reader of this site, I suspect you will recognize
many of the names of those I've spoken to. Today's post contains a whole new
selection of the LessWrong-relevant recordings that have come out since my last post
about this. I hope you enjoy them! Since last time, we've also started adding
transcripts for a number of the episodes (thanks to those of you who commented on
my last post encouraging us to do this).
The curated list below is organized according to the LessWrong-relevant topics we
cover in each conversation. All of these conversations can also be found on our
podcast website or by searching for "Clearer Thinking" in just about any podcast app. 
If there are other people you'd like to see me record conversations with, please
nominate them in the comments! The format of each episode is that I invite each
guest to bring four or ﬁve "ideas that matter" that they are excited to talk about, and
then the aim is to have a fun, intellectual discussion exploring those ideas (rather than
an interview).
 
Rationality and decision-making
Beyond cognitive biases: improving judgment by reducing noise (with Daniel
Kahneman)
How can we apply the theory of measurement accuracy to human judgments?
How can cognitive biases aﬀect both the bias term and the noise term in
measurement error? How much noise should we expect in judgments of various
kinds? Is there reason to think that machines will eventually make better decisions
than humans in all domains? How does machine decision-making diﬀer (if at all)
from human decision-making? In what domains should we work to reduce variance
in decision-making? If machines learn to use human decisions as training data,
then to what extent will human biases become "baked into" machine decisions?
And can such biases be compensated for? Are there any domains where human
judgment will always be preferable to machine judgment? What does the "fragile
families" study tell us about the limits of predicting life outcomes? What does
good decision "hygiene" look like? Why do people focus more on bias than noise
when trying to reduce error? To what extent can people improve their decision-
making abilities? How can we recognize good ideas when we have them? Humans
aren't fully rational, but are they irrational?
This particular episode is unique in that we've also made a Thought Saver deck of
ﬂashcards to help you to learn or consolidate the key insights from it. You can see a
sample of these below, but you can get the full deck by creating a Thought Saver
account here or by clicking through to the end of the sample. And if you want to

embed your own ﬂashcards in LessWrong posts (like we did below), here's a link to a
post that describes how to do that. 
 
Rationality and Cognitive Science (with Anna Riedl)
What is the Great Rationality Debate? What are axiomatic rationality and
ecological rationality? How irrational are people anyway? What's the connection
between rationality and wisdom? What are some of the paradigms in cognitive
science? Why do visual representations of information often communicate their
meaning much more eﬀectively than other kinds of representations?
Everyday Statistics and Climate Change Strategies (with Cassandra Xia)
What are "shed" and "cake" projects? And how can you avoid "shed" projects?
What is the "jobs to be done" framework? What is the "theory of change"
framework? How can people use statistics (or statistical intuition) in everyday life?
How accurate are climate change models? How much certainty do scientists have
about climate change outcomes? What are some promising strategies for
mitigating and reversing climate change?
Are you a wamb or a nerd? (with Tom Chivers)
What is a "wamb"? What are the diﬀerences between wambs and nerds? When is
it appropriate (or not) to decouple concepts from their context? What are some
common characteristics of miscommunications between journalists and
writers/thinkers in the EA and Rationalist communities? What are "crony" beliefs?
How can you approach discussions of controversial topics without immediately
getting labeled as being on one team or another? What sorts of quirks do
members of the EA and Rationalist communities typically exhibit in social
contexts?
Clearer paths and sharper ideas (with Lynette Bye)
What are "forward-chaining" and "backward-chaining," and how do they connect
with theory of change? What sorts of mental habits and heuristics prevent you
from brainstorming ideas eﬀectively? How can you harness feedback eﬀectively to
sharpen your ideas? From whom should you solicit feedback? How can you view
your own products with fresh eyes? What are some common struggles people
encounter when starting or changing careers, and how can they be overcome?
Why are small experiments so under-used? How can we construct a sustainable
work life? What are the best ways to rest and recover from overwork and burnout?

Evidence, reason, and compassion for all sentient beings (with Jamie Woodhouse)
How can we encourage people to increase their critical thinking and reliance on
evidence in the current information climate? What types of evidence "count" as
valid, useful, or demonstrative? And what are the relative strengths and
weaknesses of those types? Could someone reasonably come to believe just about
anything, provided that they live through very speciﬁc sets of experiences? What
does it mean to have a "naturalistic" epistemology? How does a philosophical
disorder diﬀer from a moral failure? Historically speaking, where does morality
come from? Is moral circle expansion always good or praiseworthy? What sorts of
entities deserve moral consideration?
Consciousness and subjective experiences
When is suﬀering good? (with Paul Bloom)
When (if ever) can suﬀering be good? Is there an optimal ratio of pleasure to pain?
What is motivational pluralism? Can large, positive incentives be coercive? (For
example, is it coercive to oﬀer to pay someone enormous amounts of money to do
something relatively benign or even painful or immoral?) How can moving from
making judgments about a person's actions to making judgments about their
character solve certain moral puzzles? Why do we sometimes make seemingly
irrational judgments about the relative badness of certain actions? How does the
level of controversy around an action factor into how much we publicly disapprove
of it? What are the diﬀerences between compassion and empathy? Is antisocial
personality disorder (AKA psychopathy or sociopathy) deﬁned only by a lack of
empathy? How have humans evolved (or not) to detect and mitigate the eﬀects of
others who feel no remorse? Is altruism especially vulnerable to remorseless
people? What are the diﬀerences between narcissists and sociopaths?
How many minds do you have? (with Kaj Sotala)
What are the advantages of viewing the mind through the multi-agent model as
opposed to (say) the rational/optimizing agent model? What is the "global
workspace" theory of consciousness? What's going on during concentration
meditation according to the global workspace theory? If our brains are composed
of multiple sub-agents, then what does it mean when I say, "I believe such-and-
such"? Are beliefs context-dependent (i.e., you believe P in one context and not-P
in a diﬀerent context)? What eﬀects do the various therapeutic modalities and
meditation practices have on our beliefs? What are the advantages of
transformational therapy over other approaches?
Accessing pure consciousness at any moment (with Loch Kelly)
What is "awakening"? What is a "stateless" state? What is nonduality? Why and
how do some spiritual practitioners experience a dissolution of their sense of self?
Do these altered or enlightened states require thousands of hours of practice to
achieve, or are they always inside us, waiting to be noticed and accessed at any
time? Can these states be accessed through a variety of paths and methods? Is
there a certain kind of person that does better or worse at achieving these states?
Psychological Models and Parenting (with Divia Eden)

What is the Internal Family Systems model? What kinds of information do our
emotions give us? How many agents live in our heads? And, if there's more than
one, how well do those agents cooperate? What is operant conditioning? What is
attachment theory? How does parenting diﬀer from animal training? Is decision
theory able to unify many diﬀerent psychological theories?
Exploring your shadow and healing your traumas (with Aurora Quinn-Elmore)
What is metamodernism? How does metamodernism relate to spiral dynamics?
What does it look like to apply a metamodern approach to large-scale problems?
What are shadow traits, and what is shadow projection? What do our reactions to
others' behavior tell us about ourselves? What's going on psychologically and
physiologically when we relive past traumas? What dosages of psychedelics are
most eﬀective in a therapeutic context? How soon will psychedelic substances
likely be decriminalized or legalized at the state and/or federal level in the United
States? How can we enter into blissful, ecstatic, intense, or other less common
psychological states without drugs or alcohol? What are the pros and cons of
(especially intergenerational) co-living?
Major and minor scales of consciousness (with Andrés Gomez Emilsson)
Should pleasure and pain be measured on logarithmic scales? How might such a
scale aﬀect utilitarian calculations? How do harmonic energy waves in the brain
correspond to states of (or intensities of) consciousness? What sorts of
conclusions can we draw about brain states given the resolutions and sampling
rates of tools like fMRI, EEG, and MEG? What is the symmetry theory of
homeostatic regulation, and how does it connect to pleasure and pain? Are
uncomfortable or confused mental states computationally useful to us? To what
extent can the concepts of musical consonance and dissonance map onto energy
states in the brain?
 
Eﬀective altruism
How can we save the world? (with Toby Ord)
What is "the precipice"? Which kinds of risks (natural or technological) pose the
greatest threats to humanity speciﬁcally or to life on Earth generally in the near
future? What other kinds of existential risks exist beyond mere extinction? What
are the diﬀerences between catastrophic risks and existential risks? How serious is
the threat of climate change on an existential scale? What are the most promising
lines of research into the mitigation of existential risks? How should funds be
distributed to various projects or organizations working on this front? What would
a world with existential security look like? What is diﬀerential technological
development? What is longtermism? Why should we care about what happens in
the very far future?
How to use your career to have a large impact (with Ben Todd)
What is 80,000 Hours, and why is it so important? Does doing the most good in
the world require being completely selﬂess and altruistic? What are the career
factors that contribute to impactfulness? How should people choose among the

various problem areas on which they could work? What sorts of long-term AI
outcomes are possible (besides merely apocalyptic scenarios), and why is it so
important to get AI right? How much should we value future generations? How
much should we be worried about catastrophic and/or existential risks? Has the
80,000 Hours organizing shifted its emphasis over time to longer-term causes?
How many resources should we devote to meta-research into discovering and
rating the relative importance of various problems? How important is personal ﬁt
in considering a career?
Why it's so hard to have conﬁdence that charities are doing good (with Elie
Hassenfeld)
How does GiveWell's approach to charity diﬀer from other charitable
organizations? Why does GiveWell list such a small number of recommended
charities? How does GiveWell handle the fact that diﬀerent moral frameworks
measure causes diﬀerently? Why has GiveWell increased its preference for health-
related causes over time? How does GiveWell weight QALYs and DALYs? How much
does GiveWell rely on a priori moral philosophy versus people's actual moral
intuitions? Why does GiveWell have such low levels of conﬁdence in some of its
most highly-recommended charities or interventions? What should someone do if
they want to be more conﬁdent that their giving is actually having a positive
impact? Why do expected values usually tend to drop as more information is
gathered? How does GiveWell think about second-order eﬀects? How much good
does the median charity do? Why is it so hard to determine how impactful
charities are? Many charities report on the eﬀectiveness of individual projects, but
why don't more of them report on their eﬀectiveness overall as an organization?
Venture capitalists often diversify their portfolios as much as possible because
they know that, even though most startups will fail, one unicorn can repay their
investments many times over; so, in a similar way, why doesn't GiveWell fund as
many projects as possible rather than focusing on a few high performers? Why
doesn't GiveWell recommend more animal charities? Does quantiﬁcation
sometimes go too far?
EA Eﬃcacy and Community Norms (with Stefan Schubert)
How can people be more eﬀective in their altruism? Is it better for people to give
to good causes in urgent situations or on a regular basis? What causes people to
donate to less eﬀective charities even when presented with evidence that other
charities might be more eﬀective? We can make geographically distant events
seem salient locally by (for example) showing them on TV, but how can we make
possible future events seem more salient? How much more eﬀective are the most
eﬀective charities than the average? How do altruists avoid being exploited (in a
game-theoretic sense)? What sorts of norms are common in the EA community?
What does humanity need to survive after a global catastrophe? (with David
Denkenberger)
What kinds of catastrophic risks could drastically impact global food supply or
large-scale electricity supply? What kinds of strategies could help mitigate or
recover from such outcomes? How can we plan for and incentivize cooperation in
catastrophic scenarios? How can catastrophic and existential risks be
communicated more eﬀectively to the average person? What factors cause people
to cooperate or not in disaster scenarios? Where should we be spending resources

right now to prepare for catastrophe? Why does it seem that governments are
largely uninterested in these questions?
How to measure impact, and why we may have all been doing it wrong (with Michael
Plant)
Researchers in the Eﬀective Altruism movement often view their work through a
utilitarian lens, so why haven't they traditionally paid much attention to the
psychological research into subjective wellbeing (i.e., people's self-reported levels
of happiness, life satisfaction, feelings of purpose and meaning in life, etc.)? Are
such subjective measures reliable and accurate? Or rather, which such measures
are the most reliable and accurate? What are the pros and cons of using QALYs
and DALYs to quantify wellbeing? Why is there sometimes a disconnect between
the projected level of subjective wellbeing of a health condition and its actual
level (e.g., some people can learn to manage and cope with "major" diseases, but
some people with "minor" conditions like depression or anxiety might be in a
constant state of agony)? What are some new and promising approaches to
quantifying wellbeing? The EA movement typically uses the criteria of scale,
neglectedness, and tractability for prioritizing cause areas; is that framework still
relevant and useful? How do those criteria apply on a personal level? And how do
those criteria taken together diﬀer conceptually from cost-eﬀectiveness? How
eﬀective are psychological interventions at improving subjective wellbeing? How
well do such interventions work in diﬀerent cultures? How can subjective
wellbeing measures be improved? How can philosophers help us do good better?
 
The future of society 
Why do civilizations collapse? And is ours next? (with Samo Burja)
What is "long" history? Why don't historians usually focus on what happened
before recorded human history? What (if anything) is special about agriculture
when it comes to the development of civilization? How far back does human
civilization go, and why should we care? Have humans always been gardeners?
What factors cause civilizations to crumble or thrive? Should we reboot
standardized tests and college admissions every few decades so that measures
don't become targets? Which destructive factors are particularly salient to modern
human civilization? Why is there such a disconnect between our intuition that
progress is inevitable and our knowledge that virtually all civilizations have
collapsed in the past? In other words, what makes us think that we'll succeed
where others have failed? How does a functional social institution diﬀer from a
failing one? What is the "great founder" theory?
An interview with an A.I. (with GPT-3 and Jeremy Nixon)
What is machine learning? What are neural networks? How can humans interpret
the meaning or functionality of the various layers of a neural network? What is a
transformer, and how does it build on the idea of a neural network? Does a
transformer have a conceptual advantage over neural nets, or is a transformer
basically the equivalent of neural nets plus a lot of computing power? Why have
we started hearing so much about neural nets in just the last few years, even
though they've existed conceptually for many decades? What kind of ML model is

GPT-3? What learning sub-tasks are encapsulated in the process of learning how to
autocomplete text? What is "few-shot" learning? What is the diﬀerence between
GPT-2 and GPT-3? How big of a deal is GPT-3? Right now, GPT-3's responses are
not guaranteed to contain true statements; is there a way to train future GPT or
similar models to say only true things (or to indicate levels of conﬁdence in the
truthfulness of its statements)? Should people whose jobs revolve around writing
or summarizing text be worried about being replaced by GPT-3? What are the
relevant copyright issues related to text generation models? A website's
"robots.txt" ﬁle or a "noindex" HTML attribute in its pages' meta tags tells web
crawlers which content they can and cannot access; could a similar solution exist
for writers, programmers, and others who want to limit or prevent their text from
being used as training data for models like GPT-3? What are some of the scarier
features of text generation models? What does the creation of models like GPT-3
tell us (if anything) about how and when we might create artiﬁcial general
intelligence?
What causes progress? And how can we stop it from slowing? (with Jason Crawford)
What is progress? How do we (and should we) measure progress? What are the
most important questions to ask in progress studies? What are the factors that
lead to progress? Why has large-scale progress taken so long (i.e., why did we not
see much progress until the Industrial Revolution)? Why did the industrial
revolution, scientiﬁc revolution, and democratic revolution all seem to start within
a relatively short period of time of each other? How can we prevent progress from
slowing down, stopping, or even reversing? What factors have contributed to the
slowing of progress in the last 50 years? What's the state of progress in nuclear
energy? What is the history of attitudes towards progress? And why is it important
for people to believe that progress is good?
Utopia on Earth and morality without guilt (with Joe Carlsmith)
What are some of the challenges of deﬁning utopia? What should a utopia look
like? What are concrete versus sublime utopias? What are some of the failure
modes related to various conceptions of utopia? Is it really that hard to create a
shared, positive vision of the future? What is the value (or disvalue) of creating
new people, especially in relation to the utopic or dystopic state of the world?
What is "whole-hearted morality" versus "morality-as-taxes"? How can we
encourage people to be more moral without harming them psychologically (e.g.,
by loading them down with guilt)? Which sorts of worldview changes are
reversible? Where does clinging ﬁt into the constellation of concepts like valuing,
caring, envying, etc.? How does non-attachment diﬀer from indiﬀerence? Is
clinging always bad? Is philosophy making tangible progress as a ﬁeld? Is
philosophy's primary function to show us how our questions are confused rather
than to give us direct answers to our questions? Has philosophy given us a clearer
picture of what consciousness is or isn't?
 
Self-improvement
The secrets of eﬀective learning (with Andy Matuschak)

How can we accelerate learning? Is spaced repetition the best way to absorb
information over the long term? Do we always read non-ﬁction works with the goal
of learning? What are some less common but perhaps more valuable types of
information that can be put on ﬂashcards? What sorts of things are worth
remembering anyway? Why is it important to commit some ideas to memory
when so much information is easily ﬁndable on the internet? What beneﬁts are
derived from being involved in all stages of a project pipeline from concept to
execution (as opposed to being involved only in one part, like the research
phase)? Why should more researchers be involved in para-academic projects?
Where can one ﬁnd funding for para-academic research?
Self-Improvement and Behavior Change (with James Norris)
How can you live your best life? What's a good deﬁnition of "wisdom"? What are
some possible taxonomies of life outcomes? What are some low-hanging fruit in
the realm of self-improvement? What are some useful behavior change
frameworks and techniques?
Productive Conversations and Feedback Loops (with Julia Carvalho)
How can we become better leaders? How can we give better feedback to others?
How can we be better listeners? How can we give good advice? How do startups
(or even existing companies) build great products? What sorts of things do experts
actually know? When is it useful to poll customers for feedback?
Risk-Driven Development and Decentralization (with Satvik Beri)
What is risk-driven development? How should we weigh advice, best practices,
and common sense in a domain? What makes some feedback loops better than
others? What's the best way to take System 2 knowledge and convert it to System
1 intuition? What are forward-chaining and backward-chaining? When is it best to
use one over the other? What are the advantages and disadvantages of
centralization and decentralization?
 
Economics
Tyler's three laws and twelve rules (with Tyler Cowen)
Why might it be the case that "all propositions about real interest rates are
wrong"? What, if anything, are most economists wrong about? Does political
correctness aﬀect what economists are willing to write about? What are the
biggest open questions in economics right now? Is there too much math in
economics? How has the loss of the assumption that humans are perfectly rational
agents shaped economics? Is Tyler's worldview unusual? Should people hold
opinions (even loosely) on topics about which they're relatively ignorant? Why is
there "something wrong with everything" (according to Cowen's First Law)? How
can we learn how to learn from those who oﬀend us? What does it mean to be a
mentor? What do we know and not know about success? What is lookism? Why is
raising someone else's aspirations a high-return activity?
Can the economy grow indeﬁnitely? (with Alyssa Vance)

How is the economy like a diﬀerential equation? Can the economy grow
indeﬁnitely? Are there economic attractor states? Or are economic outcomes
chaotic and/or extremely sensitive to certain variables? What should we know
about progress in genetic engineering? Can you (and should you) do genetic
engineering in your garage? What are some common mistakes people make when
thinking about AI? Should we expect AI abilities to converge in some domains and
diverge in others? Why do we sometimes collectively forget important ideas? Have
we as a species grown wiser over the course of our history? How can we form
high-trust communities on the internet? In the context of social media, is ease of
access at cross-purposes with membership screening and/or costs, or is it possible
to have both? What should we make of ephemeral communities that appear
brieﬂy, do something huge, and then disappear (like the WallStreetBets subreddit
phenomenon)? What are the various types of misinformation being used in the US,
Russia, China, and elsewhere?
 
Politics
The clash between social justice and anti-wokeness (with Amber Dawn and Holly
Elmore)
Is it okay for anyone to have opinions about marginalized communities even if
they're not a part of those communities? Do people in marginalized groups have
special knowledge (especially tacit knowledge) about their groups that can't be
known or experienced from the outside? To what extent can we know and
empathize with others' experiences regardless of diﬀerences in race,
socioeconomic status, gender, sexual orientation, etc.? Do oppression and
discrimination tend to be caused more by active bigotry or by mere lack of care
and awareness? What information (if any) does intersectionality fail to capture
about people? Is describing someone intersectionally an end in itself, or is it just a
way of correcting (or over-correcting) for the suppression of marginalized voices?
Should ideas be discussed absent their context or implications (see: decoupling
norms vs. contextualizing norms)? To what extent should we focus on individuals
versus groups when attempting to ﬁx inequities? Are individuals or groups
responsible for redressing the atrocities of their ancestors? Should people be
"canceled" for their views (including their past views, even if their current views
are diﬀerent)? To what extent is the shifting of moral ground around social justice
issues unpredictable and/or disorienting? How can democratic societies balance
the need to debate diﬃcult ideas with the risk of giving reprehensible ideas a
platform? Should rules about oﬀensiveness be enforced from the top-down (e.g.,
from a government, a school administration, a company's board of directors, or
even parents)? Is oﬀense only "in the eye of the beholder"?
 
Cryonics 
Freezing to (not) death: cryonics and the quest for immortality (with Max Marty)
What is cryonics? And how does it work? What do we know right now about
reversing death? And what would we have to learn to make resurrection from a

cryogenically frozen state feasible? How much does cryonics cost? What
incentives would future people have for reviving a cryo-frozen person? How likely
is it that a cryo-frozen person will be brought back in the future? Why do people
(even pro-cryonics people) "cryoprastinate" and put oﬀ considering cryonics for a
later time? What sorts of risks are involved in being frozen and later revived?
What philosophical and ethical issues are at stake with cryonics? Would a revived
person be able to integrate into a future society? Why is there a stigma around
cryonics in some cultures?
 
Some other relevant people that I've already recorded with, but haven't yet released
the episodes for, include Matt Goldenberg, Habiba Islam, Leah Edgerton, Katja Grace,
Literal Banana, Eric Schwitzgebel, Buck Shlegeris, Cate Hall, Alene Anello, Peter
Wildeford, Elizabeth Edwards-Appell, and Aaron Hamlin. Please let me know in the
comments who else you think I should record with! :)
 
 

Lessons After a Couple Months of
Trying to Do ML Research
I'm a 17 y/o who started doing transformer interpretability work around October 2021,
mainly for the learning experience, but also with the goal of potentially ﬁnding
something cool and interesting about transformers. I'm writing this post to consolidate
some lessons I learned since then, and I hope that some of these ideas will be
useful to other early career people hoping to do ML research. If you think any
of this advice is wrong/misleading, please comment!
Getting Good Research Intuitions
For the majority of the past couple months, my work was exploratory. I was reading
papers, talking to other researchers in the ﬁeld, reimplementing things, messing
around with other people's codebases or trying weird experiment ideas. Most of this
work was done to build good research intuitions: how do I set my priors such
that I have a) good ideas for what's interesting, b) principled mental models of [insert
research topic] (transformer internals in my case) and c) reasonable predictions for
my hypotheses and experiments? 
Recognizing Promising Topics & Ideas
The "good idea for what's interesting" part is what I think is the most important part
of having good research intuitions. In my mind, "good idea for what's interesting"
encompasses both having a good idea of which topics are interesting and also having
a good idea for which experiment ideas and hypotheses are promising. 
Fortunately, the ﬁrst part — knowing which areas of research are promising —
shouldn't be too hard if your ﬁeld of interest is big enough. I think the standard advice
of 1) ﬁgure out who all the important people are in the ﬁeld, 2) ﬁgure out what they
think is interesting and 3) ﬁgure out why works here. That plus an understanding of
the importance, tractability, and neglectedness of each topic should be enough
information for you to make a logical decision of which topic you want to work on. But
I don't think I would recommend going that deep on deciding which topics are
interesting. What's probably faster and better anyways is, after having a basic
overview of the ﬁeld, to just pick the topic that you vibe with the most and
double down on that for a while. Obsessive interest is a powerful indicator of
genius (bus ticket theory of genius) and you'll work harder and better on the things
that you have a strong internal compulsion towards. 
The second part — knowing which experiment ideas are promising — is a lot trickier. I
think this ability mainly boils down to practice, experience, and time. And, of course,
practice and experience involves having ideas in the ﬁrst place! You're never too early
to have ideas, and even though your initial ideas will probably be bad, that's part of
the feedback process. Have a lot of ideas! Make a lot of predictions! Think
about how to test those predictions! And after doing this a lot and getting
feedback (through mentorship, experiments, reading, etc), your intuitions will
eventually be tuned towards better and more interesting ideas. 

Also, a really nice thing about doing this kind of thinking before you "learn" the ﬁeld is
that many times, answers might already exist, and so you can get feedback on your
ideas and your mental models a lot more quickly. Instead of having to run and design
an entire experiment, a simple Google search or email might be all you need. 
Principled Mental Models 
I think I'm still pretty far from having really principled mental models of how modern
deep learning works. The steps that I know to gain these mental models are the
standard a) learn all the math very well (3b1b is gold for the basic calc & linear
algebra intuitions), b) absorb the mental models and intuitions of more established
researchers, and c) spend a lot more time doing research. 
Reasonable Predictions for Hypotheses/Experiments
Having principled mental models is one way of having reasonable predictions for your
experiments. The ﬂip side of the coin is to have more willy nilly, heuristics-based
predictions for experiments based on empirical evidence. Of course, the way
to get these heuristics is to run a bunch of experiments. 
If you don't have a lot of experiment ideas already, one class of exploratory
experiment ideas, where you can a) get practice coding and b) tune your predictive
engine, is to graph a bunch of statistics about a component of interest and predict
what happens. These experiments can range from "Yeah, I deﬁnitely know what's
going to happen" to "I have no idea what's going to happen" and still be valuable. 
Exploratory Research vs Directed Research
In general, if you want to ﬁnd something interesting, I think a directed research
agenda (i.e. I have a speciﬁc story of how X works and I'm trying to prove whether
it's true or false) is almost always better than an exploratory research agenda
(i.e. I have a bunch of random ideas and I'm just going to go down the list until I ﬁnd
something interesting). This is mainly because I think there's a lot of noise in ML
research speciﬁcally, and it's harder to distinguish between signal and noise if you
don't really know what you're looking for (as in pursuing an exploratory research
agenda). 
However, if you're not trying to ﬁnd something interesting, and just trying to
build intuitions or practice, then I think exploratory is the better option. This
is mainly because I think you need some threshold of experience and intuition before
you can perform a directed research agenda well, and I think you can get experience
and intuition and momentum faster by iterating through a set of experiment ideas
instead of focusing on one strongly. 
Machine Learning Speciﬁc Advice
Beware Bias, Bugs, and Bizarreness
Oftentimes when doing ML research, you'll get results that are very weird and
surprising, and oftentimes those results are the results of bugs. And you don't even

really need a lot of prior experience to be able to recognize bug-caused-weirdness i.e.
your model outputting "!" with 100% probability for every single input. Many times
also there'll be subtler, surprising things that you might not notice. A general good
piece of advice is for any result that is signiﬁcant, surprising or weird in any
way, go do a bunch of sanity checks to ﬁnd other potential explanations for
the result (which, besides bugs, could also be bias or some weird quirk of ML). In
general, the more surprising a result is, the more skeptical you should be of its
validity. 
Know When to Stop Investigating
Oftentimes, you'll also get results that are weird and surprising, and those results
are not the results of bugs, and they're simply just weird and surprising. Many times,
these weird results will not really have any bearing on your current research agenda,
but because they're weird and interesting, you might feel the urge to investigate and
ﬁgure out what's going on. This is a ﬁne line to walk, but, in general, I don't think
it's worthwhile to spend a lot of time trying to explain irrelevant, weird
phenomena because machine learning is full of irrelevant, weird phenomena, and it's
a really easy way to derail the momentum you've built working on your main agenda. 
Miscellaneous
Never delete code or results!!!
Especially if the results took a long time to compute
This sounds obvious, but a decent amount of times in the past I've thought
that something wasn't going to be useful, deleted it, and then realized I
later needed it for something else. 
Keep track of your codebase on Github. It's pretty useful sometimes to see
how you implemented something in the past, even if that code becomes
obsolete. 
Anytime you want to actually prove something, use a big sample size
Google Colab is always a good place to start doing exploratory stuﬀ, but once
your code base starts growing more and more complex, VScode Interactive
Mode or a Jupyter notebook is useful to be able to import your own functions
from local modules
Getting Started
Here's a list of less abstract advice on how to get started doing machine learning
research:
Find a good mentor(s) [very important]
People say it all the time, but a good mentor will drastically accelerate your growth
and progress. In my opinion, the qualities to be on the lookout for, in
descending order of importance, are 1) time spent with mentor, 2) mentor's
relevance to your speciﬁc interests, and then 3) prestige. 
Since most potential mentors will be a lot more experienced than you anyways, the
amount you can learn from a mentor is probably more directly proportional to the

amount of time you spend with them, instead of how closely related their interests are
to your research interests or how connected/prestigious/etc they are.
Reach out to the authors of any paper you liked for a call
Talking to the authors of good papers is a really good way to learn more about your
research topic and gain tacit knowledge about how the research process actually
works. It's also a good way to engineer serendipity and ﬁnd new mentors or
collaborations (my current collaboration with a group from AI2 started with one good
call). Some advice on how to make the most of a call with a researcher: 
Ask about the journey of the research presented in the paper. You'll gain a lot of
tacit knowledge that way. 
Come with your own ideas related to their paper (i.e. did you try X? What do you
think would happen if I did Y?) and ask for feedback
Form a continuous relationship and stay in touch. 
Keep a good research journal
A research journal is an important personal reminder for what you did and what you
learned, but write it as if you're communicating your results to a stranger. You might
be surprised by how much you forget about the experiments that you ran from several
months ago. 
Research Journal Tips:
Store graphs, thoughts, results, notes from calls, any content related to your
research in your notebook. Date your journal and use informative headings. 
Write your research journal with as little assumed context as possible. Always
label your axes in your graphs. Explain the exact experiment you ran as clearly
as possible. Future you will thank you. 
How to write a research agenda:
The three most important parts of any research agenda is a) the research question to
be answered b) why this research question matters and c) what concrete directions
are going to be taken to answer the question. So when writing a research agenda, I
like to follow a structure that looks something like:
Overarching Questions
Super broad questions. The kind of questions you write about in the
discussion of a paper.
Why answers to these questions are important
Research Questions
These kinds of questions suggest hypotheses/experiment ideas by their
nature. 
Usually they're about speciﬁc behavior/phenomena
Experiment Ideas
 
Thanks for reading! And thank you to Alex Gray and Oam Patel for helpful feedback on
this post. 

 

How to Make Your Article Change
People's Minds or Actions? (Spoiler:
Do User Testing Like a Startup Would)
Are you writing an article that's supposed to change people's minds and/or actions?
For example, an article about career advice, productivity, or getting people to apply
for a job? Then this post is for you!
TL;DR: For important articles I recommend doing real user testing (even with just one
person!). Try it and you'll never go back.
I speciﬁcally recommend:
(1) Watch them read it, don't just ask for
Google Doc comments
Open a zoom, share screen, and ask them to share their thoughts live as they read.
(2) Show it to your target audience, don't just
ask others what your target audience would
think
If your article is supposed to change developers' career plans, ask a dev who might
change their career plan. Don't ask a biologist what a dev might think.
(3) Check if they changed their mind, not just
if they like your writing
If they end with "I've got to do this now!": That's a good sign. 
If they say "sounds interesting, I'll think about it", that's not as good.
I personally like to ﬁrst ask the other person for their opinion (for example, "what's
your current career plan?"), then let them read the article, and then ask again.
The general intuition:
If you want to adopt my frame of mind, ask yourself: Would your idea of how to
improve [the article's persuasiveness] also improve [how engaging a startup's app] is?
Here are some exercises, do you think these would work?

Let GPT-3 look at the app (or article) and tell you how engaging the app (or how
persuasive the article) is
Try to guess in advance all the reasons why your target audience won't agree
with the article, or won't be engaged with the app
Hint: Is the test [checking if a real user from your target audience changes their mind]
or [using some proxy to guess whether a real user would change their mind] ?
How to start?
Easiest: post in Bountied Rationality, or in another group that has your target
audience.
A draft post you can copy & paste: "Hey, I'll pay $10 for someone in [describe your
target audience here] to review a post of mine for the EA/lesswrong forum. I want to
do it live, over a video call. The call is capped at 30 minutes (and expected to be less).
Comment or message me if you're interested. Tagging or referring relevant people is
also appreciated."
 
Thanks to Matt Brooks and Daniel Reeves for helping make this article better <3

How to Best Use Twitter
[Note: While I do intend to write more about Russia's invasion of Ukraine, this post is
intended to address this only indirectly rather than directly, by helping illustrate how
to ﬁnd other sources of information, and you are once again implored to rely on other
news sources. I need to get inside the war's OODA loop to write about it usefully,
which requires more dedicated time than I've had this past week.]
It is high time for me to talk about the only practical way I know about to follow
developments in the world in real time, whether they be a war, a pandemic or
something else entirely, which is Twitter and in particular Twitter Lists.
I do not know of any practical alternative. One can of course watch or read the usual
news reports, which are mostly eﬀectively State Media of various quality, for various
diﬀerent States. When you're reading about an actual war, the State part of State
Media becomes more prominent and harder to miss.
The best TV sources for international events like the war that I have are Bloomberg (as
far as I can tell, the closest thing to non-state media) for the economic side of things
and the BBC World News for the more general side. Occasionally I'll take a glance at
CNN or Fox News or various other networks to get a sense of what they are focusing
on. I have not attempted to directly watch any Russian broadcasts but am curious
what is the best option for that.
For domestic American events, there are no non-obvious TV sources I have found, and
TV is essentially useless other than to know what the Narratives are saying or to cover
discrete events like debates, elections or the State of the Union. Any kind of #Analysis
is strictly fuhgeddaboudit.
For written media, the usual suspects are what they are so choose your favorites.
None of them seem able to keep up with the pace of play other than Bloomberg
oﬀering insight into markets, so they are mostly again useful for 'how are things being
presented and sold' than insight into actual events.
For what is happening, Twitter is where it is at.
To use Twitter properly, there are four vital pieces of technology.
1. Tweetdeck or another similar alternative application.
2. Knowing who to follow and read.
3. Lists.
4. Unfollows, ﬁlters, mutes and blocks.
I'll also give advice for how to start out, and some thoughts on regulating use.
Tweetdeck
Twitter by default will present posts using an algorithm. Things it thinks you will be
more interested in, or that will generate more engagement, or that otherwise it
decides on a whim to show you, will be shown to you. Things it disfavors will be
dropped on the ﬂoor.

This is no good. The biggest issues are:
1. It makes it impossible to follow things in real time.
2. It makes it impossible to reliably see things you decide you want to see.
3. It makes it impossible to know when you're 'done' with new content.
4. It makes it impossible to avoid seeing things more than once.
5. It inserts random stuﬀ you never asked for.
Twitter's website is ﬁne for sending direct messages and chatting. It's acceptable for
checking individual proﬁles to see what they have to say and whether you want to
include such a person in your information diet. It's also ﬁne for viewing an individual
list. That's another reason to emphasize lists over following people.
However, for the purpose of following the people you follow, Twitter's website is
useless. You want to use something else.
My choice has been Tweetdeck. Tweetdeck allows you to have lots of columns, one for
people you follow, one for notiﬁcations and then any number of additional lists. Also
it's free.
It is deﬁnitely not perfect. When I want to explore a particular Tweet or screenshot it, I
still open a default Twitter window for that.
So there is probably something out there that works better, and hopefully people will
let me know about it in the comments. But Tweetdeck is easy to set up, easy to use,
free and has served me well.
Knowing Who to Follow
Following someone is a package of several goods.
1. They and potentially others see you follow them.
2. They can DM you and perhaps you can DM them.
3. The posts you want from them appear in your feed.
4. The posts you don't want from them appear in your feed.
5. You see interactions between people if and only if you follow all of them.
6. If they're locked it's the only way.
Some but not all of these also apply to adding people to lists, which I'll get to in the
next section.
Sometimes the DM clause is important, but mostly it is not. I ﬁnd it far better to leave
DMs fully open. If someone wants to talk to me, I want to see what they have to say
and decide whether to respond. There is a rate of contact at which that would become
non-viable, but I am nowhere near that threshold. So the DM thing is mostly moot.
If you follow someone they will see that you follow them. This sends them a message
that you see what they are saying and are interested in what they have to say. In
many cases, that is a message you'll want to send. It can also be important to
friendships. I don't mind doing this as a token for people who post super rarely, but if
someone posts a lot of stuﬀ you don't ﬁnd valuable, it's important to know to pull that
plug.
Most of the question is: Do you want this person's posts in your feed?

Marc Anderson's theory is to follow (and block) based on a single Tweet. I think this is
a terrible, no-good, very bad take. If I blocked people for one bad take the way he
does I would block him for it. He would, of course, approve.
If you go that route, you'll end up with a ﬁrehose of people most of whom are mostly
saying things you do not care about and/or things of not very high content value. With
too many of them, you'll start missing high value things because you can't process all
the info. Meanwhile, with too many things in the ﬁrehose, you won't notice that a
given person is low value unless they provoke you to block them, so you won't know
to get rid of them.
Thus, I follow the principle of look at their posts based on one Tweet, and form a
follow/list/no-follow decision.
What I am looking for is their average quality. A good follow often posts rarely, but
makes your life better when they do post. Having 10,000 people who post once a year
but post a lot of great stuﬀ is great. If someone almost never posts, but you know
they're someone worth engaging with in general, that's usually a great follow.
If you post every hour, that can obviously provide a lot more value, but it can also
waste a lot of time, and bad interactions also actively make your life (very slightly)
worse. The easier it is to instantly notice something is not worth your time, and the
less it pushes your buttons, the lower the cost it imposes - so for example Wordle is a
negative, but only a small one. A Tweetstorm of 100 posts in a row is much less
punishing because if you don't like it it's easy to skip, and so on.
Once you follow someone, start tracking in the back of your mind whether seeing their
posts is making your life better or worse. If it turns out you've made a mistake, you'll
want to quickly correct it, either demoting to a list or eliminating entirely. It's mostly a
lot easier to ﬁgure this out all at once before you start, so I try to focus on that, but
that can give you the wrong idea.
It's also a good idea to periodically review your list of people you follow and remove
those who you realize no longer belong or are not pulling their weight. As a rule of
thumb, once you get above about 300 people, you should be very suspicious that you
are following too many. If you decide to disagree with me and not use lists at all, that
number can likely go somewhat higher to something like 500-600, but thousands is
deﬁnitely a mistake.
Adding someone as a full follower is a high bar for me. Adding someone to a list is a
lower bar. Adding to both is an even higher bar, since it means seeing things twice,
but you may need to do this to capture their interactions with others, or if others use
your lists.
It's true for lists as well, but if you are following someone who posts at all regularly,
that entails knowing how you want to handle their Tweets. Who is to be taken
seriously? Who is coming from which perspective? Who is purely for fun? Who wants
personal interactions? If your answer is 'this is too much information to be tracking'
then you are following too many people. If someone basically never posts, then it can
be worth keeping them around despite this.
Things to Beware

What are some good reasons to consider not following someone?
1. Lots of stuﬀ you don't care about. Wordle, cooking that doesn't grab you,
personal details you don't care about, lots of stuﬀ about topics you simply don't
care about. You want to tolerate some of this, but if someone is mostly talking
about cars and you don't care about cars, don't follow them.
2. Self-promotion. Anyone who does a lot of self-promotion has to really earn
their keep in other ways.
3. Spam. Especially if it's dangerous spam, but also if it's 'retweet this to win!'
Again, I ﬁnd this is more costly than you may realize.
4. Spoilers. If people ruin TV shows, movies or sporting events you care about,
sorry, that's simply not acceptable. I have a sense of when I consider Twitter
'safe' versus 'unsafe' and during 'unsafe' periods it's my fault, but if I need to be
current on Mets games to check Twitter, that's not gonna work.
5. Not Safe For Work. You likely want to make sure these are in their own list.
6. Misinformation. Everyone makes mistakes and that's ﬁne, but there are limits
to how often is acceptable especially if it's happening on purpose... unless you
are fully aware of the problem and want to know the word on the street.
7. Politics, especially Political Advocacy. Sometimes you want politics. I
maintain a politics list, and if something suﬃciently important is happening
you'll want to know. But I believe strongly in keeping the strong forms of this
mostly distinct, especially perspectives that reliably rile you up and make you
angry, either at them or at the other side or at the world in general. That's not
something you want to happen continuously.
8. Just Not That Great. Someone had a good week and you followed them, but
mostly they're just not that great. Pull the plug.
9. Getting Old Fast. There's a bunch of things that are fun for a while, but which
get old after a while. This can include gimmick accounts that are fun for a week
and then that's enough, or less subtle stuﬀ that can last longer. It's not them,
it's you.
10. Cancel Bait. If you're in a sensitive position, 'following the wrong account'
might be an issue at some point. Also you don't want to be interacting too much
with anyone who has a tendency to take one mistake or disagreement and blow
it way out of proportion, so consider steering fully clear.
Lists
Sometimes I want to know what is happening in the political or economic worlds, or
among fellow Magic: The Gathering players, or what the broader rationalist-adjacent
world is thinking, or sports, or what is going on with Covid-19 or the war. At other
times, I very much want to ignore such matters, or only pay attention to extent they
are important.
Even if I want to read up on all the day's developments in the next hour, I'd rather do
them one area at a time. Going back and forth between games and Covid and
economics requires constantly shifting what programmers call 'state' and is best
minimized.
By default social media forces you to take whatever is on oﬀer, but Twitter lets you
avoid this via the use of lists. You should use them aggressively, whether or not you
choose to roll your own. Lists also let you oﬀer those lists to others.

This gives a very large advantage to accounts that have focus. Someone who clearly
belongs on one of your lists is far more useful than someone whose focus is split, even
if you are inclined to be ﬁne with the diﬀerent portions. If you're not interested in the
other half of what someone posts, that's a lot of de facto spam to ﬁlter out, and
anything in the wrong mode can eﬀectively be spam.
For each list, the goal is to have enough people to get a feel for what is happening in
the area, but not so many that it overwhelms, which tends to mean you want between
20-100 members depending on the details. Much more than that and it's probably
more of a ﬁrehose.
What lists should you have?
There are two basic lists I think essentially everyone should have: The NSFW List and
the Politics List.
The NSFW List is the one you keep private. By default, anyone can see who you follow,
and also by default those posts can pop up on the screen any time you check your
phone. It's good to keep at least one list where you put all the people who have things
you'd like to keep to yourself, and it will often make sense to have two or more -
either because they are diﬀerent subcategories, or because there's a 'top' one you
want to check on the regular and that is similar to the secret version of follows, and a
'normal' one for things you only check sometimes or don't mind missing.
The Politics List (here is mine) is there because you want to usually be uninterested in
politics, but that does not mean politics is uninterested in you. Anyone who riles you
up on the regular and isn't a very good personal friend goes onto this list. The usual
suspects that tell you what Narrative is from both sides should also go here even if
you ﬁnd them wrong and obnoxious - you'll want all the top presidential candidates,
many of the high oﬃcials, and representatives from not only ingroup but also
outgroup and fargroup.
This is so you know what they are saying during key developments, and by putting
them on this list you'll hopefully be in the mental state to not let it get to you. Having
someone here is very very much not an endorsement of any kind - it takes all kinds.
Right now, of course, you'll probably want a particular War list for accounts that are
covering developments there. I have one of my own, but I'm not ready yet to
transition over to it, so I've been using the ones from Noah Smith and from Bob Gurley
more, and this larger one when I want a larger ﬁrehose for a bit. 'Rolling your own' in
this situation is likely not worth it for most users.
Similar to Politics I have a list for Economics. If you have other similar technical ﬁelds
you're keeping up with, you'll want something similar.
Naturally I have a list for Covid-19. I keep it small, as it is only the people I want to
check for Covid-19 but that I do not want to keep following post-pandemic.
If you have hobbies that form their own worlds, it's good to have a list for them.
I have a list for Magic: The Gathering, which is a bunch of Magic players who often talk
about Magic things. I've become steadily less engaged over time so this has been
getting steadily more neglected and old-school, and I rarely check it.

I of course have a Rationalists list. There are some quirky inclusions and exclusions
here. and this list has the most people I also follow because it's important if you want
to get the most out of such people to get the interactions, which have a lot of the
value. In particular it's worth noting that this includes a number of people who
wouldn't self-identify as rationalists, including the meta-rationalists and the post-
rationalists but also people who simply talk and think in the same way. It also includes
one historian who goes on lots of insanely long threads because I don't know where
else to put her.
I have a list called Shameless Commerce which is a place for accounts for restaurants,
game companies and related people who are very much going to spam ads at you or
at least links to articles, but where that is something suﬃciently relevant to my
interests. I have another (private) list for musical artists and similar folks as a way to
track tour dates and such, but it's hard to use because all of them are very spammy.
I have an old list called comedians that should be self-explanatory but it turned out
that Twitter wasn't a good medium for comedy, so it's mostly deprecated now.
Finally for public consumption I have a list called Football that I'd like to have more
good members for, that is for when I am watching sports in real-time and spoilers are
now ﬁne.
Keep Your Focus and Use Replies
My approach to posting on Twitter takes this very much to heart. Know who your
followers are. For a while, my Twitter was focused on Magic: The Gathering and
games, so I would be careful about posting non-Magic content. Now it's more focused
on rationality and Covid and the blog, and the majority of my followers don't care
about games. There is certainly a time and a place to talk about games anyway - I'm
about to stream, or something is happening with my game Emergents, but too much
gaming talk would drive people away.
You only care about the followers you care about. If it's mostly about the people you
know, then it is of course ﬁne to ignore the priorities of others - if they want in, that's
their call, if not that's ﬁne too. Similarly, you may actively want to drive the wrong
people away, such as when Dick Nixon loses some followers by talking about baseball
and says "good." When this is due to politics, I'm less of a fan.
My solution to the focus problem is replies. Others only see a reply if they follow both
accounts. So if I'm replying to a gamer or a game's account, then anyone who sees it
is probably interested in games. If I'm replying to someone into rationality minutia,
then I can go into the weeds there, and so on.
One might worry that the world contains tons of accounts and everyone can only
follow a few of them so your chance of having the good pairs is small, but the world
that counts in such contexts is relatively small. If you are paying attention, you'll end
up following enough of the right people, and/or including them in your lists.
Unfollows, Filters, Mutes and Blocks
Whenever you see a post that makes your life worse, it should automatically trigger a
soft check to see if countermeasures are in order. If you don't actively manage what

you see and don't see on Twitter both on the addition end and the subtraction end,
you'll end up with more and more junk and the experience will slowly get worse.
For people who are being followed or listed, have they produced a bunch of value to
you in the past? Are they providing something you value? If not, you should have a
very itchy trigger ﬁnger when you get irritated. You can use this as a random time to
realize you weren't getting much you care about - if you can't remember them
providing value, either they're not, and/or you're following too many people. For lists
you check rarely you need to have more tolerance in case you've forgotten why
they're there, but even so, not overly much.
Also you can consider 'demoting' people from followed to a list, or 'promoting' them to
be followed whether or not you also remove them from their list.
If someone is super high value it can be worth checking who they follow in turn.
Filters are not something I use much. Once a ﬁlter is on, it usually never comes oﬀ,
and you have poor feedback on whether or not it worked out, and a poor sense of the
people in question. Sometimes it means you miss something important, so I get the
sense I should tread lightly. Still, I probably should have set up that green-square ﬁlter
a while ago for Wordle and all that.
Mutes and blocks are similarly one-way, but much less dangerous, and much more
necessary. I strongly disagree with 'follow based on a single Tweet' but I totally agree
with 'block or mute based on a single Tweet.'
There are certain kinds of being-an-asshole that people worth engaging with simply
will never do. Life is too short, pull the plug. But if the only oﬀense is that they're an
asshole, or being deeply stupid, or simply won't shut-up about various things in ways
that keep eating your time, that's what mute is for.
Block is for someone who is actively working to make your life worse, or life worse in
general. This can be commercial, like crypto-bots. It can be that they're looking for
people to cancel or yell at, or otherwise seem likely to quote tweet and start
something or similar, so you'd prefer their eyes turn elsewhere. Another reason is that
if they're making replies to your posts seem dumber or more political or enraging,
then you want to make that stop.
Or perhaps you want them to know for the satisfaction. There is that too.
The world is smaller than you think. Taking the time to block or mute people will pay
dividends if you're a power user.
Starting Out
When you start using Twitter you've got not only zero followers but zero people you
are following and no lists. The ﬁrst step is ﬁnding some people to follow.
Twitter will automatically scan your contacts for users, and they will suggest big-
following accounts like celebrities and news sources. Say yes to your real friends and
contacts, mostly say no to the big accounts.
There is an easy-to-use feature that is much better at getting you started than
Twitter's suggestions, once you ﬁnd the accounts of your good friends. That feature is

to look at who your friends are following, or what lists they have made public. Also
check who people whose thinking and taste you respect are following - especially the
ones who only follow a few hundred people despite having a lot of posts and followers.
That doesn't mean you want to trust their judgement without checking or thinking. It
does mean that you should think brieﬂy about what it means that each of them is
indeed being followed, and update your actions accordingly. When in doubt, check the
feed.
Once someone has been active for a while, the fewer people someone follows, the
more that should count for something. It occurs to me that it would be very possible to
do some sort of automatic Google-page-rank-style calculation, to see who has high-
quality followings who have a high bar before following people.
You can, of course, check who I am following, while keeping in mind my choices are a
quirky mix and likely to mostly be not for you even if I'm not making a mistake.
Setting Limits
I use and check Twitter more than would be healthy for most people. That's been
necessary for me to keep on top of Covid-19, and now Ukraine. It is not without its
downsides, so it's important to set limits to help with that.
It's not as bad as Facebook. I am of course Against Facebook. But still.
First, I highly recommend you Bring Back the Sabbath. Once a week, at a minimum,
take a full day's break from things that are not restful for you. Deﬁnitely include social
media on that list.
In similar fashion, when you are present, be present. Don't check your Twitter feed
while in conversation, or at a meal, or when hanging out for a nice evening. When
possible I literally put my phone somewhere else to help with this.
I also recommend not checking on one's phone except when things are truly
happening in real time. Don't install the app either. By using your computer, ideally a
desktop, you can process the information more eﬃciently and avoid getting your brain
and actions constantly interrupted. Deep everything, not only deep work, requires
putting such dopamine hits out of your mind.
You want to not fall into the habit of checking constantly. Email has the same issue,
but at least there there's an important trade-oﬀ - responding to people quickly has big
advantages.
Ideally go to the next level.
When there are not real time events going on (including being in active
conversations), which I've managed to use at some times but not others, is to only
check at minimum intervals of at least a day. Most of the time, the world can wait,
even if right now it does not feel that way. When I've managed to stick to this policy it
has deﬁnitely been an improvement to my day-to-day and I want to be getting back to
it soon - or at least get to checking only Ukraine lists at other times.
That goes hand in hand with limiting the size of your feed and lists. A less constant
drip of stuﬀ makes constant checking less tempting. It also means that you can check

less often and for less time, and still pick up the stuﬀ with the highest value - you
want to be able to check once a day and not feel any fear of missing out.
Note on War Bias
While Twitter is my only known technology for following something like a war in real
time, and you can customize it to aim for balance, the War makes this essentially
impossible.
The war is a situation of unusual moral clarity. The villain of the piece is (in my model
at least) intentionally playing a cartoon villain. They are also going with 'deny that
there is a war' rather than 'we are winning' or 'we are justiﬁed in a way a Westerner
might understand.' This is all a less insane choice than it ﬁrst appears, but it has its
consequences.
As a result, there is a very dominant Narrative here in the West and dissenting voices
mostly stay quiet. Russia has outright banned Twitter, although that did not seem to
have made any noticeable diﬀerence. Essentially all reports and information are from
people who stand with Ukraine or at least desire a Ukrainian victory. Selection eﬀects
are extreme, and it is all but impossible even to assemble a feed of what is happening
that isn't also full of direct appeals for help.
How do we avoid having this bias give us a warped picture of what is happening, on
every level? This is a ﬁnal exam for bounded distrust.
The good news is that there is essentially no reason to pretend to be actually ﬁguring
things out but lying about it. Sure, there's a reason to share a particular misleading or
unconﬁrmed or even rather transparently fake individual thing. But there's no reason
to also be visibly trying to ﬁgure things out. The audience for such attempts is small,
and that audience can tell. There are a lot of eyes on essentially everyone reporting
much of anything, which also helps.
Thus, in my experience so far, I believe I have been able to get a good sense oﬀ of
very little data what type of source I am dealing with, so I can respond and update
accordingly. That still leaves tons of fog of war and confusion and unknowns. On many
of the very big questions I remain deeply confused between diﬀerent models and sets
of facts. Yet things are nowhere near as epistemically hopeless as they seem.
Going beyond that would be beyond scope here.
Make It Your Own
Twitter's strength lies in its customizability. When I went looking through the people I
follow, and none of them are essential follows, or even obviously correct follows.
There's tons of knobs you can turn, and no one's Twitter need be like anyone else's
Twitter.
Similarly, these suggestions are based on how I get the most out of Twitter and try to
keep the costs reasonable. Your situation will be diﬀerent, so by all means ignore or
reverse this when it does not apply to you.

Gears-Level Mental Models of
Transformer Interpretability
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
This post aims to quickly break down and explain the dominant mental models
interpretability researchers currently use when thinking about how transformers work. 
In my view, the focus of transformer interpretability research is teleological: we care about
the functions each component in the transformer performs and how those functions interact
to yield the capabilities we see in language models today. From a functional understanding of
transformer internals, we then hope to be able to answer other important interpretability
questions such as, "Where/How is knowledge stored?," "Do transformers have beliefs?", and
"How do transformers reason?" As such, the mental models described in this post will be
functional theories about how researchers think about transformer internals, and not
hypotheses about other interpretability questions, like the ones mentioned above. 
There are three main components at work in the transformer: the attention heads, the MLP,
and the additive residual stream that connects all the layers. The functions that each play
aren't clear and are the subject of much research, and the mental models described below
will be diﬀerent ways of thinking about each main component. None of these mental models
are mutually exclusive, and in reality, transformer internals probably look like a messy
combination of many of these models.  
This post assumes a working understanding of transformers. For a primer on the transformer
architecture, I recommend The Illustrated Transformer, Transformers from Scratch, and The
Annotated Transformer. Other works that this post draws greatly from are the logit
lens, Transformer Feed-Forward Layers Are Key-Value Memories, ROME, and
the Anthropic papers.
The Models
Residual Stream as Output Accumulation
The residual stream is simply the accumulation of all the stuﬀ the transformer wants to say
at the end of its inference step. 
Clearly, the last residual hidden state is the transformer's prediction (before it gets projected
to the vocabulary), and all this mental model is suggesting is that the prior hidden states are
less-reﬁned versions of the ﬁnal residual hidden state. The strongest version of this model
states that the middle hidden states are the model's nascent predictions, and the weaker
version just states that the middle hidden states contain the model's nascent predictions.
This model seems pretty intuitive, and its weaker version seems pretty true to me. 
The strongest source of evidence for this model currently is the logit lens. In a really neat
trick, which they dubbed the logit lens, nostalgebraist found that by projecting the
intermediate hidden states of GPT2 to the vocabulary matrix, the resulting logit distributions
made a lot of sense. The logit lens might be a way to see what GPT believes at each time
step in its processing, and with it, we can see some surprisingly coherent and logical
"thought processes."

The logit lens on GPT2-medium with the ﬁrst paragraph of Harry Potter as input.
Made using the transformer-utils Python package. 
How to read this table: The token at the bottom of each column is the last token
in the input sequence; the token at the top of each column is the correct token;
the tokens in-between are the projections of GPT2's hidden states to the vocab. 
When GPT2 is faced with an easy prediction task, such as induction ("Mr. Dursley") or stop
words ("director of") — words that have super high bigram log likelihoods — the logit lens
shows that it converges to that prediction rapidly, and when it's faced with a harder task, we
see that diﬃculty reﬂected accordingly in the ﬂatness of its logit distribution. Additionally,
GPT2's intermediate projections make sense. If we just read the projections for the column
with the input token as "the," we read "same, ﬁrst, only, youngest, only" which sounds like
GPT2 thinks that the sentence is going to describe Mr. Dursley as an only child, youngest
child, ﬁrst-born, etc. 
One common perspective in deep learning is the information bottleneck perspective, which
states that deep learning models try to transform their input representations to encode as
much information about the output as possible, while removing irrelevant information about
the input. In the logit lens table above, we can see that by the ﬁrst layer, the residual hidden
state usually does not project out to the original input token. What this suggests is that GPT,
and autoregressive transformers in general, immediately converts inputs to guesses about
the output, working in an output-prediction space more than an input-compression space. 
The strong version of this mental model, which speciﬁcally states that the residual hidden
states are distributions over the output vocabulary, is also kind of intuitive. Weak evidence
for the strong version of this mental model, that the intermediate hidden states are the
model's ﬁnal prediction, is this quick PCA experiment I ran. If you run PCA on all of a model's
residual states for a given prediction, there's a single direction that explains roughly 88% of
the variance across all layers. And if you project this direction to the vocab, it's usually
(~77% of the time) the model's ﬁnal prediction. 

PCA of each of the layer residuals (12 x 768 matrix) for ~200 examples in GPT2.
X-axis is PCA components. Y-axis is explained variance ratios. 
Residual Stream as Communication Channel
Another way of thinking about the residual stream is that it's a communication channel for
individual components of the transformer to talk through. This perspective was introduced in
Anthropic's ﬁrst paper about mechanistic interpretability and seems to be the group's take
on the residual stream in general. Importantly, thinking about the residual stream as a
communication channel or as an accumulation of output is not mutually exclusive, and, in
fact, the accumulation of information about the output, seen in the logit lens, is probably
being communicated to components deeper in the transformer. 
 
Image straight from the Anthropic paper, describing the residual stream as a
communication channel. 
The nature of the residual stream is that it's a linear, vector space that is being added to and
read from by all layers. The attention heads and MLP will read from the space with a linear
projection — with the query and key matrices and the ﬁrst linear transformation in the MLP
respectively — and then write to the residual by adding another linear transformation — the
value and output matrices and second linear transformation in the MLP respectively — back

into the residual. And since the residual doesn't do any processing itself, it's intuitive to think
of it as the space where all the attention heads and MLP neurons communicate through. 
There are a couple of components that have been found in transformers that seem to be
doing communication-like things. The most notable component is, of course, induction
heads. Induction heads are pairs of attention heads that seem to implement an induction-like
algorithm, speciﬁcally an algorithm that completes the pattern AB ... A -> B. They work by
searching through the context, ﬁnding the present token, and then attending to the token
that comes after the present token. Since these induction heads involve pairs of heads
across layers, they communicate through the residual stream, where the ﬁrst induction head
copies information with its OV matrix about the present token, which the key matrix of the
second induction head reads. 
Induction Heads in action. The second induction head's QK matrix is reading from
the information that the ﬁrst induction head's OV matrix wrote into the residual
stream.
Anthropic also found evidence of neurons in the MLP and attention heads that seem to delete
information from the residual stream. Speciﬁcally, the MLP neurons have input weights that
have very high negative cosine similarity with their output weights, indicating that if a
direction is present in the residual stream, the neuron will add in the negative of that
direction into the residual stream. The attention heads had highly negative eigenvalues in
their OV matrix and seemed to attend to the present token, indicating that they delete
information about the present token.
MLP as Key-Value Pairs
The Feed-Forward (FF) in a transformer is deﬁned as 2 linear transformations of the input
with a non-linearity (ReLU, GeLU, etc.) in between, i.e:
F F ( x ) = max ( 0 , x W 1 + b 1 ) ( W 2 + b 2 )
In the paper, Transformer Feed-Forward Layers Are Key-Value Memories, Geva et al. propose
a mental model for thinking about the MLP where the ﬁrst linear transformation are keys and
the second linear transformation are values. Together, they form key-value pairs or neural
memories, which can be written as (omitting the biases):

FF(x) = f(x ⋅K⊺) ⋅V   where f is some non-linearity
Since each key-value pair corresponds to columns in the MLP weights, we can rewrite the
above as:
FF(x) =
dm
∑
i=1
f(x ⋅ki) ⋅vi where mi = f(x ⋅ki) is the coeﬃcient of vi and dm is the MLP's hidden
dimension (typically 4 times the model embedding size)
The key-value pair mental model states that when a key is activated by "something", its
corresponding value will be written strongly into the residual (since the coeﬃcient mi will be
high). This alone is pretty straightforward and seems right; the important thing is what you
interpret this "something" as. 
This "something" according to the key-value paper are patterns: each key is activated by
textual patterns in its training data, and when a key is activated, its corresponding value will
shift the residual's logit distribution (with the mental model of the residual stream as output
accumulation) towards a distribution that complements the logits that would typically appear
after the textual pattern correlated with the key.  This correlation between key and value is
particularly noticeable in higher layers i.e. the key-value pairs in higher layers are more likely
to contain semantic information, while lower layers contain shallow (syntactic or
grammatical) information. 

Key-value schematic from the key-value
memories paper. Keys are activated by
textual patterns in the text and their
corresponding values shift the logit
distribution towards tokens that follow those
patterns. 
The key-value mental model suggests that the MLP is where knowledge is stored (since you
can treat key-value pairs as a linear-associative memory). For example, maybe values
contain knowledge and write that knowledge into the residual stream. Multiple knowledge
editing methods have made use of this implication. In Rank-One-Model-Editing (ROME), they
used the key-value model for their knowledge editing procedure, and the fact that ROME
works really well supports the key-value model's validity.  
Attention as Information Movers

I think this mental model is the least controversial of all the ones mentioned in this post. The
very structure of attention suggests that its function is centered around the context and the
diﬀerent relationships between tokens in a sequence. How else would information move
across tokens?
Besides the intuitive-ness of this mental model, there's a lot of empirical evidence that backs
it up. Obviously, the existence of induction heads supports the mental model that attention
heads move information across tokens. Additionally, in the ROME paper, their causal tracing
method supports the idea that attention heads move information across tokens. Speciﬁcally,
their causal tracing suggests that attention heads move factual knowledge about tokens that
the MLP writes into the residual stream towards the prediction of the ﬁnal token in the
sequence. I'd just direct readers to the actual paper if they want to learn more about this
stuﬀ because it's pretty cool. 
 

Civilization as Self-Restraint
The following is the ﬁrst post in the Civilization and Cooperation sequence, with the goal of
laying out a coherent, explicit, and actionable model of what humans are doing when they
form societies and adhere to their rules.  Each essay is intended to communicate something
like a single step of a proof—they should stand alone, but are much more interesting in
conjunction.
As a child (not raised within a church but surrounded by friends who were), I was always
struck by the phrase thou shalt not covet in the last two Old Testament commandments.
Coveting (it seemed to me) was an entirely internal action, one which could at least in theory
have no impact on the external world—if you had strong moral boundaries against theft,
adultery, and (I guess) whining, then there seemed to be no reason why the act of coveting
itself would be a problem.  There were certainly things which I coveted which literally no one
else on Earth was aware of, so ... ?
Eventually, eight-year-old me generated a pet theory: once in the Christian heaven, people
can do anything they want, and thus it was not enough to simply block bad behavior.
 Heaven is a walled garden, and it's meant to be at least in part a wish-fulﬁllment paradise—
thus, the only people you can safely allow inside are those who not only reliably act well, but
also robustly desire only virtuous things.  If someone was holding oﬀ only because of
external prohibitions, they'd wreak havoc once they got into the wish-fulﬁllment zone.
(This theory was further reinforced by my vague understanding that swearing/cursing was
also Forbidden, along with things like "being angry for petty or trivial reasons.")
Another way to express the above theory is something like "external restraints are less
reliable than self-restraint."  Anyone who has ever been responsible for preventing a toddler
from gleefully oﬃng itself will likely agree—there is a limit to what can be accomplished by
cleaning and childprooﬁng and making decrees.  The job becomes vastly easier once you can
recruit the toddler's own motivations, and convince (or bribe) it to not want dangerous things
in the ﬁrst place.
(This is what's behind the questionable-but-not-entirely-outlandish practice of letting
toddlers touch the hot stove, once.  Usually, goes that theory of parenting, once is all they
need.)
No complex system of rules and boundaries can work via entirely external imposition (at
least, not until we're surrounded by autonomous surveillance/enforcement drones at all
times).  There are just too many ways to do something wrong, too many times when you're
unobserved and can "get away with it," too many edge cases and loopholes in any explicit
framework.  Catching and punishing transgressions is too slow and lossy; people have to
(more or less) want to adhere to the rules, because of their own values and principles.
This leads us to the central thesis of this ﬁrst essay.
Consider a range between autonomy and civility, represented below as a gradient between
red and white in accordance with the fun-to-play-with MTG color system:

(For the rest of this sequence, I'm going to use the terms "autonomy" and "civility" in a
moderately nonstandard way, as the least-wrong handles for the concepts I want to
communicate.  Therefore, if you quote a passage that includes one of them, please also
include this explanation or a summary of it.)
We can deﬁne autonomy as something like "the freedom to choose among any of the options
that are permitted by physical law."  In other words, if you can do it in the sense that it's
technically possible, then you can do it in the permissive sense as well.
(Total autonomy is something like savagery or anarchy; I've chosen to avoid those terms to
avoid motte-bailey-ish confusion over connotation.)
Civility, on the other hand, can be deﬁned as something like "the willing relinquishment of
available options."  Taking things which you could do, in theory, and instead committing to
not-doing-them.
(At least, to some nonzero degree, and within some contexts, possibly with speciﬁc
conditions.  More on this later.)
The process of blacklisting options (i.e. increasing civility) is civilization; one becomes more
civilized with each option removed from the table.  Thus Freud's excellent quote "The ﬁrst
human who hurled an insult instead of a stone was the founder of civilization." In a state of
total autonomy, there is nothing to stop me from hitting you on the head with a rock
whenever I feel like it.  Once I robustly give up that option, I become more civilized than I
was before—at least with respect to my relationship with you.
A few small points about this model.
First, it may seem strange to frame civility and civilization in terms of what actions are left
untaken, as opposed to what actions are actively preferred.  Investment bankers who always
wear well-kept, high-quality business suits are not necessarily cognizant of the fact that they
have given up the option of wearing t-shirts, or sun skirts, or kimonos, or fursuits, or no
clothes at all; the dress code seems prescriptive rather than proscriptive.
But it's worth noting that there's no special reward for conforming to the prescription "you
must wear a suit."  Rather, it's part of the price of entry to that particular subculture (that
particular civilization, if we think of each possible subset of options-relinquished as
representing a point in civilization-space).  Whereas if one were to ﬂout the prescription,
there would in most cases be immediate consequences.
This is broadly true of behavioral norms, independent of context; it's no coincidence that the
Mosaic commandments are mostly written "thou shalt not" rather than "thou shalt."
Additionally, there's model simplicity to be had in attending to the excision of various options
(which are often common to many domains) rather than to the addition of various
prescriptions (which will be wildly diﬀerent in diﬀerent contexts).
Second, note that the gradient above is open-ended on the right side.  If we deﬁne
civilization (verb) as the act of giving up options, then there's no eﬀective limit to the
process.  We can leap all the way to "you can't do anything at all" just like we can use the

word "inﬁnity," but we can't really get there by striking individual options oﬀ the list one at a
time.  There are just too many of them[1].
This leads to the third note, which should be obvious, but which is worth spelling out
explicitly: as deﬁned in this sequence, more civility is not obviously better.
This is important to underline, because the connotation of "civilization" in ordinary usage is
pretty strongly positive; most people will generically expect that "becoming more civilized" is
unambiguously good.
But civility is always costly; making a sacriﬁce of one's freedom of movement and freedom of
choice can be worthwhile on net but is still, in itself, a loss.  The next essay in this sequence
will attempt to model why people willingly choose to relinquish autonomy, and what they
expect to get in return, but for now, here are a few simple examples in which being "more
civilized" (as I'm using the term) is clearly at least locally worse than being less so:
You are trapped in a loveless, abusive marriage, but your civilization forbids the options
"get a divorce," "run away," "have an aﬀair," and "kill your spouse."
You are engaged in a debate with a conversational partner who is using fallacious
reasoning and all sorts of dirty tricks and tactics to make you look bad, but your
civilization rules out things like "interrupting," "being rude," "losing your temper," or
"cutting oﬀ the conversation," at least for people in your class.
Your sibling stole your toy and hit you just before your parent walked into the room, but
your family's civilization prohibits both "hitting" and "tattling."
In each of these situations, it's clear that violating the prohibition would at least be locally
valuable, in that it would solve the immediate problem (setting aside questions of
punishment, or damage to the broader social fabric).
It's also clear that the parties involved "could" take those options, in a strictly physical sense.
 There's no physical barrier to running away, being rude, or shouting that your sibling stole
your toy.  Instead, there's something like an expectation of greater cost that leads to
preemptive self-restraint.  The individual is, in a way, "choosing" not to take a technically-
available option, but that choice is heavily informed by a sense of realism about whether the
option will actually pay oﬀ in the long run.
Next post in sequence: Lopsided Trees
1. ^
Especially when one takes into account the fact that, seemingly paradoxically, the
relinquishment of a given option often results in a plethora of new options, such that
the list grows longer as you strike things from it.  More on this in the next essay.

Projecting compute trends in Machine
Learning
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
Summary
Using our dataset of milestone Machine Learning models, and our recent analysis of compute
trends in ML, we project forward 70 years worth of trends in the amount of compute used to
train Machine Learning models. Our simulations account for (a) uncertainty in estimates of
the growth rates in compute usage during the Deep Learning (DL)-era and Pre-DL era, and
(b) uncertainty over the 'reversion date', i.e. the date when the current DL-era compute
trend (with a ~6 month doubling time) will end and revert to the historically more common
trend associated with Moore's law. Assuming a reversion date of between 8 to 18 years, and
without accounting for algorithmic progress, our projections suggest that the median of
Cotra 2020's biological anchors may be surpassed around August 2046 [95% CI: Jun 2039, Jul
2060]. This suggests that historical rates of compute scaling, if sustained brieﬂy (relative to
how long these trends have been around so far), could result in the emergence of
transformative models.
Our work can be replicated using this Colab notebook.
Note: we present projections, not predictions. Our post answers the question of: "What would
historical trends over the past 70 years when naively extrapolated forward imply about the
future of ML compute?" It does not answer the question: "What should our all-things-
considered best guess be about how much compute we should expect will be used in future
ML experiments?"
Introduction
Recently, we put together a dataset of over a hundred milestone Machine Learning models,
spanning from 1952 to today, annotated with the compute required to train them. Using this
data, we produce simple projections of the amount of compute that might be used to train
future ML systems.
The question of how much compute we might have available to train ML systems has
received some attention in the past, most notably in Cotra's Biological Anchors report.
Cotra's report investigates TAI timelines by analyzing: (i) the training compute required for
the ﬁnal training run of a transformative model (using biological anchors), and (ii) the
amount of eﬀective compute available at year Y. This article replaces (ii) the compute
estimate by projecting 70 years worth of trends in the amount of compute used to train
Machine Learning models.
Cotra's amount of eﬀective compute available at year Y is broken down into forecasts of (a)
compute cost, (b) compute spending, and (c) algorithimic progress. By contrast, we do not
decompose the estimate, and rather project it on our previous investigation of training
compute of ML milestone systems. This trend includes the willingness to spend over time
including the reduced compute costs over time; however, it does not address algorithmic
progress. We explicitly do not forecast the cost of compute or compute spending.

 Figure 1. Contrasting our work with that of Cotra 2020
In this post, we present projections based on previously observed trends and some basic
insights about how long the current 6-month doubling time can be sustained. That is, our
post answers the question of: what would current trends imply about the future if you naively
extrapolate them forwards.
One key reason we don't expect these projections to be particularly good predictions is that
it seems likely that Moore's law might break down in some important way over the next few
decades. We therefore might expect that that the doubling-time in compute usage, when the
dollar-budgets to scale compute grow at the economic growth-rate, will be substantially
longer than the historically common ~20-month doubling period.
When will the current scaling trend revert back
to Moore's law?
In our recent analysis of compute trends in ML (Sevilla et al., 2022), we ﬁnd that, since the
advent of Deep Learning, the amount of compute used to train ML systems has been
doubling every 6 months. This is much faster than the previous historical doubling time that
we ﬁnd to be roughly 20 months (which is roughly in line with Moore's law). Previous work
(Carey, 2018, and Lohn and Musser, 2022) has pointed out that a scaling-rate that outstrips
Moore's law by a wide margin cannot be sustained for many years as a rate of growth in ML
compute spending that far exceeds economic growth cannot be sustained for many years.
A key question, then, for projecting compute used in future ML systems, is: How long can the
current fast trend continue, before it reverts to the historically much more common trend
associated with Moore's law?
To answer this question, we replicate the analysis by Carey, 2018, but instead of using the
numbers from OpenAI's AI and Compute (Amodei and Hernandez, 2018), we use the
numbers from our recent analysis (summary).[1] This analysis, roughly, points to three
scenarios:
Bearish: slow compute cost-performance improvements and very little specialized
hardware improvements. In this scenario, it takes 12 years for the cost of computation
to fall by an OOM. The current 6-month doubling period can be maintained for another
~8 years.
Middle of the road: Moderate compute cost-performance improvements and
moderate improvements in specialized computing. In this scenario, it takes roughly 7
years for the cost of computation to fall by an OOM, and progress in specialized
hardware helps sustain the trend ~3 additional years. The current 6-month doubling
period can be maintained for another ~12 years.

Bullish: Fast compute cost-performance improvements and substantial improvements
in specialized computing. In this scenario, it takes 4 years for the cost of computation
to fall by an OOM, and progress in specialized hardware helps sustain the trend ~6
additional years. The current 6-month doubling period can be maintained for another
~18 years.
Roughly, we might say that these scenarios are represented by the following distributions
over 'reversion dates', i.e. dates when the scaling trends are more similar to Moore's law
than they are to the current fast trend.
Fig 2. Distributions that roughly correspond to the three scenarios that come out
of our replication of Carey, 2018.[1]
We then produce a mixture of these distributions by creating a weighted linear pool where
"Bearish" is assigned 0.75, "Middle of the road" is assigned 0.20, and "Bullish" 0.05, based
on our best-guesses (you can apply your own weights using this Colab notebook.)

Fig 3. our best-guess for a prior over reversion dates, formed by mixing the
previous distributions
We can use this as our prior over when the fast-trend will revert to the more historically
common trend associated with Moore's law.
Projecting ML compute trends
We simulate compute paths based on (a) our estimates of the growth rates in compute
usage during the DL-era and Pre-DL era, and (b) our prior over 'reversion date', i.e. the date
when the current DL-era compute trend will end. We account for the uncertainty in both (a)
and (b) in our simulations (see details here).

Fig 4. 10,000 projected compute paths. Solid line represents the median
projected compute at each date, and the shaded region represents 2-standard
deviations around the median.
Our simulations reveal the following projections about the amount of compute used to train
ML models.
   
Year
Projected FLOPs used to
train largest ML model
Enough for how many anchor's
median compute requirements?
2025 1025.90 [1025.33, 1026.14]
0/6
2030 1028.67 [1026.71, 1029.47]
0/6
2040 1032.42 [1029.27, 1034.71]
1/6
2050 1035.26 [1031.78, 1038.86]
3/6
2060 1038.10 [1034.35, 1042.49]
5/6
2070 1040.79 [1036.83, 1045.49]
5/6
2080 1043.32 [1039.04, 1048.18]
6/6
Table 1: Projected FLOPs from 2025 to 2080
These projections suggest that, without accounting for algorithmic progress, the most
modest of Cotra 2020's biological anchors will be surpassed around August 2030 [95% CI:
Jan 2029, May 2038], the median anchor (~1034.36 FLOPS) will be surpassed around August
2046 [95% CI: Jun 2039, Jul 2060], and the strongest of anchors will be surpassed around
May 2072 [95% CI: Jan 2057, Jun 2089].

Conclusion
If we naively extrapolate the trends uncovered from 70-years worth of compute scaling in
Machine Learning, we ﬁnd that within roughly 25 years, large-scale ML experiments will use
amounts of compute that exceed the half of the compute budgets that Cotra 2020 has
suggested may be suﬃcient for training a transformative model. This highlights the fact that
historical rates of compute scaling in Machine Learning, even if sustained relatively brieﬂy
(relative to how long these trends have been around so far), could place us in novel territory
where it might be likely that transformative systems would be trained. This work also
suggests that understanding compute trends might be a promising direction for predicting
ML progress, 
Details of the simulations
We assume compute grows exponentially in time at some rate g: 
C ( t ) = C ( 0 ) e g t ,  where  t ≥ 0.
In our projections, we replace g with g∗, deﬁned as a weighted geometric mean of our best-
guess of the growth rate during Moore's law (~
g M), and the growth rate of our estimate of the
growth rate during the Deep-Learning Era (^
g DL): 
g ∗ = ^
g  
w ( t )
DL
 ~
g  
1 − w ( t )
M
 ,  where  w ( t ) ∈ [ 0 , 1 ] .
Here, ^
g DL simply denotes the growth rate during the Deep Learning Era (2010 onwards) as
estimated using OLS. In particular, we estimate the following model using our dataset: 
log  C ( t ) = β + g D L t ,  where  t > 2010.
 ~
g M is deﬁned as follows: 
~
g  M = √ ^
g  M g 20-month ,
 where ^
g M is the estimated growth rate during the Pre-DL era, and g20-month is the growth
rate implied by a 20-month doubling period. The reason we take the geometric mean of the
estimated growth rate, and the growth rate implied by a 20-month doubling period is
because Moore's law is suﬃciently well-established that the error bars around ^
g M are too
large relative to how well-established Moore's law is. We therefore artiﬁcially increase our
precision of the growth rate associated with Moore's law by taking an average of our
estimated value and the usual growth rate implied by an ~20-month doubling-time.

Our weight function, w(t), is constructed as follows: 
w ( t ) = exp  ( 
  ) 
− 1
 .
Why? Well, it's a logistic-like function with a unit-interval range, which exceeds 1/2 when 
t < reversion date, equals 1/2 when t = reversion date, and is less than 1/2 otherwise. This is
what it looks like:
We then simulate some path C j as follows: 
C j = C ( 2022 ) e g 
∗
j  t ,  where, for any   j :
^
g DL is estimated on our randomly sampled (with replacement) DL-Era Data,
^
g M is estimated on our randomly sampled (with replacement) Pre-DL Era data, and
w(t) is set based on a randomly sampled reversion date from our prior over reversion
dates.
1. ^
You can ﬁnd the details of this analysis and a comparison to Carey's results here.
( t − 2022 ) 2
2 ( reversion date − 2022 ) 2

Ways to invest your wealth if you
believe in a high-variance future?
I largely follow standard ﬁnancial advice, for example I have a moderate fraction of
my wealth in retirement accounts. But I also put substantial weight on the future
being... extremely high variance, such that I think there's a pretty good chance that
"putting money away" will end up having been useless or at least not very useful.
Generally I think it would be a pretty bad idea to have a speciﬁc belief about how the
future is going to be extremely diﬀerent and therefore wildly go against currently
accepted ﬁnancial best practices. But it seems highly reasonable, if one believes in
the most important century hypothesis, to somehow change your investment plans for
that. Here are some relevant futures I can imagine;
A straight-up fast-take-oﬀ singularity.
There's a mini-armageddon that causes some societal infrastructure to collapse.
Maybe banks fail and I lose that money, or maybe people stop using USD and
start bartering with petroleum and gold.
Society moves toward post-scarcity in a way that makes money less useful.
The legislative landscape changes. Maybe congress appropriates retirement
accounts for emergency use. Or maybe longevity tech is going really well, and
people's lifespans are longer congress extends the age at which you can take
the money out. Maybe they retroactively change capital gains such that
investing money was useless.
One problem I'm having is that while it's easy to come up with ideas that are useful in
some of these scenarios (e.g. build a nuclear bunker) they're totally wasted in the
other scenarios. It feels like there should be some way to convert current money into a
non-abstract asset that would be always useful, and also fungible in the future. Like
can I... somehow buy and store a ton of joules? Or can I somehow... buy 10 years
worth of non-perishable food, to make sure I don't starve? When I try to think through
these, the logistics seem to be more costly than any expected beneﬁt.
The category that feels most promising to me is something like, invest in making
myself a person more able to handle those high-variance scenarios. Gain skills in
operations, negotiation, maintain good health, etc. But like, "become a stronger
person" is just the kind of thing I'm always trying to do anyway, and it mostly seems
to require time and cognitive resources. The thing I'm trying to ﬁgure out is how to
convert money into being a stronger person.

Hinges and crises
This is a linkpost for
https://forum.eﬀectivealtruism.org/s/dr4yccYeH6Zs7J82s/p/DzBEfDdsd4ucxChyk
Crossposted from EA forum. The second post in the  sequence covers the importance of
crises, argues for crises as opportunities, and makes the claim that this community is
currently better at acting with longer timescale OODA loops but lacks skills and capabilities
to act with short OODA loops.
We often talk about the hinge of history, a period of high inﬂuence over the whole future
trajectory of life. If we grant that our century is such a hinge, it's unlikely that the "hinginess"
is distributed uniformly across the century; instead, it seems much more likely it will be
concentrated to some particular decades, years, and months, which will have much larger
inﬂuence. It also seems likely that some of these "hingy" periods will look eventful and be
understood as crises at the time. So understanding crises, and the ability to act during
crises, may be particularly important for inﬂuencing the long-term future.
The ﬁrst post in this sequence mentioned my main reason to work on COVID: it let me test
my models of the world, and so informed my longtermist work. This post presents some
other reasons, related to the above argument about hinges. None of these reasons would
have been suﬃcient for me personally on their own, but they still carry weight, and should
be suﬃcient for others in the next crisis.[1]
An exemplar crisis with a timescale of months
COVID has commonalities with some existential risk scenarios. (See Krakovna.) Lessons from
it could transfer to risks in which:
the crisis unfolds over a similar timescale (weeks or years, rather than seconds or
hours),
governments have some role,
the risk is at least partially visible,
the general population is engaged in some way.
This makes COVID a more useful comparison for versions of continuous AI takeoﬀ where
governments are struggling to understand an unfolding situation, but in which they have
options to act and/or regulate. Similarly, it is a useful model for versions of any x-risk where
a large fraction of academia suddenly focuses on a topic previously studied by a small group,
and resources spent on the topic increase by many orders of magnitude. This emergency
research push is likely in scenarios with a warning shot or suﬃciently loud ﬁre alarm that
gets noticed by academia.
On the other hand, lessons learned from COVID will be correspondingly less useful for cases
where few of the above assumptions hold (e.g. "an AI in a box bursts out in an intelligence
explosion on the timescale of hours"). 
Crisis and opportunity
Crises often bring opportunities to change the established order, and, for example, policy
options that were outside the Overton window can suddenly become real. (This was noted
pre-COVID by Anders Sandberg.) There can also be rapid developments in relevant
disciplines and technologies.

Some examples of Overton shifts during COVID include: total border closures (in the West),
large-scale and prolonged stay-at-home orders, mask mandates, unconditional payouts to
large fractions of the population, and automatic data-driven control policies.
Technology developments include the familiar new vaccine platforms (mRNA, DNA) going to
production, massive deployment of rapid tests, and the unprecedented use of digital contact
tracing. 
(Note that many other opportunities which opened up were not acted on.)
Taking advantage of such opportunities may depend on factors such as "do we have a
relevant policy proposal in the drawer?", "do we have a team of experts able to advise?" or
"do we have a relevant network?". These can be prepared in advance.
Default example for humanity thinking about
large-scale risk
COVID will likely become the go-to example of a large-scale, seemingly low-probability risk
we were unprepared for. The ability to shape narratives and attention around COVID could be
important for the broader problem of how humanity should deal with other such risks.
While there is a clear philosophical distinction between existential risks and merely
catastrophic risks, 1) in practice it may be diﬃcult to tell the ultimate scale of some risks,
and 2) most people will not understand the distinction between GCRs and x-risks in an
intuitive way (understanding both as merely "extremely large"). So narratives and research
surrounding GCRs are important for work on x-risk.
Conclusion
The above are why it made sense to pay attention to COVID, even if the pandemic's direct
impact on the trajectory of humanity is small. (In some ways it still makes sense to pay
attention.)
The broader conclusion is that longtermists' ability to observe, orient themselves, decide and
act during crises may be critical to inﬂuencing long-term outcomes.
The usual ontology of longtermist interventions partitions the space according to "cause
areas" or "risks", leaving room for the unknown "cause X". An alternative, almost orthogonal
view partitions interventions according to the time scale of the OODA loop (i.e. the decision
and action process) they implement.
 

 
On this view, longtermism has so far focussed on actions in the top row, that have OODA
loops on the horizon of years and decades. Typical examples might be writing books that ﬁx
the basic framing of a ﬁeld, basic research, or community building. 
While there is a lot of commonality in actions along a column (e.g. at all timescales, the AI
risk ﬁeld will want to do AI research), there is also a lot that would be common interventions
across a row (e.g. all cause areas may will need to know how governement may pass
emergency regulation on a timescale of days).
The skills and capabilities needed to act on a scale of months, weeks, or days seem relatively
undeveloped. The following posts will make speciﬁc suggestions for what to improve in this
regard, based on our experience with COVID - in particular the rather obvious suggestion of
creating a longtermist "emergency response team" devoted to fast action. 
At the same time, I suggest taking this framing as a prompt: what else are we not doing?
Where else is the table ﬁlled less than it should be?
 
1. ^
I worked on the covid crisis at the expense of working directly on AI alignment and
macro strategy at FHI, which is a very high bar.

[Intro to brain-like-AGI safety] 7. From
hardcoded drives to foresighted plans: A
worked example
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
Part of the  "Intro to brain-like-AGI safety" post series .
(This post substantially overlaps with my post from last August, Value loading in the human
brain: a worked example . Compared to that older post, this version has numerous minor
edits for clarity, correctness, and ﬁtting-into-the-ﬂow-of-this-series.)
7.1 Post summary / Table of contents
The previous post presented a big picture of how I think motivation works in the human
brain, but it was a bit abstract. In this post, I will walk through an example. To summarize,
the steps will be:
1. (Section 7.3) Our brains gradually develop a probabilistic generative model of the world
and ourselves;
2. (Section 7.4) There's a "credit assignment" process, where something in the world-
model gets ﬂagged as "good";
3. (Section 7.5) There's a reward prediction error signal roughly related to the time-
derivative of the expected probability of the "good" thing. This signal drives us to "try"
to make the "good" thing happen, including via foresighted planning.
All human goals and motivations come ultimately from relatively simple, genetically-
hardcoded circuits in the Steering Subsystem (hypothalamus and brainstem), but the details
can be convoluted in some cases. For example, sometimes I'm motivated to do a silly dance
in front of a full-length mirror. Exactly what genetically-hardcoded hypothalamus or
brainstem circuits are upstream of that motivation? I don't know! Indeed, I claim that the
answer is currently Not Known To Science. I think it would be well worth ﬁguring out! Umm,
well, OK, maybe that speciﬁc example is not worth ﬁguring out. But the broader project of
reverse-engineering certain aspects of the human Steering Subsystem (see my discussion of
"Category B" in Post #3)—especially those upstream of social instincts like altruism and
status-drive—is a project that I consider desperately important for AGI safety, and utterly
neglected. More on that in Posts #12-#13.
In the meantime, I'll pick an example of a goal that to a ﬁrst approximation comes from
an especially straightforward and legible set of Steering Subsystem circuitry. Here goes.
Let's say (purely hypothetically... 👀) that I ate a slice of prinsesstårta cake two years ago,
and it was really yummy, and ever since then I've wanted to eat one again. So my running
example of an explicit goal in this post will be "I want a slice of prinsesstårta".

Prinsesstårta cake. I suggest eating some, in order to
better understand this blog post. For science! (Image
source: my favorite local bakery.)
Eating a slice of prinsesstårta is not my only goal in life, or even a particularly important one
—so it has to trade oﬀ against my other goals and desires—but it is nevertheless a goal of
mine (at least when I'm thinking about it), and I would indeed make complicated plans to try
bring about that goal. Like, for example, dropping subtle hints to my family. In blog posts.
When my birthday is coming up. Purely hypothetically!!
7.2 Reminder from the previous post: big
picture of motivation and decision-
making
From the previous post, here's my diagram of motivation in the brain:

See previous post for details.
As also discussed in the previous post, we can split this up by which parts are "hardcoded"
by the genome, versus learned within a lifetime—i.e., Steering Subsystem versus Learning
Subsystem:

7.3 Building a probabilistic generative
world-model in the cortex
The ﬁrst step in our story is that, over my lifetime, my cortex (speciﬁcally, the Thought
Generator in the top-left of the diagram above) has been building up a probabilistic
generative model, mostly by predictive learning of sensory inputs (Post #4, Section 4.7)
(a.k.a. "self-supervised learning").
Basically, we learn patterns in our sensory input, and patterns in the patterns, etc., until we
have a nice predictive model of the world (and of ourselves)—a giant web of interconnected
entries like "grass" and "standing up" and "slices of prinsesstårta".
Predictive learning of sensory inputs is not fundamentally dependent on supervisory signals
from the Steering Subsystem. Instead, "the world" provides the ground truth about whether
a prediction was correct. Contrast this with, for example, navigating the tradeoﬀ between
searching-for-food versus searching-for-a-mate: there is no "ground truth" in the
environment for whether the animal is trading oﬀ optimally, except after generations of
hindsight. In that case, we do need supervisory signals from the Steering Subsystem, which
estimate the "correct" tradeoﬀ using heuristics hardcoded by evolution. You can kinda think
of the is/ought divide, with the Steering Subsystem providing the "ought" ("to maximize
genetic ﬁtness, what ought the organism to do?") and predictive learning of sensory inputs
providing the "is" ("what is likely to happen next, under such-and-such circumstances?")
That said, the Steering Subsystem is indirectly involved even in predictive learning of
sensory inputs—for example, I can be motivated to go learn about a topic.
Anyway, every thought I can possibly think, and every plan I can possibly plan, can be
represented as some conﬁguration of this generative world-model data structure. The data
structure is also continually getting edited, as I learn and experience new things.

When you think of this world-model data structure, imagine many terabytes of inscrutable
entries—imagine things like, for example,
"PATTERN 847836 is deﬁned as the following sequence: {PATTERN 278561, then PATTERN
657862, then PATTERN 128669}."
Some entries have references to sensory inputs and/or motor outputs. And that giant
inscrutable mess comprises my entire understanding of the world and myself.
7.4 Credit assignment when I ﬁrst bite
into the cake
As I mentioned at the top, on a fateful day two years ago, I ate a slice of prinsesstårta, and it
was really good.
Step back to a couple seconds earlier, as I was bringing the cake towards my mouth to take
my ﬁrst-ever bite. At that moment, I didn't yet have any particularly strong expectation of
what it would taste like, or how it would make me feel. But once it was in my mouth,
mmmmmmm, oh wow, that's good cake.
Relevant parts of the diagram for what happened when I took my ﬁrst surprisingly-delicio
bite of prinsesstårta, two years ago.
So, as I took that bite, my body had a suite of autonomic reactions—releasing certain
hormones, salivating, changing my heart rate and blood pressure, etc. Why? The key is that,
as described in Post #3, Section 3.2.1, all sensory inputs split:

One copy of any given sensory signal goes to the Learning Subsystem, to be integrated
into the predictive world-model. (See "Informational inputs" at the top left of the
diagram.)
A second copy of the same signal goes to the Steering Subsystem, where it serves as
an input to genetically-hardwired circuitry. (See "Informational inputs" at the bottom-
center of the diagram.)
Taste bud inputs are no exception: the former signal winds up at the gustatory cortex within
the insula (part of the neocortex, in the Learning Subsystem), the latter at the gustatory
nucleus of the medulla (part of the brainstem, in the Steering Subsystem). After its arrival at
the medulla, the taste inputs feed into various genetically-hardcoded brainstem circuits,
which, when also prompted with the taste and mouth-feel of the cake, and also accounting
for my current physiological state and so on, execute all those autonomic reactions I
mentioned.
As I mentioned, before I ﬁrst bit into the cake, I didn't expect it to be that good. Well,
maybe intellectually I expected it—if you had asked me, I would have said and believed that
the cake would be really good. But I didn't viscerally expect it.
What do I mean by "viscerally"? What's the diﬀerence? The things I viscerally expect are
over on the "Thought Assessor" side. People don't have voluntary control over their Thought
Assessors—the latter are trained exclusively by the "ground truth in hindsight" signals from
the brainstem. You do have some ability to manipulate them by controlling what you're
thinking about, as discussed in the previous post (Section 6.3.3), but to a ﬁrst approximation
they're doing their own thing, independent of what you want them to be doing. From an
evolutionary perspective, this design makes good sense as a defense against wireheading—
see my post Reward Is Not Enough.
So when I bit into the cake, my Thought Assessors were wrong! They expected the cake to
cause mild "yummy"-related autonomic reactions, but in fact the cake
caused intense "yummy"-related autonomic reactions. And the Steering
Subsystem knew that the Thought Assessors had been wrong. So it sent correction signals
up to the Thought Assessor algorithms, as shown in the diagram above. Those algorithms
then edited themselves, so that going forward, every time I bring a fork-full of prinsesstårta
towards my mouth, the Thought Assessors will be more liable to predict intense hormones,
goosebumps, reward, and all the other reactions that I did in fact get.

A cool thing just happened here. We started with a simple-ish hardwired algorithm: Steering
Subsystem circuits turning certain types of taste inputs into certain hormones and autonomic
reactions. But then we transferred that information into functions on the learned world-model
—recall that giant inscrutable database I was talking about in the previous section.
(Let me pause to spell this out a bit: The "ground truth in hindsight" signal tweaks some of
the Thought Assessors. The Thought Assessors, you'll recall from Post #5, are a set of maybe
hundreds of models, each trained by supervised learning. The inputs to those trained
models, or what I call "context" signals (see Post #4), include neurons from inside the
predictive world-model that encode "what thought is being thunk right now". So we wind up
with a function (trained model) whose input includes things like "whether my current thought
activates the abstract concept of prinsesstårta", and whose output is a signal that tells the
Steering Subsystem to consider salivating etc.)
I call this step—where we edit the Thought Assessors—"credit assignment". Much more
about that process in upcoming posts, including how it can go awry.
So now the Thought Assessors have learned that whenever the "myself eating prinsesstårta"
concept "lights up" in the world-model, they should issue predictions of the corresponding
hormones, other reactions, and reward.
7.5 Planning towards goals via reward-
shaping
I don't have a particularly rigorous model for this step, but I think I can lean on intuitions a
bit, in order to ﬁll in the rest of the story:

Remember, ever since my ﬁrst bite of prinsesstårta two years ago, the Thought Assessors in
my brain have been inspecting each thought I think, checking whether the "myself eating
prinsesstårta" concept in my world-model is "lit up" / "activated", and to the extent that it is,
issuing a suggestion to prepare for rewards, salivation, goosebumps, and so on.
The diagram above suggests a series of thoughts that I think would "light up" the world-
model concept more and more, as we go from top to bottom.
To get the intuition here, maybe try replacing "prinsesstårta" with "super-salty cracker".
Then go down the list, and try to feel how each thought would make you salivate more and
more. Or better yet, replace "eating prinsesstårta" with "asking my crush out on a date", go
down the list, and try to feel how each thought makes your heart rate jump up higher and
higher.
Here's another way to think about it: If you imagine the world-model being vaguely like
a PGM, you can imagine that the "degree of pattern-matching" corresponds roughly to the
probability assigned to the "eating prinsesstårta" node in the PGM. For example, if you're
conﬁdent in X, and X weakly implies Y, and Y weakly implies Z, and Z weakly implies "eating
prinsesstårta", then "eating prinsesstårta" gets a very low but nonzero probability, a.k.a.
weak activation, and this is akin to having a far-fetched but not completely impossible plan
to eat prinsesstårta. (Don't take this paragraph too literally, I'm just trying to summon
intuitions here.)
I'm really hoping this kind of thing is intuitive. After all, I've seen it reinvented numerous
times! For example, David Hume: "The ﬁrst circumstance, that strikes my eye, is the great
resemblance betwixt our impressions and ideas in every other particular, except their degree

of force and vivacity." And here's William James: "It is hardly possible to confound the
liveliest image of fancy with the weakest real sensation." In both these cases, I think the
authors are gesturing at the idea that imagination activates some of the same mental
constructs (latent variables in the world-model) as perception does, but that imagination
activates them more weakly than perception.
OK, if you're still with me, let's go back to my decision-making model, now with diﬀerent
parts highlighted:
Relevant parts of the diagram for the process of making and executing a foresighted plan
to procure prinsesstårta.
Again, every time I think a thought, the Steering Subsystem looks at the corresponding
"scorecard", and issues a corresponding reward. Recall also that the active thought / plan
gets thrown out when its reward signal is negative, and it gets kept and strengthened when
its reward is positive.
I'll oversimplify for a second, and ignore everything except the value function (a.k.a. The
"Will lead to reward" Thought Assessor). And I'll also assume the Steering Subsystem just
defers to that proposed value, rather than overruling it (see Post #6, Section 6.4.1). In this
case, each time our thoughts move down a notch on the purple arrow diagram above—from
idle musing about prinsesstårta, to a hypothetical plan to get prinsesstårta, to a decision to
get prinsesstårta, etc.—there's an immediate positive reward, so that the new thought gets
strengthened, and gets to establish itself. And conversely, each time we move back up the
list—from decision to hypothetical plan to to idle musing—there's an immediate
negative reward, so that thought gets thrown out and we go back to whatever we were
thinking before. It's a ratchet! The system naturally pushes its way down the list, making and
executing a good plan to eat cake.

So there you have it! From this kind of setup, I think we're well on the way to explaining the
full suite of behaviors associated with humans doing foresighted planning towards explicit
goals—including knowing that you have the goal, making a plan, pursuing instrumental
strategies as part of the plan, replacing good plans with even better plans, updating plans as
the situation changes, pining in vain for unattainable goals, and so on.
7.5.1 The other Thought Assessors. Or: The
heroic feat of ordering a cake for next week,
when you're feeling nauseous right now
By the way, what of the other Thought Assessors? Prinsesstårta, after all, is not just
associated with "will lead to rewards", but also "will lead to sweet taste", "will lead to
salivation", etc. Do those play any role?
Sure! For one thing, as I bring the fork towards my mouth, on the verge of consummating my
cake-eating plan, I'll start salivating and releasing cortisol in preparation.
But what about the process of foresighted planning (calling the bakery etc.)? I think the
other, non-value-function, Thought Assessors are relevant there too—at least to some
extent.[1]
For example, imagine you're feeling terribly nauseous. Of course your Steering
Subsystem knows that you're feeling terribly nauseous. And then suppose it sees you
thinking a thought that seems to be leading towards eating. In that case, the Steering
Subsystem may say: "That's a terrible thought! Negative reward!"
OK, so you're feeling nauseous, and you pick up the phone to place your order at the bakery.
This thought gets weakly but noticeably ﬂagged by the Thought Assessors as "likely to lead
to eating". Your Steering Subsystem sees that and says "Boo, given my current nausea, that
seems like a bad thought." It will feel a bit aversive. "Yuck, I'm really ordering this huge
cake??" you say to yourself.
Logically, you know that come next week, when you actually receive the cake, you won't feel
nauseous anymore, and you'll be delighted to have the cake. But still, right now, you feel
kinda gross and unmotivated to order it.
Do you order the cake anyway? Sure! Maybe the value function (a.k.a. the "will lead to
reward" Thought Assessor) is strong enough to overrule the eﬀects of the "will lead to
eating" Thought Assessor. Or maybe you call up a diﬀerent motivation: you imagine yourself
as the kind of person who has good foresight and makes good sensible decisions, and who
isn't stuck in the moment. That's a diﬀerent thought in your head, which consequently
activates a diﬀerent set of Thought Assessors, and maybe that gets high value from the
Steering Subsystem. Either way, you do in fact call the bakery to place the cake order for
next week, despite feeling nauseous right now. What a heroic act!
1. ^
Side note: I happen to think there's something akin to "less discounting" (discount
factor closer to 1.0) for the value function compared to the various other Thought
Assessors, such that complicated indirect distant-in-time plans are predominantly
driven by the value function. This guess comes from the "incentive learning"
psychological literature, but that's a story for a diﬀerent blog post. Anyway, it's not all-
or-nothing; I ﬁgure the other assessors are at least somewhat relevant, even for distant
plans, as in the example here.

How to Lumenate (UK Edition)
When my partner and I decided to move in together, I promised I would make our home
bright and lumenated and optimized in ways that matter. So early on I bought three lamps
(and fairy lights for the bedroom) and bought the brightest possible lightbulbs in lumens I
could get on Amazon and made a lot of mistakes because it turns out in the UK there are like
4 kinds of standard lightbulbs including like mini ones with mini little threads that made me
feel like I had bought a toy and tried to install it into a real life electronic device.
And I asked him if it was good, and he said it was.
And then four months later, I thought, "but couldn't it be a lot brighter in here?" and he said
"yes" and I realized I had not held to my promise at all, so I doubled the number of lamps in
the living room, and I asked him if it was good, and he said it was.
And then two months after that, I thought "but couldn't it be a lot brighter in here?" and he
said "yes" and I realized I had not held to my promise at all and I decided to Actually Try and
apply as much Dakka as it took and follow the instructions here but they weren't that helpful
because it's the wrong Amazon, so I found equivalents and made mistakes and did a bunch
of arts and crafts and this is what I now have in my apartment. It's likely nowhere near ideal,
and I would be thrilled to get to add better options.
Important Information: 
1. Lightbulb thread types: check the specs when you buy
E27/E26: which is what I consider "normal" and is what you'd expect if you
bought a standard lightbulb in the US. E26/27 are apparently interchangeable?
Googled it: E26 vs E27, what's the diﬀerence? E26 stands for 26 mm and
the E27 for 27 mm in diameter. These two standards are
interchangeable, meaning a US E26 will ﬁt in a European E27 base, and E27
will ﬁt in a E26 base. The only diﬀerence is the voltage (for light bulbs).
B22: No, don't buy these for anything I suggest below, they have two stick out
pieces on either side of the base and no thread, some lights in my apartment
take these but nothing I suggest below.
E14: weirdly small cylinder as a base, with threading
2. General information: 
1. Lumens are brightness, the more the better for this purpose. I aimed for 2000
lumens with each light and rarely found it
2. Lighting color temperature is measured in Kelvin, the higher the number the
cooler and more like daylight. I aimed for as high as I could get for the bright
lights, usually got 5000K or 4000K, and got 2700 for soft light, but didn't put that
much thought into it. The 2700 are too yellow for my preference if I was being
picky. Chart here.
3. I bought the dimmer as recommended in the original article but most lightbulbs
that ﬁt my purposes on Amazon UK were nondimmable so I don't recommend one
here
Products
1. [x2] Lamp with two lightbulb holders:
https://www.amazon.co.uk/gp/product/B017XQI99W/ref=ppx_yo_dt_b_search_asin_title?
ie=UTF8&psc=1
1. The key thing here is that you need diﬀerent kinds of lightbulbs for the two
sockets. I used the two sockets to have cool light in the top one and warm light in

the bottom one.
2. The top socket uses E27 threading, the bottom one uses E14 threading
2. [x2] Lamp with one lightbulb holder:
https://www.amazon.co.uk/gp/product/B00R3LQW42/ref=ppx_yo_dt_b_search_asin_title
?ie=UTF8&psc=1
1. Uses E27 threading
3. A lamp that came with the apartment that takes E27 lightbulbs
4. This string of sockets:
https://www.amazon.co.uk/gp/product/B08SVWXJN2/ref=ppx_yo_dt_b_search_asin_title?
ie=UTF8&psc=1
1. I alternated warm lightbulbs and cool lightbulbs
2. Note: it's meant for outdoors, looks a little ridiculous inside, especially because
it's so long I have it looped around my command hooks which gives a funny eﬀect
I've grown to like, but it's not top notch aesthetics
3. Caveat emptor: it takes E27 threaded lightbulbs (and comes with its own, which
are dimmable but not very bright) but it turns out that the Litake brand which I
bought ﬁrst doesn't quite reach long enough into the socket to get the threads to
meet, and so I had to return them to get the LOHAS brand.
5. My cool lights:
1.  Umi brand, 4000K (cool white), 1521 lumens, which is the best I could ﬁnd,
currently unavailable, used for lamps #1-3 above:
https://www.amazon.co.uk/gp/product/B07W8QNPPM/ref=ppx_yo_dt_b_search_asi
n_title?ie=UTF8&psc=1
2. Litake, 5000K (Daylight), 1600 Lumens, E26 threading, I returned these because
they didn't work with the string of sockets in #4 but I believe would work just ﬁne
for the lamps in #1-3
https://www.amazon.co.uk/gp/product/B0832SLQG9/ref=ppx_yo_dt_b_search_asin
_title?ie=UTF8&psc=1
3. LOHAS, 6000K (Overcast day), 1600 Lumens, E27 threading, non-dimmable,
these worked with the socket in #4:
https://www.amazon.co.uk/gp/product/B07FTJJBSP/ref=ppx_yo_dt_b_search_asin_t
itle?ie=UTF8&th=1
Annoyingly, these don't tell you what color they are on the box so if you get
these with the LOHAS warm lights you'll have to mark them or remember
1. Haven't bought these yet, but found corn bulbs at 2000 lumens in a bunch of
color temperatures: https://www.amazon.co.uk/Sauglae-Incandescent-Equivalent-
Daylight-2000Lm/dp/B0836LG3L4/ref=sr_1_6?
crid=3P1RJAKMWPEVU&keywords=corn%2Bbulb%2Be27&qid=1647180764&spre
ﬁx=corn%2Bbulb%2Be27%2Caps%2C313&sr=8-6&th=1
6. My warm lights:
1. TopLeder Brand, only 471 lumens, 2700K warm white color, E14 threading, which
you need for the second socket for lamp #1 above, nondimmable:
 https://www.amazon.co.uk/gp/product/B08ZYFLFB5/ref=ppx_yo_dt_b_search_asin
_title?ie=UTF8&psc=1
2. Litake, 2700k, 1600 lumens, E26 threading, I returned these because they didn't
work with the string of sockets in #4 but I believe would work just ﬁne for lamp
#2 above, nondimmable
https://www.amazon.co.uk/gp/product/B07T9YD7F2/ref=ppx_yo_dt_b_search_asin
_title?ie=UTF8&th=1
3. LOHAS, 2700K (warm white), 1600 Lumens, E27 threading, non-dimmable, these
worked with the socket in #4:
https://www.amazon.co.uk/gp/product/B07FTJJBSP/ref=ppx_yo_dt_b_search_asin_t
itle?ie=UTF8&th=1
Annoyingly, these don't tell you what color they are on the box so if you get
these with the LOHAS cool lights you'll have to mark them or remember
1. Haven't bought these yet, but found corn bulbs at 2000 lumens in a bunch of
color temperatures: https://www.amazon.co.uk/Sauglae-Incandescent-Equivalent-
Daylight-2000Lm/dp/B0836LG3L4/ref=sr_1_6?

crid=3P1RJAKMWPEVU&keywords=corn%2Bbulb%2Be27&qid=1647180764&spre
ﬁx=corn%2Bbulb%2Be27%2Caps%2C313&sr=8-6&th=1
7. [x3] (this one is especially non optimized): Lanterns to soften the light in the sockets:
https://www.amazon.co.uk/gp/product/B09BQWPF2L/ref=ppx_yo_dt_b_search_asin_title
?ie=UTF8&psc=1
1. These are probably not the best ones I could have gotten, to fully open them they
need to have the metal / plastic structure that comes with them in there,
propping it all up, but that makes it impossible (as far as I could ﬁgure, though I
didn't try that hard) to also have the lightbulb in from the socket, so mine are all
funny saucer shapes because there's no internal structure
2. I recommend 2 because the small ones are too small to get around the lightbulb
and the big ones fall down. I used one by using tape and scissors and tinkering,
but probably easier to just get three to have enough of the sizes that work. I
alternated the two sizes that worked naturally so that all my warm lights have the
small ones and the cool lights have the big ones, for my own aesthetic
preferences.
8. Command Hooks: You need medium and large ones to set up the sockets, both work, I
wouldn't go for anything smaller. I needed 11 total for my setup, but I'm using 13/16th
the length of the socket.
1. Large:
https://www.amazon.co.uk/gp/product/B000M3V8XI/ref=ppx_yo_dt_b_search_asin
_title?ie=UTF8&psc=1
2. Medium:
https://www.amazon.co.uk/gp/product/B000FSORW4/ref=ppx_yo_dt_b_asin_title_o
01_s00?ie=UTF8&psc=1
9. I also have a bunch of these fairy lights in my apartment, I love them for ambiance and
they are pretty bright, but not for delumenation purposes:
https://www.amazon.co.uk/gp/product/B001EA8R8M/ref=ppx_yo_dt_b_search_asin_title
?ie=UTF8&th=1
1. Warning: 16m is very very long! There are 8m and 16m options, I have both and I
can think of places I'd put the 16m but it is really quite long.
2. Be sure to get the ones with the switch so you don't have to unplug them to turn
them oﬀ
Outcome
Here's what it looks like (understanding that taking pictures of light is...uh). You can see the
wonkiness of the lanterns and the vast diﬀerences in colors. Having all cool lights would be
deﬁnitely overwhelming. Having two lamps next to each other is deﬁnitely not very aesthetic
(and that's repeated elsewhere in the room) but it's worth it for now.
With all the lights on, it's quite bright and cheery! The socket lights were so bright that as I
hang out here in the evening, I wanted them oﬀ. I like getting to have just the soft lights in
the room (in the lamps + fairy lights, not sockets since I can only turn on that whole thing at
once) for evening ambiance and adjust as necessary.




[MLSN #3]: NeurIPS Safety Paper
Roundup
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
As part of a larger community building eﬀort, I am writing a safety newsletter which is
designed to cover empirical safety research and be palatable to the broader machine
learning research community. You can subscribe here or follow the newsletter on twitter
here.
 
Welcome to the 3rd issue of the ML Safety Newsletter. In this edition, we cover:
NeurIPS ML safety papers
experiments showing that Transformers have no edge for adversarial robustness
and anomaly detection
a new method leveraging fractals to improve various reliability metrics
a preference learning benchmark
... and much more.
Robustness
Are Transformers More Robust Than CNNs?
This paper evaluates the distribution shift robustness and adversarial robustness of
ConvNets and Vision Transformers (ViTs). Compared with previous papers, its
evaluations are more fair and careful.
After controlling for data augmentation, they ﬁnd that Transformers exhibit greater
distribution shift robustness. For adversarial robustness, ﬁndings are more nuanced.
First, ViTs are far more diﬃcult to adversarially train. When successfully adversarially
trained, ViTs are more robust than oﬀ-the-shelf ConvNets. However, ViTs' higher
adversarial robustness is explained by their smooth activation function, the GELU. If

ConvNets use GELUs, they obtain similar adversarial robustness. Consequently, Vision
Transformers are more robust than ConvNets to distribution shift, but they are not
intrinsically more adversarially robust.
Paper
Video
 
Fractals Improve Robustness (+ Other Reliability Metrics)
PixMix improves both robustness (corruptions, adversaries, prediction consistency) and
uncertainty estimation (calibration, anomaly detection).
PixMix is a data augmentation strategy that mixes training examples with fractals or
feature visualizations; models then learn to classify these augmented examples.
Whereas previous methods sacriﬁce performance on some reliability axes for
improvements on others, this is the ﬁrst to have no major reliability tradeoﬀs and is
near Pareto-optimal.
Paper
 
Other Recent Robustness Papers
A new adversarial robustness state-of-the-art by ﬁnding a better way to leverage data
augmentations.
A highly eﬀective gradient-based adversarial attack for text-based models.
A new benchmark for detecting adversarial text attacks.

Adversarially attacking language models with bidirectional and large-scale
unidirectional language models.
First works on certiﬁed robustness under distribution shift: [1], [2], [3].
A dataset where in-distribution accuracy is negatively correlated with out-of-distribution
robustness.
Improving performance in tail events by augmenting prediction pipelines with retrieval.
A set of new, more realistic 3D common corruptions.
Multimodality can dramatically improve robustness.
Monitoring
Synthesizing Outlier for Out-of-Distribution Detection
The authors model the hidden feature representations of in-distribution examples as
class-conditional Gaussians, and they sample virtual outliers from the low-likelihood
region. The model is trained to separate in-distribution examples from virtual outliers.
A path towards better out-of-distribution (OOD) detection is through generating diverse
and unusual examples. As a step in that direction, this paper proposes to generate
hidden representations or "virtual" examples that are outliers, rather than generate
raw inputs that are outliers. The method is evaluated on many object detection and
classiﬁcation tasks, and it works well. It is not evaluated on the more diﬃcult setting
where anomalies are held-out classes from similar data generating processes. If the
authors evaluated their CIFAR-10 model's ability to detect CIFAR-100 anomalies, then
we would have more of a sense of its ability to detect more than just far-from-
distribution examples. Assuming no access to extra real outlier data, this method
appears to be the state-of-the-art for far-from-distribution anomaly detection.
Paper
 
Studying Malicious, Secret Turns through Trojans

ML models can be "Trojans" and have hidden, controllable vulnerabilities. Trojan models
behave correctly and benignly in almost all scenarios, but in particular circumstances
(when a "trigger" is satisﬁed), they behave incorrectly. This paper demonstrates the
simplicity of creating Trojan reinforcement learning agents that can be triggered to
execute a secret, coherent, and undesirable procedure. They modify a small fraction of
training observations without assuming any control over policy or reward. Future safety
work could try to detect whether models are Trojans, detect whether a Trojan model is
being triggered, or precisely reconstruct the trigger given the model.
Paper
 
New OOD Detection Dataset
The Species dataset contains over 700,000 images covering over 1,000 anomalous
species.
While previous papers claimed that Transformers are better at OOD detection than
ConvNets, it turns out their test-time "anomalous examples" were similar to examples
seen during pretraining. How can we properly assess OOD detection performance for
models pretrained on broad datasets? This paper creates a biological anomaly dataset
with organisms not seen in broad datasets including ImageNet-22K. The OOD dataset
shows that Transformers have no marked edge over ConvNets at OOD detection, and
there is substantial room for improvement.
Paper
 
Other Recent Monitoring Papers
Detecting far-from-distribution examples by simply ﬁrst clipping values in the
penultimate layer.

A new OOD detection dataset with 224K classes.
A new metric advances the state-of-the-art for predicting a model's performance on
out-of-distribution data, assuming no access to ground truth labels.
A diﬀerentiable calibration loss sacriﬁces a small amount of accuracy for large
calibration improvements.
In a thorough analysis of calibration, ConvNets are less calibrated than Transformers
and MLP models, and more pretraining data has no consistent eﬀect on calibration.
A dataset that can be used for detecting contradictions given long background
contexts. Such a detector could be used for preventing models from stating falsehoods
at odds with reality or their previous statements.
Factual knowledge in language models corresponds to a localized computation that can
be directly edited.
Alignment
A Benchmark for Preference Learning
Instead of assuming that the environment provides a (hand-engineered) reward, a
teacher provides preferences between the agent's behaviors, and the agent uses this
feedback to learn the desired behavior.
Preference-based RL is a framework for teaching agents by providing preferences
about their behavior. However, the research area lacks a commonly adopted
benchmark. While access to human preferences would be ideal, this makes evaluation
far more costly and slower, and it often requires navigating review board
bureaucracies. This paper creates a standardized benchmark using simulated teachers.
These simulated teachers have preferences, but they can exhibit various irrationalities.
Some teachers skip queries, some exhibit no preference when demonstrations are only
subtly diﬀerent, some make random mistakes, and some overemphasize behavior at
the end of the demonstration.
Paper
 
Other Recent Alignment News

It is sometimes easier to identify preferences when decision problems are more
uncertain.
Debate about "alignment" deﬁnitions: [1], [2], [3].
Today's optimal policies tend to be power-seeking, a failure mode that will become
more concerning with future advanced AI.
Using model look-ahead to avoid safety constraint violations.
This work proposes a policy editor to make policies comply with safety constraints;
experiments are based on Safety Gym.
Benchmarking policies that adhere to constraints speciﬁed via natural language.
 
Other News
Apply to Fathom Radiant which is working on hardware for safe machine intelligence.

AI Performance on Human Tasks
This was an independent project with guidance and support from Vael Gates and
Richard Korzekwa --thank you both for your mentorship!
Introduction
This project was inspired by a prompt from the AI Impacts project:
For intellectual skills where machines have surpassed humans, ﬁnd out how long it
took to go from the worst performance to average human skill, and from average
human skill to superhuman skill.
I initially tried to answer this question for several task areas. But over the course of this
project, I pivoted more towards analyzing the advantages and disadvantages computer
programs have over humans in each task, including the sources of error. I made this
pivot mostly for practical reasons. For some tasks, machine programs are only tested
against expert human performance, making them diﬃcult to compare to "beginner"
and "average" humans. In other tasks, the diﬀerences between human and machine
performance are qualitative rather than quantitative. It seemed most appropriate to
describe how machines and humans diﬀer in performance rather than quantify it. I
used this information to predict whether AI will replace or augment humans in
performing these task areas.
I picked ﬁve task areas, each representing diﬀerent aspects of human cognition:
1. Poker-playing (strategy and interdependent decision-making)
2. Image classiﬁcation (a key subset of vision and perception)
3. Text-summarization (interpreting and manipulating text)
4. Creating static visual art (aesthetics and ingenuity)
5. Human-like dexterity (manipulating physical objects)
Here is a summary of my conclusions:
Task
Current capabilities
Poker
Superhuman (consistently)
Image classiﬁcation
Superhuman (usually)
Text-summarization
Average human (unreliable)
Static visual art
Superhuman (but requires human input)
Human-like dexterity Below humans (except speciﬁc tasks)
See "Discussion / Personal Predictions" section for my predictions in how AI will
 machines will replace or augment humans in these tasks.
Task 1: Poker
Poker-playing is a strong case study for many aspects of human decision-making. Most
signiﬁcantly, it involves strategic decision-making against an opponent. Strong poker

players use betting strategies, each having diﬀerent consequences, which correspond
to risk management and strategy broadly. Unlike chess and checkers, poker requires
deciding under imperfect knowledge since you never know your opponent's current
hand. You must interpret your opponent's bets despite never being certain what cards
they hold. It involves identifying and exploiting your opponent's patterns - a form of
agent modeling. It involves deception through bluﬃng and switching between
strategies. Playing against an opponent who attempts to deceive you involves acting
under unreliable information. Machines encounter each of these challenges when
playing poker against humans.
The ﬁrst serious attempt at a "Poker bot" was Michael Caro's ORAC, which played both
limit and no-limit Texas Hold 'Em. ORAC reportedly "rolled over professional-level
opponents in private tests in 1983 and even won most of its matches against [Michael
Caro]." Yet at the 1984 World Series of Poker, ORAC showed mixed results against
world champion humans, mostly losing. It is diﬃcult to conclude how ORAC would play
against other humans, as its performance was never oﬃcially documented. ORAC
appears to be better than a ﬁrst-time human poker player but worse than top
professionals.
The next notable poker program was Loki, created by the University of Alberta in 1997
to play multiple-player Limit Hold'em. Loki betted by counting the potential hands that
were better than, worse than, and equal to Loki's hand. It then estimated the
probability its hand was the best one generated. It did not perform opponent modeling
- storing information about opponents' previous betting decisions to analyze its
strategy and exploit patterns. It simply assumed the opponent would always make the
best decision for its current hand. This allowed Loki to play "better than average," yet
not at a world-class level.
Later versions of Loki were improved through a neural network trained on real poker
tournament data. They were equipped with basic opponent modeling, although it
ignored much of the relevant context for simplicity. It couldn't model an opponent who
varied their strategy throughout the game. Loki's developers underscored the
importance of opponent modeling in future algorithms.
In 2002, the University of Alberta developed PsOpti to play Limit Hold'em. The
algorithm was designed to approximate the game-theoretic optimal strategy (i.e.
optimized for not losing, not for winning). It assumed the opponent was also playing
optimally rather than with a bias or alternative strategy. While playing against humans,
PsOpti showed mixed results. Most human players struggled to adjust to the program's
playing style. It often made plays that human players would consider unconventional,
resulting in a "cloud of confusion" that gave PsOpti an advantage. When Gautam Rao
played against the program, he won by exploiting the fact that it lacked opponent
modeling. The researchers concluded that "opponent modeling will almost certainly be
necessary to produce a program that surpasses all human players."
In 2006, Carnegie Mellon researchers developed GS1, a Limit Hold'em program that
analyzed the structure of the game tree and automatically generated various
strategies. Unlike other bots, GS1 used little poker-speciﬁc knowledge or expert-
designed abstractions for its strategy. It performed competitively against two other
programs (PsOpti and Vexbot). It was also competitive against humans but lost on
average by just 0.02 bets per hand. Again, the program's performance was limited by
its lack of opponent modeling.
In 2007, the University of Alberta developed Polaris, a program for playing both limit
and no-limit Texas Hold 'Em. Polaris used counterfactual regret minimization (CFR)

against abstract poker games. CFR—now considered a core framework for solving large
imperfect-information games—works by iteratively traversing the game tree and
computing the amount of "regret" for each action at each decision point. "Regret"
refers to how much better the program would have done had it pursued one action
over another.
Polaris's strategy was relatively ﬁxed and could only be adjusted by "tilting" (or
modifying) the payoﬀ function. It did not contain opponent modeling; it consisted of
several poker strategies, each corresponding to a solution to a speciﬁc "tilt." After each
hand, the program computed the relative performance of each strategy, then
determined the best strategy based on the payoﬀ function. Polaris would converge to
the optimal strategy after playing a few hands. It showed strong performance against
humans, narrowly losing against two professionals but beating a team of six. This made
Polaris the ﬁrst program that "statistically defeated a group of human professionals."
While Polaris lacked full opponent modeling, its ability to converge to the optimal
strategy for a given opponent proved eﬀective.
In January 2015, the University of Alberta developed Cepheus, an "unbeatable"
computer program for Limit Hold 'Em. Cepheus used CFRᐩ, a variant of counterfactual
regret minimization that iterates over the entire game tree and requires less
computation. While developing its strategy, it also learned to use bluﬃng. Cepheus
became the ﬁrst program to "solve" an imperfect information game. 
That same year, Carnegie Mellon researchers developed Claudico to compete against
professional players in No-Limit Hold 'Em. Most previous poker-playing programs had
been designed for Limit Hold 'Em, which is less computationally complex and involves
diﬀerent strategies. Claudico functioned by inputting the rules of poker and computing
increasingly-precise approximations of the game-theory-optimal strategy as an output.
It played well against the humans and ﬁnished with a statistical tie, although the
human professionals earned more chips. It occasionally made unusual bets such as
betting $19,000 to win a $700 pot. 
The successor to Claudico was Libratus, developed by Carnegie Mellon in 2017. It used
CFRᐩ combined with a new technique for endgame solving - the process of computing a
better strategy for just an endgame than what can be computed for the entire game.
During a competition against four professional players, Libratus won by a high margin.
One player commented, "I felt like I was playing against someone who was cheating,
like it could see my cards... It was just that good."
In 2019, Carnegie Mellon and Facebook's Pluribus "decisively" beat human
professionals in six-player no-limit Hold 'em, the most widely played poker format in
the world. Unlike previous AI programs, Pluribus played against more than two people,
hence it could not rely on a Nash equilibrium strategy. It used a limited-lookahead
strategy, computing possible plays in response to the opponents' strategy several
moves ahead of the current one. Pluribus made deliberately unpredictable decisions
and varied its behaviors to confuse the opponent. It was also far more computationally
eﬃcient than previous poker machines. 
Poker-playing algorithms were almost exclusively tested against professional human
players and other poker-playing algorithms. This makes it diﬃcult to compare their
performance to low- and average-skill human performance. However, it took from
CARO in 1983 to Loki in 1997 to go from ﬁrst major attempt to "better than average" in
Limit Hold 'Em. Since then, several advancements occurred - including opponent
modeling, improved game-theoretic optimization, CFR, and the ability to bluﬀ and
change strategies. In 2015, Cepheus became the ﬁrst unbeatable Limit Hold 'Em

program, meaning it took 18 years to go from "better than average" to superhuman
performance.
Below is a chart depicting the time for AI to cross human-relative ranges in Limit Hold
'Em (based on this template):
Limit Hold 'Em
 
 
 
Range
Start
End
Duration (years)
First attempt to above-average
<1983 1997 >14
Above-average to superhuman
1997
2015 18
Task 2: Image classiﬁcation
A foundational task in computer vision is image classiﬁcation, or assigning semantic
labels to images to reﬂect their visual content. With the amount of image data growing
rapidly, AI is and will continue to help us process this information. Early approaches
used hand-crafted feature descriptors that identify speciﬁc features (e.g. shape, color,
texture). While they oﬀered promising results, these attempts couldn't detect and
describe higher-level concepts. More recent programs have become exceptionally good
at image tagging, seeming to approach a 0% error rate.
Beginning in 2010, algorithms had been evaluated in image classiﬁcation in the
annual ImageNet Large Scale Visual Recognition Challenge.  To classify images, the
software must identify objects in the images and correctly select the most accurate
categories of 1,000 options. The winner, NEC Labs America and UIUC, achieved an
error rate of 28%. The classiﬁcation error rate for humans in the ImageNet dataset was
reported to be 5.1%. Future attempts at this type of image classiﬁcation will be
described below.
Another major comparison between humans and AI in image classiﬁcation came from
Andrej Karpathy in 2011. He compared his performance to the state-of-the-art program
at the time. The program analyzed a pool of 50,000 images, then placed them into one
of 10 categories (e.g. "dogs," "horses," "trucks"). It applied several learning algorithms
such as sparse auto-encoders, K-means clustering, and Gaussian mixtures using only
single-layer networks. It found that K-means clustering yielded the most accurate
results with about 80% accuracy. Researchers were surprised it performed as well as it
did, expecting it to perform worse due to its simplicity. When Andrej Karpathy tried the
same task manually, he scored 94% accuracy. 
Another 2011 program demonstrated that machines were far behind humans in image
classiﬁcation. The program consisted of two modules: a hand-designed feature
extractor (which computed numerical properties of the image data to discriminate
between two categories) and an Adaptive Boosting machine learning algorithm. The
program's error rate ﬂuctuated widely depending on the problem and amount of
training. But overall, it consistently performed far worse than humans. When humans
and the program were trained on the same amount of data, humans had an average
error rate of 13.7% while the program had a 50% error rate.

The greatest source of error came from the algorithm's inability to analyze information
about the overall image geometry. The program could only learn from statistical local
measurements; it had no sense of symmetry, alignment, proximity, inclusion, or
collinearity. It could not perform abstract reasoning about the arrangements of shapes.
Hence, humans performed far better in categorizing images.
Later in 2011, Google researchers created a deep learning model that accurately
identiﬁed images with human faces 81.7% of the time, human body parts 76.7% of the
time, and cats 74.8% of the time. Unlike previous algorithms, this one was not
programmed to identify speciﬁc features. This made it one of the earliest
breakthroughs in unsupervised learning. The feature detector was robust against
translation, scaling, and out-of-plane rotation. The researchers concluded that neural
networks could achieve low error rates in classiﬁcation when given enough training
data. (It is now widely known that modern deep learning requires a lot of training data.)
In 2011 and beyond, researchers continued competing in the ImageNet challenge
(mentioned above). XRCE won in 2011 with a 26% error rate, and AlexNet won in 2012
at 16.4%. The paper that introduced AlexNet is considered one of the most inﬂuential
publications in the ﬁeld of deep convolutional networks. It concluded that deeper
models can achieve higher performance but require more computational power, which
became possible as GPUs improved. The ﬁrst system to beat humans in image
classiﬁcation came from Microsoft in February 2015 achieving a 4.94% error rate. In
December 2015, Microsoft beat its previous record with a 3.6% error rate. The
ImageNet challenge winner error rates each year are depicted here:
The researchers whose algorithm surpassed human performance in February
2015 compared their program's errors to human errors. They argued that most human
errors come from ﬁne-grained recognition, or diﬀerentiating between hard-to-
distinguish object classes such as dog species. Human errors also come from class
unawareness - failing to consider a relevant class due to not knowing it exists. In
contrast, the algorithm's errors occurred in cases requiring abstract understanding or
contextual knowledge related to the image (e.g. a spotlight).
The researchers whose model achieved a 3.3% error rate in the 2016 ImageNet
challenge also described the sources of error. They determined that the top ﬁve errors
were distributed as follows:

"Similar labels" refers to cases where the predicted labels (e.g. "monitor") refer to
similar content as the ground truths (e.g. "desktop computer"). These errors could be
considered an evaluation error rather than a failure of the algorithm. "Not salient GT"
occurs when the ground truth is not salient in the image, hence the predicted class
may or may not be incorrect. This could also be mitigated by changing the evaluation
policy to allow multiple ground truths. "Challenging images" refers to images that
humans would struggle to classify. Interestingly, this was the least common error
among those identiﬁed. "Incorrect GT" refers to cases where the ground truth is simply
incorrect. (Researchers have found that around 6% of labels in the ImageNet dataset
are incorrect.) "Incorrect PC," the most common error identiﬁed, simply means the
algorithm failed to generate the correct predicted class. Most errors in this category
occurred in images with irregular distributions such as light distortion or motion blur.
Such errors may be avoidable by including more of these irregular images in the
training data. Challenging images and incorrect PC—which comprised 43% of the
algorithm's errors—were the only two categories in which the algorithm was truly
"responsible" for the mislabeling.
Compared to humans, AI programs are better at processing vast amounts of image
data quickly, and in most cases, more accurately. However, machines can
unexpectedly fail at identifying images when contextual knowledge or understanding of
a nuanced relationship is required. AI programs may stumble at distinguishing between
a man and a statue of a man, or at identifying an object from an occluded image. 
In the ImageNet challenge, it took three years to go from the ﬁrst attempt in 2010 to
beginner level - around 2x the human error rate. It took another two years to reach
average human performance in 2015. Two years later, a program achieved
superhuman performance at half the error rate of humans.
Image classiﬁcation (ImageNet)  
 
 
Range
Start End
Duration (years)
First attempt to beginner
2010
2013 3

Beginner to average
2013
2015 2
Average to superhuman
2015
2017 2
Task 3: Text-summarization
Text-summarization is a key subset of natural language processing. There are two main
types: extractive summarization (extracting the most important content from the input
text without modifying it) and abstractive summarization (generating new phrases and
representations that are closer to what a human might write). While the ﬁeld of
extractive summarization is considered mature, abstractive summarization is still
largely unsolved.
Early attempts to summarize texts relied on statistical methods to rank words and
sentences. In 1958, IBM developed an extractive summarization method using an IBM
704 data-processing machine. It computed word frequencies and distributions for
technical papers, then determined the relative signiﬁcance of individual words and
sentences. The highest-ranking sentences were outputted as an "auto-abstract"
summary. The auto abstracts lacked the sophistication of human-generated abstracts,
but they were reliable and uniform. 
In 1969, H. P. Edmundson proposed a new method for producing text extracts. Previous
methods had relied primarily on the frequency of words to determine their importance.
Edmundson's method also considered cue words (an inputted list of at most 1000
words), titles and headings, and sentence location. On average, 44% of sentences
extracted by the algorithm matched sentences extracted by humans. The study
emphasized two limitations of the method: redundancy and the lack of linguistic
complexity (i.e. inability to factor in ﬁgures, tables, phrases, clauses, footnotes, etc.).
The issue of redundancy was addressed by Hongyan Jing of the Association for
Computational Linguistics in 2000.
In 1995, researchers from the Xerox Palo Alto Research Center developed a trainable
program for producing document extracts. It involved feature detectors using a naive-
Bayes classiﬁer to discount short sentences, place a higher weight on high-frequency
words and sentences with ﬁxed phrases (eg. "in conclusion...," "in summary"), detect
uppercase words, and more. The program performed well: on average, it selected 84%
of the sentences chosen by professionals. The main issue described was the
generalizability of the method, as the document being summarized needed the title,
text, and abstract to be in a speciﬁc order.
In 2005, S.P. Yong developed a summarization method using text pre-processing,
keyword extraction, and summary production. It performed pre-processing by removing
stop words (e.g. "the," "a," "can.") and stemming (removing suﬃxes and preﬁxes). It
performed keyword extraction using a competitive neural network that took a word
frequency function as an input. Summary production translated the neural network
output into words, then selected sentences containing those words from the document.
The method performed with a "contents score" of 83.03% and produced "a fairly good
summary" that "extract[ed] the most important contents." His paper demonstrated
that extractive summarization could be performed with main steps: pre-processing,
keyword extraction, and summary production.

In 2010, Li Chengcheng of Inner Mongolia Normal University developed an extractive
summarization method based on Rhetorical Structure Theory (RST) - a framework that
analyzes text structure at the clause level. The program generated complete, accurate,
and readable summaries. Its weaknesses included the lack of discourse-structural
analysis, inability to handle documents with varied subjects (e.g. a magazine), and
ineﬃciency of analyzing each sentence at several levels. 
In 2015, research into text-summarization increased signiﬁcantly. In 2016, Bina
Nusantara University researchers used Term Frequency-Inverse Document Frequency
(TF-IDF) scoring to summarize documents. The TD-IF value of a particular word is
greater when it appears frequently in the document. To account for how some words
appear more frequently regardless of the topic, TD-IF values are lower when many
other documents in the corpus (inputted collection of documents) contain the word.
The TF-IDF-based program yielded summaries with 67% accuracy. The researchers
attributed the inaccuracies to limited sample sizes and the lack of extra weighting for
words appearing in the title. Over the next few years, the ﬁeld of extractive
summarization was considered to be reaching maturity.
In November 2017, Salesforce developed a neural network model for abstractive text
summarization by combining supervised word prediction with reinforcement learning. It
used a similar encoder-decoder mechanism as previous programs but was able to avoid
redundancy and generate summaries of diﬀerent lengths. The algorithm was not "as
good as a person," but was "dramatically better than anything developed previously"
in abstractive summarization. The most common issue was short and truncated
sentences towards the end of the text. 
In December 2019, Google's PEGASUS abstractive summarizer reached human-level
performance in generating text summaries. The program, which removed entire
sentences from the texts and tasked the model with recovering them, utilized pre-
training with self-supervised objectives. PEGASUS achieved human-level performance
on a variety of document types using relatively little training data. It consistently
generated grammatically and syntactically correct summaries. However, some have
reported that PEGASUS occasionally produces inaccurate or misleading information.
The challenges in automatic text-summarization evolved as the methods did.
Extraction-based programs were limited by the original input text, as they could not
generate new sentences. They lacked linguistic sophistication and smooth transitions
between diﬀerent ideas. They sometimes contained redundancy and were overall less
readable and cohesive. The ﬁeld of abstractive summarization is relatively new and still
suﬀers from producing factually inconsistent summaries. Current techniques seem to
"understand" grammar and syntax, not the factual content. According to a 2019 study,
8-30% of abstractive summaries (by diﬀerent systems) are factually inconsistent with
respect to the source document. Hence, I'd argue AI is currently at average-level
human performance in abstractive text-summarization.
Text-summarization
 
 
 
Range
Start
End
Duration
(years)
First attempt to beginner (extractive)
<1958 2005 >47
Beginner (extractive) to average
(abstractive)
2005
2019 14

Task 4: Creating static visual art
Some consider creativity to be "the ultimate moonshot for artiﬁcial intelligence." One
way to test AI creativity is having them create artistic images. Current AI methods
seem capable of creating art, but require human intervention.
In 1973, artist Harold Cohen developed AARON, a computer program that creates
artistic images. He began developing a simple version of AARON in C, then switched to
Lisp and added new features over time. Each style and capability of the program was
hand-coded. Yet it could generate an inﬁnite number of unique, aesthetically pleasing
drawings. It also drew with the irregularity of a freehanded human. Because features
were hard-coded, Cohen argued AARON was not "creative" but also posed the
question, "If what AARON is making is not art, what is it exactly?" Below are examples
of AARON's works:


The next major event in AI-generated visual art was the advent of Generative
Adversarial Networks (GANs) in 2014. Introduced by Ian Goodfellow and his colleagues,
GANs are a framework for using deep learning methods for generative modeling. They
contain two elements: a generator (creates new images) and a discriminator (tries to
identify which images are computer-generated). If the generator successfully "tricks"
the discriminator, that image is outputted by the GAN. Several artists—
including Helena Sarin, Anna Ridler, and Memo Akten—have used GANs over the years
to create art. Below is Portrait of Edmond de Belamy (2018), a painting created using a
GAN that sold for $432,500 at an auction:

One limitation of GANs is that they require several human interventions. The human
must select the training data, design the network, train the network, and process the
outputs. This "raises questions about where the 'artist' resides within the production of
AI art." These critiques were addressed in 2017 by Creative Adversarial Networks
(CANs) - modiﬁed GANs that maximize deviation from established styles while
minimizing deviation from art distribution. During experiments, humans could not
distinguish CAN-generated art from human-generated art. The program, called AICAN,
claims to be "the ﬁrst machine generated artist to pass the Turing test."
In 2015, Google released DeepDream, a computer vision program for manipulating
images through "algorithmic pareidolia." It used a convolutional neural network to
modify patterns in images and create a dream-like psychedelic appearance. Users
could input an image and specify the style they want to emphasize, for example, to

make it appear more "cat-like." It is limited by requiring an inputted image and only
being able to generate a speciﬁc style of art.
In 2021, OpenAI revealed DALL-E, a program that generated images from text prompts.
It used a version of GPT-3 to interpret and create images from natural language inputs.
It could create both realistic images and objects that don't exist in reality. Occasionally,
DALL-E fails by creating images that were unexpected or intended to look diﬀerent. It is
diﬃcult to pinpoint why these errors occur given the "black box" nature of GPT-3 and
the lack of open-source code. Below are examples of how DALL-E generates images
from text prompts:

The shortcomings of visual art programs vary widely. Early models had limited
"creativity" and required hard-coded styles. GAN-based programs oﬀered wider artistic
capability but still involved human input. Recent deep learning-based programs oﬀer
wider creative abilities but occasionally fail unexplainably. Throughout this entire
history, computers have never been able to determine which content is beautiful or
eye-catching to the human eye. According to artist and computer scientist William
Latham, "you can't completely remove the artist or the public from the artwork."
Because there are no real examples of art created exclusively by AI, it is diﬃcult to
compare AI versus human performance.
Generating static visual art
 
 
 
Range
Start
End
Duration
(years)
First attempt to above average (GANs)
<1973 2014 >41
Above average (GANs) to superhuman
(DALL-E)
2014
2021 7
Task 5: Human-like dexterity
Digitally operated and programmable robots have been around since the 1950s. But
only recently have robots attempted to replicate human-like dexterity in manipulating
physical objects. There's been signiﬁcant progress in repetitive, precise tasks, but
otherwise machines are far behind humans.
In 1972, Waseda University developed WABOT-1, the world's ﬁrst full-scale intelligent
humanoid robot. It had a limb control system that enabled walking, hands and tactile
sensors for gripping and moving objects, a vision system, and various other sensors.
WABOT-1 had limited abilities in all areas and was estimated to have the mental faculty
of a one-and-a-half-year-old child. Walking one step took around 45 seconds. WABOT-2,
revealed in 1984, had ﬁve-ﬁngered hands capable of performing precise movements
such as playing an electronic keyboard.
Throughout the 1970s, several robotic arms were developed including FAMULUS
(1973), The Silver Arm (1974), and SCARA (1978). These robots could assemble cars,
small parts, and electronics, but were not designed to mimic human dexterity.
In 2000, Honda released ASIMO, which could run, walk, and interact with its
environment. Its multi-ﬁngered hands contained a tactile sensor and a force sensor on
the palm and each ﬁnger. This allowed each ﬁnger to act independently and with more
dexterity. It could perform tasks such as lifting a bottle and twisting oﬀ the cap, holding
a soft paper cup (without squishing it) to pour a liquid, and making sign language
expressions.
From 2001 to 2005, Fujitsu released three robots in its HOAP series. Relying on neural
networks, these miniature robots could perform tasks such as sumo movements,
cleaning a whiteboard, and grasping thin objects.
Beginning in 2005, TOSY developed TOPIO, a humanoid robot for playing table tennis.
It used deep learning to continuously improve its performance. Over time, TOSY also

developed new models with greater degrees of freedom and more advanced motors.
In 2008, the Institute of Robotics and Mechatronics at DLR introduced Justin, an
autonomous and programmable humanoid robot. It was designed for sensitive physical
manipulation including household work and assisting astronauts in space. Justin's
multiple actuated degrees of freedom allowed it to complete several tasks at once
under a "task hierarchy." For example, it could "serve beverages while observing the
environment, moving without singularity, avoiding collisions, compliantly responding to
collisions with the environment - all without spilling the drinks."  Early versions of Justin
could perform tasks such as grasping objects handed by humans, pouring liquid, and
unscrewing a bottle cap. Later versions could skim bread crumbs oﬀ a table, collect
shards of a broken bug using a broom, make coﬀee without splashing liquid, and throw
a ball. It could also catch ﬂying objects with an 80 percent success rate using cameras,
tracking software, and precision grasping.
In 2012, Rethink Robotics introduced Baxter, a robot for handling simple industrial
tasks. Such tasks included packing and unpacking boxes, assembling plastic parts into
place, welding and stamping machines, putting caps on jars, and performing
inspections to remove defective items. Its arms were compliant: rather than being
rigid, they could sense obstacles and adapt to them. Baxter was limited by requiring
task-level programming from humans, being less fast and precise than other industrial
robots, and being unable to lift heavy objects.
In 2013, Boston Dynamics announced Atlas, a bipedal humanoid robot. It was
particularly skilled at walking over rough terrain and performing gymnastics (backﬂips,
cartwheels, handstands, somersaults, etc.). In a 2015 demonstration, it completed
tasks such as removing debris from an entryway, opening a door and entering a
building, using a tool to break through a concrete panel, and operating a ﬁre hose.
While impressive, Atlas required training from humans and may not perform as well in
an uncontrolled or unfamiliar environment.
In 2015, Moley Robotics developed a robotic chef that could cook over 5,000 recipes.
Attached to rails on the ceiling, the "R-Kitchen" used two robotic arms and hands with
tactile sensors. It was programmed to copy the exact movements of a human chef. A
human chef would cook while tracked by sensors, which provided data to be translated
into digital movements by programmers. R-Kitchen could prepare any meal that was on
its recipe list with precision. It performed relatively quickly; it took ﬁve minutes to make
a gourmet pizza from scratch.
In 2015, UC Berkeley developed BRETT, a robot that used deep learning to perform
tasks such as assembling a toy airplane, solving a puzzle, screwing a cap on a bottle,
and putting a clothes hanger on a rock. The robot learns by experimenting with tasks
like a child, with a reward function providing a score based on how well it's doing.
In 2016, UC Berkeley researchers developed DexNet, which was reportedly more
dexterous than any robot created prior. It used a robotic manipulator with a Grasp
Quality Convolutional Neural Network to learn how to grasp objects. It trained itself to
grab objects in a virtual environment through trial and error. It could generalize to
objects it hadn't seen before and learn how to grasp them on the ﬂy.
In 2018, OpenAI developed Dactyl, a human-like robot hand to manipulate physical
objects. It was fully trained via simulation using a general-purpose reinforcement
learning algorithm. Dactyl was able to manipulate blocks and solve a Rubik's cube with
one hand. This was done without developers programming speciﬁc motions. It
automatically discovered techniques such as sliding, ﬁnger pivoting, and ﬁnger gaiting

in the virtual simulation, then transferred those techniques to real-world physical
manipulation. Although Dactyl had touch sensors, it didn't need to use them,
demonstrating that tactile sensing isn't necessary for physical manipulation. Dactyl
was also robust to perturbations and could generalize to other objects with similar
properties.
In 2019, UC Berkeley developed Blue, a low-cost robot similar to the human arm. It was
trained using reinforcement learning and could perform tasks such as tying knots,
handling screws, folding laundry, and operating a Nespresso machine. It was designed
to be safe around humans; unlike most industrial robots, which are like a brick wall
when pushed, Blue could be jostled and would move aside "like a human in a crowded
subway." It could relax its arm to be compliant or tense up and become rigid. However,
this ﬂexibility meant less precision in its performance.
Since then, other groups have attempted to emulate human dexterity in limited
applications. In 2019, NVIDIA's AI Robotics Research Lab developed kitchen robots
designed for collaboration with humans. That same year, Tangible Research
developed dexterous robotic hands, although they were haptic-controlled. In 2020,
Dexterity AI created robots for automating physical tasks, primarily for repetitive
logistics and warehouse tasks.
Overall, machines are still far behind humans at dexterity and manipulating physical
objects. Many programs require extensive human training, programming, or direct
control. Most robots only perform only a small subset of the tasks humans are capable
of. Programs with wider capabilities are typically far slower than humans. For now, the
best use case for robotics is repetitive tasks such as factory operations or cooking.
Human-like dexterity
 
 
 
Range
Start
End
Duration (years)
First attempt to beginner <1972 Unknown Unknown
Conclusion
The levels of machine performance relative to humans vary widely depending on the
task. In poker, machines are better than the most-skilled humans. Starting with basic
probabilistic models, algorithms improved over time through opponent modeling,
game-theoretic approximations, neural networks, CFR, and other developments. AI has
several qualitative advantages over humans in poker. Unlike humans, AI also does not
get fatigued after many rounds or become ﬂustered after a losing streak. It also doesn't
"feel" the value of money. Whereas humans experience loss aversion, AI tends towards
the game theory optimal play.
Algorithms have also surpassed human performance in image classiﬁcation. Early
techniques were simplistic and could not analyze certain features or patterns. Over
time, deep learning systems have become increasingly good at classifying and have
approached near-0% error rates asymptotically. Much of the recent errors on the
ImageNet challenge can be ﬁxed through more training data or better evaluation
methods. Currently, the main shortcomings of AI image classiﬁers are identifying
complex relationships (e.g. a statue of a man on a horse), analyzing occluded images,
and dealing with adversarial examples. How frequent these issues arise and the
potential for overcoming them is unclear.

For text summarization, algorithms have not quite reached human-level performance.
The ﬁeld of extractive summarization has been heavily researched since 1958 and is
considered to have reached maturity. Extractive summarizers can now produce high-
quality summaries with minimal redundancy. The focus has since shifted to abstractive
summarization, which produces more human-like summaries. While models such as
PEGASUS can consistently generate grammatically and syntactically accurate
summaries, they occasionally output false or misleading information.
It is diﬃcult to characterize AI performance in generating static visual art, as programs
still require human intervention. AI began as an impersonator, not a creator. More
recent programs could replicate and blend diﬀerent styles of artwork and generate
realistic images. Arguably, AI can now produce art but can't create it. It can emulate
tactical skills, yet it lacks human taste. AI has no sense of which artwork would be
considered interesting or inspiring.
Machines are far behind human performance at dexterity and physical manipulation.
Even the most recent innovations in robotics either require human control/intervention
or perform only a small range of tasks. Some of the programs that "learn" by
experimenting have demonstrated wider capabilities, although their performance may
diminish when moving from a controlled testing environment to the real world.
Discussion / Personal Predictions
Current AI progress in the ﬁve chosen tasks may indicate how humans will interact with
AI in the future. Progress in poker-playing demonstrates that AI can outperform humans
in strategy, even with imperfect or unreliable information. AI can also bluﬀ, run game
theory approximations, and change strategies. One might extrapolate from these
results and conclude that AI will replace humans in strategic decision-making. Yet I'd
argue this will take time for two reasons. First, poker games are discrete, whereas real-
world games are often continuous or diﬃcult to model algorithmically. Second, humans
may be hesitant to yield strategic decisions to AI - particularly political, ﬁnancial, or
business decisions. In the near term, I predict that AI will augment humans in making
strategic decisions by providing the possible outcomes of various paths. Whether
humans allow AI to make the ﬁnal decision will depend on the risks associated.
In image classiﬁcation, I predict that AI programs will continue to approach a 0% error
rate asymptotically until the errors become negligible. Mass image classiﬁcation has
many applications such as object identiﬁcation in satellite images, traﬃc control,
medical imaging, image organization, visual search, and facial recognition. I predict
that AI will replace the role of humans in these tasks, except in the creation of training
datasets.
Given recent advances in abstractive summarization, AI could augment humans in
summarizing texts. Current algorithms occasionally generate summaries with factual
information that's inconsistent with the source document. However, there have been
many promising eﬀorts (including Microsoft's SpanFact and a solution from Amazon) to
generate factually correct abstractive summaries. As these systems are integrated with
other strong abstractive summarization techniques, I predict AI will obviate the need
for humans in this task. Humans would only be required when speciﬁc styles or formats
of summarization are desired. 

I chose to research the task of text summarization because I believed it could serve as
a proxy for many other tasks in natural language processing. However, I later realized
that there are several challenges in NLP that aren't necessarily present in these text
summarization tasks. Such issues include irony/sarcasm, ambiguity, colloquialisms, and
language diﬀerences. Because of these issues, the results from text-summarization
may not generalize easily to other NLP tasks such as sentiment analysis and
conversational agents.
Based on AI's progress in generating artistic images, I predict that AI will augment
artists in their work. It is currently impossible to deﬁne "creativity" into a set of rules -
perhaps by deﬁnition, or possibly due to limited research as to how and why humans
are creative. AI could be used as a tool for creators to experiment with and enhance
their work. AI may also open a new type of art, similar to how cameras led photography
to become recognized as an art genre. 
Of the ﬁve tasks, AI is the furthest away from outperforming humans in manipulating
physical objects with human-like dexterity. Current robots seem best-suited towards
repetitive tasks or tasks that require precision. But given recent advancements, I
predict that machines will perform increasingly dynamic and complex tasks in areas
such as manufacturing, surgery, and farming. Such developments will likely be
targeted towards specialized cases where high volume and scale are required. This is
supported by both the current trajectory of AI developments and the demand for
oﬄoading mundane tasks from humans.
Here is a summary of my personal predictions:
Personal predictions  
Task
Overall, AI will {augment/replace} humans.
Poker
Both
Image classiﬁcation
Replace
Text-summarization
Replace
Static visual art
Augment
Human-like dexterity Both
If you have  feedback or disagree with any of my predictions, please let me know in the
comments!

Experimental longtermism: theory needs
data
This is a linkpost for
https://forum.eﬀectivealtruism.org/s/dr4yccYeH6Zs7J82s/p/by8u954PjM2ctcve7
The series explains my part in the response to COVID, my reasons for switching from AI
alignment work to the COVID response for a full year, and some new ideas the experience
gave me. While it is written from my (Jan Kulveit's) personal perspective, I co-wrote the text
with Gavin Leech, with input from many others.
The ﬁrst post covers my main motivation: experimental longtermism.
Feedback loop
Possibly the main problem with longtermism and x-risk reduction is the weak and slow
feedback loop. 
(You work on AI safety; at some unknown time in the future, an existential catastrophe
happens, or doesn't happen, as a result of your work, or not as a result of your work.)
Most longtermists and existential risk people openly admit that the area doesn't have good
feedback loops. Still, I think the community at large underappreciates how epistemically
tricky our situation is. Disciplines that lack feedback from reality are exactly the ones that
can easily go astray.
But most longtermist work is based on models of how the world works - or doesn't work.
These models try to explain why such large risks are neglected, the ways institutions like
government or academia are inadequate, how various biases inﬂuence public perception and
decision making, how governments work during crises, and so on. Based on these models,
we take further steps (e.g. writing posts like this, uncovering true statements in decision
theory, founding organisations, working at AI labs, going into policy, or organising
conferences where we explain to others why we believe the long-term future is important
and x-risk is real). 
Covid as opportunity
Claim: COVID presented an unusually clear opportunity to put some of our models and
theory in touch with reality, thus getting more "experimental" data than is usually possible,
while at the same time helping to deal with pandemic. The impact of the actions I mentioned
above is often unclear even after many years, whereas in the case of COVID impact of similar
actions was observable within weeks and months.
For me personally, there was one more pull. My background is in physics, and in many ways,
I still think like a physicist. Physics - in contrast to most of maths and philosophy - has the
advantage of being able to put its models in touch with reality, and to use this signal as an
important driver in ﬁnding out what's true. In modern maths, (basically) whatever is
consistent is true, and a guiding principle for what's important to work on is a sense of
beauty. To a large extent, the feedback signal in philosophy is what other philosophers think.
(Except when a philosophy turns into a political movement - then the signal comes from
outcomes such as greater happiness, improved governance, large death tolls, etc.) In both
maths and philosophy, the core computation mostly happens "in" humans. Physics has the
advantage that in its experiments, "reality itself" does the computation for us.

I miss this feedback from reality in my x-risk work. Note that many of the concrete things
longtermists do, like posting on the Alignment Forum or explaining things at conferences,
actually do have feedback loops. But these are usually more like maths or philosophy: they
provide social feedback, including intuitions about what kinds of research are valuable. One
may wonder about the problems with these feedback loops, and what kind of blind-spots or
biases they entail.
At the beginning of the COVID crisis, it seemed to me that some of our "longtermist" models
were making fairly strong predictions about speciﬁc things that would fail - particularly about
inadequate research support for executive decision-making. After some hesitation, I decided
that if I trusted these models for x-risk mitigation, it made sense to use them to solve COVID
as well. And in pretty much every scenario, I learn something.   
Over the next year, I and many collaborators tried a number of interventions to limit the
damage associated with COVID. While we were motivated by trying to help, basically every
intervention was also an experiment, putting some speciﬁc model in touch with reality, or
attempting to ﬁx some perceived inadequacy. Our eﬀorts have had some direct impact, but
from  the longtermist perspective, the main source of value is  'value of information'.
A more detailed description of our work is forthcoming, but brieﬂy: we focused on
inadequacies in the world's modeling, forecasting, and decision support. Legible outputs
include our research on non-pharmaceutical interventions, advising major vaccine
manufacturers, advising multiple governments, sometimes at the executive level, consulting
with bodies such as the European CDC and multiple WHO oﬃces, and reaching millions of
educated readers with our arguments, with mostly unknowable eﬀects. I'm fairly conﬁdent
the eﬀorts made at least one country's COVID policy not suck during at least one epidemic
wave, and moderately conﬁdent our eﬀorts inﬂuenced multiple countries toward marginally
better decisions.  
Concretely, here's a causal graph of some of our eﬀorts: 
 
(Every edge has value of information.)
The sequence of posts, to be released over the next couple of weeks, will cover more detail:
1. Experimental longtermism (you are here)
2. Hinges and crises  
1. An exemplar crisis with a timescale of months; 
2. Crisis and opportunity; 
3. Default example for humanity thinking about large-scale risk;
4. Yet another drowning child thought experiment
3. What we tried

4. How we failed
5. The case for emergency response teams
6. Static and dynamic prioritisation: eﬀective altruism should switch from argmax() to
softmax()
7. Diﬀerent forms of capital
8. Miscellaneous lessons
1. Evidence in favour of trespassing
2. Evidence for crises as opportunities
3. Research distillation is neglected
9. Call to Action
 
Part of the value of my COVID year depends on whether I can pass on the data I collected,
and the updates I made from them. The posts to come discuss some of these.
 
Conclusion
A year of intense work on COVID likely gave me more macrostrategy ideas, governance
insights, and general world-modelling skills than the counterfactual (which would have been
mostly solo research from my home oﬃce and occasional zoom calls with colleagues from
FHI). My general conclusion is that such "experimental longtermist" work is useful, and
relatively neglected. 
One reason for neglectedness may be the type of reasoning where a longtermist compares
the "short-term direct impacts" of similar work with the potential "long-term direct impacts"
of a clearly longtermist project, and neglects the value of information term. (Note that a
longtermist prioritisation taking value of information into account will often look diﬀerent
from a prioritisation focused on maximising direct impact - e.g. optimising for the value of
information will lead to exploring more possible interventions). 
My rough guess of the total value of information is a >10% improvement in my decision-
making ability about large matters. Adding in what I hope you learn from me, it seems a
clearly good investment. 
On the margin, more longtermists should do experiments in this spirit; for the future, seize
the day.

On presenting the case for AI risk
[ Cross-posting from the EA Forum ]
Epistemic status: Personal anecdotal evidence, not fully thought through to my own
satisfaction. I'm posting this anyway because if I wait until I've thought it through to
my satisfaction then I might never post it at all.
People keep telling me how they've had trouble convincing others to care at all about
AI risk, or to take the concerns about misaligned AI seriously. This has puzzled me
somewhat, because my experience has been signiﬁcantly diﬀerent than that.
In my case I mostly talk to engineers who deal with real-world applications of current
or near-future AI systems, and when I talk to them about AI safety I actually don't
focus on existential risks at all. Instead I just talk about the risks from the systems we
have right now, and the challenges we face in making them safe for today's safety-
and mission-critical systems. So I'll talk about things like speciﬁcation gaming,
negative side eﬀects, robustness, interpretability, testing and evaluation challenges,
security against adversaries, and social coordination failures or races to the bottom.
And then at the end I'll throw in something about, oh yeah and obviously if we scale
up AI to be even more powerful optimizers than they are now, then clearly these
problems can have potentially catastrophic consequences.
And the thing is, I don't recall getting just about any pushback on this - including on
the longer term risks part. In fact, I ﬁnd that people tend to extrapolate to the
existential risks on their own. It really is pretty straightforward: There are huge
problems with the safety of today's AI systems that are at least partly due to their
complexity, the complexity of the environment they're meant to be deployed in, and
the fact that powerful optimizers tend to come up with surprising and unforeseen
solutions to whatever objectives we give them. So as we build ever more complex,
powerful optimizers, and as we attempt to deploy them in ever more complex
environments, of course the risks will go way up!
Sometimes I'll start adding something about the longer term existential risks and
they'll be like, "whoah, yeah, that's going to be a huge problem!" And sometimes they
do the extrapolation themselves. Sometimes I'll actually immediately follow that up
with reasons people have given why we shouldn't be worried... and the people I'm
talking to will usually shoot those arguments down immediately on their own! For
example, I might say something like, "well, some have argued that it's not such a
worry because we won't be stupid enough to deploy really powerful systems like that
without the proper safeguards..." And they'll respond with incredulous faces,
comments like, "yeah, I think I saw that movie already - it didn't end well," and
references to obvious social coordination failures (even before the pandemic).
Depending on how the conversation goes I might stop there, or we might then get a
bit more into the weeds about speciﬁc concerns, maybe mention mesa-optimization or
the like, etc. But at that point it's a conversation about details, and I'm not selling
them on anything.
Notice that nowhere did I get into anything about philosophy or the importance of the
long term future. I certainly don't lead with those topics. Of course, if the conversation
goes in that direction, which it sometimes does, then I'm happy to go into those
topics. - I do have a philosophy degree, after all, and I love discussing those subjects.

But I'm pretty sure that leading with those topics would be actively counterproductive
in terms of convincing most of the people I'm talking to that they should pay attention
to AI safety at all. In fact, I think the only times I've gotten any real pushback was
when I did lead with longer-term concerns (because the conversation was about
certain projects I was working on related to longer-term risks), or when I was talking to
people who were already aware of the philosophy or longtermist arguments and
immediately pattern-matched what I was saying to that: "Well I'm not a Utilitarian so I
don't like Eﬀective Altruism so I'm not really interested." Or, "yeah, I've heard about
Yudkowsky's arguments and it's all just fear-mongering from nerds who read too much
science ﬁction." Never mind that those aren't even actual arguments - if I've gotten to
that point then I've already lost the discussion and there's usually no point continuing
it.
Why do I not get pushback? I can think of a few possibilities (not mutually exclusive):
As above, by not leading with philosophy, longtermism, eﬀective altruism, or
science-ﬁction-sounding scenarios I avoid any cached responses to those topics.
Instead I lead with issues that nobody can really argue with since those issues
are here already, and then I segue into longer-term concerns as the conversation
allows.
My main goal in these discussions is not usually to convince people of existential
risks from future very advanced AI, but rather that they should be doing more
about safety and other risks related to today's AI. (This gets into a larger topic
that I hope to post about soon, about why I think it's important to promote near-
term safety research.) I suspect that if I were only focused on promoting
research that was explicitly related to existential risk, then this would make the
sell a lot harder.
My usual audience is engineers who are working (at least to some degree) on
systems with potential safety- and mission-critical applications in the military,
space exploration, and public health domains. These people are already quite
familiar with the idea that if the systems they come up with are anything less
than extremely reliable and trustworthy, then those systems will simply not be
deployed. (Contrary to what appears to be common perception among many
EAs, I've been told many times by colleagues familiar with the subject that the
US military is actually very risk-averse when it comes to deploying potentially
unsafe systems in the real world.) So for these people it's an easy sell - I just
need to remind them that if they want any of the cool AI toys they're working on
to be actually deployed in the real world, then we need do an awful lot more to
ensure safety. It's possible that if I were giving these talks to a diﬀerent
audience, say ML researchers at a commercial startup, then it might be a much
harder sell.
Here's a presentation I gave on this topic a few times, including (in an abridged
version) for a Foresight Institute talk. Note that the presentation is slightly out of date
and I would probably do it a bit diﬀerently if I were putting it together now. Relatedly,
here's a rough draft for a much longer report along similar lines that I worked on with
my colleague I-Jeng Wang as part of a project for the JHU Institute for Assured
Autonomy. If anybody is interested in working with me to ﬂesh out or update the
presentation and/or report, please email me (aryeh.englander@jhuapl.edu).

Ask AI companies about what they are
doing for AI safety?
Cross-posted from the EA Forum .
Today, I had the pleasure of attending a talk by Jeﬀ Dean at my university (Georgia
Tech). Titled "Five Exciting Trends in Machine Learning," it was a fascinating, engaging
presentation. Midway through the talk, I started to idly wonder, if there ends up being
a Q&A after the talk, could I ask a question about AI safety? I started drafting a
question on my phone.
After the talk, there was in fact time for questions. The moderator took one question
from one person sitting a few rows in front of me, and I wished I had raised my hand
earlier and sat closer to the front. Then, the moderator read aloud two questions from
people watching the livestream, and they asked their own question. Jeﬀ Dean was still
available for questions after the talk, however. I ended up asking him something like
the following:
"One major focus of DeepMind is aligning ML models to follow what humans want,
rather than narrowly pursuing objectives that are easy to specify. You mentioned the
trend of how AI is becoming increasingly capable and general. If this continues, and if
we had a highly advanced general AI and want it to cure cancer, one solution to that
objective would be to kill everyone, but that would be pretty bad. So it seems
important to be able to ﬁgure out how to specify the objectives that we actually want,
and some exciting approaches to this include reward modeling or iterated
ampliﬁcation. Is this a problem that Google AI is working on or plans to work on, like
its sister company DeepMind?"
I don't think that was the optimal way of asking about AI alignment, but that's what I
ended up asking. (If anyone has suggestions on how to talk about long-term AI risk in
a better way, please leave a comment!)
His response was essentially, Google AI is doing some work on AI safety. Google AI
focuses more on near-term stuﬀ, while DeepMind starts from the perspective of
thinking about super AGI. He's more optimistic that we'll be able to solve these issues
as we go and that we'll have the constraints necessary to prevent an AI from killing
everyone, while he does appreciate that some people are approaching this from a
more paranoid perspective.
I thanked him for his thoughts. In total, I think Jeﬀ Dean was asked just around ten
questions.
I remember reading a quote that went something like, "At every town hall, ask your
representatives what they are doing to address the climate crisis." I don't know how
often Jeﬀ Dean visits universities to give a talk, but if every time, just two or three
students from the local EA group asked him a polite, thoughtful question about AI
alignment, I think he might pay closer attention to it.
More generally, EAs in a local area may want to have a coordinated eﬀort to ask
speakers (carefully considered) questions about topics relevant to EA. Many social
movements have gained traction and created change from just a handful of people
raising awareness of an issue. Though there are many pitfalls we would want to avoid

- appearing uneducated, out-of-touch, aggressive, or polemical, for example - I think
we could do more to adopt helpful strategies from traditional social movements.

Phase transitions and AGI
This is a linkpost for https://www.metaculus.com/notebooks/10286/phase-transitions-
and-agi/
Take a look at the following graph, from Robin Hanson's Long-Term Growth As a
Sequence of Exponential Modes:
Here, "world product" is roughly the gross world product divided by the level of income
necessary for one person to live at a subsistence level. It measures the total production
of the human species in units of "how many people could live at a subsistence level on
that much production?"
The yellow marks are historical estimates of world product that Hanson gathered from
a variety of sources, and he's ﬁt three diﬀerent models to this data. What's notable is
the good ﬁt that the "sum of exponentials" type models have with this data. It looks
like the world economy goes through diﬀerent phases which are characterized by
diﬀerent rates of growth: in the ﬁrst phase world product doubled every ∼100, 000
years, in the second phase it doubled every ∼1000 years, and in the third phase it
doubled every ∼10 years, where we can give or take a factor of 2 from these
estimates - they are meant only to convey the order of magnitude diﬀerences.
We also see that transitions to subsequent phases are relatively fast. The transition
from the ﬁrst phase to the second phase took ∼1000 years, much less than the

doubling time of 100, 000 years characterizing this phase, and the transition from the
second phase to the third took on the order of ∼200 years, still smaller than the 1000
years of doubling time typical of the second phase. We can also observe that the
timing of these events roughly matches the First Agricultural Revolution and the
Industrial Revolution, so we might tentatively label the phases as corresponding to
"foraging", "farming" and "industry" respectively.
The study of these past transitions is important because they are the only reference
class we have for dramatic changes in the nature of the world economy and in how the
human species is organized and how we coordinate our activities. Since we have two
transitions to examine, we might also get a rudimentary sense of the variance of
outcomes: two is the minimal value we need in order to do that.
Unfortunately, many details about the foraging phase are shrouded in mystery. There's
still no consensus on the world product estimates for this phase even today: it could be
that this phase was actually ten times shorter than we think it is, and it might only date
back to around 200,000 BCE rather than 2,000,000 BCE. In this case, the doubling time
in this phase would be higher, about ∼10, 000 years. This is still much slower than
what came after, and still large compared to how long it took for the transition to take
place.
Regardless, the ﬁrst conclusion we should draw from this reference class is that such
phase transitions are possible and they can happen surprisingly quickly compared to
the pace of the changes that people who lived in a particular phase would be used to.
We can draw a second conclusion by noting that while the durations of the phases vary
quite a lot, the number of doublings of world product in each phase seems to be
similar: ∼10, give or take a factor of 2. Given the small sample size and the diﬃculties
of generalization, it's hard to extrapolate the duration of the industrial phase based on
this information, but it does suggest that the phase coming to an end soon wouldn't be
surprising from an outside point of view.
The question this essay is meant to answer is broadly this: how likely is a phase
transition in the near future, and given that one occurs, how likely is it to be brought
about by AGI? (By deﬁnition, I take transformative AI to be precisely a development in
AI which triggers such a phase transition.)
Outside view
One important question we should ask is how far in advance it's possible to see phase
transitions coming. The answer to this seems to be "less than half of a doubling time"
given the past examples. In other words, since the world economy is currently doubling
every 20 years or so, we probably shouldn't expect to see any sign of an impending
phase transition until we're less than a decade away from it. Therefore, the fact that
nothing special seems to be happening now shouldn't aﬀect our assessment of the
odds of a phase transition in the next century.
On the other hand, the outside view also should lead us to be cautious about what
mode of organization will become dominant after the phase transition. It would have
been quite diﬃcult to anticipate in the year 1400 that the next phase would be

associated with industry, since industry wasn't growing particularly fast relative to
anything else in 1400.
Can we get a more precise idea about how long we can expect the industrial phase to
last from an outside point of view? Here is one way to go about doing this: assume that
D + 1 where D is the number of doublings in a phase is drawn from a Pareto distribution
with an unknown tail exponent α. Pareto distributions have heavy right tails and allow
for a lot of uncertainty. This means the forecasts it implies will be quite conservative on
transformative AI timelines, which might be a disadvantage for reasons I'll come back
to shortly.
A Pareto distribution has one parameter: the exponent α. If we had a lot of data then
we could estimate α using frequentist methods (such as maximum likelihood
estimation) but since we don't, we have to use Bayesian methods to get anything
useful out of this analysis.
The conjugate prior of the Pareto distribution is the same as the one of the exponential
distribution, since the logarithm of a Pareto distributed random variable is
exponentially distributed. This conjugate prior is given by the gamma distribution.
We start with the Jeﬀreys prior for the Pareto distribution, which is simply an improper
prior proportional to 1/α. This formally corresponds to a gamma distribution 
Gamma(0, 0) where the distribution is characterized in terms of its shape and rate
respectively. Now, we do a Bayesian update: we have two observations of past phases
and they took approximately 8.9 and 7.5 doublings - these values are taken from
Hanson's paper - for the foraging and farming phases respectively. Using the conjugate
prior updating rule for the exponential distribution after adding 1 and taking
logarithms, we update to the posterior distribution
Gamma(2, log(9.9) + log(8.5)) = Gamma(2, log(84.15))
Now we can do a Monte Carlo simulation by ﬁrst sampling values of α from the
posterior and conditioning on there having been at least 10 doublings so far in the
current phase, and then sampling some value of the number of doublings until the end
of the current phase. This give us a sample from which we can infer what the
percentiles of various outcomes must be.
The cumulative distribution function looks like this:

The reason the percentiles after the median get so large is because of the
aforementioned property that the Pareto distribution has heavy tails. Since sustaining
doublings indeﬁnitely has a substantial chance of being outside the realm of physical
possibilities, we might want to also try using a distribution which has thinner tails. A
natural choice for this is the exponential distribution.
This calculation is remarkably similar since the exponential and Pareto distributions are
closely related. Now we assume the number of doublings D is drawn directly from an
exponential distribution with an unknown rate parameter λ. Once again the Jeﬀreys
prior for λ is Gamma(0, 0), and a similar Bayesian update gets us the posterior
Gamma(2, 7.5 + 8.9) = Gamma(2, 16.4)
Repeating the Monte Carlo simulation from before in this new context gives the
following cumulative distribution function:

Which of these is a better choice? In my judgment the exponential distribution in this
case is giving much more realistic timelines, and it's what I will be primarily relying on
in order to make my forecasts. I include both models, however, as a way to show that
our choice of model really aﬀects our view of what the timeline should be like.
The main argument against using heavy tailed priors is that the number of doublings is
already the base two logarithm of the factor by which world product increases by in a
phase, so if we assume a heavy tailed distribution for it then we have to exponentiate
that in order to get the actual growth in world product. This becomes similar to a
double exponential which has a high probability of exceeding physical limits - how
conﬁdent are we that, say, 9000 doublings of world product is even physically possible
at all, let alone it all occurring in a single phase?
I also experimented with using a model in which D is sampled from a gamma
distribution, but because its Jeﬀreys prior doesn't belong to its family of conjugate
priors Bayesian inference on it gets quite hairy. In the end the results I get are
somewhat more pessimistic than using an exponential, but the diﬀerence isn't
pronounced.
Inside view

I think conditional on there being a phase transition in the next hundred years or so,
it's likely (around 65%) that the cause of the transition will be the development of
transformative AI. However, even if this is not true, reverse causality will then become
operative: it's very hard to imagine that AGI is not achieved a short time after a phase
transition. Even a factor 10 increase in the growth rate of the economy would be
enough for AGI timelines to become quite compressed, for instance.
The reason I would give 65% odds to AGI being the driver of such a phase transition is
that it's hard for me to tell a plausible story about any other technology that's currently
on the horizon doing so. Moreover, one of the signs of a part of the economy that will
be responsible for a phase transition is that it should have a fast growth rate and a
plausible mechanism by which that fast growth rate can be sustained and take over the
whole economy, and I think the only serious contender for this position right now is AI
research. I wouldn't go higher than 65% because a technology that we can't yet see
could end up being responsible for the phase transition: this is the same as the point I
raised earlier about how industry wasn't growing fast relative to the rest of the
economy in 1400.
My opinion is that the inside view right now favors a phase transition sometime
between 2 and 5 doublings. It's diﬃcult to imagine transformative AI coming along
without at least one further doubling. Some relevant milestones here come from
Holden Karnofsky's post on transformative AI forecasting using biological anchors:

As Karnofsky says in his post:

Bio Anchors estimates a >10% chance of transformative AI by 2036, a 50% chance
by 2055, and an 80% chance by 2100.
I think this is extremely optimistic. I agree with the timeline in likelihood terms: the
maximum likelihood estimate on when we get transformative AI is probably "two to ﬁve
doublings", which is roughly the same timeline here - again, their timeline seems a bit
more optimistic, but broadly consistent. This roughly means that I think we would be
most likely to be seeing the kind of world we are seeing now if we were around two to
ﬁve doublings away from a phase transition.
However, a good Bayesian has to combine likelihoods with priors in order to get a
posterior distribution, and this is my primary point of disagreement with the Bio
Anchors timeline: the outside view, in other words the prior distribution, suggests a
phase transition occuring soon is unlikely. The industrial phase is roughly 200 years old,
and it has lasted for around 10 doublings already. Conditional on that, even if we just
assume a constant rate of arrival for the end of the current phase (which will be rather
optimistic), we should get a maximum likelihood estimate of around 10% every
doubling for it to happen. The median forecast would then be around 7 doublings until
the end of the current phase. If we want to go down from 7 to below 2, we need to
have very strong evidence that a phase transition is going to happen, and I don't think
AI developments so far provide any such evidence.
More explicitly, consider the second cumulative distribution funciton plot above. Two
doublings is roughly the 14th percentile of outcomes, so P(D ≤2) ≈0.14. The
corresponding odds ratio is 0.14/0.86 = 0.162 or so. To update from this odds ratio to
even odds requires a Bayes factor of roughly 1/0.162 ≈6. In other words, to justify a
median forecast of two more doublings, the world would have to be 6 times more likely
to look as it does under the hypothesis D ≤2 than under the alternative D > 2. In my
judgment the available evidence comes nowhere close to meeting this stringent
standard, and I'm curious to hear from people who think otherwise.
Most of the expectation of imminent transformative AI rests on extrapolations such as
the one in the graph: if we train a big enough model (human brain-sized, or more
accurately, of a similar inferential complexity to the human brain) for a long enough
time (compute used by all of evolution), we'll not only get human or superhuman
performance on diﬃcult tasks, but this performance will directly translate into a
transformation of the global economy. I think the model uncertainty here is so large
that updating too strongly away from the prior on this kind of argument is a bad idea.
Forecasts
There are three related questions that I'll forecast on: GWP growth to exceed 6%, GWP
growth to exceed 10% and When will economic growth accelerate? You can ﬁnd the
cumulative distribution function of my forecasts over at the linked post on Metaculus.
I think all three of these questions are unlikely to resolve if there is no phase transition:
I think the ﬁrst one has around 15% chance of not resolving > in the absence of one,
while the second and third are 1% or less. Therefore, my forecasts on all three
questions are based on taking my outside view estimates, adjusting them slightly

upwards due to the arguments given in the inside view section, and then making
further adjustments based on the speciﬁc question.
I think mean GWP growth exceeding 10% per year for a suﬃciently long time is
approximately equivalent to there being a phase transition - it's highly unlikely that any
phase transition would have a doubling time factor over the current phase that's less
than 3. However, 30% growth in a single year is a stronger demand, so I've adjusted
the distribution downwards to account for that. You shouldn't take the exact
distribution too seriously, since it's diﬃcult to input exact distributions and I haven't
taken the eﬀort to do so, but I've made sure that everything is consistent.
Mean GWP growth exceeding 6% could happen without a phase transition, but it's
rather unlikely. It would require major governments around the world enacting wide-
reaching economic reforms, or an unprecedented economic boom across most of the
underdeveloped world. I put the odds of this at around 15%, and my forecast is more or
less a combination of this with my estimate of the arrival time of a phase transition.
Discussion
Most transformative AI timelines focus strongly on the inside view: how long until
neural networks become as big as the human brain, how long until we reach certain
compute thresholds, how long do researchers in the ﬁeld think we have until
transformative AI, et cetera. I think the inside view is useful, but in the process the
outside view is either ignored or not weighted strongly enough to balance out inside
considerations.
This essay is meant to be a corrective for that: using Bayesian methods it's actually
possible to get information about the timeline of when we can expect another phase
transition purely based on the past two examples of such transitions. The distributions
we get this way do end up being somewhat sensitive to assumptions about priors,
especially at the tails, but overall I think using any standard "uninformative" prior is
superior to just saying there's no outside view on the problem and focusing only on the
inside view.
Addendum: Someone over at Metaculus linked Semi-informative priors over AI
timelines by Tom Davidson in the comments, which has a similar ﬂavor to what I do
here and ends up with similar timelines for a phase transition & transformative AI as I
do. If you're interested in this outside view perspective, his article is also worth
reading.

When should you relocate to mitigate
the risk of dying in a nuclear war?
Epistemic Status: "Thinking out loud" is probably a good way to describe it. I've been
researching and thinking about this stuﬀ on and oﬀ for the past few days, and this is
the best I've got. I haven't vetted it too closely and probably have made some
mistakes.
Some people see me as a very risk-averse person. I wouldn't say that. It's more that
I'm death-averse. Risks that involve the possibility of death, I'm very averse to, relative
to anyone I've ever met or can think of oﬀ the top of my head. Risks that don't involve
the possibility of death, I think I'm pretty tolerant of. Financially, I start startups and
play poker. Not that either of those things are particularly risky. Physically, an example
is that I really don't mind the risks of things like getting mugged or breaking my leg.
I remember the ﬁrst conversation I had with someone about the Ukraine/Russia conﬂict
(other than my girlfriend). It was with my mom. I was talking on the phone with her and
I think I said something like "You see what's going on in the Ukraine?". Her response
was something like "Ugh don't remind me. Gas prices are going to skyrocket."
That's not where my mind went. My mind went to death. Nuclear bombs are a thing.
Escalation of conﬂict is a thing. Irrationality is a thing. This very well might end up in a
nuclear war that ends up killing me. How high is that risk, and is it worth me doing
anything to mitigate it?
My initial instinct was a begrudging yes. I place an extremely high value on life. The
risk is actually real. It probably is high enough to justify moving. Which really sucks. I
just moved to Portland about a month ago. We're just setting in here. It's been
amazing. It's the ﬁrst time in my life I've been able to live in an urban area, which is
awesome because I don't drive. And it's the ﬁrst time in my life I've gotten to choose
my location based on where I want to be, not where I have to be, eg. because of a job.
Next, I ran some quick numbers to get a ballpark of where we're at and whether I need
to act fast. Suppose that we value life at $10B (1-3 orders of magnitude higher than
you probably do). Suppose that there is a 1/10 chance of me dying if the US is attacked
and a 0% chance if I move. If there is a 1/1,000 chance of us getting attacked, staying
here would cost $1M. 1/10,000 it costs $100k. 1/100,000 it costs $10k. With that initial
analysis, it's not immediately clear to me what I should do. Which is a success, I think. I
wanted to see if an initial analysis yielded an obvious action. It didn't. Now it's time for
a closer look.
The next thing I did is, after spending a little bit of time googling around for stuﬀ and
talking it through a bit with a friend, I wrote RFC WWIII. Writing really helps me think,
and I thought it'd be good for the community as well. In that post I dove a little bit
deeper into things, but didn't really make it much further than my initial analysis.
Talking some things through in the comments was pretty helpful though.
Then I came across 80,000 Hours' problem proﬁle on nuclear security. That seems to
contain a ton of useful resources and information. I listened to the thing they push the
most there, a podcast episode with Daniel Ellsberg.

That signiﬁcantly changed my view. From what Ellsberg said, it seems quite likely that
if there is a nuclear exchange, it will be large enough to trigger a nuclear winter, and if
there is a nuclear winter, we'll all end up dead within a year, save for a few people
surviving oﬀ of ﬁsh and seaweed (and wood?!) in New Zealand, living a Hatchet-style
life. If those things are true, moving doesn't really do much for you. Plus, the reason I
value life so highly is because of the possiblity of life extension and an awesome post-
singularity future. But in the scenario where I'm living oﬀ of ﬁsh and seaweed in New
Zealand, the probability of life extension is near zero. Plus the quality of that life is
quite low.
But then, to continue this roller coaster, I came across a tweet by Rob Wiblin saying
that he is leaving London due to the threat of a nuclear exchange.
After listing all the imaginable nuclear escalation scenarios I've decided to leave
London for now.
The lost life expectancy from remaining narrowly outweighs the inconvenience of
leaving for me (which isn't so big).
To help calibrate, a 1 in 1000 chance = ~2 weeks life lost.
Rob is a guy who's opinion I have to take seriously. He is the director of research at
80,000 Hours. I only have a limited impression of him, but from what I can tell he
seems to have solid epistemics. And he is one of the more informed people on the topic
of nuclear security. He did three 3+ hour long interviews of various experts in the ﬁeld,
and seemingly did a bunch of research to prepare for those interviews. And there's
probably more that I'm not aware of. Also, major props to Rob for taking ideas
seriously, sticking his neck out, and having the courage and altruism to share his
position with the world.
Let's try to look at this with a focus on where the cruxes are. One crux is how likely it is
that an attack leads to a nuclear winter. What is nuclear winter? I'll let Daniel Ellsberg
explain.
Well, that our policy has actually been the threat of an insane action, an action that
essentially we now know for the last 35 years has involved killing nearly everyone
on earth by the smoke from the burning cities that are planned to be hit in our war
plan. And that smoke, we now know on the nuclear winter calculations, would be
lofted into the stratosphere, would spread around the world globally. I'm talking
now about a war between the U.S. and Russia, where thousands of weapons would
be involved. And a few hundred of those weapons on cities which are targeted
would be enough to cause smoke that would reduce the sunlight reaching the
earth's surface by about 70%, killing all the harvests worldwide and for a period as
a long as a decade.
But that wouldn't be necessary, killing all the harvests for about a year or even less
would exhaust our food supplies, which globally are about 60 days, and nearly
everyone would starve to death except for a small fraction, perhaps 1% a little
more or less, of humans would survive, in Australia or New Zealand, southern
hemisphere is somewhat less aﬀected, eating ﬁsh and mollusks. And that could be
a sizable number of people. One percent is 70 million people, but 99% gone and
virtually all the larger animals other than humans. They're not as adaptable as we
are, and they can't move thousands of miles and wear clothes, light ﬁres, have
houses. They would go extinct altogether, as they did when an asteroid hit the

earth 67 or 65 million years ago and created a very similar eﬀect, blotting out the
sunlight by the dust that was sent up.
If an attack leads to such a scenario, it looks like I'll be dead within a year. In which
case dying immediately from the attack in Portland vs escaping to Greenland but dying
a year later as the crops all fail, it's not like escaping to Greenland gets me a much
better outcome.
Actually, it's probably a worse outcome. As much as I don't want to die, I am a believer
that there are outcomes worse than death, and a post apocolyptic life in Greenland
dreading my inevitable doom as the crops slowly fail, I shiver just thinking about it. It's
probably worse than being dead. So that option isn't looking very good.
Similar story with moving to New Zealand in hopes of being one of the survivors in this
nuclear winter scenario. I feel like there's a good chance I wouldn't survive anyway
even if I did make it there. In a diﬀerent 80,000 Hours podcast episode with David
Denkenberger, they talked about how New Zealand is probably overrated. There's a
risk that people ﬁght over a limited food supply and things basically get too crazy and
don't work out.
But even if I did survive, that Hatchet-style life doesn't sound very good. It probably
isn't worse than dying, but it'd be pretty bad. Let's just say 20% as good as normal life.
Suppose we use the typical $10M to value normal life (life extension won't happen in
this scenario). That means this New Zealand life would be worth $2M. And let's say,
generously, there's only a 50% chance I survive if I make it there. So call it $1M instead
of $2M. By moving to New Zealand right now, the logic would be that there is a small
chance of losing out on this life valued at $1M. If there is a 1/1,000 chance of being
killed in a nuclear attack here in Portland, then staying here is like taking a 1/1,000
chance at losing out on $1M. So an EV of -$1,000. Not worth the hassel of moving. Not
even close. Even if it was a 1/100 chance of an attack, the $10k wouldn't be worth it to
me either. We'd have to start getting into the 1/10 territory, and I really don't think
we're there right now, so it's looking like this is an option that I can shelve for now, and
perhaps revisit if things get really crazy.
What a terrible thing to seriously think about though. I really hope I don't have to.
I did say this was a crux though. So far I've been assuming that it is true. But is it? Will
a nuclear winter actually happen? Maybe it is only a smaller exchange that doesn't
actually lead to a nuclear winter. Ellsberg doesn't think so.
So the war plans of both U.S. and Russia have contemplated as sending not just
hundreds but thousands of warheads at each other and hitting hundreds of cities.
And something between 100 and 200 cities hit that way, by thermonuclear
weapons, would cause this nuclear winter. The likelihood of a limited nuclear war
between the U.S. and Russia is not quite zero, but it's very small. Any armed
conﬂict between U.S. and Russia, which has never occurred yet, would bear a high
likelihood or a real risk of erupting and escalating into use or nuclear weapons by
one or the other. Once that happened, the change of keeping it limited is very low.
Each would worry that the other was about to escalate. And another major point in
the book is that our planning on both sides has been aimed, delusionally for this
entire period, at limiting damage to one's own side by counterforce, by hitting the
forces of the other side in addition to its cities and its urban industrial centers. In
fact, most of the targets on both sides are of military targets, many of them near
cities or in the cities actually.

He calls it "not quite zero, but it's very small". That's too bad. He is just one guy
though. My experience has slowly been teaching me that seemingly smart people can
be wrong, and that I trust them too much. I'd be much more comfortable getting more
data points on this. If people in the comments can help, that'd be appreciated.
Something that pushes me closer to the possibility of a smaller nuclear exchange that
didn't trigger a nuclear winter is that, it just wouldn't make sense strategically to have
a large exchange. Ellsberg actually talks a lot about this throughout the podcast. You
can nuke the crap out of all their land based nuclear launch places and stuﬀ, and even
cripple their air force so that they can't launch nukes by air, but there are still nuclear
submarines. It'd be impossible to disarm all of them. So the enemy would still be able
to hit you back with their submarines. Hard. So then, if it doesn't make strategic sense
to nuke the crap out of them and start a large exchange, why does Ellsberg think that it
is basically inevitable? It's not quite clear to me, actually.
Yeah, I guess I don't understand this very well. In which case, I suppose I should assign
something like a 20% probablity of a limited exchange.
In this scenario, I personally would be safe in Portland. I think. From the googling I've
done, it looks like it'd be low enough on the list of targets where it wouldn't get hit. The
targets are places like military bases, critial infrastructure, government buildings like
the Pentagon, and major population centers like New York.
Check this out:
It shows the cities, towns and military sites which will be pulverised in the case of a
500-warhead or 2,000-warhead nuclear attack.
The larger attack, which would target 2,000 locations across the USA, would most
likely be an unprovoked attack by an enemy.
In this case the enemy would have the element of surprise and would attempt to
hit as many big cities and important military sites as possible.
If America attacks ﬁrst and is hit as response, the enemy would most likely know a
victory would be impossible and would simply attempt to kill as many people as
possible.
This map, put together by the Federal Emergency Management Agency (FEMA) and
the National Resources Defense Council, shows the areas most likely to be attacked
in these scenarios.
The purple triangles indicate big cities, while the black circles are smaller cities and
towns, as well as military sites and missile launch sites.

Remember, Ellsberg said it would only take a few hundred nukes to trigger a nuclear
winter. And Russia would have to attack a bunch of other NATO countries as well, and
the various military bases NATO countries have scattered around the globe. So looking
at those purple triangles and thinking about all the other places Russia would have to
hit, it doesn't look like Portland would make the cut.
I'm not 100% conﬁdent in this though. Maybe I'm... 90% sure? Let's run some numbers
on this. Suppose there's a 10% chance that in this scenario, Portland gets hit. In this
scenario, it's not guaranteed that I die. I remember reading on some survival blog that
something like 50% of people surived in Nagasaki. Looking at these maps of air ﬂow for
nuclear fallout, Oregon is one of the best places to be. And looking at this diagram, it
looks like if you can manage to get a few miles away from the impact site and stay
covered up for 48 hours or so, you're actually in decent shape. So looking at that 50%
number for Nagasaki and factoring in that I'm analytical and paranoid enough to
perhaps, whether by car, bike or foot, run away from the city center before an attack,
maybe the chance I die is something like 20%?

So, we have a 20% chance of a smaller scale attack rather than a larger scale one,
10% chance that Portland gets hit in this scenario, and a 20% chance I die if Portland
gets hit. Suppose that in this smaller scale scenario we value life at $2B instead of that
$10B I default to, because life extension is less likely. If there is a 1/1,000 chance of an
attack in general, then we have an EV of 0.1% * 20% * 10% * 20% * $2B = -$8k.
So, would I live in, say, rural Oregon for a year in exchange for $8k? No, I wouldn't. I
like where I'm at. What about for $80k, ie if there was a 1/100 risk? Yeah, I would. So
1/100 seems like the right order of magnitude to target for when I should consider a
move. There's also the idea of getting an Airbnb somewhere for a week or so if
tensions are particularly high.
What about for people in places like New York? Suppose New York has something like a
80% chance of getting hit instead of Portland's 10% chance. And instead of a 20%
chance of dying, it's 40%, because of air ﬂow and nuclear fallout. That's a 8 * 2 = 16
times bigger risk. So I suppose 1/1000 chance of an attack is the order of magnitude I'd
target if I lived there.
Let's return to that tweet by Rob Wiblin. He did mention that he doesn't buy the idea of
there being no chance of surviving long term, and linked to two podcast episodes he
did: one with David Denkenberger and the other with Luisa Rodriguez. Let's explore
those and see what we ﬁnd.
The title of Denkenberger's podcast is "We could feed all 8 billion people through a
nuclear winter. Dr David Denkenberger is working to make it practical." The opening
quote is:
I'm very concerned that if people don't know about resilient foods then they could
conclude that most people are going to die.

It could be an incentive for countries to do very bad things, like steal food from
your neighboring countries.
That's why I want to get the message out that we could actually feed everyone if
we cooperate.
The opening paragraph is:
If there's a nuclear war followed by nuclear winter, and the sun is blocked out for
years, most of us are going to starve, right? Well, currently, probably we would,
because humanity hasn't done much to prevent it. But it turns out that an ounce of
forethought might be enough for most people to get the calories they need to
survive, even in a future as grim as that one.
The podcast is three hours long and I haven't listened to the whole thing (or read the
whole transcript), but it sounds like he's saying that we could do things to prevent
starvation in the event of a nuclear winter. I'm not seeing anything about, as things are
today, it being likely that we would avoid starvation. In fact, in that opening paragraph,
it says that most of us would starve. And looking through the rest of the intro, it seems
that if we were to survive, it would mean getting creative with things like mushrooms.
That sounds like a pretty bad life to me.
So this isn't helping my estimate of survival in a nuclear winter. In fact, it's hurting it. If
nuclear winter was more survivable, I would not expect to see this guy dedicate his
career to this and make all of these claims about creative ways to farm mushrooms and
the need to extract sugar from wood. I take this as pretty strong evidence that a
nuclear winter would be quite hard to survive (which I mostly factored in to my
probability estimates above actually, I'm just explaining it in the post here).
There was a section in the podcast that the transcript titled "Should listeners be doing
anything to prepare for possible disasters?". I was confused by it. Denkenberger brings
up the idea of avoiding popular cities. But he isn't very enthusiastic about it. He does
say:
It really depends on how many nuclear weapons are used and whether it's just the
US as the target or all of NATO. But I think that just living on the outskirts of a city
is quite a bit lower risk.
But what about nuclear winter? Isn't his whole thing that, as things currently stand,
there wouldn't be enough food for people in the event of a nuclear winter, in which
case it doesn't matter where you're located? So living on the outskirts would only help
in the case of a smaller scale attack. To be charitable, I'll assume that's what he had in
mind. I wish he would have mentioned and elaborated on that though. It's a very
important question how plausible this smaller scale attack is. Ellsberg called it a "not
quite zero" chance. If this is what Denkenberger dedicated his career to, I'd think he'd
have spent quite a bit of time thinking about this question. So I'm disappointed to not
see more discussion of it.
Let's move to the episode with Luisa Rodriguez. The title of that one is "Luisa Rodriguez
on why global catastrophes seem unlikely to kill us all". And yeah, that's what the
podcast is about. The idea that not every human would die, and that humanity would
slowly build itself back up. Ie. we'd be back to where we are now within 1,000 years.
I don't see how that is relevant though. Or, maybe I should say helpful. I don't see how
it is helpful. It sounds like more evidence that we would in fact have almost all of

humanity die in a nuclear winter, and the ones who surive would be grasping at straws.
But Luisa has a series of articles on the Eﬀective Altruism forum. Cool! Let's see if
there's anything good there.
Opens articles. Skims through them. Wow! The roller coaster continues! Another
signiﬁcant belief update.
Aparently a lot of EAs and people in general just assume that a US-Russia nuclear
exchange would result in this nuclear winter we've been talking about that would kill
almost everyone. But Luisa doesn't believe that, and her reasoning seems sound.
Let's see what her estimate of an only 11% chance of a nuclear winter would mean.
Actually, let's kinda split the diﬀerence and assume a 20% chance. She seems more
optimistic than other people, and a higher number helps account for the fact that living
in a world where a nuclear attack happened but it didn't trigger a nuclear winter
wouldn't be as good a world to be a part of. Plus, it's seeming that recent events are
evidence against Putin being rational, which I think means a larger exchange is more
likely.
Previously I said the following:
So, we have a 20% chance of a smaller scale attack rather than a larger scale one,
10% chance that Portland gets hit in this scenario, and a 20% chance I die if
Portland gets hit. Suppose that in this smaller scale scenario we value life at $2B
instead of that $10B I default to, because life extension is less likely. If there is a
1/1,000 chance of an attack in general, then we have an EV of 0.1% * 20% * 10% *
20% * $2B = -$8k.
Now we're saying an 80% chance it's a smaller scale attack. So it's 0.1% * 80% * 10% *
20% * $2B = -$32k. Huh. I guess that doesn't change things too much. I previously was
giving a 20% chance of a smaller scale attack even though Ellsberg said the chance
was "not quite zero" because I've learned not to trust people. Pats self on the back.
Thanks Professor Quirrell.
By the way, I'm feeling much better about these probabilities now. Getting another
data point is great. Various cool people in the EA community reviewed Rodriguez's
work, so it's implicitly even more data points. I like that I've seen, presumably, people
on both far ends of the spectrum. Ie. Ellsberg seems close to the end of "it's gonna be a
nuclear winter and everyone is gonna die" whereas Rodriguez seems close to the
opposite end.
Let's try another adjustment. Rodriguez leans against thinking that an exchange would
target cities and instead it'd be military bases. Previously I was assuming a 10%
chance that Portland would be hit in this small scale exchange scenario. If feel like I can
update away from that. Maybe to 5% instead? Sure. That gives us -$16k. Still in the
same vicinity.
I could continue diving into this, but I think this is a good place to stop. Orders of
magnitude are what matter. These smaller changes don't matter too much. It seems
that a roughly 1/100 chance of an attack is where it'd make sense to relocate for
someone like me in a city like Portland. For someone in a city like New York, 1/1,000
seems more appropriate.

Edit: That assumes the super high $2B value on life. Other people probably value it 1-3
orders of magnitude less. I probably shouldn't have used such a high value for myself
either. I think I overestimated/didn't really think about the chance of an aligned AI
happening in this world where a smaller scale nuclear attack happens. In such a world,
it's possible that technological progress is halted, but perhaps more concerningly, I
think I'd expect it to be more likely for governments to seek to use AI as a weapon,
increasing the chance that it is unaligned. I'll guess that this adds up to a 10x reduction
in the value I place on life, so $200M instead of $2B, in which case it'd be 1/10 for
Portland, but I could easily be way oﬀ.

A survey of tool use and workﬂows in
alignment research
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
TL;DR: We are building language model powered tools to augment alignment
researchers and accelerate alignment progress. We could use your feedback on what
tools would be most useful. We've created a short survey that can be ﬁlled
out here.
We are a team from the current iteration of the AI Safety camp and are planning to
build a suite of tools to help AI Safety researchers.
We're looking for feedback on what kinds of tools would be most helpful to you as an
established or prospective alignment researcher. We've put together a short survey to
get a better understanding of how researchers work on alignment. We plan to analyze
the results and make them available to the community (appropriately anonymized).
The survey is here. If you would also be interested in talking directly, please feel free
to schedule a call here.
This project is similar in motivation to Ought's Elicit, but more focused on human-in-
the-loop and tailored for alignment research. One example of a tool we could create
would be a language model that intelligently condenses existing alignment research
into summaries or expands rough outlines into drafts of full Alignment Forum posts.
Another idea we've considered is a brainstorming tool that can generate new
examples/counterexamples, new arguments/counterarguments, or new directions to
explore.
In the long run, we're interested in creating seriously empowering tools that fall under
categorizations like STEM AI, Microscope AI, superhuman personal assistant AI, or
plainly Oracle AI. These early tools are oriented towards more proof-of-concept work,
but still aim to be immediately helpful to alignment researchers. Our prior that this is
a promising direction is informed in part by our own very fruitful and interesting
experiences using language models as writing and brainstorming aids.
One central danger of tools with the ability to increase research productivity is dual-
use for capabilities research. Consequently, we're planning to ensure that these tools
will be speciﬁcally tailored to the AI Safety community and not to other scientiﬁc
ﬁelds. We do not intend to publish the speciﬁcs methods we use to create these tools.
We welcome any feedback, comments, or concerns about our direction. Also, if you'd
like to contribute to the project, feel free to join us at the #accelerating-alignment
channel in the EleutherAI channel.
Thanks in advance!

Exploring Finite Factored Sets with
some toy examples
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
This is a linkpost for https://tm.kehrenberg.net/a/ﬁnite-factored-set-example/
Seeing as there is little secondary literature for the Finite Factored Set formalism, I
thought I'd write up my experience of exploring it through some toy examples that are
classic examples in the Pearlian paradigm. My goal was to see how these models that
I understood very well in the Pearlian paradigm would work in Finite Factored Sets.
As a warning, this doesn't make any use of the more exciting properties of Finite
Factored Sets. It's just an exploration of how this formalism handles the mundane
stuﬀ. This also means that I'm using the factored set directly, without the abstractions
of the orthogonality database. Which I think is ﬁne here, because these are tiny toy
examples whose structure is fully known. (However, it's possible that I've missed the
entire point of Finite Factored Sets.)
The ﬁrst example is the 3-variable collider that is very central to Pearl's formalism. It
is given by the following Causal Diagram:
A, B, and C are all binary variables (0=false, 1=true).
The intended meaning of a Causal Diagram (or rather, function causal model) is that
the value of a node xi is given by a deterministic function that takes as input the
parents, pa(xi), (indicated by the arrows) and an "error", or "noise", variable ui that is
governed by a probability distribution that is independent from the other error/noise
variables: xi = fi(pa(xi), ui). Thus, the value of C is given by c = fc(a, b, uC) where uC is
noise or uncertainty that is not explicitly modeled, which we can visualize like this:
We could also split up A and B into a deterministic and a random part, but as they are
root nodes, there is little point. It would just be a = fa(uA).
The Pearlian formalism runs on graphs, but Finite Factored Sets run on the set S of all
possible outcomes - the sample space. So, the goal is now to construct a sample
space that is consistent to the above graph. After that, we'll ﬁnd a factorization of that
sample space.
I think it should be clear that to cover the whole sample space S, it is suﬃcient to
consider all possible combinations of the outcomes of A, B, and UC (but not C),

because if we know the value of these three, then we also know the value of C, via fc.
So, we simply deﬁne S as the Cartesian product of the sets of possible values of A and
B: A = {0, 1}, B = {0, 1}, and the possible values of UC: UC, which I'll leave undeﬁned
for the moment (except to note that it must be a ﬁnite set):
S = A × B × UC,
(a, b, uC) ∈S
(We use the lowercase letters to represent elements of sets that make up the
Cartesian product: a ∈A, b ∈B, and uC ∈UC.)
Then - as is custom in the formalism of Finite Factored Sets - the variables A, B, UC
are deﬁned as partitions on S. For example, A is a partition consisting of two parts: 1)
the set of elements of S where a = 0 and 2) those where a = 1:
A = {{(a, b, uC) ∈S|a = 0}, {(a, b, uC) ∈S|a = 1}}
This captures exactly what the variable A is supposed to represent: the question of
whether the ﬁrst element of the Cartesian product is 0 or 1. B is deﬁned analogously:
B = {{(a, b, uC) ∈S|b = 0}, {(a, b, uC) ∈S|b = 1}}
And UC as well:
UC = {{(a, b, uC) ∈S|uC = 0}, ...}
with an as of yet undeﬁned number of parts in the partition.
Now, given the way we constructed S, the set of partitions G = {A, B, UC} is a
factorization of S: F = (S, G), because any element of S can be uniquely identiﬁed by
knowing in which part of A, B, and UC it is, because S is just the Cartesian product of A
, B, and UC.
Now that we have the set, let's look at the probability distribution over S. Let P(A = a′)
be shorthand for P({(a, b, uC) ∈S|a = a′}), i.e., the probability that the outcome lands

in the subset {(a, b, uC) ∈S|a = a′} of the sample space S, where the ﬁrst element of
the Cartesian product is equal to a′. We deﬁne P(B = b) and P(UC = uc) analogously.
Finally, let P(a, b, uC) refer to the probability of the individual outcome (a, b, uC) ∈S.
The fact that we want our ﬁnite factored set model to be consistent with the Causal
Diagram above, implies some things about P. In particular, the diagram implies that A,
B, and UC should be independent, which means that the joint probability should
factorize like this:
P(a, b, uC) = P(A = a)P(B = b)P(UC = uC)
But this is exactly how our (ﬁnite) set S factorizes! Thus, P factorizes the same way
that S does.
This concludes the translation of the graphical model into a semantically-equivalent
ﬁnite factored set. To recap what we have accomplished: we turned the original model
with the variables A, B, and C into one with three independent variables: A, B, and UC.
And deﬁned C as a deterministic function of these. We then constructed a ﬁnite
factored set with the factors A, B, and UC.
Now, let's deﬁne C on S as well. We again deﬁne it as a partition of S:
C = {{(a, b, uC) ∈S|fC(a, b, uC) = 0},
{(a, b, uC) ∈S|fC(a, b, uC) = 1}}
We simply partition S depending on the output of the function fC. This also allows us
to deﬁne P(C = c) as P({(a, b, uC) ∈S|fC(a, b, uC) = c}).
In the Pearlian formalism, we can read oﬀ the fact that A and B are independent from
the graph structure. With Finite Factored sets, we have to look at histories. The history
of a partition (aka a variable) is roughly speaking the minimal set of factors in the
factorization G that is suﬃcient to fully specify the partition (aka variable). A and B are
factors in their own right, so their history consists just of themselves: h(A) = {A} and 
h(B) = {B}, because surely knowing A is enough to know A. As h(A) ∩h(B) = ∅, we can

conclude that A and B are orthogonal, but this is just because we deﬁned the
factorization that way, so this is no new information. Still, it's a good consistency
check.
The history of C is more interesting, but still pretty trivial to determine. As long as fC
makes use of all its arguments and is generally non-pathological, all the factors are
needed to determine C. So: h(C) = {A, B, UC}. This implies h(A) ⊂h(C) and h(B) ⊂h(C)
which implies A and B are both strictly before C in the time deﬁned by our
factorization. This is a non-trivial result which matches what we would have expected
from the graphical model that I drew in the beginning. So, that's good.
But what if we condition on C? Colliders are special because they have A ⊥B but 
A ⊥/ B|C. Can we recover this results from our ﬁnite factored set model?
To compute conditional orthogonality, we have to compute conditional histories. In
order to condition on C, we need the histories conditioned on the subsets of S where 
C = 0 and where C = 1. We'll call these subsets C0 and C1:
C0 := {(a, b, uC) ∈S|fC(a, b, uC) = 0}
C1 := {(a, b, uC) ∈S|fC(a, b, uC) = 1}
Let's start with the latter: let's determine h(A|C1) - the conditional history of A given 
C1. However, the deﬁnition of conditional history is not as straightforward as the
deﬁnition of history. I'm reproducing it here:
Let F = (S, G) be a ﬁnite factored set, let X, Y , Z ∈Part(S), and let E ⊆S. We use 
s ∼X t to say that two elements s and t are in the same part in X. The conditional
history of X given E, written hF(X|E), is the smallest set of factors H ⊆G satisfying
the following two conditions:
1. For all s, t ∈E, if s ∼b t for all b ∈H, then s ∼X t.
2. For all s, t ∈E and r ∈S, if r ∼b0 s for all b0 ∈H and r ∼b1 t for all b1 ∈G∖H,
then r ∈E.

The ﬁrst condition is easy. It just says that the factors in h(A|C1) should be suﬃcient to
pin down A in C1. We can safely assume that A itself will be enough. So, is 
H := h(A|C1) just {A} again? Well there is that second condition. To show its eﬀect,
let's assume that indeed H = {A}. The condition then speciﬁes something that has to
be true for all s, t ∈C1 and r ∈S. However, given the assumption we just made, I can
construct a counterexample to the stated condition. (Thus showing by contradiction
that h(A|C1) is not just {A}.)
To make things concrete, I'll give a concrete deﬁnition of how C is computed. First,
let's say that UC is the result of a 6-sided die; so: UC = {1, 2, 3, 4, 5, 6}. We'll then say
that C is XOR of A and B, except when UC rolls a 6, in which case C is just 0:
fC(a, b, uC) = {
a ⊕b
if uC ≠6
0
else.
The counterexample is then:
s = (as, bs, uCs) = (1, 0, 1)
t = (at, bt, uCt) = (0, 1, 2)
r = (ar, br, uCr) = (1, 1, 2)
Let's ﬁrst conﬁrm that s and t are indeed in C1. To be in C1, we need fC(a, b, uC) = 1.
This is true for both, because 1 ⊕0 = 1 and 0 ⊕1 = 1 and in neither case is uC = 6.
Now, in the if-statement in the second condition, we ﬁrst ask whether r and s can be
distinguished by the factors in H. We're assuming for now that H consists only of A, so
the question becomes whether r and s can be distinguished by looking at A. The
answer is no, because in both cases, the ﬁrst entry is 1 (so, a = 1). Thus, as far as the
factors in H are concerned r and s are the same.
Next we must investigate r and t in light of the the factors that are not in H. In our
case, that's B and UC. As we can see, r and t are indeed indistinguishable under B and

UC because r and t only diﬀer in the ﬁrst entry of the tuple (which corresponds to A).
The if-statement is thus true, and so according to the condition, we should have r ∈C1
. However, this is not the case, because 1 ⊕1 = 0, and so r is in C0. In other words, we
have a contradiction and h(A|C1) can't consist of only A.
So, what does the second condition in the deﬁnition of conditional history look out for?
It seems to want to prevent that both H and its complement are incomplete when it
comes to being able to answer the question of whether an element is in C1 or not.
That is, if both the factors within H and the factors without seem to indicate that an
element r is in C1, then - the deﬁnition says - it should really be in C1. The factors
should not be split in such a way that neither H nor its complement (G∖H where G is
the set of all factors) can reconstruct C; otherwise, the border itself leaks information
about the likely values of factors.
The problem can be easily ﬁxed by adding B and UC to h(A|C1) as well, so that H has
all the factors needed to fully pin down the border of C1: h(A|C1) = {A, B, UC}. Via
symmetry, we also get h(A|C0) = {A, B, UC} and also the same for h(B|C0,1). We thus
deﬁnitely don't have h(A|C0/1) ∩h(B|C0/1) = ∅ anymore. And so, A and B are not
orthogonal given C.
This means we have recovered the two most important independence statements
about the collider: A ⊥B and A ⊥/ B|C, as well as C ⊥/ A and C ⊥/ B (just from the fact
that A and B are strictly before C in time). What remains to be conﬁrmed are C ⊥/ A|B
and C ⊥/ B|A. I'll leave that as an exercise for the reader.
As our next example, we'll have a look at the 3-variable chain:
Separating out the noise/uncertainty:
I won't repeat all the steps now and just skip to constructing the sample space S as
the Cartesian product of the possible values of A, UB, and UC. The elements of S are
then tuples like this one:

(a, uB, uC) ∈S
We deﬁne the partitions A, UB, and UC on this in the obvious way, and so they are our
factorization of S. The variables B and C are deﬁned as partitions of S according to
some deterministic function of (a, uB) and (a, uB, uC) respectively. For the histories, it
follows then that h(A) = {A}, h(B) = {A, UB}, and h(C) = {A, UB, UC}, which implies 
A < B < C, as one would expect.
(It might seem remarkable here that we can reconstruct the exact order of A, B, and C
, but that is only because we deﬁned S that way. Nothing interesting has happened
yet. This is just a self-consistency check.)
I tried visualizing these histories but had limited success:
visualization of history
visualization of history
The interesting independence statement in this model is A ⊥C|B. So, to investigate
this, let's look at h(C|B1) - the conditional history of C on the subset of S where B = 1.
The ﬁrst step is to clarify that C is computed via B:
fC(a, uB, uC) = gC(fB(a, uB), uC)
That is, a and uB are only used via fB. But if we already know the output of fB (because
we're in the subset B1 ⊂S where fB = 1), then we only need uC to compute the value
of C. Thus, it would be consistent with condition 1 of the conditional histories
deﬁnition if we had h(C|B1) = {UC}. But is it also consistent with condition 2?
In condition 2, we have ﬁrst this part: "if r ∼UC s ..." However, UC has no bearing on
whether r is in B1 or not, because fB doesn't take UC as an input. So, we can ignore
that part. Without that part we are left with: if r ∼X t for all X ∈{A, UB}, then r ∈B1. Is
this true for all t ∈B1 and r ∈S? Yes, because what it is saying is, if r seems to be in 

B1 according to A and UB, then it really is in B1. And that is true, because A and UB
already fully determine B, so if they say you are in B1, then you are.
So, we avoided the trap we had above, where neither H nor its complement could
reconstruct the boundary of the set we were conditioning on. Here, A and UB (which
are both not in H) are able to reconstruct the boundary of B1 perfectly, so condition 2
is fulﬁlled. Thus, h(C|B1) = {UC} (and analogously h(C|B0) = {UC}), which implies that
h(A|B0/1) ∩h(C|B0/1) = ∅. (I didn't check that h(A|B0/1) doesn't contain UC, but a quick
argument shows that it won't: UC is neither needed to pin down A nor to pin down the
border of B0/1.) So, A ⊥C|B as expected.
There is one interesting 3-variable model left - the common cause:
The reader may want to try this out for themself before reading on.
Splitting oﬀ the randomness, we get:
As before, we can construct a sample space that is factorized by G = {UB, A, UC},
giving us the ﬁnite factored set F = (S, G). The histories for A, B, and C are then again
obvious: h(A) = {A}, h(B) = {UB, A}, and h(C) = {A, UC}. We can see that B and C are
not independent, because their histories both include A. But they also don't have a
deﬁnite temporal order; we can neither say B <F C nor C <F B.
From Pearl's theory, we expect that B and C become independent when conditioned
on A. So let's look at those conditional histories. As we know all the tricks by now, it
will be a very high-level analysis.
We recall the ﬁrst rule of conditional histories: the history should contain those factors
that are needed to pin down the variable given that we already know the value of the
conditioned variable. If we know the value of A, then it suﬃces to know UB in order to
know B. So, the conditional history, H, of B given that we know the value of A,
contains UB at the least.

The second rule of conditional histories demands that either H or its complement, 
G∖H, (or both) is on its own suﬃcient to determine the value of the conditioned
variable (A in our case). Assuming H = {UB}, the complement of H, {A, UC}, contains 
A itself, and so is deﬁnitely suﬃcient to determine A. Thus, when conditioning on
values for A, H = {UB} is a permitted history by the second rule.
By symmetry, we get {UC} for the conditional history for C. This all then implies 
B ⊥C|A as expected.
Conclusions
I hope this was useful to someone. And I hope I didn't completely mess up the
intended use of this formalism.
One thing I appreciate about this formalism is that I ﬁnd it easy to drop to the base
level (the sample space with the factorization) to explicitly check my higher-level
thoughts when I get confused. It's nice to have that solid ground level available
whenever I need it.
The rule about conditional histories is not exactly easy, but it feels closer to a
fundamental law than the d-separation rules of the Pearlian paradigm, which always
felt a bit arbitrary.
Finally, I still kind of think that a DAG is a really nice way of visualizing dependencies
and independencies of random variables. I wonder if there is a visualization that feels
more native to Finite Factored Sets while still looking nice.

Brain preservation to prevent
involuntary death: a possible cause
area
(Cross-posted at the Eﬀective Altruism Forum) 
Previous EA discussions of this topic: here, here, here, and here. Note that these
primarily focus on cryonics, although I prefer the term brain preservation because it is
also compatible with non-cryogenic methods and anchors the discussion around the
preservation quality of the brain. See here for more discussion of terminology.  
This post is split up into two sections: 
(a) Technical aspects, which discusses why I think preserving brains with methods
available today may allow for revival in the future with long-term memories and
personality traits intact. 
(b) Ethical aspects, which discusses why I think the ﬁeld may be among the most cost-
eﬀective ways to convert money into long-term QALYs, given certain beliefs and
values. 
In this post, I'm not discussing whether individuals should sign themselves up for
brain preservation, but rather whether it is a good use of altruistic resources to
preserve people and perform research about brain preservation. 
Technical aspects of brain preservation
1. What is the idea behind brain preservation? 
a. Brain preservation is the process of carefully preserving and protecting the
information in someone's brain for an indeﬁnite length of time, with the goal of
reviving them if technologic and civilizational capacity ever progresses to the point
where it is feasible and humane to do so.
b. Our society's deﬁnition of death has shifted over time. It depends upon the
available medical technology, such as CPR and artiﬁcial respiration. In the future, the
deﬁnition of death will almost certainly be diﬀerent than it is today. One possible
improved deﬁnition of death would be when the information in the person's brain that
they value is irreversibly lost, which is known as information-theoretic death.
c. Pausing life without causing information-theoretic death could be done with a long-
term preservation method that is not yet known to be reversible today, but which has
the goal of preserving enough information in the brain so that it could potentially
become reversible in the future with improvements in technology. This is brain
preservation. 
d. With current methods, we can potentially preserve enough structure in the brain
over the long term to retain the information for valued cognitive functions like long-
term memories. There are multiple possible methods to attempt to accomplish this,
each with upsides and downsides. 

e. Plausible methods for the revival of people following brain preservation include
whole brain emulation or oﬀ-board molecular nanotechnology-based repair. 
2. What in the brain is necessary to try to preserve? 
a. It is already possible to stop electrochemical neuronal activity (in humans) and
biological time (in other animals) without loss of long-term memories. The cognitive
functions that most people care about seem to be encoded by static structures in the
brain. More on this here. 
b. Adequately preserving the brain alone would be enough to retain the information
for long-term memories and core personality traits, because it is the only part of the
body that is known to be irreplaceable without massive eﬀects on this information.
More on this here. 
c. Despite currently lacking complete models, we can use existing knowledge in
neuroscience to evaluate the hypothetical process by which structural information for
valued cognitive functions could be mapped in the future. More on this here. 
d. A wealth of evidence suggests engrams are encoded in neural structures
distributed across the brain. More speciﬁcally, it seems to be the distributed activity of
neuronal ensembles communicating through the biomolecule-annotated connectome
that instantiates long-term memory recall. More on this here. 
e. Because of the correlated nature of structural information in the brain, it is likely
that there are numerous topological maps of the biomolecule-annotated connectome
that could retain the information needed for long-term memories. Even if many of
these maps were damaged or destroyed by aspects of the brain preservation
procedure, if at least one could still be inferred, then the information content would
still be present. In the future, our inference capacities are likely to improve
dramatically as a result of AI, making the inference of suﬃcient structural information
in the preserved brain much more plausible. More on this here. (This is assuming that
humanity survives the transition to transformational AI.)
3. What are the methods we can use to try to preserve structural
information in the brain? 
There are numerous possible ways to do so. Broad categories include: 
a. Unprotected cryopreservation (also known as "straight freeze"). In the
absence of cryoprotective agents, ice will certainly form during low subzero cooling
and will mechanically crush cellular structures, leading to damage to the connectome.
The question is whether that damage is enough to cause information-theoretic death.
There is some evidence that structural preservation quality following unprotected
cryopreservation may not actually be that bad (for example, Aschwin de Wolf
discusses this here), although this is one of the most controversial questions in the
ﬁeld, and others vehemently disagree. Neural Archives Foundation oﬀers this option in
Australia, which costs AUS$30,000 (~$22,000 USD). 
b.  Cryopreservation with cryoprotective agents. By perfusing cryoprotective
agents into the brain, ice formation and associated damage can potentially be
prevented. Unfortunately, getting cryoprotective agents into the brain is a diﬃcult
problem. Also, this causes dehydration and associated damage to brain structure.
However, Greg Fahy's team at 21st Century medicine has reported that they can
achieve good preservation quality with their cryoprotective agent perfusion protocol;

for the most recent details on this, see page 496 of Robert Freitas's book. Cryonics
Institute oﬀers this option for ~$30,000 for people who live locally in the Clinton
Township, Michigan area. 
c. Aldehyde-stabilized cryopreservation. Aldehyde stabilized cryopreservation
was introduced as a whole-brain preservation method in 2015. It has two main steps
(to oversimplify quite horribly): aldehyde ﬁxation, followed by cryopreservation. The
ﬁxation step seems to help with the distribution of cryoprotectants to the brain. The
reasons for this aren't quite clear to me but likely involve quickly stabilizing the
structure of blood vessels and/or cell membranes. 
For this method, 21st Century Medicine (where Robert McIntyre and Greg Fahy
worked) won the Brain Preservation Foundation prizes -- the small mammal prize in
2016 and the large mammal prize in 2018. These prizes were for showing that the
method could preserve the microstructural anatomy (i.e. the connectome) of whole
brains using a method that would be capable of storage for the long term -- at least
100 years. Clearly, aldehyde stabilized cryopreservation is an extremely promising
approach for people who prefer brain structure preservation. Oregon Cryonics is
currently oﬀering an option similar to this for $28,000. 
d. Fixation followed by long-term liquid preservation in aldehyde. Aldehyde
ﬁxatives act to preserve tissue primarily by covalently cross-linking biomolecules,
mainly proteins. This dramatically strengthens natural gel-like structures in the brain
such as the cytoskeleton and extracellular matrix and has been shown to cause gel
formation in brain tissue. Gel formation is a way for a liquid to have solid-like
properties and stability. Vitriﬁcation and gel formation actually share many biophysical
properties. 
Might gel formation alone be suﬃcient to preserve brain structure for the long term?
There are some studies suggesting that neuronal morphology is maintained for
decades when preserved even at room temperature in formalin. One study found that
neuronal morphology was well-preserved in brain tissue stored in formalin for 50
years. Another study found that there was "excellent preservation of ﬁne and even
cellular details in the tissue" in brain tissue preserved in formalin for ~80 years.
Formalin only came into use in the 1890s, and there isn't that much brain tissue
preserved this way for such a long time, so it's unclear how long this tissue would
maintain its structure. There are also a lot of biomolecular alterations resulting from
formalin storage, although they may not be suﬃcient to cause information-theoretic
death. 
Oregon Cryonics is currently oﬀering liquid preservation in aldehyde for $1000. When
combined with the fees of a local funeral director or pathology specialist to do the
initial preservation, the total cost for this is likely around $3000-10,000. Note that the
average funeral cost in the US is around $7000. 
e. Fixation followed by plastic embedding. For example, brain plastination. This
oﬀers the potential for more stable long-term preservation at ambient temperatures.
As far as I know, this is not currently oﬀered anywhere in the world. 
Fixation-based options (#3-5) are potentially more scalable because they oﬀer the
possibility of tapping into the existing brain banking infrastructure, which exists in
much of the world. However, doing so also depends on how long one can wait
following legal death but prior to starting the preservation procedure, which is another
unknown question.  

I want to be clear that I am not endorsing any of these organizations. I am just listing
these prices for the purposes of discussing how much brain preservation currently
costs because this is clearly relevant to its potential as an eﬀective altruism cause
area. Hopefully, it is apparent that there is a range of possible approaches and that
room temperature methods in particular have the potential to be quite cheap. 
Open questions: 
- Which structural aspects of the brain are necessary for preserving the information
contained in long-term memories and personality? 
- Which brain preservation method is the most cost-eﬀective way to preserve a
suﬃcient amount of brain structure for long-term memories and other valued aspects
of personal identity? 
- Can we improve the expected structural preservation quality of room temperature
options using methods that are still cheap? 
- How long can one wait after legal death but prior to starting the brain preservation
procedure, without causing information-theoretic death? 
Ethical aspects of brain preservation as an EA cause area 
I'm going to discuss four areas here: importance, tractability, neglectedness, and
scalability. 
Importance: To the extent that it might work, the ethical importance of brain
preservation is very high, from both a QALY improvement and QALY extension
perspective. 
1. QALY improvement: The badness of involuntary death - for oneself, one's loved
ones, and others (including non-human animals) - is a huge psychological burden on
most people. Involuntary death is widely considered one of the great harms of life,
even by anti-natalists. An inﬂuential psychological hypothesis, Terror Management
Theory, shows that people ﬁnd the idea of death so aversive as to ward oﬀ even
thinking about related topics. Even if brain preservation was credibly seen as giving
people an optional and realistic chance at living longer, this could be a valuable
improvement to people's quality of life, as Tim Urban has discussed. 
2. QALY extension: If brain preservation is successful and the person is revived, then
they are likely to be revived in a future in which aging and many other forms of death
are no longer a problem, either through improved biomedicine or via whole brain
emulation. This has the potential to create an enormous number of QALYs. 
There is a question of how long humans can live in a way that achieving a QALY would
still be possible. Some feel that living a high-quality life for thousands of years is not
possible. However, I'm not sure where people are getting this from, especially given
the vast and obvious quality of life improvements that one could imagine in the
future. For the purposes of these calculations, I will assume 10,000 QALYs is possible,
given the potential for playing games, exploring the universe, understanding the
secrets of the universe, learning about the past and future, connecting with other
people, etc. One can imagine 10,000 QALYs as being like living 100 diﬀerent lives,
each 100 years long. Given the vast diversity in lives imaginable, if anything, 100
seems to me like an underestimate. If one assumes that more QALYs than 10,000 is
possible, then subsequent calculations become stronger (and vice versa). 

As of 2016, one estimate is that via bed nets for malaria prevention, $3500 buys 35
QALYs, or $100 per QALY. On the other hand, if a brain preservation procedure costs
~$10,000 and yields 10,000 QALYs, it has the potential to have a cost of $1 per QALY.
Naturally, a key question is what is the probability of successful revival from brain
preservation. If the probability of success is 1%, then brain preservation would be at
$100 per probabilistic QALY, on par with bed nets for malaria prevention. Given our
uncertainty in probability estimates, then more research to determine the probability
of the success of brain preservation also seems very valuable. 
One might argue that it is unfair to distribute all of the QALYs to one person and that
they should be shared more broadly. However, there is no reason that  brain
preservation  could not be made available to everyone who wants it. 
Others might argue that these QALY arguments are misguided because future society
will be able to create more people at will. For example, as Nuno Sempere describes: 
> One could also argue "that cryonics doesn't create many additional QALYs because
by revival time we've probably hit Malthusian limits. So any revived cryonics patients
would be traded oﬀ against other future lives." 
As I discuss above in QALY improvement, I disagree with this viewpoint that humans
are fungible and one can just create more people to achieve the same QALYs, because
of the psychological and relational harms caused by involuntary death. But, if
someone views humans as replaceable, then I can see why they might think that brain
preservation doesn't really create many QALYs over alternatives possible in the
future. 
3. Brain preservation may also have eﬀects on people's propensity for long-term
thinking. Having more "skin in the game" for humanity's future may make people's
behavior more pro-social. This is much discussed and seems plausible. But it also
could have complicated second-order eﬀects and seems hard to precisely estimate, so
I don't consider it an EA reason to promote brain preservation.   
4. Brain preservation doesn't seem to contribute to S-risks. For example, if an S-risk
were to occur via a malevolent AI that wanted to torture humans, then that AI could
simply simulate human minds sampled from mindspace, regardless of whether those
people had chosen to pursue brain preservation. 
Tractability: Uncertain. Structural brain preservation is already possible and already
happening. On the other hand, revival is very much not happening. In my opinion, it is
very diﬃcult to know what the probability of success of brain preservation is. There
are ways to bound it based on societal factors (for example, the probability of social
collapse), but the core problem of suﬃcient preservation quality is very challenging to
predict and is correlated with other forms of uncertainty in the project. 
I have studied this topic for years and I have no idea how people are making these
probability estimates. Aggregating success probability estimates by those individuals
who are willing to speculate despite our current uncertainty doesn't seem very
helpful. Revival also seems to be too far away for a working prediction market given
today's prediction market methods. Instead, all we can have today is feelings about
how likely it is to work. So for tractability, opinions likely vary signiﬁcantly. 
Neglectedness: High. There are very few people in the world working full-time on
structural brain preservation research for the purposes of preventing involuntary

death; probably less than ﬁve to ten people. There are also few working full-time on
preserving people; probably less than twenty-ﬁve to ﬁfty people. 
The practice is also highly stigmatized. Brain preservation is thought to run contrary
to societal mores and break taboos regarding death. For example, Tucker Carlson
called cryonics "obviously grotesque and ghoulish and kind of revolting". This is a win
from a neglectedness perspective because it suggests a clear reason why it is
neglected that is orthogonal to its likelihood of success. 
Scalability: High. One survey suggests that around 6% of people have already
decided to pursue cryopreservation, although they currently lack options to do so.
With 178,000 people dying per day, a naive extrapolation means that brain
preservation has the potential to probabilistically dramatically extend the QALYs of
10,000 people a day if the infrastructure to perform this was expanded. 
Another survey suggests that around 20% of people could imagine doing brain
preservation. People would probably be more likely to actually do it if it were a more
widespread practice and cheaper. 
Open questions: 
How much might the possibility of not deﬁnitely undergoing involuntary death
improve people's quality of life today? 
How many QALYs are possible for a human to experience assuming an indeﬁnite
lifespan?  
Are lives fungible, so that if one person dies we can just create another one with
no signiﬁcant loss to QALYs? If not, how should this aﬀect our QALY calculations
for brain preservation? 
How will future society feel about creating more people until reaching Malthusian
limits? 
Given low costs (perhaps around the same cost as a typical funeral or less),
what percent of people would be interested in brain preservation? 
Conclusion 
It seems to me that: 
(a) Most current technical arguments against brain preservation, to the extent that
there are any at all, don't grapple with the possibility of structural inference.  
(b) Most extant arguments in the eﬀective altruism community against brain
preservation as a cause area don't grapple with QALY improvement/extension, and for
unclear reasons treat humans as replaceable units, neglecting relational and
psychological factors. If people think humans are replaceable, I think they should
justify this, and also consider whether they are being consistent about it. 
(c) With today's methods, brain preservation may already be among the best altruistic
investments available from a QALY improvement/extension perspective, given
reasonable estimates about the probability of success. 
(d) With more research, substantially cheaper methods for structural brain
preservation could potentially be developed, which could further improve the
cost/beneﬁt calculus. With more research, our uncertainty about diﬀerent aspects of
the brain preservation project could also be better clariﬁed. 

As a result of the above, and given its neglectedness, I think brain preservation for the
prevention of involuntary death is one of the best areas for people interested in
helping others to work in. I also think it is a great place for people who are interested
in helping others to donate money. If you disagree, I would love to hear from you why
that is. If you agree, I would love to discuss with you practical topics of how to best
improve the ﬁeld. Thanks for reading! 
Further reading: 
- Francesca Minerva is a leader in the ethics of cryonics. She has a book on the topic
here and has a video discussing QALY extension via cryopreservation here; both are
recommended if you're interested in this topic. 
- The Brain Preservation Foundation website: https://www.brainpreservation.org/
- John Smart's essay: "Do we need a noncryogenic brain preservation prize?" 
- My in-progress essays on brain preservation (feedback welcome!):
https://brainpreservation.github.io/ 

Eﬀective Ideas is announcing a
$100,000 blog prize
This is a linkpost for
https://forum.eﬀectivealtruism.org/posts/xapRLBTpMYokrpd9q/we-re-announcing-a-
usd100-000-blog-prize
From the EA Forum:
We want to encourage a broader, public conversation around eﬀective altruism and
longtermism. To that end, we're oﬀering up to 5 awards of $100,000 each for the best
new and recent blogs. We're also making grants to promising young writers in the
community.
You can learn more about the project and get on our radar here. 
Why does this matter?
Top-of-funnel community growth in EA is slower than it could and should be. At the
same time, EA is relatively underrepresented in intellectual discourse compared to
newer and smaller movements like Progress Studies. EA is producing a ton of
thoughtful writing, but the majority takes place in internal discussions and private
documents. For some discussions, this would be the only sensible way to have them.
But having other discussions in public should help to raise the salience of EA in the
broader discourse and bring more people in. It could also help spark new ideas. 
 Further, we think EA needs more strong writers who can share key ideas in
prestigious and popular venues — to persuade people to work on the most pressing
issues of our time and to advance our thinking about them. We want to incentivize
EAs to develop those skills. 
What's the plan?
We want to jumpstart these ambitions with the Blog Prize. Over the course of 2022,
we want to ﬁnd the very best new blogs exploring themes related to eﬀective
altruism and longtermism. Up to 5 winning bloggers will receive a prize of $100,000
each. (We were inspired by Tyler Cowen's "Liberalism 2.0" blog prize). You can read
more about our rules and guidelines on our website.
The judging panel for the blog prize is me (Nick Whitaker), Leopold Aschenbrenner,
Avital Balwit, and Fin Moorhouse. Most blogs will be considered via our self-nomination
form, but please feel free to send us recommendations. 
What next?
We hope this can be a ﬁrst step towards a more ambitious eﬀort to support an
ecosystem of public-facing writing for EA and longtermism. We believe that EA
blogs could soon make up a major part of the general blogosphere, ﬁnding audiences

(and potential EAs) we wouldn't have found otherwise. Hopefully, we will also inspire
the writing of foundational blog posts and posts that evolve into great projects. 
To help potential bloggers, we've compiled a "How to start a blog" guide. We're also
oﬀering mentorship on writing and editorial strategy to bloggers amid our private
Slack community of bloggers. Self-nominate your blog on our website if you'd like to
join. We have fostered a lively community for discussion, cross-promotion, and peer-
to-peer feedback. Eventually, we hope to oﬀer bloggers seminars from established EA
writers.
While this is our ﬁrst big announcement, stay tuned for future plans and follow
ongoing eﬀorts from Eﬀective Ideas to build and foster a written media ecosystem for
EA. Watch this space.
This project is supported by FTX Future Fund and Longview Philanthropy .

ELK Computational Complexity: Three
Levels of Diﬃculty
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
ELK is described as a problem we wish to solve in the "worst case"; IE, for any
potential failure case we can describe, we want to have a strong argument about why
we won't run into that particular problem. The ELK document lists several cases we
want to handle in that way. First in this list is the case where the correct translation
is arbitrarily computationally complex.
However, this assumption implies that ELK is intractable. At best, we might hope to
deﬁne some notion of approximate solution, and prove that there are eﬃcient
approximate solutions. So how are we supposed to solve ELK, if we are to assume that
it's intractable?
The answer (at least, as I see it) is by arguing that the correct translation cannot be
too computationally complex. (Or, failing that, by providing a strong argument that it
is possible, at which point we would have to narrow the scope of ELK ambitions.)
A proposed algorithm for solving ELK which (1) has a strong argument for its
correctness, and (2) is itself a tractable algorithm, is usually already an argument that
correct translations are tractable. For example, a search procedure usually has to
evaluate candidate translations by running them; so if we could prove that the search
terminates in a reasonable amount of time, we've probably already proven that the
solution it ﬁnds is tractable (at least in certain cases, EG, on the training data). 
But this doesn't really answer the question. Since we don't already have such a
solution, we may need to explicitly think about tractability in order to produce one.
So, in this post, I'm going to think about how we might possibly bound the
computational complexity of ELK.
Level 3: Probabilistic Knowledge
The third level of diﬃculty for ELK (IE, the most computationally diﬃcult that I'll
discuss here) is the most conceptually simple to understand: we deﬁne the "correct
translation" of a pattern of activity in the Predictor to be any and all things correlated
with such activity. 
In other words, we deﬁne the "meaning" of Predictor activity as just whatever we can
predictably infer from it. In my previous post I formalized this by imagining probability
distributions called "third person perspectives" which can be conditioned on the full
execution trace of the Predictor, and infer as much as possible about the world from
this information. 
This seems intractable for two main reasons:
1. It requires logical omniscience. For example, this criterion seemingly requires
that the correct translation knows all the predictable consequences of a plan,

not just the immediate consequences. So long as the consequences can be
predicted by running the world-model forward, they count as probabilistic
implications of the Predictor's knowledge. In particular, the future outcomes
of any computations running in the world are included in this. Therefore,
computing this form of ELK requires access to a halting oracle in the worst case. 
2. Even if we can side-step the ﬁrst issue, this form of ELK is at least as hard as
ﬁnding marginals in arbitrary graphical models, which is #P-complete. As
complexity classes go, this is worse than NP-complete. 
By trying to detect any correlation, we impose an arbitrarily high requirement of
detailed simulation. As a consequence, this Reporter would know far more than the
Predictor can realistically be said to know. If the Predictor knows how the dominoes
have been set up, the Reporter is required to know how they fall. 
From a safety perspective, we're ﬁne with the reporter knowing too much -- we're only
concerned with cases where the Predictor knows something which the humans don't
know. However, this does seem like a sign that we don't have to solve such a hard
version of the problem.
So, can we specify a more tractable version?
Level 2: Computationally Accessible
Knowledge
The intuition I'm trying to capture with 'level 2' is: to "know" something means that a
(computationally) relatively close version of you could report the information. This
rules out computationally intractable "knowledge" which follows probabilistically from
the Predictor's state, but which cannot be easily computed from it.
To put it a diﬀerent way: knowledge is only knowledge if it can feasibly be used to do
something. For example, if I have a book telling me how to use the SuperSort
algorithm (a hypothetical sorting algorithm that's better than linear time), then there's
a pragmatic sense in which I know how to write SuperSort -- if you ask me to optimize
some code which includes a sorting step, I'll look up the SuperSort algorithm and
implement it, to see if it improves the performance of your code.
However, if I instead had an encrypted copy of the book, then I wouldn't possess the
knowledge in the pragmatic sense. If you asked me to optimize the same code, I
would try a diﬀerent approach.
This could be formalized in many diﬀerent ways. Here is a provisional attempt:
Deﬁnition: The  L-knowledge of a latent variable X based on the existence of a
program, as follows. The program takes in an execution trace of the Predictor in a
given situation. It outputs a pair of booleans. The ﬁrst boolean indicates whether the
program is able to decide the latent variable in this case. (Think of this as "whether
the predictor has the latent knowledge".) If so, the second boolean indicates the state
of the latent variable X. (This could be easily generalized to non-boolean variables.)
Call the length of the program K, and the time to run the program T. L-knowledge
exists (in a speciﬁc situation) when K+log(T)<L, and the ﬁrst output bit is true.
The ﬁrst bit is needed to allow programs to opt out of telling us the status of the latent
variable in cases where it is impossible for them to do so. Otherwise, 'knowledge'

would only exist when a reporter could infallibly tell us the status of the latent variable
in every case. We can't just deﬁne L-knowledge as the existence of a program which
tells us the status of the latent variable in this speciﬁc case (and satisﬁes K+log(T)
<L), because if we deﬁned it in this way, L-knowledge would be too easy. There's
always a short program that outputs 1 and a short program which outputs 0, so
whichever value the variable actually has, a program certifying L-knowledge for very
low L would exist.
Possibly, the above deﬁnition is still too easy to achieve, because we can still do that
exact same trick by making the ﬁrst bit of the program recognize only one target
situation. (This allows us to create a bound above which L-knowledge always exists;
but, at least the bound is not constant. It depends on the Levin complexity of
recognizing the speciﬁc scenario.) 
However, as I've already argued for Level 3, it's ﬁne (for safety purposes) if our
deﬁnition of knowledge includes too many cases. If we can construct a reporter who
can tell us L-knowledge whenever it exists, and we have conﬁdence that knowledge
implies L-knowledge, then we can create a reporter at least as informative as the
direct reporter. (It might sometimes tell us the truth when the predictor doesn't
actually know the truth, but that's ﬁne!)
However, unfortunately, it's not obvious that meaningful "knowledge" implies L-
knowledge. My Level 3 deﬁnition seemed to safely over-estimate the Predictor's
knowledge; but that is not nearly so obvious with L-knowledge.
Knowledge Without L-Knowledge?
L-knowledge allows the translator to self-select cases when knowledge exists and
when it doesn't; but, within those cases, knowledge is required to be perfect. A
translation which predicts the target variable 95% of the time does not count for
anything (because it is wrong inﬁnitely often). We could also deﬁne some sort of
relaxed p,L-knowledge which only required the knowledge to be p% correct, or
something along those lines. I won't discuss this possibility any further in the current
essay, however.
Another potential failure case I'm worried about relates to using knowledge for
planning. 
There are roughly two types of concerning cases for ELK. In one case, the SafeVault AI
uses latent knowledge which the human lacks, to enact a plan which drives apart the
proxy reward from what humans would like to optimize. Call this "malign use of latent
knowledge." Example: the SafeVault AI executes an undetectable exploit to jettison
itself (with the diamond) into space. Everything looks ﬁne on the sensors, and from
the outside of the vault, but if someone tries to go inside (eg, to retrieve the diamond)
they'll discover that the building is now totally empty.
In another case, the SafeVault AI fails to use latent knowledge. For example, a burglar
executes an undetectable sensor exploit in order to take the diamond. The SafeVault
understands what's going on but doesn't care, because its reward is not impacted.
Call this "malign negligence of latent knowledge."
One way to understand malign negligence is, "AI systems should not ignore mission-
critical information that they could have taken into account". But how do we judge
when they "could have" taken information into account? L-knowledge seems like a

reasonable way to cash out this "could". Where L-knowledge exists, there is a "close"
version of the system which takes this knowledge into account. (This close version is
the policy we want to incentivise, using the reporter's information.) Where L-
knowledge doesn't exist, such a variant is either too costly to run or too hard to ﬁnd. 
However, malign use of latent knowledge is in some sense more concerning. Can we
provide any guarantees against it?
Informal deﬁnition: planning-knowledge of X is the ability to make plans that
importantly rely on X. This means something like: in worlds where X is diﬀerent, the
actions that the SmartVault AI takes are diﬀerent (while, presumably, achieving similar
ends).
Conjecture: Planning-knowledge implies L-knowledge (for some fairly low L).
If something like this can be established, it would provide signiﬁcant reassurance that
uncovering the Predictor's L-knowledge protects us from malign use of latent
knowledge.
Eﬃciency of L-knowledge-based algorithms?
L-knowledge, or some suitable variation, could formalize the term "knowledge" in ELK
without requiring the Reporter to be logically omniscient. However, might it still imply
severe computational problems for ELK?
Although the concept of L-knowledge implies that it is relatively easy to run the
correct translation once found, it could still be computationally intractable to ﬁnd the
correct translation. 
Rice's Theorem says that non-trivial 'semantic' questions (questions about a
program's behavior) are always undecidable (ie, lack decision procedures).
Attempts to determine L-knowledge may run into this.
Even if this issue can be sidestepped, it still seems like determining L-knowledge
(or something similar) could be computationally intractable. Naively, to test a
candidate program, we will have to check how reliably its (second) output
correlates with the target variable X. This involves nearly the same kind of
intractable calculations discussed in level 3. We may have only given ourselves
more work, rather than improving the situation!
Can these concerns help us to further reﬁne our deﬁnition of knowledge?
Level 1: Knowledge of Knowledge
The intuition behind level 1 is: for computational knowledge ("level 2") to really count,
the Predictor has to know what it knows. Consider belief-in-belief. If I'm deluded into
thinking that water is made of tiny water-demons, but all of my predictions line up
with the standard hydrogen hydroxide hypothesis, then at some level you might say I
"know" the hydrogen hydroxide hypothesis. There's probably a computationally close
version of me who would answer questions in line with the hydrogen hydroxide
hypothesis, instead of the water-demon hypothesis. However, I may not know that this
is the case. 

I guess another way to put it is: deception requires knowledge of deception. To
deceive you, I need to know that there's a computationally close version of me who
could tell the truth. We may be able to eliminate deception (which seems good!)
without solving harder versions of ELK.
The main advantage of this idea is that it could allow us to bound the diﬃculty of
ﬁnding the correct translation, rather than just bounding the diﬃculty of executing the
correct translation. This could solve the computational complexity problem entirely
(perhaps at the cost of delivering a weaker version of ELK, with weaker safety
guarantees).
I don't have any idea how to formalize level 1 at the moment, so I'll leave it at this for
now.

Duels & D.Sci March 2022: It's time
for D-d-d-d-d-d-d-d-d-d-d-d-d-d-data!
This is an entry in the 'Dungeons & Data Science' series, a set of puzzles where
players are given a dataset to analyze and an objective to pursue using information
from that dataset. 
STORY (skippable)
You may have experienced a meteoric rise to be the billionaire CEO of DataCorp, but
you can still enjoy the ﬁner things in life.  Like children's card games.
The world-famous gaming company 'Sorcerers of the Shore' is about to release a new
card game, supposedly based upon the 'Ancient Sumerian Game of Shadows'.  They're
holding an opening tournament next week, and you mean to attend.  
Rumors that the game absorbs the souls of its players, and can destroy them if they
lose, are obvious nonsense.  That strange lady who appeared in your DataCorp oﬃce
out of a cloud of mist and warned you that 'you unleash a power beyond your
comprehension' was a transparent con artist.   You're not sure how she got in past
your security unnoticed, or how she got back out afterwards, but you don't have time
to worry about that!  You need to submit your deck list for the opening tournament!
Unfortunately, the Sorcerers of the Shore have been very tight-lipped about the whole
thing.  They haven't actually published the ruleset of the game, only the names of
cards available for you to select from.  (They said something about how you should
select 'the cards that call out to your soul.'  This is also clearly nonsense.  You don't
want cards your soul is somehow compatible with.  You want cards that will win.)
Fortunately, the ruleset is apparently 'unchanged from the ancient Sumerian game',
and so you have funded a series of archeological expeditions to look for records of
past plays of this game.  (Is this a good use of your DataCorp billions?  Of course!  This
is Serious Business!)
Your expeditions have come back with...quite impressive results, actually.  Apparently
they've uncovered a wider Ancient Sumerian civilization than anyone anticipated, with
more advanced technology than anyone expected.  The archeological community is
fascinated, but more importantly for you, you've uncovered a very large dataset of
games.  Apparently this game was used by the Sumerians for a wide variety of things
- there are records of it being used for gambling, divination, as a justice system in
trials, and apparently 'holding back the Great Devourer lest it bring an end to this
earth'.  You're not quite sure what this last one means - you think your archaeologists
have probably messed up the translation.
Even more fortunately, it appears the Sumerians had some strange ritual around
selecting cards for this game - they have done so entirely randomly, giving you a very
clean sample.  This is fortunate, because you've just heard that your long-time rival
will be attending the tournament as well.  (Some would call it undigniﬁed for a
billionaire CEO to have an ongoing rivalry with a middle schooler who is 3 feet tall, or
4 if you count his hair.  Some would call it more undigniﬁed that you keep losing.)

Your rival has declared his intention to bring a deck with one of every card available.
 This pathetic deck should be easy for you to beat - is he just hoping that he can get
lucky and draw exactly the right card whenever he needs it?  He does get lucky
annoyingly often though - you want to make sure your odds of winning are as high as
possible, to make sure you beat him this time even if he gets lucky again.
DATA & OBJECTIVE
You need to build a deck of 12 cards from the following available cards:
Alessin, Adamant Angel
Bold Battalion
Dreadwing, Darkﬁre Dragon
Evil Emperor Eschatonus, Empyreal Envoy of Entropic End
Gentle Guard
Horrible Hooligan
Kindly Knight
Lilac Lotus
Murderous Minotaur
Patchy Pirate
Sword of Shadows
Virtuous Vigilante
You can include any number of copies of any card.  So '12 copies of Alessin,
Adamant Angel' is a valid deck, as is '10 copies of Alessin, Adamant Angel plus 1
of Bold Battalion plus 1 of Dreadwing, Darkﬁre Dragon'.
Your objective is to maximize your win rate against a deck that consists of 1
copy of each card.
THIS DATASET lists past games (what cards were played on each side, and who
won).
The decks used in that dataset were randomly generated - each deck is a
random set.*
*Note for nerds: speciﬁcally, each possible deck is equally likely.  This is not quite the
same as 'pick 12 random cards each time', as that would make '12 copies of Virtuous
Vigilante' much less likely than '1 copy each of 12 cards' due to the number of
diﬀerent orderings available for the second.  The takeaway for you is the same either
way though - there isn't any hidden structure in the decks you'll see in the dataset,
don't waste time looking for it.
BONUS PVP OBJECTIVE
As in a past scenario, you may also submit a PVP deck.  I recommend sending it as a
PM to me, but if you don't mind other people seeing it you can just put it in your
answer.  The PVP deck with the best overall record (sum of performances against all
other submitted teams) will win the right to specify the theme of an upcoming
D&D.Sci scenario.  I can't guarantee success, but at some point in the next few
months (possibly after other scenarios in the pipeline have been produced) I will try to
write a scenario around whatever theme (either a general genre or a speciﬁc work)
you want.  Our previous winner, simon, selected the SCP Foundation canon as a theme
for his scenario.

I don't want the existence of a PVP objective to incentivize people too strongly against
posting ﬁndings in the chat, so as an eﬀort to reduce the risk of your ﬁndings being
used against you: if multiple people submit identical PVP decks, I will break the tie
in favor of whoever submits it earlier.
I'll aim to post the ruleset and results  on April 4th (giving one week and both
weekends for players).  PVP decks should be submitted by April 3rd to give me time to
test them.  You may edit a submitted solution at any time before the deadline.  If you
ﬁnd yourself wanting extra time, comment below and I can push these deadlines
back.
As usual, working together is allowed, but for the sake of anyone who wants to work
alone, please spoiler parts of your answers (type a '>' followed by a '!' at the
start of a line to open a spoiler block) that contain information or questions
about the dataset. 
Thank you to abstractapplic and RavenclawPrefect, who reviewed drafts of this
scenario.  (For the avoidance of doubt, they do not have inside information on the
scenario, and are free to play it).

What are the best elementary math
problems you know?
I'm looking for math problems of a speciﬁc kind. Here are the conditions I hope the
problems satisfy:
A good mathematician who hasn't seen the problem before should take
anywhere from 30 minutes to 2 hours to solve it.
The solution should only involve undergraduate level maths. Diﬃcult Putnam
problems are a good benchmark for what kind of maths background should be
required to solve the problems. The background required can be much less than
this, but it shouldn't be more.
For whatever reason you think the problem should be more widely known. The
reason is completely up to you: the solution might include some insight which
you ﬁnd useful, it might be particularly elegant, it might involve some surprising
elements that you wouldn't expect to appear in the context of the problem, et
cetera.
It's ﬁne if the problem is well known, your examples don't have to be original or
obscure.
Here are some examples:
If a polynomial with rational coeﬃcients deﬁnes an injective map Q →Q, must it
also deﬁne an injective map R →R? If yes then prove this is true, if no then ﬁnd
an explicit counterexample.
Prove that Hom(∏k∈N Z, Z) ≅⨁k∈N Z. In words, prove that homomorphisms of
abelian groups from the direct product of countably many copies of Z to Z
themselves form a group that's isomorphic to the direct sum of countably many
copies of Z.
If f : R →R is a continuous function such that the sequence f(α), f(2α), f(3α), ...
converges to 0 for every α > 0, must it be the case that limx→∞f(x) = 0? If yes
then prove this is true, if no then ﬁnd an explicit counterexample.
They are all relatively famous but they should give a sense of the ﬂavor of what I'm
looking for.

Don't Let Personal Domains Expire
It's common to see advice along these lines:
Don't build your stuﬀ in someone else's sandbox. Get your own domain and point
it to whatever service you choose to use. Your email address should be
@yourdomain, where you have full control and no one can lock you out. Don't fall
into the trap of digital sharecropping.
There are complicated tradeoﬀs here and diﬀerent choices will make sense for
diﬀerent people, but it's close to what I do personally. My writing and projects are
hosted on my own domain [1] and my email is jeff@jefftk.com.
On the other hand, I don't think this is something to do lightly. Say you register
you.example and start going by you@you.example. A few years later you decide this is too
much hassle, switch to using you@fastmail.com or you@gmail.com, and let you.example
expire. Someone else can register it, send email legitimately as you@you.example, and
Angular gets compromised. If there is anywhere you forgot to remove your former
email from your proﬁle, now you are open to being impersonated.
This problem isn't unique to personal domains, but it's much more likely: the major
email services don't make abandoned email addresses open to reregistration, to avoid
exactly this issue.
If you're considering registering a domain to use as your online identity, make sure
you're willing to take on the cost and hassle of keeping the domain registered
indeﬁnitely.
[1] I do cross-post to Facebook, LessWrong, and occasionally other places. I also rely
on them to host discussions on my posts, though I attempt to archive those
discussions back on my site.

Christopher Alexander's architecture
for learning
This is a linkpost for https://escapingﬂatland.substack.com/p/christopher-alexanders-
architecture?s=w
On the 17th of March 2022, Christopher Alexander, the architect and mathematician,
passed away. Alexander, whose intellectual inﬂuence extended far beyond
architecture and urban planning, gave the impetus to several central ideas in modern
software development, such as wikis, agile, and object-oriented programming. But his
main contribution was in vernacular architecture - that is architecture without
architects - where houses are built gradually by the people who live in them.
To honor his memory, I would like to take this moment to reﬂect on some of his ideas
in the area where he has inﬂuenced me the most: on how architecture can unlock
learning in society. How can we best make sure that the knowledge we need is
sustained over generations?
For knowledge to pass from one generation to the next we fundamentally need three
things. Firstly, we need ways for the young to access the environments they are to
master so they can learn through imitation. Secondly: for skills that are hard to learn
through imitation, we need deliberate instruction. And ﬁnally, for emotional support,
we need houses where children can seek refuge from their families or get support
when their parents are busy.
These points are not original on their own; schools try to do at least the last two. What
is more interesting is how thoroughly Alexander went about solving it. He proposed
not a new type of school but a series of architectural patterns that would weave the
functions into the very fabric of society. Access to environments, deliberate
instruction, and safe homes would be a seamless part of everything - an "educational
system so radically decentralized" that it "becomes congruent with the urban
structure itself."
At the heart of this was a pattern he called network of learning.
Network of learning
The book where Alexander most clearly lays out his ideas about learning is in his 1977
cult classic A Pattern Language: Towns, Buildings, Construction, which he co-wrote
with Sara Ishikawa and Murray Silverstein.
It is a peculiar tome: a choose your own adventure for how to build a modern
medieval city-state.
It is quite dizzying in scope and detail. Together with its companion pieces, The
Timeless Way of Building and The Oregon Experiment, the book spans 1912 pages
and contains 253 architectural "patterns". Each pattern describes "a problem that
occurs over and over again in our environment, and then describes the core of the
solution to that problem, in a way that you can use this solution a million times,
without ever doing it the same way twice". These patterns range from the world
deﬁning to the mundane - from how to beak up the countries in the world into city-

states and incorporate them in a world federation to how to keep weeds from stone
walls and what colors to use to make a room homely (red, yellow, and brown!).
The patterns are mostly derived from close observation of places from the past that
not only function well but are full of life - Alexander and his students were practicing a
sort of ethnography of vernacular architecture.
The most fundamental educational pattern in the book (18: NETWORK OF LEARNING)
is derived from Ivan Illich, the catholic priest and historian famous for his critique of
institutionalized care. Illich had purposed turning schools - the epitome of
institutionalized care and its ills - inside out and bringing forth instead learning webs.
Alexander took this idea and turned it into a design pattern. The way he did this is
akin to how an object is created from a class in object-oriented programming: he
created an instance of the Illichian idea, by pasting several page long quotes, and
then he set about modifying it. I will return to how he modiﬁed it. But ﬁrst, let's look at
what Alexander and his collaborators inherited from Illich.
The basic impulse behind the network of learning pattern is an observation. Most of
what we know we have learned not through formal educational structures - like
schools - but simply by living. We soak up the culture that surrounds us; we pick up
things from blogs we read; we watch how-to videos on YouTube; we ﬁgure stuﬀ out by
talking with our friends. Most knowledge, in this manner, reaches us not through a
curriculum but through a decentralized network of connections to other humans and
their artifacts.
By consciously designing infrastructure and services to facilitate the growth of these
connections we can create a society where "living and learning are the same", writes
Alexander. By crafting the right infrastructure we can enable and even unleash
learning.
Instead of "social control through the schools", he quotes Illich, what we want is to
enable "voluntary participation in society through networks which provide access to
all its resources for learning."
The main diﬀerence then between this pattern and the educational pattern used in
modern societies is that learning is voluntary in the network model but mandatory in
the school model. This change from mandatory to voluntary learning changes the
entire design space. Instead of trying to steer learning, you have to ﬁnd ways to
unblock it. You have to, as I have argued elsewhere, enable the learning system.
Now, what is the diﬀerence between Illich and Alexander? It lies in how they purpose
to facilitate voluntary learning. Illich focuses on reference libraries and services that
allow people to access tools, information, mentors, and peers interested in similar
things. Alexander is more interested in designs that encourage people to work in
public, making their knowledge visible to others.
Working in public
The need for children to have access to the world of adults is so obvious that it
goes without saying. The adults transmit their ethos and their way of life to
children through their actions, not through statements. Children learn by doing
and by copying. If the child's education is limited to school and home, and all the
vast undertakings of a modern city are mysterious and inaccessible, it is
impossible for the child to ﬁnd out what it really means to be an adult and

impossible, certainly, for him to copy it by doing.
- Christopher Alexander, PATTERN 57: CHILDREN IN THE CITY
If children cannot navigate their community on their own, they are severely limited in
their capacity to pursue their interests and form the connections that facilitate
learning. Enabling this is largely a question of culture. In post-war Germany, children
were allowed to roam the ruins of bombed-out cities - an experience Werner Herzog
still raves about. In Glasgow, in the mid-twentieth century, children let loose in the
city would coordinate in giant playgroups (sometimes numbering in the thousands!)
going on vampire hunts, trekking Springheeled Jack and the Grey Lady, forming
militias, stalking graveyards - and other such things that are entirely out of bounds
today and which most likely taught them plenty about organization and collective
action.
Not unlikely, quite a few of these children got traumatized or hurt or died in the ruins
and the churchyards. So wanting to reduce the risks for young people navigating the
community is not unreasonable - all things equal.
One hazard in modern cities is cars. To circumvent this problem, Alexander purposes
the construction of special bike lanes suitable for children. These bike lanes would
cross the roads in tunnels or on bridges when necessary, but generally keep away
from cars altogether, instead passing "along and even through those functions and
parts of a town which are normally out of reach: the place where newspapers are
printed, the place where milk arrives from the countryside and is bottled, the pier, the
garage where people make doors and windows, the alley behind restaurant row, the
cemetery."
In this way, the city can be, yet again, opened up to children.
In every domain, we want to make sure that activities are as mixed as possible so
knowledge can spill across. Talking about universities, for example, Alexander argues,
we need to keep them from forming secluded enclaves. We want the university, like
all work, to spread out through the city and operate openly (PATTERN 43: UNIVERSITY
AS MARKETPLACE) - scholars congregating in cafés and public libraries, perhaps, or in
labs in residential areas, where children playing in the park can observe the
researchers breeding mice or doing brain scans or splitting atoms, just like they can
observe garbage collectors and policemen.
There are encouraging signs of this ethos of working in public spreading online. Live
streaming is growing across all knowledge domains. Communities of practice are
forming on Discords - where people share learning resources, work on projects
together in public, and do community calls with experts. There is also an increasing
number of researchers that keep open notebooks where you can glimpse into their
ongoing work (see here, here, here, and here). Some independent researchers even
use hanging out in public forums as their main research strategy.
A few weeks ago, talking to José Ricón, an independent researcher mainly focused on
longevity studies at the moment, I asked him how a random Hungarian teenager can
get involved in anti-aging research. "Well", said José, "they can just google it."
(Ricón's Longevity FAQ is also a good place to start.) After having read the popular
sources, you go to the research papers they cite (which you can unlock with Sci-Hub).
You read those and look up the researchers that are active on Twitter - where you can
see how they debate, search through their past dialogues, and ask questions.

Since many important seminars are held online, a random Hungarian teenager can
easily, by lurking in the community, sign up and join the seminars to listen to the
latest ﬁndings and the gossip and locate the open questions. This was how Ricón
himself got involved in the area. There are of course still obstacles to be overcome -
how to get hands-on experience breeding mice? how to access a PCR machine? - but
we are seeing teenagers move into the space. There is a pathway.
How can we bring this openness into the oﬄine world?
Alexander purposes a provision for people that build street-facing home workshops
(PATTERN 157: HOME WORKSHOP):
[This pattern] brings the workshop out of the realm of backyard hobbies and into
the public domain. The people working there have a view of the street; they are
exposed to the people passing by. And the people passing learn something about
the nature of the community. The children especially are enlivened by this contact.
And according to the nature of the work, the public connection takes the form of a
shopfront, a driveway for loading and unloading materials, a workbench in the
open, a small meeting room . . .
University as marketplace
However, not everything can be learned through immersion. Sometimes we can
accelerate our growth by submitting to a master, or teacher, who disciplines us to
practice - a violin teacher demanding we run through scales for hours; a combat
simulation during military training; a math professor doing a problem on the
blackboard to tease out the mental operations we need to internalize.
(This is best seen as playing second ﬁddle to what we discussed above: it is the
doings in the real world that gives structure and motivation to our learning. But
deliberate instruction can unblock us in our work.)
To accelerate learning in society, we, therefore, need to make it easy, and aﬀordable,
for people to get access to classes, mentors, and teachers that can give them support.
And we need to provide a large range of diﬀerent support structures - since people
diﬀer in their needs and aims.
One metaphor that Christopher Alexander uses when describing the infrastructure
needed for this is a university as marketplace (PATTERN 43). By this, he does not
mean, primarily, a market in the economic sense. He means the actual, physical
marketplace with its stalls and its small vendors, its smells and shouts, the alleys that
meander from shop to shop, and the bustle of it that bleeds out into the surrounding
city. The learner should navigate this market, looking for the support structures that
will help her accelerate toward her goals.
This was how the early universities were organized. Writes Alexander:
The original universities in the middle ages were simply collections of teachers
who attracted students because they had something to oﬀer. They were
marketplaces of ideas, located all over the town, where people could shop around
for the kinds of ideas and learning which made sense to them.
If we want to help learners access environments where they can work deliberately on
their skills, we need to nurture these types of decentralized exchanges - oﬄine, as
well as online. We should have teachers walking the city (PATTERN 85: SHOPFRONT

SCHOOLS) and apprenticeships (PATTERN 83: MASTERS AND APPRENTICES). We need
to shoulder the work of connecting people with each other and the resources they
need. To use the Illichian terminology: we need to take on the roles of network
administrators, pedagogies, and skill models:
While network administrators would concentrate primarily on the building and
maintenance of roads providing access to resources, the pedagogue would help
the student to ﬁnd the path which for him could lead fastest to his goal. If a
student wants to learn spoken Cantonese from a Chinese neighbor [a skill model],
the pedagogue would be available to judge their proﬁciency, and to help them
select the textbook and methods most suitable to their talents, character, and the
time available for study. He can counsel the would-be airplane mechanic on
ﬁnding the best places for apprenticeship. He can recommend books to somebody
who wants to ﬁnd challenging peers to discuss African history. [Italics added.]
When connected, people can meet in each other's homes, in libraries with special
equipment, or in simple shopfront schools, which can be kept much cheaper than big
mass institutions since there is no administrative overhead and special building
requirements. In this way, we can create a rich and decentralized ecosystem of
learning services, that help people in their diverse aims.
And now we almost have an education - the only thing lacking is emotional support
and logistics.
Second homes for children
...in a society where most children are in the care of single adults or couples, the
mothers and fathers must be able to have their children looked after while they
work or when they want to meet their friends. This is ... the adult's view of the
situation. But the fact is that the children themselves have unsatisﬁed needs
which are equally pressing. They need access to other adults beyond their
parents, and access to other children; and the situations in which they meet these
other adults and other children need to be highly complex, subtle, full of the same
complexities and intensities as family life - not merely "schools" and
"kindergarten" and "playgrounds."
- Christopher Alexander, PATTERN 86: CHILDREN'S HOME
To meet these needs, Alexander purposes we set oﬀ large, rambling homes in each
neighborhood where children can congregate. There should be a core staﬀ of two or
three adults; but more than an institution, it should be a home, where at least one of
the adults live. It does not open or close. It simply is.
And unlike schools, it is not to be fenced oﬀ from the surrounding world. It is to be
built so that people walking by pass through it, rather than around it, to tie it naturally
into the city (PATTERN 101: BUILDING THOROUGHFARE). Ideally, it is co-run with a
café or a small store, or some other community facility, that brings people into its
orbit.
Using this home (as well as their family home) as a base, the children can make
exertions into the city to observe the goings-on - perhaps by biking in a glass tunnel
through a research lab! - or to join a class or an apprenticeship. And then they can
return home - to be safe, to play, to talk, and be heard.
Like all of Alexander's visions, there is something deeply human about this image. He
wanted to shape our cities after us, rather than discipline us to serve our cities. There

is also something utopian about this. But it is a utopia that is within reach - each
individual pattern is simple, and can be implemented from the bottom up. We can,
and do, shape the world we live in, at least locally.
22 march 2022. It is the evening of the ﬁfth day. As I'm writing these ﬁnal words, A
Pattern Language ﬂung open on the ﬂoor, my two daughters are sleeping in the
summer house. Not knowing how to end things, I close the laptop and walk up the
terrace to look in on them. The stars are clear tonight. The four-year-old has dropped
an arm from the bed, the ﬁngertips precisely reaching the ﬂoor. Alexander is no longer
among us, I scribble on a piece of paper I hold against the window. He will never learn
again, never grow.
But we live on in his pattern - I can see them shape my daughters.
In the distance, a tail light shines red across the valley. A low horn. I crumble the
paper and go in to lay down beside them.

[Intro to brain-like-AGI safety] 6. Big
picture of motivation, decision-making,
and RL
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
6.1 Post summary / Table of contents
Part of the  "Intro to brain-like-AGI safety" post series .
Thus far in the series, Post #1 set out some deﬁnitions and motivations (what is "brain-like
AGI safety" and why should we care?), and Posts #2 & #3 split the brain into a Learning
Subsystem (telencephalon and cerebellum) that "learns from scratch" using learning
algorithms, and a Steering Subsystem (hypothalamus and brainstem) that is mostly
genetically-hardwired and executes innate species-speciﬁc instincts and reactions.
Then in Post #4, I talked about the "short-term predictor", a circuit which learns, via
supervised learning, to predict a signal in advance of its arrival, but only by perhaps a
fraction of a second. Post #5 then argued that if we form a closed loop involving both a set
of short-term predictors in the Learning Subsystem and a corresponding set of hardwired
circuits in the Steering Subsystem, we can get a "long-term predictor". I noted that the
"long-term predictor" circuit is closely related to temporal diﬀerence (TD) learning.
Now in this post, we ﬁll in the last ingredients—roughly the "actor" part of actor-critic
reinforcement learning (RL)—to get a whole big picture of motivation and decision-making in
the human brain. (I'm saying "human brain" to be speciﬁc, but it would be a similar story in
any other mammal, and to a lesser extent in any vertebrate.)
The reason I care about motivation and decision-making is that if we eventually build brain-
like AGIs (cf. Post #1), we'll want to build them so that they have some motivations (e.g.
being helpful) and not others (e.g. escaping human control and self-reproducing around the
internet). Much more on that topic in later posts.
Teaser for upcoming posts: The next post (#7) will walk through a concrete example of the
model in this post, where we can watch an innate drive lead to the formation of an explicit
goal, and adoption and execution of a plan to accomplish it. Then starting in Post #8 we'll
switch gears, and from then on you can expect substantially less discussion of neuroscience
and more discussion of AGI safety (with the exception of one more neuroscience post
towards the end).
Unless otherwise mentioned, everything in this post is "things that I believe right now", as
opposed to neuroscience consensus. (Pro tip: there is never a neuroscience consensus.)
Relatedly, I will make minimal eﬀort to connect my hypotheses to others in the literature, but
I'm happy to chat about that in the comments section or by email.
Table of contents:
In Section 6.2, I'll present a big picture of motivation and decision-making in the human
brain, and walk through how it works. The rest of the post will go through diﬀerent
parts of that picture in more detail. If you're in a hurry, I suggest reading to the end of
Section 6.2 and then quitting.
In Section 6.3, I'll talk about the so-called "Thought Generator", comprising (I think) the
dorsolateral prefrontal cortex, sensory cortex, and other areas. (For ML readers familiar

with "actor-critic model-based RL", the Thought Generator is more-or-less a
combination of the "actor" and the "model".) I'll talk about the inputs and outputs of
this module, and brieﬂy sketch how its algorithm relates to neuroanatomy.
In Section 6.4, I'll talk about how values and rewards work in this picture, including the
reward signal that drives learning and decision-making in the Thought Generator.
In Section 6.5, I'll go into a bit more detail about how and why thinking and decision-
making needs to involve not only simultaneous comparisons (i.e., a mechanism for
generating diﬀerent options in parallel and selecting the most promising one), but also
sequential comparisons (i.e., thinking of something, then thinking of something else,
and comparing those two thoughts). For example, you might think: "Hmm, I think I'll go
to the gym. Actually, what if I went to the café instead?"
In Section 6.6, I'll comment on the common misconception that the Learning
Subsystem is the home of ego-syntonic, internalized "deep desires", whereas the
Steering Subsystem is the home of ego-dystonic, externalized "primal urges". I will
advocate more generally against thinking of the two subsystems as two agents in
competition; a better mental model is that the two subsystems are two interconnected
gears in a single machine.
6.2 Big picture
Yes, this is literally a big picture, unless you're reading on your cell phone. You saw a chunk
of it in the previous post (Section 5.4), but now there are a few more pieces.
The big picture—The whole post will revolve around this diagram. Note that the
bracketed neuroanatomy labels in the top two boxes are a bit provisional and
certainly oversimpliﬁed. (dlPFC = dorsolateral prefrontal cortex; mPFC = medial
prefrontal cortex.)
There's a lot here, but don't worry, I'll walk through it bit by bit.

6.2.1 Relation to "two subsystems"
Here's how this diagram ﬁts in with my "two subsystems" perspective, ﬁrst discussed in Post
#3:
Same as above, but the two subsystems are highlighted in diﬀerent colors.
6.2.2 Quick run-through
Before getting bogged down in details later in the post, I'll just talk through the diagram:
1. Thought Generator generates a thought: The Thought Generator settles on a "thought",
out of the high-dimensional space of every thought you can possibly think at that moment.
Note that this space of possibilities, while vast, is constrained by current sensory input, past
sensory input, and everything else in your learned world-model. For example, if you're sitting
at a desk in Boston, it's generally not possible for you to think that you're scuba-diving oﬀ
the coast of Madagascar. But you can make a plan, or whistle a tune, or recall a memory, or
reﬂect on the meaning of life, etc.
2. Thought Assessors distill the thought into a "scorecard": The Thought Assessors are a set
of perhaps hundreds or thousands of "short-term predictor" circuits (Post #4), which I
discussed more speciﬁcally in the previous post (#5). Each predictor is trained to predict a
diﬀerent signal from the Steering Subsystem. From the perspective of a Thought Assessor,
everything in the Thought Generator (not just outputs but also latent variables) is context—
information that they can use to make better predictions. Thus, if I'm thinking the thought
"I'm going to eat candy right now", a thought-assessor can predict "high probability of
tasting something sweet very soon", based purely on the thought—it doesn't need to rely on
either external behavior or sensory inputs, although those can be relevant context too.
3. The "scorecard" solves the interface problem between a learned-from-scratch world
model and genetically-hardwired circuitry: Remember, the current thought and situation is

an insanely complicated object in a high-dimensional learned-from-scratch space of "all
possible thoughts you can think". Yet we need the relatively simple, genetically-hardwired
circuitry of the Steering Subsystem to analyze the current thought, including issuing a
judgment of whether the thought is high-value or low-value (see Section 6.4 below), and
whether the thought calls for cortisol release or goosebumps or pupil-dilation, etc. The
"scorecard" solves that interfacing problem! It distills any possible thought / belief / plan /
etc. into a genetically-standardized form that can be plugged directly into genetically-
hardcoded circuitry.
4. The Steering Subsystem runs some genetically-hardwired algorithm: Its inputs are (1) the
scorecard from the previous step and (2) various other information sources—pain, metabolic
status, etc., all coming from its own brainstem sensory-processing system (see Post #3,
Section 3.2.1). Its outputs include emitting hormones, motor commands, etc., as well as
sending the "ground truth" supervisory signals shown in the diagram.[1] 
5. The Thought Generator keeps or discards thoughts based on whether the Steering
Subsystem likes them: More speciﬁcally, there's a ground-truth value (a.k.a. reward, yes I
know those don't sound synonymous, see Post #5, Section 5.3.1). When the value is very
positive, the current thought gets "strengthened", sticks around, and can start controlling
behavior and summoning follow-up thoughts, whereas when the value is very negative, the
current thought gets immediately discarded, and the Thought Generator summons a new
thought instead.
6. Both the Thought Generator and the Thought Assessor "learn from scratch" over the
course of a lifetime, thanks in part to these supervisory signals from the Steering Subsystem.
Speciﬁcally, the Thought Assessors learn to make better and better predictions of their
"ground truth in hindsight" signal (a form of Supervised Learning—see Post #4), while the
Thought Generator learns to disproportionately generate high-value thoughts. (The Thought
Generator learning-from-scratch process also involves predictive learning of sensory inputs—
Post #4, Section 4.7.)
6.3 The "Thought Generator"
6.3.1 Overview
Go back to the big-picture diagram at the top. At the top-left, we ﬁnd the Thought Generator.
In terms of actor-critic model-based RL, the Thought Generator is roughly a combination of
"actor" + "model", but not "critic". ("Critic" was discussed in the previous post, and more on
it below.)
At our somewhat-oversimpliﬁed level of analysis, we can think of the "thoughts" generated
by the Thought Generator as a combination of constraints (from predictive learning of
sensory inputs) and choices (guided by reinforcement learning). In more detail:
Constraints on the Thought Generator  come from sensory input information, and
ultimately from predictive learning of sensory inputs (Post #4, Section 4.7). For
example, I cannot think the thought: There is a cat on my desk and I'm looking at it
right now. There is no such cat, regrettably, and I can't just will myself to see
something that obviously isn't there. I can imagine seeing it, but that's not the same
thought.
But within those constraints, there's more than one possible thought my brain can
think at any given time. It can call up a memory, it can ponder the meaning of life, it
can zone out, it can issue a command to stand up, etc. I claim that these "choices" are
decided by a reinforcement learning (RL) system. This RL system is one of the main
topics of this post.

6.3.2 Thought Generator inputs
The Thought Generator has a number of inputs, including sensory inputs and
hyperparameter-shifting neuromodulators. But the main one of interest for this post is
ground-truth value, a.k.a. reward. I'll talk about that in more detail later, but we can think of
it as an estimate of whether a thought is good or bad, operationalized as "worth sticking with
and pursuing" versus "deserving to be discarded so we can re-roll for a new thought". This
signal is important both for learning to think better thoughts in the future, and for thinking
good thoughts right now:
6.3.3 Thought Generator outputs
There are meanwhile a lot of signals going out of the Thought Generator. Some are what we
intuitively think of as "outputs"—e.g., skeletal motor commands. Other outgoing signals are,
well, a bit funny...
Recall the idea of "context" from Section 4.3 of Post #4: The Thought Assessors are short-
term predictors, and a short-term predictor can in principle grab any signal in the brain and
leverage it to improve its ability to predict its target signal. So if the Thought Generator has a
world-model, then somewhere in the world-model is a conﬁguration of latent variable
activations that encode the concept "baby kittens shivering in the cold rain". We wouldn't
normally think of those as "output signals"—I just said in the last sentence that they're latent
variables! But as it happens, the "will lead to crying" Thought Assessor has grabbed a copy
of those latent variables to use as context signals, and gradually learned through experience
that these particular signals are strong predictors of me crying.
Now, as an adult, these "baby kittens in the cold rain" neurons in my Thought Generator are
living a double-life:
They are latent variables in my world-model—i.e., they and their web of connections
will help me parse an image of baby kittens in the rain, if I see one, and to reason
about what would happen to them, etc.
Activating these neurons, e.g. via imagination, is a way for me to call up tears on
command.

The Thought Generator (top left) has two types of outputs: the "traditional"
outputs associated with voluntary behavior (green arrows) and the "funny"
outputs wherein even latent variables in the model can directly impact
involuntary behaviors (blue arrows).
6.3.4 Thought Generator neuroanatomy sketch
AUTHOR'S NOTE: When I ﬁrst published this blog post, this section contained a discussion
and diagrams of cortico-basal ganglia-thalamo-cortical loops, but it was very speculative and
turned out to be wrong in various ways. It's not too relevant for the series anyway, so I'm
deleting it. I'll write a corrected version at some point. Sorry!
Here's the updated dopamine diagram from the previous post:
The "mesolimbic" dopamine signals on the right were discussed in the previous
post (Section 5.5.6). The "mesocortical" dopamine signal on the left is new to this
post. (I think there are even more dopamine signals in the brain, not shown here.
They're oﬀ-topic for this series, but see discussion here.)

There are many more implementation details inside the Thought Generator that I'm not
discussing. However, this bare-bones section is more-or-less suﬃcient for my forthcoming
posts on AGI safety. The gory details of the Thought Generator, like the gory details of almost
everything else in the Learning Subsystem, are mainly helpful for building AGI.
6.4 Values and rewards
6.4.1 The cortex proposes a "value" estimate,
but the Steering Subsystem may choose to
override
There are two "values" in the diagram (it looks like three, but the two red ones are the
same):
Two types of "value" in my model
The blue-circled signal is the value estimate from the corresponding Thought Assessor in the
cortex. The red-circled signal (again, it's one signal drawn twice) is the corresponding
"ground truth" for what the value estimate should have been. (Recall that "ground-truth
value" is a synonym for "reward"; yes I know that sounds wrong, see previous post (Section
5.3.1) for discussion.)
Just like the other "long-term predictors" discussed in the previous post, the Steering
Subsystem can choose between "defer-to-predictor mode" and "override mode". In the
former, it sets the red equal to the blue, as if to say "OK, Thought Assessor, sure, I'll take
your word for it". In the latter, it ignores the Thought Assessor's proposal, and its own
internal circuitry outputs some diﬀerent value.[2]

Why might the Steering Subsystem override the Thought Assessor's value
estimate? Two factors:
First, the Steering Subsystem might be acting on information from other (non-value)
Thought Assessors. For example, in the Dead Sea Salt Experiment (see previous post,
Section 5.5.5), the value estimator says "bad things are going to happen", but
meanwhile the Steering Subsystem is getting an "I'm about to taste salt" prediction in
the context of a state of salt-deprivation. So the Steering Subsystem says to itself
"Whatever is happening now is very promising; the value estimator doesn't know what
it's talking about!"
Second, the Steering Subsystem might be acting on its own information sources,
independent of the Learning Subsystem. In particular, the Steering Subsystem has its
own sensory-processing system (see Post #3, Section 3.2.1), which can sense
biologically-relevant cues like pain status, hunger status, taste inputs, the sight of a
slithering snake, the smell of a potential mate, and so on. All these things and more
can be possible bases for overruling the Thought Assessor, i.e., setting the red-circled
signal to a diﬀerent value than the blue-circled one.
Interestingly (and unlike in textbook RL), in the big picture, the blue-circled signal doesn't
have a special role in the algorithm, as compared to the other Thought Assessors. It's just
one of many inputs to the Steering Subsystem's hardwired algorithm for deciding what to
put into the red-circled signal. The blue-circled signal might be an especially important signal
in practice, weighed more heavily than the others, but ultimately everything is in the same
pot. In fact, my longtime readers will recall that last year I was writing posts that omitted the
blue-circled value signal from the list of Thought Assessors! I now think that was a mistake,
but I retain a bit of that same attitude.
6.5 Decisions involve not only
simultaneous but also sequential
comparisons of value
Here's a "simultaneous" model of decision-making, as described by The Hungry Brain by
Stephan Guyenet in the context of studies on lamprey ﬁsh:
Each region of the pallium [= lamprey equivalent of cortex] sends a connection to a
particular region of the striatum, which (via other parts of the basal ganglia) returns a
connection back to the same starting location in the pallium. This means that each
region of the pallium is reciprocally connected with the striatum via a speciﬁc loop that
regulates a particular action.... For example, there's a loop for tracking prey, a loop for
ﬂeeing predators, a loop for anchoring to a rock, and so on. Each region of the pallium is
constantly whispering to the striatum to let it trigger its behavior, and the striatum
always says "no!" by default. In the appropriate situation, the region's whisper becomes
a shout, and the striatum allows it to use the muscles to execute its action. 
I endorse this as part of my model of decision-making, but only part of it. Speciﬁcally, this is
one of the things that's happening when the Thought Generator generates a thought.
Indeed, my diagram in Section 6.3.4 above takes obvious inspiration from the model above.
Diﬀerent simultaneous possibilities are being compared.
The other part of my model is comparisons of sequential thoughts. You think a thought, and
then you think a diﬀerent thought (possibly very diﬀerent, or possibly a reﬁnement of the
ﬁrst thought), and the two are implicitly compared (by the Steering Subsystem picking a
ground-truth value based on the temporal dynamics of Thought Assessors jumping up and
down, for example), and if the second thought is worse, it gets weakened such that a new
thought can replace it (and the new thought might be the ﬁrst thought re-establishing itself).

I could cite experiments for the sequential-comparison aspect of decision-making (e.g. Figure
5 of this paper, which is arguing the same point as I am), but do I really need to?
Introspectively, it's obvious! You think: "Hmm, I think I'll go to the gym. Actually, what if I
went to the café instead?" You're imagining one thing, and then another thing.
And I don't think this is is a humans-vs-lampreys thing. My hunch is that comparisons of
sequential thoughts is universal in vertebrates. As an illustration of what I mean:
6.5.1 Made-up example of what comparison-of-
sequential-thoughts might look like in a simpler
animal
Imagine a simple, ancient, little ﬁsh swimming along, navigating to the cave where it lives. It
gets to a fork in the road, ummm, "fork in the kelp forest"? Its current navigation plan
involves continuing left to its cave, but it also has the option of turning right to go to the reef,
where it often forages.
Seeing this path to the right, I claim that its navigation algorithm reﬂexively loads up a plan:
"I'm will turn right and go to the reef." Immediately, this new plan is evaluated and
compared to the old plan. If the new plan seems worse than the old plan, then the new
thought gets shut down, and the old thought ("I'm going to my cave") promptly
reestablishes itself. The ﬁsh continues to its cave, as originally planned, without skipping a
beat. Whereas if instead the new plan seems better than the old plan, then the new plan
gets strengthened, sticks around, and orchestrates motor commands. And thus the ﬁsh turns
to the right and goes to the reef instead.
(In reality, I don't know much about little ancient ﬁsh, but rats at a fork in the road maze are
known to imagine both possible navigation plans in succession, based on measurements of
hippocampus neurons—ref.)
6.5.2 Comparison-of-sequential-thoughts: why
it's necessary
In my view, thoughts are complicated. To think the thought "I will go to the café", you're not
just activating some tiny cluster of dedicated go-to-the-café neurons. Instead, it's a
distributed pattern involving practically every part of the cortex. You can't simultaneously
think "I will go to the café" and "I will go to the gym", because they would involve diﬀerent
activity patterns of the same pools of neurons. They would cross-talk. Thus, the only
possibility is thinking the thoughts in sequence.
As a concrete example of what I have in mind, think of how a Hopﬁeld network can't recall
twelve diﬀerent memories simultaneously. It has multiple stable states, but you can only
explore them sequentially, one after the other. Or think about grid cells and place cells, etc.
6.5.3 Comparison-of-sequential-thoughts: how it
might have evolved
From an evolutionary perspective, I imagine that comparison-of-sequential-thoughts is a
distant descendent of a very simple mechanism akin to the run-and-tumble mechanism in
swimming bacteria.

In the run-and-tumble mechanism, a bacterium swims in a straight line ("runs"), and
periodically changes to a new random direction ("tumbles"). But the trick is: when the
bacterium's situation / environment is getting better, it tumbles less frequently, and when
it's getting worse, it tumbles more frequently. Thus, it winds up moving in a good direction
(on average, over time).
Starting with a simple mechanism like that, one can imagine adding progressively more bells
and whistles. The palette of behavioral options can get more and more complex, eventually
culminating in "every thought you can possibly think". The methods of evaluating whether
the current plan is good or bad can get faster and more accurate, eventually involving
learning-algorithm-based predictors as in the previous post. The new behavioral options to
tumble into can be picked via clever learning algorithms, rather than randomly. Thus, it
seems to me that there's a smooth path all the way from something-akin-to-run-and-tumble
to the intricate, ﬁnely-tuned, human brain system that I'm talking about in this series. (Other
musings on run-and-tumble versus human motivation: 1, 2.)
6.6 Common misconceptions
6.6.1 The distinction between internalized ego-
syntonic desires and externalized ego-dystonic
urges is unrelated to Learning Subsystem vs.
Steering Subsystem
(See also: my post  (Brainstem, Neocortex) ≠ (Base Motivations, Honorable Motivations) .)
Many people (including me) have a strong intuitive distinction between ego-syntonic drives
that are "part of us" or "what we want", versus ego-dystonic drives that feel like urges which
intrude upon us from the outside.
For example, a food snob might say "I love ﬁne chocolate", while a dieter might say "I have
an urge to eat ﬁne chocolate".
6.6.1.1 The explanation I like
I would claim that these two people are basically describing the same feeling, with
essentially the same neuroanatomical locations and essentially the same relation to low-level
brain algorithms. But the food snob is owning that feeling, and the dieter is externalizing that
feeling.
These two diﬀerent self-concepts go hand-in-hand with two diﬀerent "higher-order
preferences": the food snob wants to want to eat ﬁne chocolate while the dieter wants to not
want to eat ﬁne chocolate.
This leads us to a straightforward psychological explanation for why the food snob and dieter
conceptualize their feelings diﬀerently:

The food snob ﬁnds it appealing to think of "the desire I feel for ﬁne chocolate" as "part
of who I am". So he does.
The dieter ﬁnds it aversive to think of "the desire I feel for ﬁne chocolate" as "part of
who I am". So he doesn't.
6.6.1.2 The explanation I don't like 
Many people (including Jeﬀ Hawkins, see Post #3) notice the distinction described above,
and separately, they endorse the idea (as I do) that the brain has a Learning Subsystem and
Steering Subsystem (again see Post [Intro to brain-like-AGI safety] 6. Big picture of
motivation, decision-making, and RL 
 
Most people I talk to, including me, have
separate concepts in our learned world-
models for "me" and "my urges". I claim
that these concepts did NOT come out of
veridical introspective access to our own
neuroanatomy. And in particular, they
do not correspond respectively to the
Learning & Steering Subsystems.
I think this model is wrong. At the very least, if you want to endorse this model, then you
need to reject approximately everything I've written in this and my previous four posts.
In my story, if you're trying to abstain from chocolate, but also feel an urge to eat chocolate,
then:

You have an urge to eat chocolate because the Steering Subsystem approves of the
thought "I am going to eat chocolate right now"; AND
You're trying to abstain from chocolate because the Steering Subsystem approves of
the thought "I am abstaining from chocolate".
(Why would the Steering Subsystem approve of the latter? It depends on the individual, but
it's probably a safe bet that social instincts are involved. I'll talk more about social instincts
in Post #13. If you want an example with less complicated baggage, imagine a lactose-
intolerant person trying to resist the urge to eat yummy ice cream right now, because it will
make them feel really sick later on. The Steering Subsystem likes plans that result in not
feeling sick, and also likes plans that result in eating yummy ice cream.)
6.6.2 The Learning Subsystem and Steering
Subsystem are not two agents
Relatedly, another frequent error is treating either the Learning Subsystem or Steering
Subsystem by itself as a kind of independent agent. This is wrong on both sides:
The Learning Subsystem cannot think any thoughts unless the Steering Subsystem has
endorsed those thoughts as being worthy of being thunk.
Meanwhile, the Steering Subsystem does not understand the world, or itself. It has no
explicit goals for the future. It's just a relatively simple, hardcoded input-output
machine.
As an example, the following is entirely possible:
1. The Learning Subsystem generates the thought "I am going to surgically alter my own
Steering Subsystem".
2. The Thought Assessors distill that thought down to the "scorecard".
3. The Steering Subsystem gets the scorecard and runs it through its hardcoded
heuristics, and the result is: "Very good thought, go right ahead and do it!"
Why not, right? I'll talk more about that example in later posts.
If you just read the above example, and you're thinking to yourself "Ah! This is a case where
the Learning Subsystem has outwitted the Steering Subsystem", then you're still not getting
it.
(Maybe instead try imagining the Learning Subsystem & Steering Subsystem as two
interconnected gears in a single machine.)
1. ^
 As in the previous post, the term "ground truth" here is a bit misleading, because
sometimes the Steering Subsystem will just defer to the Thought Assessors.
2. ^
As in the previous post, I don't really believe there is a pure dichotomy between "defer-
to-predictor mode" and "override mode". In reality, I'd bet that the Steering Subsystem
can partly-but-not-entirely defer to the Thought Assessor, e.g. by taking a weighted
average between the Thought Assessor and some other independent calculation.

Food manufacturers are out to get
you
[epistemic status: n=1, but the theory seems ﬁne.  Mixture of theory and personal
narrative.  Trigger warnings:  body image issues, money, and info-dumping without a
great narrative ﬂow.  I am not a good rationalist yet, haven't even ﬁnished reading the
sequences yet, and this is my ﬁrst long post on here.]
The obesity epidemic emerges from processed food manufacturers competing to get
you to eat more of their product by any means necessary.  They try to make their
product be digested as quickly as possible so you will be hungry again sooner (largely
by pulverizing biomass before forming it into products).  They add salt/sugar/fat to
make their product so delicious that it is literally addictive.  Both of these make you
eat more of the product and enrich the food manufacturer, with a side eﬀect of
making you fat.
After years of failed attempts at calorie restriction without categorically changing
which foods I eat, my solution was to boycott all these manufacturers completely.
As of six weeks ago, and continuing indeﬁnitely, I only eat intact solid biomass, and I
only stock my kitchen with what I am allowed to eat.  I do not eat any liquids, particles
<5mm, or derivatives thereof.  The emphasis is on a wide variety of fruits, vegetables,
and meats.  The intactness of the ﬁbrous cellular matrix of the plant requires me to
chew more and slows digestion.  Slowing digestion has two beneﬁts: staying full
longer, and preventing insulin spikes which cause leptin-resistance in the brain which
makes you hungrier.  Leptin is the hormone that your fat cells make to tell the brain
that you're fat so you should reduce appetite and increase energy expenditure.  Leptin
is a really good hormone to have if you're trying to lose weight.  High insulin blocks
the brain's ability to see leptin.
I can't stress enough how important the intactness of plants is.  If the carbs are
located inside intact cellulosic cell walls, which are clustered together as an entire
intact tissue of cell walls, it takes longer for the enzymes and bacteria in your gut to
penetrate those cell walls and liberate the carbs.  This keeps insulin low, which makes
the brain more sensitive to leptin.  Intactness also forces you to slow down and chew
more, which allows time for other lagging signals of satiety to reach your brain.   If
you've done much chemistry lab, you've seen that the smaller the particle size, the
faster a substance can dissolve, ceteris paribus, because smaller particles have more
surface area per unit volume.  I don't know of any other diets that explicitly prescribe
larger particle size, but it seems very important.
Cooking can weaken or rupture cell walls, reduce the amount of chewing required, and
accelerate digestion in general.  I still cook half my plants, but if necessary I will eat
more raw vegetables when I want to run a deeper caloric deﬁcit.  I avoid eating mush
that has >8g carbs per gram of ﬁber (e.g., bananas, white rice, white potatoes).  This
seems like a better rule than the arbitrary chronological cutoﬀ that paleo uses to ban
grains and legumes.  I allow grains and legumes iﬀ they follow the above rule.
 Quinoa, lentils, and nuts are ﬁne but they're not a large part of my diet.
Another diﬀerence from paleo is that I drink unsweetened keﬁr as an unprincipled
exception to the above rules.  It seems to cause a better ratio of satiety:calories than

any other beverage, for reasons that are not well understood by me.  To facilitate long
term compliance I also allow myself one whole wheat hamburger bun per day and a
trivial number of calories from condiments/spices that break the ﬁrst rule.
In spite of all the ﬂaws in the evolutionary just so story about paleo, it seems to
perform well for weight loss in empirical tests.  This diet I invented is sorta like paleo
except I have a better rationale for which plants to exclude, based on a diﬀerent story
of how the ﬁber of an intact cellular matrix slows down digestion and enhances satiety
signaling.   Negative studies of ﬁber were just using supplemental ﬁber isolate, which
makes them irrelevant to my proposed mechanism.  The only appropriate tests in the
literature so far are RCTs of diets that eat lots of intact plants and avoid processed
food, e.g. Paleo and Mediterranean.  These studies are very positive.
Eating out is easier than expected, since most sit-down restaurants will let me order
oﬀ-menu from ingredients they have on hand, and they always have
fruits/vegetables/meats.  One can order a fruit bowl and an a la carte chicken breast
for example.
Vegetables may be better for me than fruits due to avoiding fructose, but I don't really
worry about eating a pound of mixed fruit every day.  Actually the way I got started
with this was reading Walter Isaacson's biography about Steve Jobs, and his obsession
with fruit.  I started buying ~9oz containers of fresh prepped mixed fruit and eating
that for breakfast instead of cereal, and I noticed a huge improvement in my mood
and energy levels.  Fruit is on the menu for 1-2 meals every day.  It may be less
convenient for someone without the budget to spend $20/day on produce -- they'd
have to do a lot of prep work or sacriﬁce variety.  Compliance has been way easier
than I thought it would be, partly because I'm paying for convenience.   For motivation
to comply, seeing the way girls swoon over Paxton Hall-Yoshida in the ﬁrst two
episodes of Never Have I Ever gave me a burning desire to do whatever it takes to get
a six-pack.  Paxton has a hot body and not much else but a conﬁdent personality that
derives largely from the way people treat him because of his hot body.
On the role of willpower, I envision the exercise of willpower as choosing a a point
within a bell curve of possibilities of what similarly situated people would do in my
shoes.  The farther from the center of the bell curve, the greater the eﬀort required.
 The location of the bell curve is determined by your environment, endocrinology,
ﬁtness, and genetics.  I can't rely on having enough willpower to consistently select a
z>2 outcome,  so I try to shape my external environment and endocrine environment
so that I don't need to rely overmuch on willpower.  A very important part of this is to
only stock my kitchen with the allowed foods, and to have a deﬁnite plan of what I can
order before I eat out.  But I still use a nonzero amount of willpower to limit intake.
 One phrase that really stuck with me from "Gut Reactions" by Simon Quellen Field
[trigger warning: fat-shaming and nitty gritty details of biochemistry/endocrinology]
was that if you ﬁnd yourself passing over some foods and searching for tastier foods,
then you're not really hungry, you're just addicted to the taste.  I force myself to eat a
lot of vegetables even though they're not the most delicious thing in my kitchen
(although garlic salt makes them ok).  It's enough of a habit now that it doesn't
require much eﬀort.
For exercise, I have two Beeminder goals.  The strength goal is
Pushups+Rows+Situps+2xPullups+2xDips > 75/day.   The cardio goal is ﬂoors of
stairs + 50x horizontal miles on foot + 16.66x horizontal miles on bike> 85/day.  Any
elevation gain is equated to ﬂoors of stairs.  Both of these feel pretty easy and
unambitious since I have a home gym 6 feet from my computer, but they're enough to

see a very signiﬁcant impact.   For weight loss, I have a bodyfat% goal with a graph-
editor step function that drops ~2% on the 15th of every month until I reach 10%.  I
measure bodyfat% with bodpod on the 14th of every month.
This setup has worked extremely well for me.  In fact it's working so well that I bet my
local rationalist friends $1000 for charity that I could get my bf% down from 20% on
Feb 14 to 10% on July 14.  On July 14 I will publish whether or not I won the bet.  Long
term I aim to stay at 10-12% indeﬁnitely and stay on this diet indeﬁnitely to optimize
my SMV and health.

Scientiﬁc Wrestling: Beyond Passive
Hypothesis-Testing
Epistemic status:  exploratory
In 1907, Ross Harrison grew a frog nerve in vitro, inventing modern tissue culture. He
extracted cells from the relevant part of a frog embryo, put them in a contraption from
bacteriology called a hanging drop, and observed as the cells grew and diﬀerentiated into
nerves.
Drawings from Harrison's paper, showing the hanging drop and the growth of
nerves.
Based on his paper and later comments, Harrison only wanted to settle a controversy about
the cellular nature of nerves. Where others had to cut frogs at diﬀerent stages of
development and interpret their dead bodies, Harrison could just watch the nerve grow. But
he did more than that — he shattered many assumptions about the conditions and
necessities of life, by making cells live and grow outside the body.
When I read about it, that's what I found exciting: Harrison had intervened in the world, in
doing so uncovering deep principles of cell biology. This was no passive measure to
conﬁrm a hypothesis, but a wrestling with nature to make her reveal her secrets.
I started to see this everywhere: chemical properties revealed through synthesis, natural
selection probed by Darwin's experiments, complexity classes characterized by artiﬁcial
problems... I realized that many of the experiments and ideas that excited me included
wrestling with nature to discover its secrets. And understanding the possibilities and limits of
this "scientiﬁc wrestling" will deﬁnitely prove a powerful tool in any epistemic toolkit, even
more so in a ﬁeld like Alignment where passive observation is hopeless.
So in this post, I discuss ﬁve ways I've noticed in which such wrestling can create knowledge,
with examples from chemistry, cell biology, complexity theory, evolutionary biology, network
architecture, and more.

Thanks to Flora for a great discussion on synthesis and for helping me clarify the point of this
post.
Studying hard-to-observe processes
through instantiation
Many fundamental processes we want to understand are just hard to observe: what's
happening inside the body (because of opacity), genes (because of scale), natural selection
(because of temporal and spatial scale), chemical bonds (because of microscopic scale)...
Yet we can reveal these processes by instantiating them in settings where they
can be observed.
The already mentioned invention of tissue culture by Ross Harrison served exactly this
purpose for internal body processes: to make them happen within a transparent glass
enclosure. Follow up work on maintaining cultures alive for longer and longer periods further
helped with making the underlying processes move at the speed and scale adapted for
observation.
(Culturing Life: How Cells Became Technologies, Hannah Landecker, 2007)
Herein lies the important shift from in vivo to in vitro. In observing the living subject over
time and the living process as it happened, the assumptions embedded in histological
practice were confounded. One did not have to kill the animal or the tissue to observe
the development course or experimental alteration of internal structures and processes.
Internal processes could be placed on the exterior, and watched, given the appropriate
technical substitution of particular functions of the body: asepsis, ﬂuid, structural
support, warmth. In substituting a glass enclosure and a drop of lymph for the body,
something opaque was replaced by something transparent, and the enclosure did not
have to be opened or halted in order to observe what was going on inside it. In not just
taking the animal body apart, but leaving it apart, cellular life that was autonomous,
external, and dynamic came into being for biology.
Gregory Mendel's experiments on pea plants provide another great example. By creating a
bare-bones setting and carefully controlling the breeding, Mendel extracted fundamental
principles about inheritance and genetics, without ever observing genes.
A few other examples:
Darwin leveraged knowledge and experiments about breeding when inventing
evolutionary theory, because he understood that selection by humans was an
instantiation of a more general selection process.
Chemistry abounds with examples, where reactions were and are used for revealing
underlying aﬃnities and bonding mechanisms that couldn't (and sometimes still can't)
be observed directly.
To summarize, hard-to-observe phenomena can be revealed to the scientiﬁc eye by
creating controlled and scaled versions of them.
This in turn points to the main risk: that the artiﬁcial instantiation comes apart with the
natural process. It shouldn't be a problem for my examples above,  because they either
directly interact with the natural phenomenon (chemistry, inheritance, tissue culture) or
instantiate the same general mechanism (breeding). Yet it's good to remember that these
are just proxies, especially when interpreting results.
Revealing principles through invention

Not all created proxies look as natural as tissue cultures or inheritance experiments —
sometimes their artiﬁciality itself can be productive. The idea is to make something up
that directly aims for an important aspect of the phenomenon at hand, without
having to deal with the constraints nature puts on its creations.
My ﬁrst example deﬁnitely looks weird, but I think it works: artiﬁcial-looking problems in
complexity theory. You see, my ﬁrst impression when I learned about complexity theory
years ago was that people got obsessed with irrelevant toy problems, like computing the
parity of a binary string or coloring a graph using 3 colors, but only in a certain sequence,
and never in some god-forbidden pairs. Some of these looked everything but natural, in the
sense that I didn't expect any programmer to ever have to solve them. How did working on
that help with understanding computation and its cost?
What I missed was the paradigm of complexity classes. These are sets of problems which
satisfy a certain resource constraint: the problems solvable in polynomial time for example,
or the ones taking logarithmic space. And these classes are characterized by their complete
problems — problems of the class to which every other problem can be reduced (under the
right constraints).
So if a weird problem is complete for some class, studying it actually tells us something
about the whole class (for example, proving that a single NP-complete problem can't be
solved in polynomial time would settle P vs NP). And sometimes artiﬁcial problems even
create a new complexity class, if they end up complete for an unknown and interesting set of
problems.
Another example, this time more experimental, is synthesis in chemistry. Many syntheses
don't aim to recreate natural substances, but instead to make something that was never
observed, and might never have existed, like molecular hydrogen within
buckminsterfullerene.

Molecular hydrogen inside a "bucky ball". Because we can.
Why? Many reasons: the fun of creation, practical applications, testing a theory, or helping
make further molecules. But in general, synthesis conﬁrms rather than invalidates; it shows
what can be done, conﬁrms the design, and reveals what is possible at the extreme.
To summarize, making up new things (from computational problems to molecules)
can reveal deep principles of the underlying phenomena by pushing it to its limits
or by isolating one aspect of it.
How can it fail? Both examples show the same risk: to focus too much on the creation itself
instead of treating it like a tool to investigate the original phenomenon.
Revealing properties through change
What if we're interested in understanding a concrete natural object or phenomenon, like a
molecule or a cell? Then one form of scientiﬁc wrestling consists in altering this
object to learn how it works.

Still in chemistry for example, synthesis can help diﬀerentiate structurally equivalent isomers
— molecules with the same composition and the same structure up to speciﬁc permutations.
The trick is that in some cases, the symmetry can be broken by a synthesis operation (say
replacing an hydrogen atom with a bigger group). Depending on the isomer, the group has
access to diﬀerent replacement sites, which lead to diﬀerent numbers of possible distinct
structures outcomes (technically up to symmetry). So counting the number of diﬀerent
isomers after the synthesis reveals the initial structure.
Illustration of structure inference through synthesis and counting.
Beyond this example, philosopher of chemistry Joachim Schummer argues that chemistry
studies exactly these "changeabiliies" of chemical substances:
(Knowing-Through-Making in Chemistry and Biology, Joachim Schummer, 2021)
However, at its core, chemistry is about chemical properties which are about how one or
more chemical substances can react to form one or more other chemical substances
(Schummer 1998). That is, all chemical properties are chemical reactivities, or more
generally, changeabilities (which is beyond the received philosophy of science). (footnote
Schummer, like every philosopher of chemistry, can't resist a diss on traditional
philosophy of science, that is philosophy of physics).
Cell fusion in tissue culture is a version of this scientiﬁc wrestling in biology. The natural
boundaries and compatibilities of diﬀerent cells were examined by fusing diﬀerent somatic
cells, then cells from diﬀerent individuals, then cells from diﬀerent species. Each success
unveiled new insights about the plasticity of the cell, and the fundamental compatibility
between cells from vastly diﬀerent origins.
To summarize, changing natural objects or phenomena can reveal their underlying
structure and how they can be combined to make other objects.
Paradoxically, this sounds to me like the least likely to fail, because there is not really a proxy
here.

Shattering assumptions through
successful design
Nothing dispels an impossibility as well as creating the impossible. This means that
designing new objects and phenomena can upend long-held assumptions of the
ﬁeld.
As I mentioned in the introduction, this is precisely what Harrison's ﬁrst tissue culture did.
And this speciﬁc ﬁeld of study did it again (Oops), at least twice more!
Harrison's experiment and follow-up work by Alexis Carrel and others destroyed the
early 20th century belief that complex life processes could only happen inside the
body.
Alexis Carrel's work on making an immortal culture and the 40s advances on freezing
and cloning that lead to immortal cell lines like HeLa shifted the assumptions about the
lifespan of cells and other biological material.
Starting in the 60s, the work on cell fusion (including between species) shattered the
assumed fundamental diﬀerences between individuals and species that immune
rejection and sexual infertility implied.
Another example from chemistry is the synthesis of urea from inorganic components, which
invalidated vitalist claims that organic molecules were special.
Lastly, I think of all the attempts to push the boundaries of what was considered possible in
engineering: building bigger monuments, faster CPUs and GPUs, more resistant materials... I
expect that in many cases this sort of pushing the boundaries moved what was considered
possible.
To summarize, building what is considered impossible precipitates a reconsideration
of long-held assumptions and a release from limiting constraints.
The way I expect this particular scientiﬁc wrestling to fail is to keep trying to make what's
actually impossible, which never ends well. This can be addressed by focusing on ideas that
haven't been try to death and should provide new information (for example a new technique
or an unexplored theoretical path). If you want a perpetual motion machine, you better start
with an insight in thermodynamics rather than a random "innovative" design that 2483
people came up with before.
Uncovering constraints through
diﬃculties in design
Sometimes, we end up learning about the world as a consequence of purely practical aims.
The point was to solve a concrete problem, to make things work. Yet what parts of the
solution work and which don't can uncover the underlying constraints of the
problem, which sometimes are fundamental principles.
Tissue culture is at least partially like that: most of its proponents wanted to make cultures
work in order to solve other problems, not to directly probe biological principles (although
Carrel wanted to study biological time and operationalize Bergson's duration, because why
not).
Another set of ﬁelds where I expect this to happen are engineering disciplines before
powerful scientiﬁc theories. Nowadays a lot of engineering is based on mathematical models

and simulation of the underlying physics or biology; but that's a recent innovation, and one
that isn't even applied to all subﬁelds (software engineering for example).
Yet I had trouble ﬁnding good concrete examples of this.
When I read about cathedral builders, it sounds like they learned from previous
buildings and from what happened during the decades long constructions, but guilds of
master builders were notoriously secretive, so I don't know to what extent they did
that.
Looking quickly at the history of material science, it sounds like the initial formalization
started by explaining empirical metallurgical knowledge; but I didn't invest enough
time to be conﬁdent of this.
Barriers for P vs NP in complexity theory, which are theorems about the inability of
speciﬁc proof techniques to separate P and NP, might count. But it's debatable and
debated whether they're fundamental insights into computation or just contingent to
our approaches.
And there's also John Day's Patterns in Network Architecture which attempts to create
a paradigm of network design based on 60 years of building and messing up with
networks; I plan to analyze this more in detail for my study of paradigms, but I haven't
done it yet.
To summarize, solving a concrete, practical problem can be a tool for understanding
the underlying structure of the solution space and the phenomena at hand, by
ﬁnding constraints and their extent.
This approach has two main risks that I can see:
Because it's motivated by solving practical problems, this approach risks stopping
early when things work decently well (one of Day's claims is that the ARPANET
project got things too right too quickly, which resulted in being satisﬁed and not
iterating further on the fundamental design).
If this approach unearths a constraint, it's possible that it doesn't capture an
important underlying property, just the inadequacy of current methods. I
mentioned above the barriers to P vs NP, which don't sound that fundamental to me or
many complexity theorists.
Unexplored avenues
Here is a bunch of references and topics that might be relevant to scientiﬁc wrestling, but
which I haven't explored:
Ian Hacking's Representing and Intervening is supposed to be a classic of philosophy of
science on intervention and active experiments, but his focus sounds more about
physics and realism. So I didn't prioritize reading it.
Physics arguably contains examples of scientiﬁc wrestling, like the ﬁrst nuclear
reaction, the experiments around electricity, and particle accelerators. But knowing the
tendency in philosophy of science to overemphasize physics, I decided to explore
diﬀerent examples.
Another topic that came to mind was economics, but I already had my hands full with
reading on tissue culture and biology.
More out there, creating to understand makes me think of art, but I plan to investigate
the comparative epistemologies of art and science at another time.

