
Best of LessWrong: November 2020
1. Pain is not the unit of Eﬀort
2. When Money Is Abundant, Knowledge Is The Real Wealth
3. Embedded Interactive Predictions on LessWrong
4. Some AI research areas and their relevance to existential safety
5. Luna Lovegood and the Chamber of Secrets - Part 1
6. Where do (did?) stable, cooperative institutions come from?
7. Working in Virtual Reality: A Review
8. the scaling "inconsistency": openAI's new insight
9. Generalized Heat Engine
10. Inner Alignment in Salt-Starved Rats
11. A review of Where Is My Flying Car? by J. Storrs Hall
12. The 300-year journey to the covid vaccine
13. Nuclear war is unlikely to cause human extinction
14. My intellectual inﬂuences
15. Probability vs Likelihood
16. AGI Predictions
17. Beware Experiments Without Evaluation
18. The Pointers Problem: Human Values Are A Function Of Humans' Latent Variables
19. Anatomy of a Gear
20. Gifts Which Money Cannot Buy
21. Being Productive With Chronic Health Conditions
22. Impostor Syndrome as skill/dominance mismatch
23. When Hindsight Isn't 20/20: Incentive Design With Imperfect Credit Allocation
24. It's not economically ineﬃcient for a UBI to reduce recipient's employment
25. Clarifying inner alignment terminology
26. Evading Mind Control
27. Learning Normativity: A Research Agenda
28. Snyder-Beattie, Sandberg, Drexler & Bonsall (2020): The Timing of Evolutionary
Transitions Suggests Intelligent Life Is Rare
29. Commentary on AGI Safety from First Principles
30. Final Version Perfected: An Underused Execution Algorithm
31. Does SGD Produce Deceptive Alignment?
32. Sunzi's《Methods of War》- Introduction
33. Persuasion Tools: AI takeover without AGI or agency?
34. Announcing the Forecasting Innovation Prize
35. How can I bet on short timelines?
36. How to get the beneﬁts of moving without moving (babble)
37. Continuing the takeoﬀs debate
38. Non-Obstruction: A Simple Concept Motivating Corrigibility
39. Open Problems Create Paradigms
40. Writing to think
41. Why philosophy of science?
42. Pain is the unit of Eﬀort
43. A Self-Embedded Probabilistic Model
44. Sunzi's《Methods of War》- War
45. Spend twice as much eﬀort every time you attempt to solve a problem
46. Are the social sciences challenging because of fundamental diﬃculties or
because of imposed ones?

47. Why those who care about catastrophic and existential risk should care about
autonomous weapons
48. Squiggle: An Overview
49. Confucianism in AI Alignment
50. A guide to Iterated Ampliﬁcation & Debate

Pain is not the unit of Eﬀort
This is a linkpost for https://radimentary.wordpress.com/2020/11/24/pain-is-not-the-
unit-of-eﬀort/
(Content warning: self-harm, parts of this post may be actively counterproductive for
readers with certain mental illnesses or idiosyncrasies.)
What doesn't kill you makes you stronger. ~ Kelly Clarkson.
No pain, no gain. ~ Exercise motto.
The more bitterness you swallow, the higher you'll go. ~ Chinese proverb.
I noticed recently that, at least in my social bubble, pain is the unit of eﬀort. In other
words, how hard you are trying is explicitly measured by how much suﬀering you put
yourself through. In this post, I will share some anecdotes of how damaging and
pervasive this belief is, and propose some counterbalancing ideas that might help
rectify this problem.
I. Anecdotes
1. As a child, I spent most of my evenings studying mathematics under some amount
of supervision from my mother. While studying, if I expressed discomfort or fatigue,
my mother would bring me a snack or drink and tell me to stretch or take a break. I
think she took it as a sign that I was trying my best. If on the other hand I was smiling
or joyful for extended periods of time, she took that as a sign that I had eﬀort to spare
and increased the hours I was supposed to study each day. To this day there's a
gremlin on my shoulder that whispers, "If you're happy, you're not trying your best."
2. A close friend who played sports in school reports that training can be harrowing.
He told me that players who fell behind the pack during for daily jogs would be singled
out and publicly humiliated. One time the coach screamed at my friend for falling
behind the asthmatic boy who was alternating between running and using his inhaler.
Another time, my friend internalized "no pain, no gain" to the point of losing his
toenails.
3. In high school and college, I was surrounded by overachievers constantly making
(what seemed to me) incomprehensibly bad life choices. My classmates would sign up
for eight classes per semester when the recommended number is ﬁve, jigsaw
extracurricular activities into their calendar like a dynamic programming knapsack-
solver, and then proceed to have loud public complaining contests about which
libraries are most comfortable to study at past 2am and how many pages they have
left to write for the essay due in three hours. Only later did I learn to ask: what
incentives were they responding to?
4. A while ago I became a connoisseur of Chinese webnovels. Among those written for
a male audience, there is a surprisingly diverse set of character traits represented
among the main characters. Doubtless many are womanizing murderhobos with no
redeeming qualities, but others are classical heroes with big hearts, or sarcastic
antiheroes who actually grow up a little, or ambitious empire-builders with grand plans

to pave the universe with Confucian order, or down-on-their-luck starving artists who
just want to bring happiness to the world through song.
If there is a single common virtue shared by all these protagonists, it is their
superhuman pain tolerance. Protagonists routinely and often voluntarily dunk
themselves in vats of lava, have all their bones broken, shattered, and reforged, get
trapped inside alternate dimensions of freezing cold for millennia (which conveniently
only takes a day in the outside world), and overdose on level-up pills right up to the
brink of death, all in the name of becoming stronger. Oftentimes the deﬁning
diﬀerence between the protagonist and the antagonist is that the antagonist did not
have enough pain tolerance and allowed the (unbearable physical) suﬀering in his life
to drive him mad.
5. I have a close friend who often asks for my perspective on personal problems. A
pattern arose in a couple of our conversations:
alkjash: I feel like you're not actually trying. [Meaning: using all the tools at your
disposal, getting creative, throwing money at the problem to make it go away.]
alkjash's friend: What do you mean I'm not trying? I think I'm trying my best, can't
you tell how hard I'm trying? [Meaning: piling on time, energy, and willpower to
the point of burnout.]
After several of these conversations went nowhere, I learned that asking this friend to
try harder directly translated in his mind to accusing him of low pain tolerance and
asking him to hurt himself more.
II. Antidotes
I often hear on the internet laments like "Why is nobody actually trying?" Once upon a
time, I was honestly and genuinely confused by this question. It seemed to me that
"actually trying" - aiming the full force of your being at the solution of a problem you
care about - is self-evidently motivating and requires zero extra justiﬁcation if you care
about the problem.
I think I ﬁnally understand why so few people are "actually trying." The reason is this
pervasive and damaging belief that pain is the unit of eﬀort. With this belief, the
injunction "actually try" means "put yourself in as much pain as you can handle."
Similarly, "she's trying her best" translates to "she's really hurting right now." Even
worse, people with this belief optimize for the appearance of suﬀering. Answering
emails at midnight and appearing fatigued at meetings are somehow taken to be
more credible signals of eﬀort than actual results. And if you think that's pathological,
wait until you meet someone for whom telling them about opportunities actively hurts
them, because you've just created another knife they feel pressured to cut
themselves with.
I see a mob of people walking up to houses and throwing themselves bodily at the
closed front doors. I walk up to block one man and ask, "Stop it! Why don't you try the
doorknob ﬁrst? Have you rung the doorbell?" The man responds in tears, nursing his
bloody right shoulder, "I'm trying as hard as I can!" With his one good arm, he shoves
me aside and takes a running start to lunge at the door again. Finally, the timber
shatters and the man breaks through. The surrounding mob cheers him on, "Look how
hard he's trying!"

Once you understand that pain is how people deﬁne eﬀort, the answer to the question
"why is nobody actually trying?" becomes astoundingly obvious. I'd like to propose
two beliefs to counterbalance this awful state of aﬀairs.
1. If it hurts, you're probably doing it wrong.
If your wrists ache on the bench press, you're probably using bad form and/or too
much weight. If your feet ache from running, you might need sneakers with better
arch support. If you're consistently sore for days after exercising, you should learn to
stretch properly and check your nutrition.
Such rules are well-established in the setting of physical exercise, but their analogs in
intellectual work seem to be completely lost on people. If reading a math paper is
actively unpleasant, you should ﬁnd a better-written paper or learn some background
material ﬁrst (most likely both). If you study or work late into the night and it disrupts
your Circadian rhythm, you're trading oﬀ long-term productivity and well-being for
low-quality work. That's just bad form.
If it hurts, you're probably doing it wrong.
2. You're not trying your best if you're not happy.
Happiness is really, really instrumentally useful. Being happy gives you more energy,
increases your physical health and lifespan, makes you more creative and risk-
tolerant, and (even if all the previous eﬀects are unreplicated pseudoscience) causes
other people to like you more. Whether you are tackling the Riemann hypothesis,
climate change, or your personal weight loss, one of the ﬁrst steps should be to
acquire as much happiness as you can get your hands on. And the good news is: at
least anecdotally, it is possible to substantially raise your happiness set-point through
jedi mind tricks.
Becoming happy is a fully general problem-solving strategy. And although one can in
principle trade oﬀ happiness for short bursts of productivity, in practice this is never
worth it.
Culturally, we've been led to believe that over-stressed and tired people are the ones
trying their best. It is right and proper to be kind to such people, but let's not go so far
as to support the delusion that they are inputting as much eﬀort as their joyful,
boisterous peers bouncing oﬀ the walls.
You're not trying your best if you're not happy.
[Edit: Antidotes #1 and #2 are not primarily to be interpreted as truth claims, see
Anna Salamon's comment.]

When Money Is Abundant, Knowledge
Is The Real Wealth
First Puzzle Piece
By and large, the President of the United States can order people to do things, and
they will do those things. POTUS is often considered the most powerful person in the
world. And yet, the president cannot order a virus to stop replicating. The president
cannot order GDP to increase. The president cannot order world peace.
Are there orders the president could give which would result in world peace, or
increasing GDP, or the end of a virus? Probably, yes. Any of these could likely even be
done with relatively little opportunity cost. Yet no president in history has known which
orders will eﬃciently achieve these objectives. There are probably some people in the
world who know which orders would eﬃciently increase GDP, but the president cannot
distinguish them from the millions of people who claim to know (and may even believe
it themselves) but are wrong.
Last I heard, Jeﬀ Bezos was the oﬃcial richest man in the world. He can buy basically
anything money can buy. But he can't buy a cure for cancer. Is there some way he
could spend a billion dollars to cure cancer in ﬁve years? Probably, yes. But Jeﬀ Bezos
does not know how to do that. Even if someone somewhere in the world does know
how to turn a billion dollars into a cancer cure in ﬁve years, Jeﬀ Bezos cannot
distinguish that person from the thousands of other people who claim to know (and
may even believe it themselves) but are wrong.
When non-experts cannot distinguish true expertise from noise, money cannot buy
expertise. Knowledge cannot be outsourced; we must understand things ourselves.
Second Puzzle Piece
The Haber process combines one molecule of nitrogen with three molecules of
hydrogen to produce two molecules of ammonia - useful for fertilizer, explosives, etc.
If I feed a few grams of hydrogen and several tons of nitrogen into the Haber process,
I'll get out a few grams of ammonia. No matter how much more nitrogen I pile in - a
thousand tons, a million tons, whatever - I will not get more than a few grams of
ammonia. If the reaction is limited by the amount of hydrogen, then throwing more
nitrogen at it will not make much diﬀerence.
In the language of constraints and slackness: ammonia production is constrained by
hydrogen, and by nitrogen. When nitrogen is abundant, the nitrogen constraint is
slack; adding more nitrogen won't make much diﬀerence. Conversely, since hydrogen
is scarce, the hydrogen constraint is taut; adding more hydrogen will make a
diﬀerence. Hydrogen is the bottleneck.
Likewise in economic production: if a medieval book-maker requires 12 sheep skins
and 30 days' work from a transcriptionist to produce a book, and the book-maker has
thousands of transcriptionist-hours available but only 12 sheep, then he can only

make one book. Throwing more transcriptionists at the book-maker will not increase
the number of books produced; sheep are the bottleneck.
When some inputs become more or less abundant, bottlenecks change. If our book-
maker suddenly acquires tens of thousands of sheep skins, then transcriptionists may
become the bottleneck to book-production. In general, when one resource becomes
abundant, other resources become bottlenecks.
Putting The Pieces Together
If I don't know how to eﬃciently turn power into a GDP increase, or money into a cure
for cancer, then throwing more power/money at the problem will not make much
diﬀerence.
King Louis XV of France was one of the richest and most powerful people in the world.
He died of smallpox in 1774, the same year that a dairy farmer successfully
immunized his wife and children with cowpox. All that money and power could not buy
the knowledge of a dairy farmer - the knowledge that cowpox could safely immunize
against smallpox. There were thousands of humoral experts, faith healers, eastern
spiritualists, and so forth who would claim to oﬀer some protection against smallpox,
and King Louis XV could not distinguish the real solution.
As one resource becomes abundant, other resources become bottlenecks. When
wealth and power become abundant, anything wealth and power cannot buy become
bottlenecks - including knowledge and expertise.
After a certain point, wealth and power cease to be the taut constraints on one's
action space. They just don't matter that much. Sure, giant yachts are great for social
status, and our lizard-brains love politics. The modern economy is happy to provide
outlets for disposing of large amounts of wealth and power. But personally, I don't
care that much about giant yachts. I want a cure for aging. I want weekend trips to the
moon. I want ﬂying cars and an indestructible body and tiny genetically-engineered
dragons. Money and power can't eﬃciently buy that; the bottleneck is knowledge.
Based on my own experience and the experience of others I know, I think knowledge
starts to become taut rather quickly - I'd say at an annual income level in the low
hundred thousands. With that much income, if I knew exactly the experiments or
studies to perform to discover a cure for cancer, I could probably make them happen.
(Getting regulatory approval is another matter, but I think that would largely handle
itself if people knew the solution - there's a large proﬁt incentive, after all.) Beyond
that level, more money mostly just means more ability to spray and pray for solutions
- which is not a promising strategy in our high-dimensional world.
So, two years ago I quit my monetarily-lucrative job as a data scientist and have
mostly focused on acquiring knowledge since then. I can worry about money if and
when I know what to do with it.
A mindset I recommend trying on from time to time, especially for people with
$100k+ income: think of money as an abundant resource. Everything money can buy
is "cheap", because money is "cheap". Then the things which are "expensive" are the
things which money alone cannot buy - including knowledge and understanding of the
world. Life lesson from Disney!Rumplestiltskin: there are things which money cannot
buy, therefore it is important to acquire such things and use them for barter and

investment. In particular, it's worth looking for opportunities to acquire knowledge and
expertise which can be leveraged for more knowledge and expertise.
Investments In Knowledge
Past a certain point, money and power are no longer the limiting factors for me to get
what I want. Knowledge becomes the bottleneck instead. At that point, money and
power are no longer particularly relevant measures of my capabilities. Pursuing more
"wealth" in the usual sense of the word is no longer a very useful instrumental goal. At
that point, the type of "wealth" I really need to pursue is knowledge.
If I want to build long-term knowledge-wealth, then the analogy between money-
wealth and knowledge-wealth suggests an interesting question: what does a
knowledge "investment" look like? What is a capital asset of knowledge, an
investment which pays dividends in more knowledge?
Enter gears-level models.
Mapping out the internal workings of a system takes a lot of up-front work. It's much
easier to try random molecules and see if they cure cancer, than to map out all the
internal signals and cells and interactions which cause cancer. But the latter is a
capital investment: once we've nailed down one gear in the model, one signal or one
mutation or one cell-state, that informs all of our future tests and model-building. If we
ﬁnd that Y mediates the eﬀect of X on Z, then our future studies of the Y-Z interaction
can safely ignore X. On the other hand, if we test a random molecule and ﬁnd that it
doesn't cure cancer, then that tells us little-to-nothing; that knowledge does not yield
dividends.
Of course, gears-level models aren't the only form of capital investment in knowledge.
Most tools of applied math and the sciences consist of general models which we can
learn once and then apply in many diﬀerent contexts. They are general-purpose gears
which we can recognize in many systems.
Once I understand the internal details of how e.g. capacitors work, I can apply that
knowledge to understand not only electronic circuits, but also charged biological
membranes. When I understand the math of microeconomics, I can apply it to
optimization problems in AI. When I understand shocks and rarefactions in nonlinear
PDEs, I can see them in action at the beach or in traﬃc. And the "core" topics -
calculus, linear algebra, diﬀerential equations, big-O analysis, Bayesian probability,
optimization, dynamical systems, etc - can be applied all over. General-purpose
models are a capital investment in knowledge.
I hope that someday my own research will be on that list. That's the kind of wealth I'm
investing in now.

Embedded Interactive Predictions on
LessWrong
Ought and LessWrong are excited to launch an embedded interactive prediction feature. You
can now embed binary questions into LessWrong posts and comments. Hover over the
widget to see other people's predictions, and click to add your own. 
Try it out
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
How to use this
Create a question
1. Go to elicit.org/binary and create your question by typing it into the ﬁeld at the top
2. Click on the question title, and click the copy button next to the title - it looks like this:
1. Paste the URL into your LW post or comment. It'll look like this in the editor:
Troubleshooting: if the prediction box fails to appear and the link just shows up as text, go to
you LW Settings, uncheck "Activate Markdown Editor", and try again.
 
Make a prediction
1. Click on the widget to add your own prediction
2. Click on your prediction line again to delete it
Link your accounts
Linking your LessWrong and Elicit accounts allows you to:
Filter for and browse all your LessWrong predictions on Elicit
Add notes to your LessWrong predictions on Elicit
See your calibration for your LessWrong predictions on Elicit
Predict on LessWrong questions in the Elicit app
To link your accounts:
Make an Elicit account

Send me (amanda@ought.org) an email with your LessWrong username and your
Elicit account email
Motivation
We hope embedded predictions can prompt readers and authors to:
1. Actively engage with posts. By making predictions as they read, people have to
stop and think periodically about how much they agree with the author.
2. Distill claims. For writers, integrating predictions challenges them to think more
concretely about their claims and how readers might disagree.
3. Communicate uncertainty. Rather than just stating claims, writers can also
communicate a conﬁdence level.
4. Collect predictions. As a reader, you can build up a personal database of predictions
as you browse LessWrong.
5. Get granular feedback. Writers can get feedback on their content at a more granular
level than comments or upvotes.
By working with LessWrong on this, Ought hopes to make forecasting easier and more
prevalent. As we learn more about how people think about the future, we can use Elicit to
automate larger parts of the workﬂow and thought process until we end up with end-to-end
automated reasoning that people endorse. Check out our blog post to see demos and more
context.
Some examples of how to use this
1. To make speciﬁc predictions, like in Zvi's post on COVID predictions
2. To express credences on claims like those in Daniel Kokotajlo's soft takeoﬀ post
3. Beyond LessWrong - if you want to integrate this into your blog or have other ideas for
places you'd want to use this, let us know!

Some AI research areas and their
relevance to existential safety
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
Followed by: What Multipolar Failure Looks Like, and Robust Agent-Agnostic Processes
(RAAPs) , which provides examples of multi-stakeholder/multi-agent interactions leading to
extinction events.
Introduction
This post is an overview of a variety of AI research areas in terms of how much I think
contributing to and/or learning from those areas might help reduce AI x-risk.  By research
areas I mean "AI research topics that already have groups of people working on them and
writing up their results", as opposed to research "directions" in which I'd like to see these
areas "move". 
I formed these views mostly pursuant to writing AI Research Considerations for Human
Existential Safety (ARCHES).  My hope is that my assessments in this post can be helpful to
students and established AI researchers who are thinking about shifting into new research
areas speciﬁcally with the goal of contributing to existential safety somehow.  In these
assessments, I ﬁnd it important to distinguish between the following types of value:
The helpfulness of the area to existential safety, which I think of as a function of what
services are likely to be provided as a result of research contributions to the area, and
whether those services will be helpful to existential safety, versus
The educational value of the area for thinking about existential safety, which I think of
as a function of how much a researcher motivated by existential safety might become
more eﬀective through the process of familiarizing with or contributing to that area,
usually by focusing on ways the area could be used in service of existential safety.
The neglect of the area at various times, which is a function of how much technical
progress has been made in the area relative to how much I think is needed.
Importantly:
The helpfulness to existential safety scores do not assume that your contributions to this
area would be used only for projects with existential safety as their mission.  This can
negatively impact the helpfulness of contributing to areas that are more likely to be
used in ways that harm existential safety.
The educational value scores are not about the value of an existential-safety-motivated
researcher teaching about the topic, but rather, learning about the topic.
The neglect scores are not measuring whether there is enough "buzz" around the topic,
but rather, whether there has been adequate technical progress in it.  Buzz can predict
future technical progress, though, by causing people to work on it.
Below is a table of all the areas I considered for this post, along with their entirely subjective
"scores" I've given them. The rest of this post can be viewed simply as an
elaboration/explanation of this table:
Existing
Research
Area
Social
Application
Helpfulness
to
Existential
Safety
Educational
Value
2015
Neglect
2020
Neglect
2030
Neglect

Out of
Distribution
Robustness
Zero/
Single
1/10
4/10
5/10
3/10
1/10
Agent
Foundations
Zero/
Single
3/10
8/10
9/10
8/10
7/10
Multi-agent RL
Zero/
Multi
2/10
6/10
5/10
4/10
0/10
Preference
Learning
Single/
Single
1/10
4/10
5/10
1/10
0/10
Side-eﬀect
Minimization
Single/
Single
4/10
4/10
6/10
5/10
4/10
Human-Robot
Interaction
Single/
Single
6/10
7/10
5/10
4/10
3/10
Interpretability
in ML
Single/
Single
8/10
6/10
8/10
6/10
2/10
Fairness in ML
Multi/
Single
6/10
5/10
7/10
3/10
2/10
Computational
Social Choice
Multi/
Single
7/10
7/10
7/10
5/10
4/10
Accountability
in ML
Multi/
Multi
8/10
3/10
8/10
7/10
5/10
The research areas are ordered from least-socially-complex to most-socially-complex.  This
roughly (though imperfectly) correlates with addressing existential safety problems of
increasing importance and neglect, according to me.  Correspondingly, the second column
categorizes each area according to the simplest human/AI social structure it applies to:
Zero/Single: Zero-human / Single-AI scenarios
Zero/Multi: Zero-human / Multi-AI scenarios
Single/Single: Single-human / Single-AI scenarios
Single/Multi: Single-human / Multi-AI scenarios
Multi/Single: Multi-human / Single-AI scenarios
Multi/Multi: Multi-human / Multi-AI scenarios

Epistemic status & caveats
I developed the views in this post mostly over the course of the two years I spent writing and
thinking about AI Research Considerations for Human Existential Safety (ARCHES).  I make the
following caveats:
1. These views are my own, and while others may share them, I do not intend to speak in
this post for any institution or group of which I am part.
2. I am not an expert in Science, Technology, and Society (STS).  Historically there hasn't
been much focus on existential risk within STS, which is why I'm not citing much in the
way of sources from STS.  However, from its name, STS as a discipline ought to be
thinking a lot about AI x-risk.  I think there's a reasonable chance of improvement on
this axis over the next 2-3 years, but we'll see.
3. I made this post with essentially zero deference to the judgement of other researchers. 
This is academically unusual, and prone to more variance in what ends up being
expressed.  It might even be considered rude.  Nonetheless, I thought it might be
valuable or at least interesting to stimulate conversation on this topic that is less ﬁltered
through patterns deference to others.  My hope is that people can become less inhibited
in discussing these topics if my writing isn't too "polished".  I might also write a more
deferent and polished version of this post someday, especially if nice debates arise from
this one that I want to distill into a follow-up post.
Deﬁning our objectives
In this post, I'm going to talk about AI existential safety as distinct from both AI alignment and
AI safety as technical objectives.  A number of blogs seem to treat these terms as near-
synonyms (e.g., LessWrong, the Alignment Forum), and I think that is a mistake, at least when
it comes to guiding technical work for existential safety.  First I'll deﬁne these terms, and then
I'll elaborate on why I think it's important not to conﬂate them.
AI existential safety (deﬁnition)
In this post, AI existential safety means "preventing AI technology from posing risks to
humanity that are comparable to or greater than human extinction in terms of their moral
signiﬁcance."  
This is a bit more general than the deﬁnition in ARCHES.  I believe this deﬁnition is fairly
consistent with Bostrom's usage of the term "existential risk", and will have reasonable
staying power as the term "AI existential safety" becomes more popular, because it directly
addresses the question "What does this term have to do with existence?".
AI safety (deﬁnition)
AI safety generally means getting AI systems to avoid risks, of which existential safety is an
extreme special case with unique challenges.  This usage is consistent with normal everyday
usage of the term "safety" (dictionary.com/browse/safety), and will have reasonable staying
power as the term "AI safety" becomes (even) more popular.  AI safety includes safety for self-
driving cars as well as for superintelligences, including issues that these topics do and do not
share in common.
AI ethics (deﬁnition)
AI ethics generally refers to principles that AI developers and systems should follow.  The
"should" here creates a space for debate, whereby many people and institutions can try to
impose their values on what principles become accepted.  Often this means AI ethics
discussions become debates about edge cases that people disagree about instead of
collaborations on what they agree about.  On the other hand, if there is a principle that all or

most debates about AI ethics would agree on or take as a premise, that principle becomes
somewhat easier to enforce.
AI governance (deﬁnition)
AI governance generally refers to identifying and enforcing norms for AI developers and AI
systems themselves to follow.  The question of which principles should be enforced often
opens up debates about safety and ethics.  Governance debates are a bit more action-
oriented than purely ethical debates, such that more eﬀort is focussed on enforcing agreeable
norms relative to debating about disagreeable norms.  Thus, AI governance, as an area of
human discourse, is engaged with the problem of aligning the development and deployment
of AI technologies with broadly agreeable human values.  Whether AI governance is engaged
with this problem well or poorly is, of course, a matter of debate.
AI alignment (deﬁnition)
AI alignment usually means "Getting an AI system to {try | succeed} to do what a human
person or institution wants it to do". The inclusion of "try" or "succeed" respectively creates a
distinction between intent alignment and impact alignment.   This usage is consistent with
normal everyday usage of the term "alignment" (dictionary.com/browse/alignment) as used to
refer to alignment of values between agents, and is therefore relatively unlikely to undergo
deﬁnition-drift as the term "AI alignment" becomes more popular.  For instance, 
(2002) "Alignment" was used this way in 2002 by Daniel Shapiro and Ross Shachter, in
their AAAI conference paper User/Agent Value Alignment, the ﬁrst paper to introduce the
concept of alignment into AI research.  This work was not motivated by existential safety
as far as I know, and is not cited in any of the more recent literature on "AI alignment"
motivated by existential safety, though I think it got oﬀ to a reasonably good start in
deﬁning user/agent value alignment.
(2014) "Alignment" was used this way in the technical problems described by Nate
Soares and Benya Fallenstein in Aligning Superintelligence with Human Interests: A
Technical Research Agenda.  While the authors' motivation is clearly to serve the
interests of all humanity, the technical problems outlined are all about impact alignment
in my opinion, with the possible exception of what they call "Vingean Reﬂection" (which
is necessary for a subagent of society thinking about society).
(2018) "Alignment" is used this way by Paul Christiano in his post Clarifying AI
Alignment, which is focussed on intent alignment.
A broader meaning of "AI alignment" that is not used here
There is another, diﬀerent usage of "AI alignment", which refers to ensuring that AI
technology is used and developed in ways that are broadly aligned with human values.  I think
this is an important objective that is deserving of a name to call more technical attention to it,
and perhaps this is the spirit in which the "AI alignment forum" is so-titled.  However, the term
"AI alignment" already has poor staying-power for referring to this objective in technical
discourse outside of a relatively cloistered community, for two reasons:
1. As described above, "alignment" already has a relatively clear technical meaning that AI
researchers have already gravitated towards interpreting "alignment" to mean, that is
also consistent with natural language meaning of the term "alignment", and
2. AI governance, at least in democratic states, is basically already about this broader
problem.  If one wishes to talk about AI governance that is beneﬁcial to most or all
humans, "humanitarian AI governance" is much clearer and more likely to stick than "AI
alignment".
Perhaps "global alignment", "civilizational alignment", or "universal AI alignment" would
make sense to distinguish this concept from the narrower meaning that alignment usually
takes on in technical settings.  In any case, for the duration of this post, I am using
"alignment" to refer to its narrower, technically prevalent meaning.

Distinguishing our objectives
As promised, I will now elaborate on why it's important not to conﬂate the objectives above. 
Some people might feel that these arguments are about how important these concepts are,
but I'm mainly trying to argue about how importantly diﬀerent they are.  By analogy: while
knives and forks are both important tools for dining, they are not usable interchangeably.
Safety vs existential safety (distinction)
"Safety" is not robustly usable as a synonym for "existential safety".  It is true that AI
existential safety is literally a special case of AI safety, for the simple reason that avoiding
existential risk is a special case of avoiding risk.  And, it may seem useful for coalition-building
purposes to unite people under the phrase "AI safety" as a broadly agreeable objective. 
However, I think we should avoid declaring to ourselves or others that "AI safety" will or
should always be interpreted as meaning "AI existential safety", for several reasons:
1. Using these terms as synonyms will have very little staying power as AI safety research
becomes (even) more popular.
2. AI existential safety is deserving of direct attention that is not ﬁltered through a lens of
discourse that confuses it with self-driving car safety.
3. AI safety in general is deserving of attention as a broadly agreeable principle around
which people can form alliances and share ideas.
Alignment vs existential safety (distinction)
Some people tend to use these terms as as near-synonyms, however, I think this usage has
some important problems:
1. Using "alignment" and "existential safety" as synonyms will have poor staying-power as
the term "AI alignment" becomes more popular.  Conﬂating them will oﬀend both the
people who want to talk about existential safety (because they think it is more
important and "obviously what we should be talking about") as well as the people who
want to talk about AI alignment (because they think it is more important and "obviously
what we should be talking about").
2. AI alignment refers to a cluster of technically well-deﬁned problems that are important
to work on for numerous reasons, and deserving of a name that does not secretly mean
"preventing human extinction" or similar.
3. AI existential safety (I claim) also refers to a technically well-deﬁnable problem that is
important to work on, and deserving of a name that does not secretly mean "getting
systems to do what the user is asking".
4. AI alignment is not trivially helpful to existential safety, and eﬀorts to make it helpful
require a certain amount of societal-scale steering to guide them.  If we treat these
terms as synonyms, we impoverish our collective awareness of ways in which AI
alignment solutions could pose novel problems for existential safety.
This last point gets its own section.
AI alignment is inadequate for AI existential
safety
Around 50% of my motivation for writing this post is my concern that progress in AI
alignment, which is usually focused on "single/single" interactions (i.e., alignment for a single
human stakeholder and a single AI system), is inadequate for ensuring existential safety for
advancing AI technologies.  Indeed, among problems I can currently see in the world that I
might have some ability to inﬂuence, addressing this issue is currently one of my top
priorities.

The reason for my concern here is pretty simple to state, via the following two diagrams:

Of course, understanding and designing useful and modular single/single interactions is a
good ﬁrst step toward understanding multi/multi interactions, and many people (including
myself) who think about AI alignment are thinking about it as a stepping stone to
understanding the broader societal-scale objective of ensuring existential safety.  
However, this pattern mirrors the situation AI capabilities research was following before
safety, ethics, and alignment began surging in popularity.  Consider that most AI (construed to
include ML) researchers are developing AI capabilities as stepping stones toward
understanding and deploying those capabilities in safe and value-aligned applications for
human users.  Despite this, over the past decade there has been a growing sense among AI
researchers that capabilities research has not been suﬃciently forward-looking in terms of
anticipating its role in society, including the need for safety, ethics, and alignment work.  This
general concern can be seen emanating not only from AGI-safety-oriented groups like those at
DeepMind, OpenAI, MIRI, and in academia, but also AI-ethics-oriented groups as well, such as
the ACM Future of Computing Academy:
https://acm-fca.org/2018/03/29/negativeimpacts/
Just as folks interested in AI safety and ethics needed to start thinking beyond capabilities,
folks interested in AI existential safety need to start thinking beyond alignment.  The next
section describes what I think this means for technical work.

Anticipating, legitimizing and fulﬁlling
governance demands
The main way I can see present-day technical research beneﬁtting existential safety is by
anticipating, legitimizing and fulﬁlling governance demands for AI technology that will arise
over the next 10-30 years.  In short, there often needs to be some amount of traction on a
technical area before it's politically viable for governing bodies to demand that institutions
apply and improve upon solutions in those areas.  Here's what I mean in more detail:
By governance demands, I'm referring to social and political pressures to ensure AI
technologies will produce or avoid certain societal-scale eﬀects.  Governance demands
include pressures like "AI technology should be fair", "AI technology should not degrade civic
integrity", or "AI technology should not lead to human extinction."  For instance, Twitter's
recent public decision to maintain a civic integrity policy can be viewed as a response to
governance demand from its own employees and surrounding civic society.
Governance demand is distinct from consumer demand, and it yields a diﬀerent kind of
transaction when the demand is met.  In particular, when a tech company fulﬁlls a
governance demand, the company legitimizes that demand by providing evidence that it is
possible to fulﬁll.  This might require the company to break ranks with other technology
companies who deny that the demand is technologically achievable.  
By legitimizing governance demands, I mean making it easier to establish common
knowledge that a governance demand is likely to become a legal or professional standard. 
But how can technical research legitimize demands from a non-technical audience?
The answer is to genuinely demonstrate in advance that the governance demands are
feasible to meet.  Passing a given professional standard or legislation usually requires the
demands in it to be "reasonable" in terms of appearing to be technologically achievable. 
Thus, computer scientists can help legitimize a governance demand by anticipating the
demand in advance, and beginning to publish solutions for it.  My position here is not that the
solutions should be exaggerated in their completeness, even if that will increase 'legitimacy'; I
argue only that we should focus energy on ﬁnding solutions that, if communicated broadly
and truthfully, will genuinely raise conﬁdence that important governance demands are
feasible.  (Without this ethic against exaggeration, common knowledge of the legitimacy of
legitimacy itself is degraded, which is bad, so we shouldn't exaggerate.)
This kind of work can make a big diﬀerence to the future.  If the algorithmic techniques
needed to meet a given governance demand are 10 years of research away from discovery---
as opposed to just 1 year---then it's easier for large companies to intentionally or
inadvertently maintain a narrative that the demand is unfulﬁllable and therefore illegitimate. 
Conversely, if the algorithmic techniques to fulﬁll the demand already exist, it's a bit harder
(though still possible) to deny the legitimacy of the demand.  Thus, CS researchers can
legitimize certain demands in advance, by beginning to prepare solutions for them.
I think this is the most important kind of work a computer scientist can do in service of
existential safety.  For instance, I view ML fairness and interpretability research as responding
to existing governance demand, which (genuinely) legitimizes the cause of AI governance
itself, which is hugely important.  Furthermore, I view computational social choice research as
addressing an upcoming governance demand, which is even more important.
My hope in writing this post is that some of the readers here will start trying to anticipate AI
governance demands that will arise over the next 10-30 years.  In doing so, we can begin to
think about technical problems and solutions that could genuinely legitimize and fulﬁll those
demands when they arise, with a focus on demands whose fulﬁllment can help stabilize
society in ways that mitigate existential risks.

Research Areas
Alright, let's talk about some research!
Out of distribution robustness (OODR)
Existing
Research
Area
Social
Application
Helpfulness
to
Existential
Safety
Educational
Value
2015
Neglect
2020
Neglect
2030
Neglect
Out of
Distribution
Robustness
Zero/Single
1/10
4/10
5/10
3/10
1/10
This area of research is concerned with avoiding risks that arise from systems interacting with
contexts and environments that are changing signiﬁcantly over time, such as from training
time to testing time, from testing time to deployment time, or from controlled deployments to
uncontrolled deployments.
OODR (un)helpfulness to existential safety:  
Contributions to OODR research are not particularly helpful to existential safety in my opinion,
for a combination of two reasons:
1. Progress in OODR will mostly be used to help roll out more AI technologies into active
deployment more quickly, and
2. Research in this area usually does not involve deep or lengthy reﬂections about the
structure of society and human values and interactions, which I think makes this ﬁeld
sort of collectively blind to the consequences of the technologies it will help build.
I think this area would be more helpful if it were more attentive to the structure of the multi-
agent context that AI systems will be in.  Professor Tom Dietterich has made some attempts to
shift thinking on robustness to be more attentive to the structure of robust human institutions,
which I think is a good step:
Robust artiﬁcial intelligence and robust human organizations (2018) Dietterich, Thomas
G.
Unfortunately, the above paper has only 8 citations at the time of writing (very little for
AI/ML), and there does not seem to be much else in the way of publications that address
societal-scale or even institutional-scale robustness.
OODR educational value:
Studying and contributing to OODR research is of moderate educational value for people
thinking about x-risk, in my opinion.  Speaking for myself, it helps me think about how society
as a whole is receiving a changing distribution of inputs from its environment (which society
itself is creating).  As human society changes, the inputs to AI technologies will change, and
we want the existence of human society to be robust to those changes.  I don't think most
researchers in this area think about it in that way, but that doesn't mean you can't.
OODR neglect:  

Robustness to changing environments has never been a particularly neglected concept in the
history of automation, and it is not likely to ever become neglected, because myopic
commercial incentives push so strongly in favor of progress on it.  Speciﬁcally, robustness of
AI systems is essential for tech companies to be able to roll out AI-based products and
services, so there is no lack of incentive for the tech industry to work on robustness.  In
reinforcement learning speciﬁcally, robustness has been somewhat neglected, although less
so now than in 2015, partly thanks to AI safety (broadly construed) taking oﬀ.  I think by 2030
this area will be even less neglected, even in RL. 
OODR exemplars:
Recent exemplars of high value to existential safety, according to me:
(2018) Robust artiﬁcial intelligence and robust human organizations, Dietterich, Thomas
G*
*The above paper is not really about out of distribution robustness, but among papers
I've found appreciably valuable to x-safety, it's the closest.
Recent exemplars of high educational value, according to me:
(2016) Doubly robust oﬀ-policy value evaluation for reinforcement learning, Jiang, Nan;
Li, Lihong.*
*Not directly about distributional shift, but valuable to this area in my opinion.
(2016) A baseline for detecting misclassiﬁed and out-of-distribution examples in neural
networks, Hendrycks, Dan; Gimpel, Kevin.
(2017) Enhancing the reliability of out-of-distribution image detection in neural
networks, Liang, Shiyu; Li, Yixuan; Srikant, R.
(2017) Training conﬁdence-calibrated classiﬁers for detecting out-of-distribution
samples, (2017), Lee, Kimin; Lee, Honglak; Lee, Kibok; Shin, Jinwoo.
(2018) Learning conﬁdence for out-of-distribution detection in neural networks , DeVries,
Terrance; Taylor, Graham W.
Agent foundations (AF)
Existing
Research
Area
Social
Application
Helpfulness
to
Existential
Safety
Educational
Value
2015
Neglect
2020
Neglect
2030
Neglect
Agent
Foundations Zero/Single
3/10
8/10
9/10
8/10
7/10
This area is concerned with developing and investigating fundamental deﬁnitions and
theorems pertaining to the concept of agency.  This often includes work in areas such as
decision theory, game theory, and bounded rationality.  I'm going to write more for this
section because I know more about it and think it's pretty important to "get right".
AF (un)helpfulness to existential safety:  
Contributions to agent foundations research are key to the foundations of AI safety and ethics,
but are also potentially misusable.  Thus, arbitrary contributions to this area are not
necessarily helpful, while targeted contributions aimed at addressing real-world ethical
problems could be extremely helpful.  Here is why I believe this:
I view agent foundations work as looking very closely at the fundamental building blocks of
society, i.e., agents and their decisions.  It's important to understand agents and their basic

operations well, because we're probably going to produce (or allow) a very large number of
them to exist/occur.  For instance, imagine any of the following AI-related operations
happening at least 1,000,000 times (a modest number given the current world population):
1. A human being delegates a task to an AI system to perform, thereby ceding some
control over the world to the AI system.
2. An AI system makes a decision that might yield important consequences for society, and
acts on it.
3. A company deploys an AI system into a new context where it might have important side
eﬀects.
4. An AI system builds or upgrades another AI system (possibly itself) and deploys it.
5. An AI system interacts with another AI system, possibly yielding externalities for society.
6. An hour passes where AI technology is exerting more control over the state of the Earth
than humans are.
Suppose there's some class of negative outcomes (e.g. human extinction) that we want to
never occur as a result of any of these operations.  In order to be just 55% sure that all of
these 1,000,000 operations will be safe (i.e., avoid the negative outcome class), on average
(on a log scale) we need to be at least 99.99994% sure that each instance of the operation is
safe (i.e., will not precipitate the negative outcome).  Similarly, for any accumulable quantity
of "societal destruction" (such as risk, pollution, or resource exhaustion), in order to be sure
that these operations will not yield "100 units" of societal destruction, we need each
operation on average to produce at most "0.00001 units" of destruction.*
(*Would-be-footnote: Incidentally, the main reason I think OODR research is educationally
valuable is that it can eventually help with applying agent foundations research to societal-
scale safety.  Speciﬁcally: how can we know if one of the operations (a)-(f) above is safe to
perform 1,000,000 times, given that it was safe the ﬁrst 1,000 times we applied it in a
controlled setting, but the setting is changing over time?  This is a special case of an OODR
question.)
Unfortunately, understanding the building blocks of society can also allow the creation of
potent societal forces that would harm society.  For instance, understanding human decision-
making extremely well might help advertising companies to control public opinion to an
unreasonable degree (which arguably has already happened, even with today's rudimentary
agent models), or it might enable the construction of a super-decision-making system that is
misaligned with human existence.   
That said, I don't think this means you have to be super careful about information security
around agent foundations work, because in general it's not easy to communicate fundamental
theoretical results in research, let alone by accident. 
Rather, my recommendation for maximizing the positive value of work in this area is to apply
the insights you get from it to areas that make it easier to represent societal-scale moral
values in AI.  E.g., I think applications of agent foundations  results to interpretability, fairness,
computational social choice, and accountability are probably net good, whereas applications
to speed up arbitrary ML capabilities are not obviously good.
AF educational value:
Studying and contributing to agent foundations research has the highest educational value for
thinking about x-risk among the research areas listed here, in my opinion.  The reason is that
agent foundations research does the best job of questioning potentially faulty assumptions
underpinning our approach to existential safety.  In particular, I think our understanding of
how to safely integrate AI capabilities with society is increasingly contingent on our
understanding of agent foundations work as deﬁning the building blocks of society.
AF neglect:

This area is extremely neglected in my opinion.  I think around 50% of the progress in this
area, worldwide, happens at MIRI, which has a relatively small staﬀ of agent foundations
researchers.  While MIRI has grown over the past 5 years, agent foundations work in academia
hasn't grown much, and I don't expect it to grow much by default (though perhaps posts like
this might change that default).
AF exemplars:
Below are recent exemplars of agent foundations work that I think is of relatively high value to
existential safety, mostly via their educational value to understanding the foundations of how
agents work ("agent foundations").  The work is mostly from three main clusters: MIRI, Vincent
Conitzer's group at Duke, and Joe Halpern's group at Cornell.
(2015) Translucent players: Explaining cooperative behavior in social dilemmas ,
Capraro, Valerio; Halpern, Joseph Y.
(2016) Logical induction, Garrabrant, Scott; Benson-Tilsen, Tsvi; Critch, Andrew; Soares,
Nate; Taylor, Jessica. *
*COI note: I am a coauthor on the above paper.  If many other people were writing
existential safety appraisals like this post, I'd omit my own papers from this list and
defer to others to judge them.
(2016) Reﬂective oracles: A foundation for game theory in artiﬁcial intelligence ,
Fallenstein, Benja; Taylor, Jessica; Christiano, Paul F.
(2017) Functional decision theory: A new theory of instrumental rationality , Yudkowsky,
Eliezer; Soares, Nate.
(2017) Disarmament games , Deng, Yuan; Conitzer, Vincent.
(2018) Game theory with translucent players , Halpern, Joseph Y; Pass, Rafael.
(2019) Embedded agency , Demski, Abram; Garrabrant, Scott.
(2019) A parametric, resource-bounded generalization of loeb's theorem, and a robust
cooperation criterion for open-source game theory (2019) Critch, Andrew.*
*COI note: I am the author of the above paper.  If many other people were writing
existential safety appraisals like this post, I'd omit my own papers from this list and
defer to others to judge them.
(2019) Risks from Learned Optimization in Advanced Machine Learning Systems ,
Hubinger, Evan; van Merwijk, Chris; Mikulik, Vladimir; Skalse, Joar; Garrabrant, Scott.
Multi-agent reinforcement learning (MARL)
Existing
Research
Area
Social
Application
Helpfulness
to
Existential
Safety
Educational
Value
2015
Neglect
2020
Neglect
2030
Neglect
Multi-
agent RL
Zero/Multi
2/10
6/10
5/10
4/10
0/10
MARL is concerned with training multiple agents to interact with each other and solve
problems using reinforcement learning.  There are a few varieties to be aware of:
Cooperative vs competitive vs adversarial tasks: do the agents all share a single
objective, or separate objectives that are imperfectly aligned, or completely opposed
(zero-sum) objective?
Centralized training vs decentralized training: is there a centralized process that
observes the agents and controls how they learn, or is there a separate (private)
learning process for each agent?

Communicative vs non-communicative: is there a special channel the agents can use to
generate observations for each other that are otherwise inconsequential, or are all
observations generated in the course of consequential actions?
I think the most interesting MARL research involves decentralized training for competitive
objectives in communicative environments, because this set-up is the most representative of
how AI systems from diverse human institutions are likely to interact.
MARL (un)helpfulness to existential safety: 
Contributions to MARL research are mostly not very helpful to existential safety in my opinion,
because MARL's most likely use case will be to help companies to deploy ﬂeets of rapidly
interacting machines that might pose risks to human society.  The MARL projects with the
greatest potential to help are probably those that ﬁnd ways to achieve cooperation between
decentrally trained agents in a competitive task environment, because of its potential to
minimize destructive conﬂicts between ﬂeets of AI systems that cause collateral damage to
humanity.  That said, even this area of research risks making it easier for ﬂeets of machines to
cooperate and/or collude at the exclusion of humans, increasing the risk of humans becoming
gradually disenfranchised and perhaps replaced entirely by machines that are better and
faster at cooperation than humans.
MARL educational value: 
I think MARL has a high educational value, because it helps researchers to observe directly
how diﬃcult it is to get multi-agent systems to behave well.  I think most of the existential risk
from AI over the next decades and centuries comes from the incredible complexity of
behaviors possible from multi-agent systems, and from underestimating that complexity
before it takes hold in the real world and produces unexpected negative side eﬀects for
humanity.
MARL neglect: 
MARL was somewhat neglected 5 years ago, but has picked up a lot.  I suspect MARL will keep
growing in popularity because of its value as a source of curricula for learning algorithms.  I
don't think it is likely to become more civic-minded, unless arguments along the lines of this
post lead to a shift of thinking in the ﬁeld.
MARL exemplars:
Recent exemplars of high educational value, according to me:
(2015) Cooperating with unknown teammates in complex domains: A robot soccer case
study of ad hoc teamwork , Barrett, Samuel; Stone, Peter.
(2016) Learning to communicate with deep multi-agent reinforcement learning ,
Foerster, Jakob; Assael, Ioannis Alexandros; de Freitas, Nando; Whiteson, Shimon.
(2017) Emergent complexity via multi-agent competition , Bansal, Trapit; Pachocki,
Jakub; Sidor, Szymon; Sutskever, Ilya; Mordatch, Igor.
(2017) Making friends on the ﬂy: Cooperating with new teammates , Barrett, Samuel;
Rosenfeld, Avi; Kraus, Sarit; Stone, Peter.
(2017) Multi-agent actor-critic for mixed cooperative-competitive environments , Lowe,
Ryan; Wu, Yi; Tamar, Aviv; Harb, Jean; Abbeel, OpenAI Pieter; Mordatch, Igor.
(2017) Multiagent cooperation and competition with deep reinforcement learning ,
Tampuu, Ardi; Matiisen, Tambet; Kodelja, Dorian; Kuzovkin, Ilya; Korjus, Kristjan; Aru,
Juhan; Aru, Jaan; Vicente, Raul.
(2017) Stabilising experience replay for deep multi-agent reinforcement learning ,
Foerster, Jakob; Nardelli, Nantas; Farquhar, Gregory; Afouras, Triantafyllos; Torr, Philip
HS; Kohli, Pushmeet; Whiteson, Shimon.
(2017) Counterfactual multi-agent policy gradients , Foerster, Jakob; Farquhar, Gregory;
Afouras, Triantafyllos; Nardelli, Nantas; Whiteson, Shimon.

(2017) Learning with opponent-learning awareness , Foerster, Jakob N; Chen, Richard Y;
Al-Shedivat, Maruan; Whiteson, Shimon; Abbeel, Pieter; Mordatch, Igor.
(2018) Autonomous agents modelling other agents: A comprehensive survey and open
problems , Albrecht, Stefano V; Stone, Peter.
(2018) Learning to share and hide intentions using information regularization ,  Strouse,
DJ; Kleiman-Weiner, Max; Tenenbaum, Josh; Botvinick, Matt; Schwab, David J.
(2018) Inequity aversion improves cooperation in intertemporal social dilemmas ,
Hughes, Edward; Leibo, Joel Z; Phillips, Matthew; Tuyls, Karl; Duenez-Guzman, Edgar;
Castaneda, Antonio Garcia; Dunning, Iain; Zhu, Tina; McKee, Kevin; Koster, Raphael;
others.
(2019) Social inﬂuence as intrinsic motivation for multi-agent deep reinforcement
learning , Jaques, Natasha; Lazaridou, Angeliki; Hughes, Edward; Gulcehre, Caglar;
Ortega, Pedro; Strouse, DJ; Leibo, Joel Z; De Freitas, Nando.
(2019) Policy-gradient algorithms have no guarantees of convergence in continuous
action and state multi-agent settings , Mazumdar, Eric; Ratliﬀ, Lillian J; Jordan, Michael I;
Sastry, S Shankar.
Preference learning (PL)
Existing
Research
Area
Social
Application
Helpfulness
to
Existential
Safety
Educational
Value
2015
Neglect
2020
Neglect
2030
Neglect
Preference
Learning
Single/Single 1/10
4/10
5/10
1/10
0/10
This area is concerned with learning about human preferences in a form usable for  guiding
the policies of artiﬁcial agents.  In an RL (reinforcement learning) setting, preference learning
is often called reward learning, because the learned preferences take the form of a reward
function for training an RL system.
PL (un)helpfulness to existential safety:
Contributions to preference learning are not particularly helpful to existential safety in my
opinion, because their most likely use case is for modeling human consumers just well enough
to create products they want to use and/or advertisements they want to click on.  Such
advancements will be helpful to rolling out usable tech products and platforms more quickly,
but not particularly helpful to existential safety.* 
Preference learning is of course helpful to AI alignment, i.e., the problem of getting an AI
system to do something a human wants.  Please refer back to the sections above on Deﬁning
our objectives and Distinguishing our objectives for an elaboration of how this is not the same
as AI existential safety.  In any case, I see AI alignment in turn as having two main potential
applications to existential safety:
1. AI alignment is useful as a metaphor for thinking about how to align the global eﬀects of
AI technology with human existence, a major concern for AI governance at a global
scale, and
2. AI alignment solutions could be used directly to govern powerful AI technologies
designed speciﬁcally to make the world safer.
While many researchers interested in AI alignment are motivated by (1) or (2), I ﬁnd these
pathways of impact problematic.  Speciﬁcally, 

(1) elides the complexities of multi-agent interactions I think are likely to arise in most
realistic futures, and I think the most diﬃcult to resolve existential risks arise from those
interactions.
(2) is essentially aiming to take over the world in the name of making it safer, which is
not generally considered the kind of thing we should be encouraging lots of people to
do.
Moreover, I believe contributions to AI alignment are also generally unhelpful to existential
safety, for the same reasons as preference learning.  Speciﬁcally, progress in AI alignment
hastens the pace at which high-powered AI systems will be rolled out into active deployment,
shortening society's headway for establishing international treaties governing the use of AI
technologies.
Thus, the existential safety value of AI alignment research in its current technical formulations
—and preference learning as a subproblem of it—remains educational in my view.*
(*Would-be-footnote: I hope no one will be too oﬀended by this view.  I did have some
trepidation about expressing it on the "alignment' forum, but I think I should voice these
concerns anyway, for the following reason. In 2011 after some months of reﬂection on a
presentation by Andrew Ng, I came to believe that that deep learning was probably going to
take oﬀ, and that, contrary to Ng's opinion, this would trigger a need for a lot of AI alignment
work in order to make the technology safe.  This feeling of worry is what triggered me to
cofound CFAR and start helping to build a community that thinks more critically about the
future.  I currently have a similar feeling of worry toward preference learning and AI
alignment, i.e., that it is going to take oﬀ and trigger a need for a lot more "AI civility" work
that seems redundant or "too soon to think about" for a lot of AI alignment researchers today,
the same way that AI researchers said it was "too soon to think about" AI alignment.  To the
extent that I think I was right to be worried about AI progress kicking oﬀ in the decade
following 2011, I think I'm right to be worried again now about preference learning and AI
alignment (in its narrow and socially-simplistic technical formulations) taking oﬀ in the 2020's
and 2030's.)
PL educational value: 
Studying and making contributions to preference learning is of moderate educational value for
thinking about existential safety in my opinion.  The reason is this: if we want machines to
respect human preferences—including our preference to continue existing—we may need
powerful machine intelligences to understand our preferences in a form they can act on.  Of
course, being understood by a powerful machine is not necessarily a good thing.  But if the
machine is going to do good things for you, it will probably need to understand what "good for
you" means.  In other words, understanding preference learning can help with AI alignment
research, which can help with existential safety.  And if existential safety is your goal, you can
try to target your use of preference learning concepts and methods toward that goal.
PL neglect: 
Preference learning has always been crucial to the advertising industry, and as such it has not
been neglected in recent years.  For the same reason, it's also not likely to become
neglected.  Its application to reinforcement learning is somewhat new, however, because until
recently there was much less active research in reinforcement learning.  In other words,
recent interest in reward learning is mainly a function of increased interest in reinforcement
learning, rather than increased interest in preference learning.  If new learning paradigms
supersede reinforcement learning, preference learning for those paradigms will not be far
behind.
(This is not a popular opinion; I apologize if I have oﬀended anyone who believes that
progress in preference learning will reduce existential risk, and I certainly welcome debate on
the topic.)

PL exemplars:
Recent works of signiﬁcant educational value, according to me:
(2017) Deep reinforcement learning from human preferences , Christiano, Paul F; Leike,
Jan; Brown, Tom; Martic, Miljan; Legg, Shane; Amodei, Dario.
(2018) Reward learning from human preferences and demonstrations in Atari , Ibarz,
Borja; Leike, Jan; Pohlen, Tobias; Irving, Geoﬀrey; Legg, Shane; Amodei, Dario.
(2018) The alignment problem for Bayesian history-based reinforcement learners ,
Everitt, Tom; Hutter, Marcus.
(2019) Learning human objectives by evaluating hypothetical behavior , Reddy,
Siddharth; Dragan, Anca D; Levine, Sergey; Legg, Shane; Leike, Jan.
(2019) On the feasibility of learning, rather than assuming, human biases for reward
inference , Shah, Rohin; Gundotra, Noah; Abbeel, Pieter; Dragan, Anca D.
(2020) Reward-rational (implicit) choice: A unifying formalism for reward learning , Jeon,
Hong Jun; Milli, Smitha; Dragan, Anca D.
Human-robot interaction (HRI)
Existing
Research
Area
Social
Application
Helpfulness
to
Existential
Safety
Educational
Value
2015
Neglect
2020
Neglect
2030
Neglect
Human-
Robot
Interaction
Single/Single 6/10
7/10
5/10
4/10
3/10
HRI research is concerned with designing and optimizing patterns of interaction between
humans and machines—usually actual physical robots, but not always.
HRI helpfulness to existential safety:
On net, I think AI/ML would be better for the world if most of its researchers pivoted from
general AI/ML into HRI, simply because it would force more AI/ML researchers to more
frequently think about real-life humans and their desires, values, and vulnerabilities. 
Moreover, I think it reasonable (as in, >1% likely) that such a pivot might actually happen if,
say, 100 more researchers make this their goal.
For this reason, I think contributions to this area today are pretty solidly good for existential
safety, although not perfectly so: HRI research can also be used to deceive humans, which
can degrade societal-scale honesty norms, and I've seen HRI research targeting precisely
that.  However, my model of readers of this blog is that they'd be unlikely to contribute to
those parts of HRI research, such that I feel pretty solidly about recommending contributions
to HRI.
HRI educational value:
I think HRI work is of unusually high educational value for thinking about existential safety,
even among other topics in this post.  The reason is that, by working with robots, HRI work is
forced to grapple with high-dimensional and continuous state spaces and action spaces that
are too complex for the human subjects involved to consciously model.  This, to me, crucially
mirrors the relationship between future AI technology and human society: humanity,
collectively, will likely be unable to consciously grasp the full breadth of states and actions
that our AI technologies are transforming and undertaking for us.  I think many AI researchers
outside of robotics are mostly blind to this diﬃculty, which on its own is an argument in favor

of more AI researchers working in robotics.  The beauty of HRI is that it also explicitly and
continually thinks about real human beings, which I think is an important mental skill to
practice if you want to protect humanity collectively from existential disasters.
HRI neglect: 
A neglect score for this area was uniquely diﬃcult for me to specify.  On one hand, HRI is a
relatively established and vibrant area of research compared with some of the more nascent
areas covered in this post.  On the other hand, as mentioned, I'd eventually like to see the
entirety of AI/ML as a ﬁeld pivoting toward HRI work, which means it is still very neglected
compared to where I want to see it.  Furthermore, I think such a pivot is actually reasonable to
achieve over the next 20-30 years.  Further still, I think industrial incentives might eventually
support this pivot, perhaps on a similar timescale.  
So: if the main reason you care about neglect is that you are looking to produce a strong
founder eﬀect, you should probably discount my numerical neglect scores for this area, given
that it's not particularly "small" on an absolute scale compared to the other areas here.  By
that metric, I'd have given something more like {2015:4/10; 2020:3/10; 2030:2/10}.  On the
other hand, if you're an AI/ML researcher looking to "do the right thing" by switching to an
area that pretty much everyone should switch into, you deﬁnitely have my "doing the right
thing" assessment if you switch into this area, which is why I've given it somewhat higher
neglect scores.
HRI exemplars:
(2015) Shared autonomy via hindsight optimization , Javdani, Shervin; Srinivasa,
Siddhartha S; Bagnell, J Andrew.
(2015) Learning preferences for manipulation tasks from online coactive feedback , Jain,
Ashesh; Sharma, Shikhar; Joachims, Thorsten; Saxena, Ashutosh.
(2016) Cooperative inverse reinforcement learning , Hadﬁeld-Menell, Dylan; Russell,
Stuart J; Abbeel, Pieter; Dragan, Anca.
(2017) Planning for autonomous cars that leverage eﬀects on human actions. , Sadigh,
Dorsa; Sastry, Shankar; Seshia, Sanjit A; Dragan, Anca D.
(2017) Should robots be obedient? , Milli, Smitha; Hadﬁeld-Menell, Dylan; Dragan, Anca;
Russell, Stuart.
(2019) Where do you think you're going?: Inferring beliefs about dynamics from
behavior , Reddy, Sid; Dragan, Anca; Levine, Sergey.
(2019) Literal or Pedagogic Human? Analyzing Human Model Misspeciﬁcation in
Objective Learning , Milli, Smitha; Dragan, Anca D.
(2019) Hierarchical game-theoretic planning for autonomous vehicles , Fisac, Jaime F;
Bronstein, Eli; Stefansson, Elis; Sadigh, Dorsa; Sastry, S Shankar; Dragan, Anca D.
(2020) Pragmatic-pedagogic value alignment , Fisac, Jaime F; Gates, Monica A; Hamrick,
Jessica B; Liu, Chang; Hadﬁeld-Menell, Dylan; Palaniappan, Malayandi; Malik, Dhruv;
Sastry, S Shankar; Griﬃths, Thomas L; Dragan, Anca D.
Side-eﬀect minimization (SEM)
Existing
Research
Area
Social
Application
Helpfulness
to
Existential
Safety
Educational
Value
2015
Neglect
2020
Neglect
2030
Neglect
Side-eﬀect
Minimization Single/Single 4/10
4/10
6/10
5/10
4/10

SEM research is concerned with developing domain-general methods for making AI systems
less likely to produce side eﬀects, especially negative side eﬀects, in the course of pursuing
an objective or task.
SEM helpfulness to existential safety:
I think this area has two obvious applications to safety-in-general:
1. ("accidents") preventing an AI agent from "messing up" when performing a task for its
primary stakeholder(s), and
2. ("externalities") preventing an AI system from generating problems for persons other
than its primary stakeholders, either
1. ("unilateral externalities") when the system generates externalities through its
unilateral actions, or
2. ("multilateral externalities") when the externalities are generated through the
interaction of an AI system with another entity, such as a non-stakeholder or
another AI system.
I think the application to externalities is more important and valuable than the application to
accidents, because I think externalities are (even) harder to detect and avoid than accidents. 
Moreover, I think multilateral externalities are (even!) harder to avoid than unilateral
externalities.  
Currently, SEM research is focussed mostly on accidents, which is why I've only given it a
moderate score on the helpfulness scale.  Conceptually, it does make sense to focus on
accidents ﬁrst, then unilateral externalities, and then multilateral externalities, because of the
increasing diﬃculty in addressing them.  
However, the need to address multilateral externalities will arise very quickly after unilateral
externalities are addressed well enough to roll out legally admissible products, because most
of our legal systems have an easier time deﬁning and punishing negative outcomes that have
a responsible party.  I don't believe this is a quirk of human legal systems: when two
imperfectly aligned agents interact, they complexify each other's environment in a way that
consumes more cognitive resources than interacting with a non-agentic environment. (This is
why MARL and self-play are seen as powerful curricula for learning.)  Thus, there is less
cognitive "slack" to think about non-stakeholders in a multi-agent setting than in a single-
agent setting.  
For this reason, I think work that makes it easy for AI systems and their designers to achieve
common knowledge around how the systems should avoid producing externalities is very
valuable.
SEM educational value:
I think SEM research thus far is of moderate educational value, mainly just to kickstart your
thinking about side eﬀects.
SEM neglect:
Domain-general side-eﬀect minimization for AI is a relatively new area of research, and is still
somewhat neglected.  Moreover, I suspect it will remain neglected, because of the
aforementioned tendency for our legal system to pay too little attention to multilateral
externalities, a key source of negative side eﬀects for society.
SEM exemplars:
Recent exemplars of value to existential safety, mostly via starting to think about the
generalized concept of side eﬀects at all:

(2018) Penalizing side eﬀects using stepwise relative reachability , Krakovna, Victoria;
Orseau, Laurent; Kumar, Ramana; Martic, Miljan; Legg, Shane
(2019) Safelife 1.0: Exploring side eﬀects in complex environments , Wainwright, Carroll
L; Eckersley, Peter
(2019) Preferences Implicit in the State of the World , Shah, Rohin; Krasheninnikov,
Dmitrii; Alexander, Jordan; Abbeel, Pieter; Dragan, Anca 
(This paper is about preference inference, but I think it applies more speciﬁcally to
inferring how not to have negative side eﬀects.) 
(2020) Conservative agency via attainable utility preservation , Turner, Alexander Matt;
Hadﬁeld-Menell, Dylan; Tadepalli, Prasad
Interpretability in ML (IntML)
Existing
Research
Area
Social
Application
Helpfulness
to
Existential
Safety
Educational
Value
2015
Neglect
2020
Neglect
2030
Neglect
Interpretability
in ML
Single/Single 8/10
6/10
8/10
6/10
2/10
Interpretability research is concerned with making the reasoning and decisions of AI systems
more interpretable to humans.  Interpretability is closely related to transparency and
explainability.  Not all authors treat these three concepts as distinct; however, I think when
useful distinction is drawn between between them, it often looks something like this:
a system is "transparent" if it is easy for human users or developers to observe and
track important parameters of its internal state;
a system is "explainable" if useful explanations of its reasoning can be produced after
the fact.
a system is "interpretable" if its reasoning is structured in a manner that does not
require additional engineering work to produce accurate human-legible explanations.
In other words, interpretable systems are systems with the property that transparency is
adequate for explainability: when we look inside them, we ﬁnd they are structured in a
manner that does not require much additional explanation.  I see Professor Cynthia Rudin as
the primary advocate for this distinguished notion of interpretability, and I ﬁnd it to be an
important concept to distinguish.
IntML helpfulness to existential safety:
I think interpretability research contributes to existential safety in a fairly direct way on the
margin today.  Speciﬁcally, progress in interpretability will
decrease the degree to which human AI developers will end up misjudging the
properties of the systems they build,
increase the degree to which systems and their designers can be held accountable for
the principles those systems embody, perhaps even before those principles have a
chance to manifest in signiﬁcant negative societal-scale consequences, and
potentially increase the degree to which competing institutions and nations can
establish cooperation and international treaties governing AI-heavy operations.
I believe this last point may turn out to be the most important application of interpretability
work.  Speciﬁcally, I think institutions that use a lot of AI technology (including but not limited
to powerful autonomous AI systems) could become opaque to one another in a manner that

hinders cooperation between and governance of those systems.  By contrast, a degree of
transparency between entities can facilitate cooperative behavior, a phenomenon which has
been borne out in some of the agent foundations work listed above, speciﬁcally:
(2015) Translucent players: Explaining cooperative behavior in social dilemmas ,
Capraro, Valerio; Halpern, Joseph Y.
(2018) Game theory with translucent players , Halpern, Joseph Y; Pass, Rafael.
(2019) A parametric, resource-bounded generalization of loeb's theorem, and a robust
cooperation criterion for open-source game theory (2019) Critch, Andrew.
In other words, I think interpretability research can enable technologies that legitimize and
fulﬁll AI governance demands, narrowing the gap between what policy makers will wish for
and what technologists will agree is possible.
IntML educational value:
I think interpretability research is of moderately high educational value for thinking about
existential safety, because some research in this area is somewhat surprising in terms of
showing ways to maintain interpretability without sacriﬁcing much in the way of
performance.  This can change our expectations about how society can and should be
structured to maintain existential safety, by changing the degree of interpretability we can
and should expect from AI-heavy institutions and systems.
IntML neglect:
I think IntML is fairly neglected today relative to its value.  However, over the coming decade,
I think there will be opportunities for companies to speed up their development workﬂows by
improving the interpretability of systems to their developers.  In fact, I think for many
companies interpretability is going to be a crucial bottleneck for advancing their product
development.  These developments won't be my favorite applications of interpretability, and I
might eventually become less excited about contributions to interpretability if all of the work
seems oriented on commercial or militarized objectives instead of civic responsibilities.  But in
any case, I think getting involved with interpretability research today is a pretty robustly safe
and valuable career move for any up and coming AI researchers, especially if they do their
work with an eye toward existential safety.
IntML exemplars:
Recent exemplars of high value to existential safety:
(2015) Interpretable classiﬁers using rules and bayesian analysis: Building a better
stroke prediction model , Letham, Benjamin; Rudin, Cynthia; McCormick, Tyler H;
Madigan, David; others.
(2017) Towards a rigorous science of interpretable machine learning , Doshi-Velez,
Finale; Kim, Been.
(2018) The mythos of model interpretability , Lipton, Zachary C.
(2018) The building blocks of interpretability ,Olah, Chris; Satyanarayan, Arvind;
Johnson, Ian; Carter, Shan; Schubert, Ludwig; Ye, Katherine; Mordvintsev, Alexander.
(2019) Stop explaining black box machine learning models for high stakes decisions and
use interpretable models instead , Rudin, Cynthia.
(2019) This looks like that: deep learning for interpretable image recognition , Chen,
Chaofan; Li, Oscar; Tao, Daniel; Barnett, Alina; Rudin, Cynthia; Su, Jonathan K.
(2019) A study in Rashomon curves and volumes: A new perspective on generalization
and model simplicity in machine learning , Semenova, Lesia; Rudin, Cynthia; Parr,
Ronald.
Fairness in ML (FairML)

Existing
Research
Area
Social
Application
Helpfulness
to
Existential
Safety
Educational
Value
2015
Neglect
2020
Neglect
2030
Neglect
Fairness
in ML
Multie/Single 6/10
5/10
7/10
3/10
2/10
Fairness research in machine learning is typically concerned with altering or constraining
learning systems to make sure their decisions are "fair" according to a variety of deﬁnitions of
fairness.
FairML helpfulness to existential safety:
My hope for FairML as a ﬁeld contributing to existential safety is threefold:
1. (societal-scale thinking) Fairness comprises one or more human values that exist in
service of society as a whole, and which are currently diﬃcult to encode algorithmically,
especially in a form that will garner unchallenged consensus.  Getting more researchers
to think in the framing "How do I encode a value that will serve society as a whole in a
broadly agreeable way" is good for big-picture thinking and hence for society-scale
safety problems.
2. (social context awareness) FairML gets researchers to "take oﬀ their blinders" to the
complexity of society surrounding them and their inventions.  I think this trend is
gradually giving AI/ML researchers a greater sense of social and civic responsibility,
which I think reduces existential risk from AI/ML.
3. (sensitivity to unfair uses of power)  Simply put, it's unfair to place all of humanity at
risk without giving all of humanity a chance to weigh in on that risk.  More focus within
CS on fairness as a human value could help alleviate this risk.  Speciﬁcally, fairness
debates often trigger redistributions of resources in a more equitable manner, thus
working against the over-centralization of power within a given group.  I have some
hope that fairness considerations will work against the premature deployment of
powerful AI/ML systems that would lead to the hyper-centralizing power over the world
(and hence would pose acute global risks by being a single point of failure).
4. (Fulﬁlling and legitimizing governance demands) Fairness research can be used to fulﬁll
and legitimize AI governance demands, narrowing the gap between what policy makers
wish for and what technologists agree is possible.  This process makes AI as a ﬁeld more
amenable to governance, thereby improving existential safety.
FairML educational value:
I think FairML research is of moderate educational value for thinking about existential safety,
mainly via the opportunities it creates for thinking about the points in the section on
helpfulness above.  If the ﬁeld were more mature, I would assign it a higher educational
value.  
I should also ﬂag that most work in FairML has not been done with existential safety in mind. 
Thus, I'm very much hoping that more people who care about existential safety will learn
about FairML and begin thinking about how principles of fairness can be leveraged to ensure
societal-scale safety in the not-too-distant future.
FairML neglect:
FairML is not a particularly neglected area at the moment because there is a lot of excitement
about it, and I think it will continue to grow.  However, it was relatively neglected 5 years ago,
so there is still a lot of room for new ideas in the space.  Also, as mentioned, thinking in FairML
is not particularly oriented toward existential safety, so I think research on fairness in service
of societal-scale safety is quite neglected in my opinion.  

FairML exemplars:
Recent exemplars of high value to existential safety, mostly via attention to the problem of
diﬃcult-to-codify societal-scale values:
(2017) Inherent trade-oﬀs in the fair determination of risk scores , Kleinberg, Jon;
Mullainathan, Sendhil; Raghavan, Manish.
(2017) On fairness and calibration , Pleiss, Geoﬀ; Raghavan, Manish; Wu, Felix;
Kleinberg, Jon; Weinberger, Kilian Q.
(2018) Fairness and accountability design needs for algorithmic support in high-stakes
public sector decision-making , Veale, Michael; Van Kleek, Max; Binns, Reuben.
(2018) Delayed impact of fair machine learning ,Conitzer, Vincent; Freeman, Rupert;
Shah, Nisarg.
(2018) Fairness deﬁnitions explained , Verma, Sahil; Rubin, Julia.
(2019) Fairness and abstraction in sociotechnical systems , Selbst, Andrew D; Boyd,
Danah; Friedler, Sorelle A; Venkatasubramanian, Suresh; Vertesi, Janet.
Computational Social Choice (CSC)
Existing
Research
Area
Social
Application
Helpfulness
to
Existential
Safety
Educational
Value
2015
Neglect
2020
Neglect
2030
Neglect
Computational
Social Choice
Multi/Single
7/10
7/10
7/10
5/10
4/10
Computational social choice research is concerned with using algorithms to model and
implement group-level decisions using individual-scale information and behavior as inputs.  I
view CSC as a natural next step in the evolution of social choice theory that is more attentive
to the implementation details of both agents and their environments.  In my conception, CSC
comprises subservient topics in mechanism design and algorithmic game theory, even if
researchers in those areas don't consider themselves to be working in computational social
choice.
CSC helpfulness to existential risk:
In short, computational social choice research will be necessary to legitimize and fulﬁll
governance demands for technology companies (automated and human-run companies alike)
to ensure AI technologies are beneﬁcial to and controllable by human society.  The process of
succeeding or failing to legitimize such demands will lead to improving and reﬁning what I like
to call the algorithmic social contract: whatever broadly agreeable set of principles (if any)
algorithms are expected to obey in relation to human society.
In 2018, I considered writing an article drawing more attention to the importance of
developing an algorithmic social contract, but found this point had already been quite
eloquently by Iyad Rahwan in the following paper, which I highly recommend:
(2018) Society-in-the-loop: programming the algorithmic social contract, Rahwan, Iyad
Computational social choice methods in their current form are certainly far from providing
adequate and complete formulations of an algorithmic social contract.   See the following
article for arguments against tunnel-vision on computational social choice as a complete
solution to societal-scale AI ethics:
(2020) Social choice ethics in artiﬁcial intelligence , Baum, Seth D

Notwithstanding this concern, what follows is a somewhat detailed forecast of how I think
computational social choice research will still have a crucial role to play in developing the
algorithmic social contract throughout the development of individually-alignable
transformative AI technologies, which I'll call "the alignment revolution".
First, once technology companies begin to develop individually-alignable transformative AI
capabilities, there will be strong economic and social and political pressures for its developers
to sell those capabilities rather than hoarding them.  Speciﬁcally:
(economic pressure) Selling capabilities immediately garners resources in the form of
money and information from the purchasers and users of the capabilities;
(social pressure) Hoarding capabilities could be seen as anti-social relative to
distributing them more broadly through sales or free services;
(sociopolitical pressure) Selling capabilities allows society to become aware that those
capabilities exist, enabling a smoother transition to embracing those capabilities.  This
creates a broadly agreeable concrete moral argument against capability hoarding, which
could become politically relevant.
(political pressure) Political elites will be happier if technical elites "share" their
capabilities with the rest of the rest of the economy rather than hoarding them.
Second, for the above reasons, I expect individually-alignable transformative AI capabilities to
be distributed fairly broadly once they exist, creating an "alignment revolution" arising from
those capabilities.  (It's possible I'm wrong about this, and for that I reason I also welcome
research on how to align non-distributed alignment capabilities; that's just not where most of
my chips lie, and not where the rest of this argument will focus.)
Third, unless humanity collectively works very hard to maintain a degree of simplicity and
legibility in the overall structure of society*, this "alignment revolution" will greatly complexify
our environment to a point of much greater incomprehensibility and illegibility than even
today's world.  This, in turn, will impoverish humanity's collective ability to keep abreast of
important international developments, as well as our ability to hold the international economy
accountable for maintaining our happiness and existence.
(*Would-be-footnote: I have some reasons to believe that perhaps we can and should work
harder to make the global structure of society more legible and accountable to human
wellbeing, but that is a topic for another article.)
Fourth, in such a world, algorithms will be needed to hold the aggregate global behavior of
algorithms accountable to human wellbeing, because things will be happening too quickly for
humans to monitor.  In short, an "algorithmic government" will be needed to govern
"algorithmic society".  Some might argue this is not strictly unnecessary: in the absence of a
mathematically codiﬁed algorithmic social contract, humans could in principle coordinate to
cease or slow down the use of these powerful new alignment technologies, in order to give
ourselves more time to adjust to and govern their use.  However, for all our successes in
innovating laws and governments, I do not believe current human legal norms are quite
developed enough to stably manage a global economy empowered with individually-alignable
transformative AI capabilities.  
Fifth, I do think our current global legal norms are much better than what many  computer
scientists naively proﬀer as replacements for them.  My hope is that more resources and
inﬂuence will slowly ﬂow toward the areas of computer science most in touch with the
nuances and complexities of codifying important societal-scale values.  In my opinion, this
work is mostly concentrated in and around computational social choice, to some extent
mechanism design, and morally adjacent yet conceptually nascent areas of ML research such
as fairness and interpretability.  
While there is currently an increasing ﬂurry of (well-deserved) activity in fairness and
interpretability research, computational social choice is somewhat more mature, and has a lot
for these younger ﬁelds to learn from.  This is why I think CSC work is crucial to existential

safety: it is the area of computer science most tailored to evoke reﬂection on the global
structure of society, and the most mature in doing so.
So what does all this have to do with existential safety?  Unfortunately, while CSC is
signiﬁcantly more mature as a ﬁeld than interpretable ML or fair ML, it is still far from ready to
fulﬁll governance demand at the ever-increasing speed and scale needed to ensure existential
safety in the wake of individually-alignable transformative AI technologies.  Moreover, I think
punting these questions to future AI systems to solve for us is a terrible idea, because doing
so impoverishes our ability to sanity-check whether those AI systems are giving us reasonable
answers to our questions about social choice.  So, on the margin I think contributions to CSC
theory are highly valuable, especially by persons thinking about existential safety as the
objective of their research.
CSC educational value:
Learning about CSC is necessary for contributions to CSC, which I think are currently needed
to ensure existentially safe societal-scale norms for aligned AI systems to follow after "the
alignment revolution" if it happens.  So, I think CSC is highly valuable to learn about, with the
caveat that most work in CSC has not been done with existential safety in mind.  Thus, I'm
very much hoping that more people who care about existential safety will learn about and
begin contributing to CSC in ways that steer CSC toward issues of societal-scale safety.
CSC neglect:
As mentioned above, I think CSC is still far from ready to fulﬁll governance demands at the
ever-increasing speed and scale that will be needed to ensure existential safety in the wake of
"the alignment revolution".  That said, I do think over the next 10 years CSC will become both
more imminently necessary and more popular, as more pressure falls upon technology
companies to make societal-scale decisions.  CSC will become still more necessary and
popular as more humans and human institutions become augmented with powerful aligned AI
capabilities that might "change the game" that our civilization is playing.  I expect such
advancements to raise increasingly deep and urgent questions about the principles on which
our civilization is built, that will need technical answers in order to be fully resolved in ways
that maintain existential safety.
CSC exemplars:
CSC exemplars of particular value and relevance to existential safety, mostly via their
attention to formalisms for how to structure societal-scale decisions:
(2014) Dynamic social choice with evolving preferences , Parkes, David C; Procaccia,
Ariel D.
(2016) Handbook of computational social choice , Brandt, Felix; Conitzer, Vincent;
Endriss, Ulle; Lang, Jerome; Procaccia, Ariel D.
(2016) The revelation principle for mechanism design with reporting costs , Kephart,
Andrew; Conitzer, Vincent,
(2016) Barriers to Manipulation in Voting , Conitzer, Vincent; Walsh, Toby
(2016) Proportional justiﬁed representation , Sanchez-Fernandez, Luis; Elkind, Edith;
Lackner, Martin; Fernandez, Norberto; Fisteus, Jesus A; Val, Pablo Basanta; Skowron,
Piotr.
(2017) Fair public decision making , Conitzer, Vincent; Freeman, Rupert; Shah, Nisarg.
(2017) Fair social choice in dynamic settings , Freeman, Rupert; Zahedi, Seyed Majid;
Conitzer, Vincent.
(2017) Justiﬁed representation in approval-based committee voting , Aziz, Haris; Brill,
Markus; Conitzer, Vincent; Elkind, Edith; Freeman, Rupert; Walsh, Toby.
(2020) Preference elicitation for participatory budgeting , Benade, Gerdus; Nath,
Swaprava; Procaccia, Ariel D; Shah, Nisarg.
(2020) Almost envy-freeness with general valuations , Plaut, Benjamin; Roughgarden,
Tim.

Accountability in ML (AccML)
Existing
Research
Area
Social
Application
Helpfulness
to
Existential
Safety
Educational
Value
 2015
Neglect
 2020
Neglect
 2030
Neglect
Accountability
in ML
Multi/Multi
8/10
3/10
8/10
7/10
5/10
Accountability (AccML) is aimed at making it easier to hold persons or institutions accountable
for the eﬀects of ML systems.  Accountability depends on transparency and explainability for
evaluating the principles by which a harm or mistake occurs, but it is not subsumed by these
objectives.  
AccML helpfulness to existential safety:
The relevance of accountability to existential safety is mainly via the principle of
accountability gaining more traction in governing the technology industry.  In summary, the
high level points I believe in this area are the following, which are argued for in more detail
after the list:
1. Tech companies are currently "black boxes" to outside society, in that they can develop
and implement (almost) whatever they want within the conﬁnes of privately owned
laboratories (and other "secure" systems), and some of the things they develop or
implement in private settings could pose signiﬁcant harms to society.
2. Soon (or already), society needs to become less permissive of tech companies
developing highly potent algorithms, even in settings that would currently be considered
"private", similar to the way we treat pharmaceutical companies developing highly
potent biological specimens.
3. Points #1 and #2 mirror the way in which ML systems themselves are black boxes even
to their creators, which fortunately is making some ML researchers uncomfortable
enough to start holding conferences on accountability in ML.
4. More researchers getting involved in the task of deﬁning and monitoring accountability
can help tech company employees and regulators to reﬂect on the principle of
accountability and whether tech companies themselves should be more subject to it at
various scales (e.g., their software should be more accountable to its users and
developers, their developers and users should be more accountable to the public, their
executives should be more accountable to governments and civic society, etc.).
5. In futures where transformative AI technology is used to provide widespread services to
many agents simultaneously (e.g., "Comprehensive AI services" scenarios), progress on
deﬁning and monitoring accountability can help "infuse" those services with a greater
degree of accountability and hence safety to the rest of the world.
What follows is my narrative for how and why I believe the ﬁve points above.
At present, society is structured such that it is possible for a technology company to amass a
huge amount of data and computing resources, and as long as their activities are kept
"private", they are free to use those resources to experiment with developing potentially
misaligned and highly potent AI technologies.  For instance, if a tech company tomorrow
develops any of the following potentially highly potent technologies within a privately owned
ML lab, there are no publicly mandated regulations regarding how they should handle or
experiment with them:
misaligned superintelligences
fake news generators

powerful human behavior prediction and control tools
... any algorithm whatsoever
Moreover, there are virtually no publicly mandated regulations against knowingly or
intentionally or developing any of these artifacts within the conﬁnes of a privately owned lab,
despite the fact that the mere existence of such an artifact poses a threat to society.  This is
the sense in which tech companies are "black boxes" to society, and potentially harmful as
such.
(That's point #1.)
Contrast this situation with the strict guidelines that pharmaceutical companies are required
to adhere to in their management of pathogens.  First, it is simply illegal for most companies
to knowingly develop synthetic viruses, unless they are certiﬁed to do so by demonstrating a
certain capacity for safe handling of the resulting artifacts.  Second, conditional on having
been authorized to develop viruses, companies are required to follow standardized safety
protocols.  Third, companies are subject to third-party audits to ensure compliance with these
safety protocols, and are not simply trusted to follow them without question.
Nothing like this is true in the tech industry, because historically, algorithms have been
viewed as less potent societal-scale risks than viruses.  Indeed, present-day accountability
norms in tech would allow an arbitrary level of disparity to develop between
the potency (in terms of potential impact) of algorithms developed in privately owned
laboratories, and
the preparedness of the rest of society to handle those impacts if the algorithms were
released (such as by accident, harmful intent, or poor judgement).
This is a mistake, and an increasingly untenable position as the power of AI and ML
technology increases.  In particular, a number of technology companies are intentionally
trying to build artiﬁcial general intelligence, an artifact which, if released, would be much
more potent than most viruses.  These companies do in fact have safety researchers working
internally to think about how to be safe and whether to release things.  But contrast this again
with pharmaceuticals.  It just won't ﬂy for a pharmaceutical company to say "Don't worry, we
don't plan to release it; we'll just make up our own rules for how to be privately safe with it.". 
Eventually, we should probably stop accepting this position from tech companies at well.
(That's point #2.)
Fortunately, even some researchers and developers are starting to become uncomfortable
with "black boxes" playing important and consequential roles in society, as evidenced by the
recent increase in attention on both accountability and interpretability in service of it, for
instance:
(2016) Accountability in algorithmic decision making , Diakopoulos, Nicholas.
(2017) Accountability of AI under the law: The role of explanation , Doshi-Velez, Finale;
Kortz, Mason; Budish, Ryan; Bavitz, Chris; Gershman, Sam; O'Brien, David; Schieber,
Stuart; Waldo, James; Weinberger, David; Wood, Alexandra.
(2018) It's time to do something: Mitigating the negative impacts of computing through
a change to the peer review process, Hecht, Brent; Wilcox, Lauren; Bigham, Jeﬀrey P;
Schoning, Johannes; Hoque, Ehsan; Ernst, Jason; Bisk, Yonatan; Yarosh, Lana; Amjam,
Bushra; Wu, Cathy.
This kind of discomfort both fuels and is fueled by decreasing levels of blind faith in the
beneﬁts of technology in general.  Signs of this broader trend include:
(2020) NeurIPS 2020 FAQ --- includes references to Hecht et al (2018) , Hecht (2020) ,
and GovAI (2020) for writing about the potentially negative impacts of AI technology.
(2020) NSF America's Seed Fund Technology Topic: AI , Atheron, Peter.

(2020) The Social Dilemma , a NetFlix documentary by Jeﬀ Orlowski, Davis Coombe, and
Vickie Curtis.
Together, these trends indicate a decreasing level of blind faith in the addition of novel
technologies to society, both in the form of black-box tech products, and black-box tech
companies.  
(That's point #3.)
The European General Data Protection Regulation (GDPR) is a very good step for regulating
how tech companies relate with the public.  I say this knowing that GDPR is far from perfect. 
The reason it's still extremely valuable is that it has initialized the variable deﬁning
humanity's collective bargaining position (at least within Europe, and replicated to some
extent by the CCPA) for controlling how tech companies use data.  That variable can now be
amended and hence improved upon without ﬁrst having to ask the question "Are we even
going to try to regulate how tech companies use data?"  For a while, it wasn't clear any action
would ever be taken on this front, outside of speciﬁc domains like healthcare and ﬁnance.
However, while GDPR has deﬁned a slope for regulating the use of data, we also need
accountability for private uses of computing.  As AlphaZero demonstrates, data-free
computing alone is suﬃcient to develop super-human strategic competence in a well-speciﬁed
domain.
When will it be time to disallow arbitrary private uses of computing resources, irrespective of
its data sources?  Is it time already?  My opinions on this are outside the scope of what I
intend to argue for in this post.  But whenever the time comes to develop and enforce such
accountability, it will probably be easier to do that if researchers and developers have spent
more time thinking about what accountability is, what purposes are served by various
versions of accountability, and how to achieve those kinds of accountability in both fully-
automated and semi-automated systems.  In other words, optimistically, more technical
research on accountability in ML might result in more ML researchers transferring their
awareness that «black box tech products are insuﬃciently accountable» to become more
aware/convinced that «black box tech companies are insuﬃciently accountable».
(That's point #4.)
But even if that transfer of awareness doesn't happen, automated approaches to
accountability will still have a role to play if we end up in a future with large numbers of
agents making use of AI-mediated services, such as in the "Comprehensive AI Services"
model of the future.  Speciﬁcally, 
individual actors in a CAIS economy should be accountable to the principle of not
privately developing highly potent technologies without adhering to publicly legitimized
and auditable safety procedures, and
systems for reﬂecting on and updating accountability structures can be used to detect
and remediate problematic behaviors in multi-agent systems, including behaviors that
could yield existential risks from distributed systems (e.g., extreme resource
consumption or pollution eﬀects).
(That's point #5)
AccML educational value:
Unfortunately, technical work in this area is highly undeveloped, which is why I have assigned
this area a relatively low educational value.  I hope this does not trigger people to avoid
contributing to it.
AccML neglect:

Correspondingly, this area is highly neglected relative to where I'd like it to be, on top of being
very small in terms of the amount of technical work at its core.
AccML exemplars:
Recent examples of writing in AccML that I think are of particular value to existential safety
include:
(2017) Accountability of AI under the law: The role of explanation , Doshi-Velez, Finale;
Kortz, Mason; Budish, Ryan; Bavitz, Chris; Gershman, Sam; O'Brien, David; Schieber,
Stuart; Waldo, James; Weinberger, David; Wood, Alexandra.
(2017) Value Alignment or Misalignment--What Will Keep Systems Accountable? , Arnold,
Thomas; Kasenberg, Daniel; Scheutz, Matthias.
(2018) Towards formal deﬁnitions of blameworthiness, intention, and moral
responsibility, Halpern, Joseph Y; Kleiman-Weiner, Max.*
* Note: I don't currently agree with the deﬁnitions of blameworthiness, intention, and
responsibility in this paper, but I am glad to see people working toward agreeable
deﬁnitions of these concepts, and I like that the title begins with "toward".
(2018) Trends and trajectories for explainable, accountable and intelligible systems: An
HCI research agenda , Abdul, Ashraf; Vermeulen, Jo; Wang, Danding; Lim, Brian Y;
Kankanhalli, Mohan.
(2019) Policy certiﬁcates: Towards accountable reinforcement learning , Dann,
Christoph; Li, Lihong; Wei, Wei; Brunskill, Emma.
Conclusion
Thanks for reading!  I hope this post has been helpful to your thinking about the value of a
variety of research areas for existential safety, or at the very least, your model of my
thinking.  As a reminder, these opinions are my own, and are not intended to represent any
institution of which I am a part.
Reﬂections on scope & omissions
This post has been about:
Research, not individuals. Some readers might be interested in the question "What
about so-and-so's work at such-and-such institution?"  I think that's a fair question, but I
prefer this post to be about ideas, not individual people.  The reason is that I want to say
both positive and negative things about each area, whereas I'm not prepared to write up
public statements of positive and negative judgements about people (e.g., "Such-and-
such is not going to succeed in their approach", or "So-and-so seems fundamentally
misguided about X".)
Areas, not directions. This post is an appraisal of active areas of research—topics with
groups of people already working on them writing up their ﬁndings.  It's primarily not an
appraisal of potential directions—ways I think areas of research could change or be
signiﬁcantly improved (although I do sometimes comment on directions I'd like to see
each area taking).  For instance, I think intent alignment is an interesting topic, but the
current paucity of publicly available technical writing on it makes it diﬃcult to critique. 
As such, I think of intent alignment as a "direction" that AI alignment research could be
taken in, rather than an "area".
Papers, not legislation, books, or TV shows.  Many intellectual artifacts aside from
research papers matter to existential safety, including legislation, as well as ﬁction and
non-ﬁction books, TV shows, and movies.  Such works are not beyond the scope of my
opinions, but are beyond this scope of this post.

Luna Lovegood and the Chamber of
Secrets - Part 1
Luna Lovegood walked through the barrier between Platforms Nine and Ten to
Platform Nine and Three-Quarters. Luna wondered what happened to Platform Nine
and One-Half. Numbers like "three quarters" only appear when you divide an integer
in half twice in a row.
Luna looked around for someone who might know the answer and spied a unicorn.
She wore clothes, walked on two feet and had curly brown hair. None of that fooled
Luna. The unicorn radiated peace and her ﬁngernails were made out of alicorn.
"What happened to Platform Nine and One-Half?" Luna asked the unicorn.
"There is no Platform Nine and One-Half," the unicorn replied.
"How do you know?" Luna asked.
"It would have been in Hogwarts: A History," the unicorn replied, "nor is there mention
of a Platform Nine and One-Half in Modern Magical History, Important Modern Magical
Discoveries, or any other book in the Hogwarts library. There is only a Platform Nine
and Three Quarters."
"What about Platform Nine and Seven Eighths?" Luna asked.
"There is no Platform Nine and Seven Eights either." The unicorn turned around and
walked away before Luna could ask "How do you know?"
If Platform Nine and Three Quarters does not appear in Muggle libraries then Platform
Nine and One-Half is unlikely to appear in wizard libraries, except for double-witches'
libraries. The Hogwarts library is not a double-witch library.
"How are you?" a Weasley-headed ﬁrst-year girl asked Luna.
"I'm trying to ﬁnd Platform Nine and One-Half. The unicorn told me it doesn't exist. If it
does exist then it must be hidden by powerful magics. How are you?" said Luna.
"What unicorn?" the ﬁrst-year girl asked.
"That one, right there," Luna said, pointing.
The girl invented an excuse to end the conversation.
Luna didn't know how to make friends. She had a vague idea that as a ﬁrst-year, the
Hogwarts Express was a one-time opportunity to do so. She wore a necklace she had
painted herself which nobody else seemed to notice. She had brought kettle of home-
brewed Comed-Tea, but it had got her jeered out of a compartment.
Nobody was interested in the troll she had smelled at Platform Nine and Three
Quarters or her discovery of a lich in the second year or that the Chamber of Secrets

had been opened or any of Dark Lord Harry Potter's various plots. The other ﬁrst-years
seemed unfocused and confused.
Confused....
Wrackspurts are invisible creatures that ﬂoat into your ears and make your brain go
fuzzy. The train could be full of them. They could be ﬂoating into her ears right now.
Luna stuck her index ﬁngers in her ears to block the possible Wrackspurts. The ﬁrst-
years in the nearby compartment looked at Luna as if she were already unpopular.
Wrackspurts are cognitohazardous which means they mess with your thoughts. Luna
learned all about Wrackspurts and other cognitohazards in her work on The Quibbler.
The most important thing about cognitohazards is to check yourself regularly and
ﬁgure out if you've already been aﬀected by one.
Luna observed her own mind. Fuzzy? No. Unfocused? No. Confused? No. Wrackspurts
had not yet entered her brain. (Unless it already had and was inhibiting her meta-
cognition—but she didn't think that was happening.) Luna observed the other
students. Maybe they were infected by Wrackspurts or maybe they were behaving
normally. It was hard to tell without a Wrackspurt-free baseline to compare them to.
Before she could unplug her ears, Luna had to ﬁgure out if there were Wrackspurts
roaming the train. But Wrackspurts are invisible. How can she test whether such a
thing exists?
Wrackspurts are attracted to people so the safest place to go would be an empty
compartment. Smaller would be better since that would decrease the likelihood of a
Wrackspurt having drifted in randomly. Luna walked past cabin after cabin with her
ﬁngers in her ears. Eventually she found a suitable compartment, boring and
cramped. She shut the door with her knee and then unplugged her ears. She counted
to eighty-nine and then observed her mind again. Still no Wrackspurt symptoms.
She had hoped Hogwarts would be an opportunity to make friends. But the other girls
and boys her age all seemed wrapped up in amassing power by forming alliances.
Even the Muggle-borns were more interested in visible charms like chocolate frogs
than invisible creatures like Wrackspurts.
Luna wondered if she was the most invisible ﬁrst-year in the school. No other
compartments had fewer than three students. Presumably, everyone else in ﬁrst-year
was making friends or already had some.
It could be worse. Luna could have a curse that made her unpopular or made people
forget who she was. Muggles had a hard time seeing witches and wizards. If double-
witches were real then it would be hard for witches to see them just like it's hard for
Muggles to see witches. Luna's eyes would drift from one side of the double-witch to
the other without noticing the witch in front of her.
Luna looked around her cramped compartment. To her left, in the opposite cabin, four
girls were pointing at her and laughing again. Out the window, to her right, the
countryside drifted by. Luna was getting distracted.
Luna stuﬀed her ﬁngers back into her ears. The left index ﬁnger went right in. The
right index ﬁnger didn't.

Luna covered her right ear and then gently squeezed the Wrackspurt into her right
hand. From there, she guided the Wrackspurt into an empty Comed-Tea can and
wrapped her scarf over the opening.
Luna wondered where the Wrackspurt could have come from. It had to have been
there before she shut the door. But the Wrackspurt had not immediately entered
Luna's ears when she had ﬁrst entered the compartment. There may not have been
any free Wrackspurts in the compartment when Luna had entered it. Luna's trapped
Wrackspurt must have been in someone else's ear.
But who? Luna's eyes slid from one end of her compartment to the other, as if nothing
important existed in-between.

Where do (did?) stable, cooperative
institutions come from?
The United States has a bunch of nice things whose creation/maintenance requires
coordinated eﬀort from a large number of people across time. For example: bridges
that stay up; electrical grids that provide us with power; the rule of law; newspapers
that make it easier to keep tabs on recent events; ﬁre ﬁghting services that stop most
ﬁres in urban areas; roads; many functioning academic ﬁelds; Google; Amazon;
grocery stores; the postal service; and so on.
The ﬁrst question I'd like to pose is: how does this coordination work? What keeps
these large sets of people pulling in a common direction (and wanting to pull in a
common direction)? And what keeps that "common direction" grounded enough that
an actual nice thing results from the pulling (e.g., what causes it to be that you get a
working railway system, rather than a bunch of tracks that don't quite work? what
causes you to sometimes get a functioning ﬁeld of inquiry and not a cargo cult)? Is it
that:
Many people independently value the nice thing, and they altruistically decide to
put their own eﬀorts toward creating/maintaining the nice thing? (E.g., some
large set of people wishes there were good ﬁre-ﬁghting institutions, and so each
of them altruistically and independently decides to found a ﬁre-ﬁghting branch,
to work at that branch, to tweak that branch's habits into a more eﬀective
conﬁguration, etc.?)
A small number of rich and powerful people (who are somehow also
knowledgeable about institution design) value the nice thing, and they
altruistically decide to set up incentives such that other people, purely via self-
interest, will do the work that is needed to create/maintain the nice thing? (E.g.,
a small number of people altruistically donate to ﬁre-ﬁghting groups and set up
incentives at those groups, and then other people do the ﬁre-ﬁghting work
because they want a job?)
Something else?
One reason I'd like to pose this question is that it seems plausible to me that the
magic that used to enable such cooperative institutions is fading. If so, it seems useful
to know about that fading for quite a variety of reasons.
My own lead candidate answer to "what is the magic that lets these cooperative
institutions run?" is this:
Somehow, people have sometimes known how to craft "institutional cultures" that
aligned an individual's desire for (glory/$/prestige/etc.) with the actions that will allow
the institution as a whole to acquire redistributable (glory, $, prestige, etc.) in the long
run. More speciﬁcally, cooperative institutions arise in cases where some set of
designers (either a few people, or a larger distributed set) magically manage several
things at once:
0. There is an institutional culture that is distinct from the formal workings of the
institution, but that exists alongside it, helping to animate it. For example,
alongside the formal workings of the old NYT (the printing presses, newspaper

subscriptions, staﬀ payroll, explicit assignments, etc.) there was an ethic of
journalism that helped direct staﬀ actions at many junctures (an ethic of e.g. "all
the news that's ﬁt to print," putting in shoe-leather, protecting one's sources,
etc.).
1. The installed "institutional culture" is pretty good at picking out actions that, if
taken, will tend to cause the institution as a whole to gain redistributable
(glory/$/prestige/etc.) in the long-term. In our example: The old NYT will in fact
gain more long-run prestige, customers, incoming staﬀ talent, etc. if it follows its
journalistic ethics. In other words, the culture gestured at by ""all the news that's
ﬁt to print," putting in shoe-leather, protecting one's sources, etc." oﬀered pretty
good on-the-ground answers to the question "What can I do now, as an NYT
reporter/manager/editor/etc., that will most improve the NYT's long-term
standing?"
2. The installed "institutional culture" both teaches people how to detect which
staﬀ members do/don't have that same culture, and prompts people to
diﬀerentially reward/punish (and promote/ﬁre) staﬀ members who do/don't have
that same culture.
3. Via 2), an individual staﬀ member will be able to succeed best on personal goals
(in terms of some combination of $, prestige, being thought attractive by
potential mates, etc.) via following the institutional culture.
I am curious whether this 0-3 account of how stable, cooperative institutions work
seems right to you guys (or whether there are caveats, or errors, or important
omissions. I'd really like an accurate model here).
Separately (but relatedly - if the above account is importantly wrong, I'll probably be
wrong about this too) - I would like to pose a second question: Is it getting harder to
create stable, cooperative institutions in the above sense? If so, why/how?
Some evidence that it is getting harder:
Governmental institutions: There seems to be some degree of institutional
failure (mild-ish, so far) in a number of American and especially Californian
institutions: California's electricity is less reliable than it used to be, due
basically to bad governance. San Francisco, especially, is seeing rising crime,
due more or less to decriminalizing a lot of crime. Many aspects of the covid-19
response also cast our institutions in a worse light than I'd previously
anticipated, though it is plausible (given my ignorance) that my anticipations
were the silly thing here and that we would not in fact have done better in
previous eras. (I'm thinking here of: America being slower than I'd anticipated re:
acquiring testing and PPE; putting very little money in the extensive stimulus bill
to reducing covid via testing/research/etc.; America staying in semi-lockdown for
an extended time instead of trying harder either to head toward actual zero (via
border control, testing + tracing, etc.) so that we could relax again, or toward
something more like herd immunity (while metering it out; but it seems to me
that as a country we probably lost more to the costs many parts of America
seeming not to lock down for extended periods of time without a plan to use that
time to do anything constructive, and without (I think?) adequate accounting for
what that would cost in terms of social stability and mental health.)

Non-governmental parts of our national sense-making apparatus: Most brand
names, e.g. the NYT, Harvard, Science and Nature magazines, the Democrats,
the Republicans, the police, the CDC, etc. seem less well-regarded than they
used to be. I can't think of many brands of any sort that are instead better-
regarded (Amazon, SpaceX and bitcoin, probably).
Subcultures: David Chapman claims that subcultures are much harder to form
now / more or less don't exist anymore. I have also tried to look myself, and this
matches my own experience: rationality and EA seem among the few things that
are sort-of here, and even we are only sort-of here, I think. ("The rationalist
diaspora," not "the happening applied rationality scene.") (I can think of some
others, e.g. the authentic relating / circling communities; some other parts of the
Thiel-o-sphere; maybe the group at the Stoa; surely some others. But... fewer
than I would have expected, and I think fewer than I would have found in past
decades?)
California/ the blue coasts have historically been a place where trends originate,
then hit the US as a whole, then also the rest of the West. So I'm not sure how
local this stuﬀ is or isn't right now, but I'm worried regardless.
All of this is disputable. And I would love to see your disputes. Even more so, I would
love to see your unjustiﬁable stab-in-the-dark intuitions as to where the center of all
this is. From my perspective, the diﬃculty we are having lately in forming/sustaining
institutional cultures (especially, ones adequate to get much done) seems like one of
the central canaries in a puzzle that I badly need to fathom. I'll put my own
hypotheses in the comments.
Acknowledgments: Thanks to lots of folks at Sunday's town hall discussion for relevant
remarks (no fault to them for my errors), and to Ben Hoﬀman for his essay "Bob the
Builder, and the Neo-Puritan Deal."

Working in Virtual Reality: A Review
For the past three days, I started experimenting with working in Virtual Reality. I'm quite
impressed. My guess is that it's not good for most people yet, but that 1 to 10% of people
reading this would gain a 2 to 20% increase in computer productivity by using a VR working
setup. The upper end is for people who get distracted easily or have a diﬃcult time with
SAD.
This feels like the most radical experiment I've made to my setup so far, so I'm quite happy
with how it's worked out. I've used to dream of similar setups and it's really cool that the
technology is basically there. I've given demos to a few people in my house who haven't
been close to VR and their responses varied from fairly impressed to incredibly impressed. 
I'm fairly convinced that there's an extremely promising future for work in VR. The VR
ecosystem seems to be improving much more quickly than the alternatives. It strikes me as
surprisingly possible that within 2 to 5 years, VR work setups will be the generally
recommended work setups, at least for "people in the know."[1] This could both lead to
direct improvements and lead the way for radical rethinkings of what work setups are
possible.
My Setup
My speciﬁc setup is an Oculus Quest 2 ($300), a 2016 Macbook Pro, and the application
Immersed VR. Immersed works using WiFi. My router is around 15 feet away from my
headset, and my computer is connected directly to the router via Ethernet. In the app I use
two "monitors"; I downscale a 4K monitor to 2048x1280 and use a side monitor of
1920x1080. It's suggested to keep resolutions rather low both because the Oculus Quest 2
doesn't itself have a high resolution (1832×1920 per eye), and because higher resolution
means higher latency. You can have up to ﬁve "virtual" monitors with Immersed, but I prefer
one or two big ones.

This is me editing this post. My setup is pretty simple when writing. I have a
second screen on the right, but I'm not using it at the moment. I typically have
the main screen a bit closer to me, but zoomed it out to make this image more
interesting.

Me browsing LessWrong in the Spaceship area.
I think I used this setup for around 5 working hours on Wednesday, 6 on Thursday, and
maybe 2 so far today (but it's still early). It didn't seem to get particularly tiring over that
time.
I've been getting latency of around 5ms to 15ms, but every minute or so there are some
frustrating 1-5 second hiccups. It's possible to watch videos but I have seen large decreases
in frame rate from time to time. They have instructions about using WiFi direct to make
things smoother. I've ordered the necessary module (it's around $25) and should be getting
it shortly.
I'm not sure how long I'm going to continue using it. I ﬁnd the Quest a bit uncomfortable to
wear for long periods and sometimes a bit tiring for my eyes. I'm going to continue tinkering
to try to make it better.
Beneﬁts

Focus
I have a roommate now and ﬁnd visual stimuli distracting. I'm also in a room that's a bit of a
mess. I like having a lot of things (a lot of small experiments), and that makes it diﬃcult to
have a clean workspace.
VR setups can isolate away everything that's not the monitors. There's an option to see a
keyboard, but I don't use it (I recommend spending eﬀort to not need to). There's a handful
of decent virtual room options. On Immersed there seems to be a few that prominently
feature space and space travel.
Light / SAD
LessWrong now has a full tag on lighting, with 6 popular posts on the topic. I've been
considering setting up a system myself. 
I'm not sure how to best measure the amount of light experienced in VR vs. the sun, but
things seem relatively bright to me with the Quest. VR glasses use curved lenses and a dark
environment to focus the LCD light on your eyes, unlike regular monitors that are meant to
be visible at any angle. So even a relatively VR small screen can produce more eye-lumens
than something much larger. I recently purchased a 350nit 4k monitor and found that that
hasn't been quite enough for some parts of the day. With the VR headset, I often turn the
monitor brightness down. 
The only thread I could ﬁnd on the topic was this one on Reddit, but it doesn't seem that
great to me. I found this beginning of a scientiﬁc study on "VR for Seasonal Aﬀective
Disorder", but no completed version. I'd hypothesize that living mostly in VR would have
some signiﬁcant beneﬁts for some people with signiﬁcant SAD (if you're in VR, how does it
even matter what the season is?), though I could imagine that it has some downsides too.
Ergonomics
VR headsets can be a bit heavy, but besides that can be highly ergonomic. In virtual
environments you can conﬁgure screens to be anywhere you want them. I have a decent
monitor arm that I ﬁnd decently suboptimal. I often have a hard time bringing my monitor
just where I want, so move my neck to compensate (a bad idea!). It can also be fairly shaky
when my desk is in standing mode. In VR I can easily position and reposition my monitors
exactly where I want them in the sizes I want them, it's great.
I've previously thought about trying to work while laying down, when my back was
particularly sore. There are some intense $6k++ setups for this, and gerry-rigging solutions
can be quite awkward. With a VR headset you still would need some solution to position the
keyboard, but the monitor issue is of course dramatically simpliﬁed. I tried reading a bit while
lying down and it worked ﬁne.
Portability
One of the worst things about monitors is that they are a pain to transport. They're quite
large and heavy, and I've had a sequence of bad luck moving them without causing at least
some considerable damage. The way things are going, with a VR headset, you could have a
stellar setup anywhere at all, which is unheard of. Maybe outdoor setups on warm days
would be possible, though of course, you'd have to replace the visuals with some similar or
superior theme on your device (You'd still get the sounds, sent, and breeze.) Perhaps at
some point laptops will forgo the screens, or maybe all the hardware will be in the headset
and you'd carry a separate mouse keyboard combo.
Coworking

I haven't tried this yet, but apparently, you can cowork with Immersed. I believe you get the
beneﬁt(?) of being able to see the screens of other coworkers. The options are quite
conﬁgurable depending on the program.
Coworking in VR has the obvious beneﬁt of allowing people to live anywhere, but also the
obvious cost of not being able to see people's faces. In Immersed there is one feature where
you can have a "digital webcam" that uses an avatar of you in a format that's accessible for
online video chats in Google Meet and similar. It's neat but faily basic. 
Facebook has an impressive demo of Photorealistic Face Tracked Avatars, but I imagine it
won't be released for a while.
Negatives
Resolution & Latency
As mentioned, the resolution is rather poor compared to modern monitors. The latency is
signiﬁcantly worse, though Wiﬁ direct should help, and Windows setups with direct

connections should be ﬁne. This seems quite bad for high-bandwidth tasks like video editing
or video games, but useable for typing and a lot of coding.
Discomfort
VR headsets are still a bit uncomfortable to wear for long periods. I imagine this will improve
a lot over time. I think that future prototypes look a lot like sunglasses. Apple apparently is
getting into the space, so I imagine their take will be particularly lightweight. 
Facebook (Quest only)
The Quest 2 requires Facebook login and the operating system is heavily integrated with
Facebook. To share a screenshot of my in-game setup I actually had to post it to my
Facebook wall, then copy and paste that image. In general the on-system OS is useable but
quite basic.
Other Discussions
There are a few neat videos of people showing oﬀ their VR oﬃce setups:
This one is a nice overview of Immersed, though it's about a year old.
This video shows oﬀ the Immersed webcam feature.
This one shows oﬀ Virtual Desktop with a wired connection.
Facebook is working on "Inﬁnite Oﬃce" which seems interesting but isn't yet available.
It at least demonstrates their optimism and dedication to the area. It's pretty easy for
me to imagine it being better than Immersed after it launches.
Here's a discussion of someone who didn't ﬁnd working in VR particularly usable, in part
because they needed to see the keyboard and apparently had a lot of in-person distractions. 
The Immersed Blog is interesting, though short and biased. They claim that their team works
for 8+ hours a day in VR, and point out that apparently, some users reported using VR to
eﬀectively live in diﬀerent time zones. 
There's an Immersed Discord and it has most of the discussion I've seen from actual users.
The setup is highly biased to favor positive messages, but there is a long list of very
enthusiastic users. Generally, people are most positive about the focus beneﬁts and the use
of extra monitors. There seems to be almost no discussion from users who have used it for
collaboration; most have used it solo.
 
Conclusion
Working in VR is clearly in its "early days", but it's deﬁnitely happening. There seem to be at
least dozens of people working full-time in VR at this point, most have started in the last ~2
years.  The technology is already quite inexpensive and useable. The advantages going
forward are numerous and signiﬁcant. 
I'd expect the VR headsets coming out this next year to continue to get better, so waiting a
while is a safe option. But I suggest keeping an eye out and planning accordingly. If you've
been thinking about buying a fancy monitor setup or SAD light setup, you might want to
reevaluate.
 
[1] By this, I mean what I and many smart startups would recommend. Often very good ideas
take a long time to become popular. Popularity seems harder to predict than quality.

the scaling "inconsistency": openAI's
new insight
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
I've now read the new OpenAI scaling laws paper.  Also, yesterday I attended a fun and
informative lecture/discussion with one of the authors.
While the topic is on my mind, I should probably jot down some of my thoughts.
This post is mostly about what the new paper says about the "inconsistency" brought up in
their previous paper.
The new paper has a new argument on this topic, which is intuitive and appealing, and
suggests that the current scaling trend will indeed "switch over" soon to a new one where
dataset size, not model size, is the active constraint on performance.  Most of this post is an
attempt to explain and better understand this argument.
——
The new paper is mainly about extending the scaling laws from their earlier paper to new
modalities.
In that paper, they found scaling laws for transformers trained autoregressively on text data. 
The new paper ﬁnds the same patterns in the scaling behavior of transformers
trained autoregressively on images, math problems, etc.
So the laws aren't telling us something about the distribution of text data, but about
something more fundamental.  That's cool.
They also have a new, very intuitive hypothesis for what's going on with the "scaling
inconsistency" they described in the previous paper - the one I made a big deal about at the
time.  So that's the part I'm most excited to discuss.
I'm going to give a long explanation of it, way longer than the relevant part of their paper. 
Some of this is original to me, all errors are mine, all the usual caveats.
——
1. L(C) and L(D)
To recap: the "inconsistency" is between two scaling laws:
The law for the best you can do, given a ﬁxed compute budget.
This is L(C), sometimes called L(C_min).  L is the loss (lower = better), C is your
compute budget.
The law for the best you can do, given a ﬁxed dataset size.
This is L(D), where D is the number of examples (say, tokens) in the dataset.
Once you reach a certain level of compute, these two laws contradict each other.

I'll take some time to unpack that here, as it's not immediately obvious the two can even be
compared to one another - one is a function of compute, the other of data.
2. C sets E, and E bounds D
Budget tradeoﬀs
Given a compute budget C, you can derive the optimal way to spend it on diﬀerent things. 
Roughly, you are trading oﬀ between two ways to spend compute:
Use C to buy "N": Training a bigger model - "N" here is model size
Use C to buy "S": Training for more steps "S" (gradient updates)
The relationship between S (steps) and D (dataset size) is a little subtle, for several reasons.
From step count to update count
For one thing, each single "step" is an update on the information from more than one data
point.  Speciﬁcally, a step updates on "B" diﬀerent points - B is the batch size.
So the total number of data points processed during training is B times S.  The papers
sometimes call this quantity "E" (number of examples), so I'll call it that too.
From update count to data count
Now, when you train an ML model, you usually update on each data point more than once. 
Typically, you'll do one pass over the full dataset (updating on each point as you go along),
then you'll go back and do a second full pass, and then a third, etc.  These passes are
called "epochs."
If you're doing things this way, then for every point in the data, you get (number of epochs)
updates out of it.  So
E = (number of epochs) * D.  
Some training routines don't visit every point the exact same number of times - there's
nothing forcing you to do that.  Still, for any training procedure, we can look at the quantity E
/ D.
This would be the number of epochs, if you're doing epochs.  For a generic training routine,
you can can think of E / D as the "eﬀective number of epochs": the average number of times
we visit each point, which may not be an integer.
Generally, E ≠ D, but we always have E≥D.  You can't do fewer than one epoch; you can't
visit the average point less than once.
This is just a matter of deﬁnitions - it's what "dataset size" means.  If you say you're training
on a million examples, but you only update on 100 individual examples, then you simply
aren't "training on a million examples."
3. The inconsistency
L(D): information
OpenAI derives a scaling law called L(D).  This law is the best you could possibly do - even
with arbitrarily large compute/models - if you are only allowed to train on D data points.

No matter how good your model is, there is only so much it can learn from a ﬁnite sample. 
L(D) quantiﬁes this intuitive fact (if the model is an autoregressive transformer).
L(C): budgeting
OpenAI also derives another a scaling law called L(C).  This is the best you can do with
compute C, if you spend it optimally.
What does optimal spending look like?  Remember, you can spend a unit of compute on 
a bigger model (N), or 
training the same model for longer (S)
(Sidenote: you can also spend on bigger batches B.  But - to simplify a long, complicated
story - it turns out that there are really just 2 independent knobs to tune among the 3
variables (B, N, S), and OpenAI frames the problem as tuning (N, S) with B already "factored
out.")
In the compute regime we are currently in, making the model bigger is way more eﬀective
than taking more steps.
This was one of the punchlines of the ﬁrst of these two papers: the usual strategy, where you
pick a model and then train it until it's as good as it can get, is actually a suboptimal use of
compute.  If you have enough compute to train the model for that long ("until
convergence"), then you have enough compute to train a bigger model for fewer steps, and
that is a better choice.
This is kind of counterintuitive!  It means that you should stop training your model before it
stops getting better.  ("Early stopping" means training your model until it stops getting
better, so this is sort of "extra-early stopping.")  It's not that those extra steps wouldn't
help - it's that, if you are capable of doing them, then you are also capable of doing
something else that is better.
Here's something cool: in Appendix B.2 of the ﬁrst paper, they actually quantify exactly how
much performance you should sacriﬁce this way.  Turns out you should always stop at a test
loss about 10% higher than what your model could asymptotically achieve.  (This will be
relevant later, BTW.)
Anyway, OpenAI derives the optimal way to manage the tradeoﬀ between N and S.  Using
this optimal plan, you can derive L(C) - the test loss you can achieve with compute C, if you
allocate it optimally.
N goes up fast, S goes up slowly...
The optimal plan spends most incremental units of compute on bigger models (N).  It spends
very little on more steps (S).
The amount it spends on batch size (B) is somewhere in between, but still small enough that
the product E = B*S grows slowly.
But remember, we know a relationship between E and "D," dataset size.  E can't possibly be
smaller than D.
So when your optimal plan chooses its B and its S, it has expressed an opinion about how big
its training dataset is.
The dataset could be smaller than B*S, if we're doing many (eﬀective) epochs over it.  But it
can't be any bigger than B*S: you can't do fewer than one epoch.
... and you claim to achieve the impossible

L(C), the loss with optimally allocated C, goes down very quickly as C grows.  Meanwhile, the
dataset you're training with that compute stays almost the same size.
But there's a minimum loss, L(D), you can possibly achieve with D data points.
The compute-optimal plan claims "by training on at most B*S data points, with model size N,
I can achieve loss L(C)."
The information bound says "if you train on at most B*S data points, your loss can't get any
lower than the function L(D), evaluated at D = B*S."
Eventually, with enough compute, the L(C) of the compute-optimal plan is lower than the
L(D) of the dataset used by that same plan.
That is, even if the compute-optimal model is only training for a single epoch, it is claiming to
extract more value that epoch than any model could ever achieve, given any number of
epochs.
That's the inconsistency.
4. The resolution
In the new paper, there's an intuitive hypothesis for what's going on here.  I don't think it
really needs the multimodal results to motivate it - it's a hypothesis that could have been
conceived earlier on, but just wasn't.
Bigger models extract a resource faster
The idea is this.  As models get bigger, they get more update-eﬃcient: each time they
update on a data point, they get more out of it.  You have to train them for fewer (eﬀective)
epochs, all else being equal.
This fact drives the choice to scale up the model, rather than scaling up steps.  Scaling up
the model makes your steps more valuable, so when you choose to scale the model rather
than the steps, it's almost like you're getting more steps anyway.  (More "step-power," or
something.)
The resource is ﬁnite
Each data point has some information which a model can learn from it.  Finite models,
trained for a ﬁnite amount of time, will miss out on some of this information.
You can think about the total extractable information in a data point by thinking about what
an inﬁnitely big model, trained forever, would eventually learn from that point.  It would
extract all the information - which is more than a lesser model could extract, but still ﬁnite. 
(A single data point doesn't contain all the information in the universe.)
This is literally the deﬁnition of L(D): what an inﬁnitely big model, trained forever, could learn
from D separate data points.  L(D) quantiﬁes the total extractable information of those
points.
(More precisely, the total extractable information is the gap between L(D) and the loss
achieved by a maximally ignorant model, or something like that.)
Converging in the very ﬁrst step
As models get bigger, they extract more information per update.  That is, each time they
see a data point, they extract a larger fraction of its total extractable information.

Eventually, your models are getting most of that information the very ﬁrst time they see the
data point.  The "most" in that sentence gets closer and closer to 100%, asymptotically.
How does this relate to optimal compute allocation?
The logic of the "optimal compute plan" is as follows:
Your model is an imperfect resource extractor: it only gets some of the resources locked up in
a data point from the ﬁrst update.  So you could extract more by running for more steps ... 
...  but if you have the compute for that, you can also spend it by making your steps more
eﬃcient.  And, in the current compute regime, that's the smarter choice.
It's smarter by a speciﬁc, uniform proportion.  Remember, you should stop training when
your loss is 10% higher than the converged loss of the same model.  If the converged loss is
L, you should stop at 1.1*L.
Can you always do that?  If your model is eﬃcient enough, you can't!  As the ﬁrst epoch gets
closer to 100% eﬃcient, the loss after the ﬁrst epoch gets arbitrarily close to the converged
loss.  Your loss goes under 1.1*L by the end of the ﬁrst epoch.
At this point, the story justifying the L(C) law breaks down.
The L(C) law goes as fast is it does because upgrading the eﬃciency of your extractor is
cheaper - in terms of compute spent per unit of resource extracted - than actually running
the extractor.
This works as long as your extractor is ineﬃcient.  But you can't push eﬃciency above
100%.  Eventually, the only way to extract more is to actually run the damn thing.
Getting a bigger quarry
When you're extracting a resource, there's a diﬀerence between "improve the extractor"
and "get a bigger quarry."
If your quarry has 100 resource units in it, the strategy of "improving the extractor" can
never get you more than 100 units.  It can get them to you faster, but if you want more than
100 units, you have to get a bigger quarry.
"N" sets the eﬃciency of the extractor.  "S" sets ... well, it doesn't exactly set the size of the
quarry (that's D).  There is an ambiguity in the S: it could mean running for more epochs on
the same data, or it could mean getting more data.
But S does, at least, set an upper bound on the size of the quarry, D.  (Via D≤E and E = B*S,
with B set optimally as always.)
With high enough compute (and thus model size), you've pushed the "extractor upgrades
are cheap" lifehack as far as it can go.  With this eﬃcient extractor, taking S steps (thus
making E = B*S updates) sucks up most of the information theoretically extractable from E
individual data points.
The learning curve L(E) of your model, as it makes its ﬁrst pass over the dataset, starts to
merge with L(D), the theoretical optimum achievable with that same dataset.  You trace out
L(D) as you train, and the relevant constraint on your performance is the maximum data size
D you can obtain and train on.
Where we are now 
In the compute regime that spans GPT-2 and the smaller variants of GPT-3, extraction is far
less than maximally eﬃcient.  The L(C) strategy applies, and the smart move is to spend

compute mostly on model size.  So you make GPT-2, and then GPT-3.
Once we get to the full GPT-3, though, the extractor is eﬃcient enough that the justiﬁcation
for L(C) has broken down, and the learning curve L(E) over the ﬁrst epoch looks like L(D).
Here is that as a picture, from the new paper:
The yellowest, lowest learning curve is the full GPT-3.  (The biggest GPT-2 is one of the
green-ish lines.)  The black line is L(D), maximally eﬃcient extraction.
You can see the whole story in this picture.  If you're in one of the smaller-model learning
curves, running for more steps on more data will get you nowhere near to the total
extractable info in that data.  It's a better use of your compute to move downwards, toward
the learning curve of a bigger model.  That's the L(C) story.
If the L(C) story went on forever, the curves would get steeper and steeper.  Somewhere a
little beyond GPT-3, they would be steeper than L(D).  They would cross L(D), and we'd be
learning more than L(D) says is theoretically present in the data.
According to the story above, that won't happen.  We'll just converge ever closer to L(D).  To
push loss further downward, we need more data.
Implications
Since people are talking about bitter lessons a lot these days, I should make the following
explicit: none of this means "the scaling hypothesis is false," or anything like that.
It just suggests the relevant variable to scale with compute will switch: we'll spent less of our
marginal compute on bigger models, and more of it on bigger data.
That said, if the above is true (which it may not be), it does suggest that scaling transformers
on text alone will not continue productively much past GPT-3.

The GPT-3 paper says its choices were guided by the "grow N, not S" heuristic behind the
L(C) curve:
Based on the analysis in Scaling Laws For Neural Language Models [KMH+20] we train
much larger models on many fewer tokens than is typical.
("KMH+20″ is the ﬁrst of the two scaling papers discussed here.)  Even following this
heuristic, they still picked a huge dataset, by human standards for text datasets.
In the above terms, their "E" was 300 billion tokens and their "D" was ~238 tokens, since
they updated multiple times on some tokens (cf. Table 2.2 in the GPT-3 paper).  The whole of
Common Crawl is 410 billion tokens, and Common Crawl might as well be "all the text in the
universe" from the vantage point of you and me.
So, there's room to scale D up somewhat further than they did with GPT-3, but not many
orders of magnitude more.  To me, this suggests that an intuitively "smarter" GPT-4 would
need to get its smartness from being multimodal, as we really can't go much further with
just text.

Generalized Heat Engine
I'd like to be able to apply more of the tools of statistical mechanics and
thermodynamics outside the context of physics. For some pieces, that's pretty
straightforward - a large chunk of statistical mechanics is just information theory, and
that's already a ﬂourishing standalone ﬁeld which formulates things in general ways.
But for other pieces, it's less obvious. What's the analogue of a refrigerator or a carnot
cycle in more general problems? How do "work" and "heat" generalize to problems
outside physics? The principle of maximum entropy tells us how to generalize
temperature, and oﬀers one generalization of work and heat, but it's not immediately
obvious why we can't extract "work" from "heat" without subsystems at diﬀerent
temperatures, or how to turn that into a useful idea in non-physics applications.
This post documents my own exploration of these questions in the context of a
relatively simple problem, with minimal reference to physics (other than by analogy).
Speciﬁcally: we'll talk about how to construct the analogue of a heat engine using
biased coins.
Intuition
The main idea I want to generalize here is that we can "move uncertainty around"
without reducing uncertainty. This is exactly what e.g. a refrigerator or heat engine
does.
Consider the viewpoint of a refrigerator-designer. All the microscopic dynamics of the
(fridge + environment) system must be reversible, so the number of possible
microscopic states will never decrease on its own as time passes. The only way to
reduce uncertainty about the microscopic state is to observe it. But the fridge
designer is designing the system, deciding in advance how it will behave. The
designer has no direct access to the environment in which the fridge will run, no way
to measure the exact positions the atoms will be in when the fridge ﬁrst turns on. The
designer, in short, cannot directly observe the system. So, from the designer's
perspective, there's uncertainty which cannot be reduced.
(In statistical mechanics, there are several entirely diﬀerent justiﬁcations for why
observations can't reduce microscopic uncertainty/entropy - for instance, in one
approach, macroscopic variables are chosen in such a way that we can
deterministically predict future macroscopic observations. Another comes from
Maxwell's demon-style arguments, where the demon's memory has to be included as
part of the system. I'll use the designer viewpoint, since it's conceptually simple and
easy to apply in other areas - in particular, we can easily apply it to the design of AIs
embedded in their environment.)
While we can't reduce our total uncertainty, we can move it around. We design the
machine to apply transformations to the system which leave us more certain about
some subsystems (e.g. the inside of the refrigerator), but less certain about other
subsystems (e.g. heat baths used to power the system).
Setup

We'll imagine two large sets of IID biased coins. One is the "cold pool", in which each
coin comes up 1 (i.e. heads) with probability 0.1 and 0 with probability 0.9. The other
is the "hot pool", in which each coin comes up 1 with probability 0.2. We'll call the
coins in the cold pool X
C
1 ... X
C
n , and the coins in the hot pool X
H
1 ... X
H
n .
We're going to apply transformations to these coins. Each transformation replaces
some set of coins with new values which are a function of their old values. For
instance, one transformation might be
( X 
C
1  , X 
H
3  , X 
H
7  ) ← ( X 
C
1  , X 
H
3  X 
C
1  + X 
H
7  
¯ ¯¯¯¯¯¯ ¯
X 
C
1  , X 
H
7  X 
C
1  + X 
H
3  
¯ ¯¯¯¯¯¯ ¯
X 
C
1  )
(Here the bar denotes logical not - i.e. 
¯¯¯¯¯
X  means "not X".) This transformation swaps 
X
H
3  with X
H
7  if X
C
1  is 0, and leaves everything unchanged if X
C
1  is 1.
We'll mostly be able to use any transformations we want, but with two big constraints.
First: all transformations must be reversible. If we know the ﬁnal state of the
coins and which transformations were applied, then we must be able to reconstruct
the initial state of the coins. (This is the analogue of microscopic reversibility.) Our
example transformation above is reversible - since it doesn't change X
C
1 , we can
always tell whether X
H
3  and X
H
7  were swapped, and we can swap them back if they
were (indeed, we can do so by simply reapplying the same transformation).
Second constraint: all transformations must conserve the number of heads;
heads can be neither created nor destroyed on net. Here the number of heads is our
analogue of energy, and heads-conservation is our analogue of microscopic energy
conservation. (In physics, we'd probably describe this as some kind of spin system in
an external magnetic ﬁeld.) Our example transformation above conserves the number
of heads: it either swaps two coins or leaves everything alone, so the total number of
heads stays the same.
One more key rule: while we will be able to choose what transformation to apply, we
do not get to look at the coins before choosing our transformation. Physical
analogy: if we're building a heat engine or refrigerator or the like, we can't just freely
observe the microscopic state of the system. More generally, if we're designing some
machine (like a heat engine), we have to decide up-front how the machine will
behave, before we have perfect information about the environment in which it will
run. The machine itself can "observe" variables while running, but the machine is part
of the system, so those "observations" need to be reversible and energy-conserving
just like any other transformations.
Writing it all out mathematically: we choose some transformation T for which

( X H , X C ) ′ = T ( X H , X C )
( ∑ k X 
H
k  + ∑ k X 
C
k  ) ′ = ∑ k X 
H
k  + ∑ k X 
C
k
T is invertible
We'll want to choose this T to do something interesting, like reduce the uncertainty of
particular coins.
Extracting "Work"
General problem: choose a transformation to produce some coins which are 1 with
near-zero uncertainty (i.e. asymptotically zero uncertainty). We'll call these
deterministic coins "work", and use w to denote the number of work-coins produced.
We'll look at two subproblems to this problem. First, we'll try to do it using just one of
the two pools of coins (the hot one, though it doesn't matter). This is the equivalent of
"turning heat directly into work", i.e. a type-2 perpetual motion machine; we'd expect
it to be impossible. Second, we'll tackle the problem using both pools, and ﬁgure out
how much work we can extract. This is the equivalent of a heat engine.
Extracting Work From One Heat Bath
The ﬁrst key thing to notice is that this is inherently an information compression
problem. I have n random coins with heads-probability 0.2. I want to make w of those
coins near-certainly 1, while still making the transformation reversible - therefore the
remaining n −w transformed coins must contain all of the information from the
original n coins. In other words, I need to compress the info from the original n coins
into n −m bits with near-certainty.
If we whip out our information theory, that compression is fairly straightforward. Our
biased coins have entropy of −(0.2 ∗log(0.2) + 0.8 ∗log(0.8)) ≈0.72 bits per coin. So,
with a reversible transformation we can compress all of the info into 0.73n of the
coins, and the remaining 0.27n coins can all be nearly-deterministic.
(We're fudging a bit here - we may need to add one or two extra coins from outside to
make the compression algorithm handle unlikely cases without loss - but for current
purposes that's not a big deal. I'll be fudging this sort of thing throughout the post.)
However, we also need to conserve the number of heads. That's a problem: fully
compressed bits are 50/50 in general, so our 0.73n compressed bits include roughly 
0.36n heads. We started with only 0.2n heads, so we have no way to balance the

books - even if all of our 0.27n deterministic bits are tails, we still end up with too
many heads and too few tails.
This generalizes: we won't be able to compress our information without producing
more tails. Hand-wavy proof: the initial distribution of coins is maxentropic subject to a
constraint on the total number of heads. So, we can't compress it without violating
that constraint.
Let's spell this out a bit more carefully.
A maxentropic variable contains as much information as possible - there is no other
distribution over the same outcomes with higher entropy. In general, mutual
information I(X, Y ) is at most the entropy of one variable H(X) - i.e. the information in 
X about Y  is at most all of the information in X, so the higher the entropy H(X) the
more information X can potentially contain about any other variable Y .
In our case, we have an initial state X and a ﬁnal state X′. We want to compress all the
info in X into X′, I(X, X′) = H(X), so we must have H(X′) ≥I(X, X′) = H(X). Initial state X
 is maxentropic: its possible outcomes are all values of n coin ﬂips with a ﬁxed number
of heads, and X has the highest possible H(X) over those outcomes. Final state X′ we
choose to be maxentropic - we need H(X′) ≥H(X), so we make H(X′) as large as
possible. However, note that the possible outcomes of X′ are a strict subset of the
possible outcomes of X: possible outcomes of X′ are all values of n coin ﬂips with a
ﬁxed number of heads AND the ﬁrst w coins are all heads. So, we choose X′ to be
maxentropic on this set of outcomes, but it's a strictly smaller set of outcomes than
for X, so the maximum achievable entropy H(X′) will be less than H(X). Thus: our
condition H(X′) ≥H(X) cannot be achieved. 
We cannot extract deterministic bits (i.e. work) from a single pool of maxentropic-
subject-to-constraint random bits (i.e. heat), while still respecting the constraint.
Even more generally: if we have a pool of random variables which are maxentropic
subject to some constraint, we won't be able to compress them without violating that
constraint. If the constraint ﬁxes a value of ∑k fk(Xk), and we want to deterministically
ﬁx f1(X1), then that reduces the number of possible values of ∑k>1 fk(Xk), and
therefore reduces the amount of information which the remaining variables can
contain. Since they didn't have any "spare" entropy before (i.e. initial state is
maxentropic subject to the constraint), we won't be able to "ﬁt" all the information
into the remaining entropy.

That's a very general analogue of the idea that we can't extract work from a single-
temperature heat bath. How about two heat baths?
Extracting Work From Two Heat Baths
Now we have 2n coins to play with: n with probability 0.1, and n with probability 0.2.
The entropy is roughly 0.73 bits per "hot" coin, and 0.47 bits per "cold" coin. So, we'd
need 1.19n coins with a roughly 50/50 mix of heads and tails to contain all the info.
That's still too many heads: full compression would require roughly .59n heads, and
we only have about (0.1 + 0.2)n = 0.3n. But our initial distribution is no longer
maxentropic given the overall constraint, so maybe it could work if we only partially
compress the information?
Let's set up the problem more explicitly, to maximize the work we can extract.
Our ﬁnal distribution will contain w deterministic bits and 2n −w information-
containing bits. The information-containing bits must contain a total of 0.3n −w
 heads. In order to contain as much information as possible, the ﬁnal distribution of
those 2n −w bits should be maxentropic subject to the constraint on the number of
heads. So, they should be roughly (remember, large n) IID with probability 
 of
heads, with total entropy −(2n −w)(
log(
) +
log(
)). We set that
equal to the amount of entropy we need (i.e. 1.19n bits), and solve for w. In this case,
I ﬁnd w = 0.011n. Since we started with about 0.3n heads, we're able to extract about
3.7% of them as "work" (or 5.5% of the "hot" heads).
So we can indeed extract work from two heat baths at diﬀerent temperatures.
Notably, the "eﬃciency" we calculated is not the usual theoretical optimal eﬃciency
from thermodynamics. That "optimal eﬃciency" comes from a slightly diﬀerent
problem - rather than converting all our bits into as much work as possible, that
problem considers the optimal conversion of random bits into work at the margin,
assuming our heat baths don't run out. In particular, that means we usually wouldn't
be using equal numbers of bits from the hot and cold pools.
This post is already plenty long, so I'll save further discussion of thermodynamic
eﬃciency and temperatures for another day.
Takeaway
The point of this exercise is to cast core ideas of statistical mechanics - especially the
more thermo-esque ideas - in terms which are easier to generalize beyond physics. To
that end, the key ideas are:
0.3n−w
2n−w
1.7n
2n−w
1.7n
2n−w
0.3n−w
2n−w
0.3n−w
2n−w

Thermo-like laws apply when we can't gain information about a system (e.g.
because we're designing a machine to operate in an environment which we can't
observe directly at design time), can't lose information about a system at a low
level (either due to physical reversibility constraints or because we don't want to
throw out info), and the system has some other constraints (like energy
conservation).
We can operate on the system in ways which move uncertainty around, without
decreasing it.
If we want to move uncertainty around in a way which makes certain variables
nearly deterministic (i.e. "extract work"), that's a compression problem.
We can't compress a maxentropic distribution, so we can't extract work from a
single maxentropic-subject-to-constraint pool of variables without violating the
constraint.
We can extract work from two pools of variables which are initially maxentropic
under diﬀerent constraints, while still respecting the full-system constraint.
The follow-up post on thermodynamic eﬃciency and temperatures is here .

Inner Alignment in Salt-Starved Rats
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
(See comment here for some corrections and retractions. —Steve, 2022)
Introduction: The Dead Sea Salt
Experiment
In this 2014 paper by Mike Robinson and Kent Berridge at University of Michigan (see also
this more theoretical follow-up discussion by Berridge and Peter Dayan), rats were raised in
an environment where they were well-nourished, and in particular, where they were never
salt-deprived—not once in their life. The rats were sometimes put into a test cage with a
lever which, when it appeared, was immediately followed by a device spraying ridiculously
salty water directly into their mouth. The rats were disgusted and repulsed by the extreme
salt taste, and quickly learned to hate the lever—which from their perspective would seem to
be somehow causing the saltwater spray. One of the rats went so far as to stay tight against
the opposite wall—as far from the lever as possible!
Then the experimenters made the rats feel severely salt-deprived, by depriving them of salt.
Haha, just kidding! They made the rats feel severely salt-deprived by injecting the rats with a
pair of chemicals that are known to induce the sensation of severe salt-deprivation. Ah, the
wonders of modern science!
...And wouldn't you know it, almost instantly upon injection, the rats changed their behavior!
When shown the lever (this time without the salt-water spray), they now went right over to
that lever and jumped on it and gnawed at it, obviously desperate for that super-salty water.
The end.
Aren't you impressed? Aren't you ﬂoored? You should be!!! I don't think any standard
ML algorithm would be able to do what these rats just did!
Think about it:
Is this Reinforcement Learning? No. RL would look like the rats randomly stumbling
upon the behavior of "nibbling the lever when salt-deprived", ﬁnd it rewarding, and
then adopt that as a goal via "credit assignment". That's not what happened. While
the rats were nibbling at the lever, they had never in their life had an
experience where the lever had brought forth anything other than an utterly
repulsive experience. And they had never in their life had an experience
where they were salt-deprived, tasted something extremely salty, and found
it gratifying. I mean, they were clearly trying to interact with the lever—this is a
foresighted plan we're talking about—but that plan does not seem to have been
reinforced by any experience in their life.
Update for clariﬁcation: Speciﬁcally, it's not any version of RL where you learn
about the reward function only by observing past rewards. This category includes
all model-free RL and some model-based RL (e.g. MuZero). If, by contrast, you
have a version of model-based RL where the agent can submit arbitrary
hypothetical queries to the true reward function, then OK, sure, now you can get
the rats' behavior. I don't think that's what's going on here for reasons I'll mention
at the bottom.
Is this Imitation Learning? Obviously not; the rats had never seen any other rat around
any lever for any reason.

Is this an innate, hardwired, stimulus-response behavior? No, the connection between a
lever and saltwater was an arbitrary, learned connection. (I didn't mention it, but the
researchers also played a distinctive sound each time the lever appeared. Not sure how
important that is. But anyway, that connection is arbitrary and learned, too.)
So what's the algorithm here? How did their brains know that this was a good plan? That's
the subject of this post.
What does this have to do with inner alignment? What is inner alignment anyway?
Why should we care about any of this?
With apologies to the regulars on this forum who already know all this, the so-called "inner
alignment problem" occurs when you, a programmer, build an intelligent, foresighted, goal-
seeking agent. You want it to be trying to achieve a certain goal, like maybe "do whatever I,
the programmer, want you to do" or something. The inner alignment problem is: how do you
ensure that the agent you programmed is actually trying to pursue that goal? (Meanwhile,
the "outer alignment problem" is about choosing a good goal in the ﬁrst place.) The inner
alignment problem is obviously an important safety issue, and will become increasingly
important as our AI systems get more powerful in the future.
(See my earlier post mesa-optimizers vs "steered optimizers" for speciﬁcs about how I frame
the inner alignment problem in the context of brain-like algorithms.)
Now, for the rats, there's an evolutionarily-adaptive goal of "when in a salt-deprived state,
try to eat salt". The genome is "trying" to install that goal in the rat's brain. And apparently,
it worked! That goal was installed! And remarkably, that goal was installed even before that
situation was ever encountered! So it's worth studying this example—perhaps we can learn
from it!
Before we get going on that, one more boring but necessary thing:
Aside: Obligatory post-replication-crisis discussion
The dead sea salt experiment strikes me as trustworthy. Pretty much all the rats—and for key
aspects literally every tested rat—displayed an obvious qualitative behavioral change almost
instantaneously upon injection. There were sensible tests with control levers and with control
rats. The authors seem to have tested exactly one hypothesis, and it's a hypothesis that was
a priori plausible and interesting. And so on. I can't assess every aspect of the experiment,
but from what I see, I believe this experiment, and I'm taking its results at face value. Please
do comment if you see anything questionable.
Outline of the rest of the post
Next I'll go through my hypothesis for how the rat brain works its magic here. Actually, I've
come up with three variants of this hypothesis over the past year or so, and I'll talk through
all of them, in chronological order. Then I'll speculate brieﬂy on other possible explanations.
My hypothesis for how the rat brain did
what it did
The overall story
As I discussed in My Computational Framework for the Brain, my starting-point assumption is
that the rat brain has a "neocortex subsystem" (really the neocortex, hippocampus, parts of
thalamus and basal ganglia, maybe other things too). The neocortex subsystem takes
sensory inputs and reward inputs, builds a predictive model from scratch, and then chooses

thoughts and actions that maximize reward. The reward, in turn, is issued by a diﬀerent
subsystem of the brain that I'll call "subcortex".
To grossly oversimplify the "neocortex builds a predictive model" part of that, let's just say
for present purposes that the neocortex subsystem memorizes patterns in the inputs, and
then patterns in the patterns, and so on.
To grossly oversimplify the "neocortex chooses thoughts and actions that maximize reward"
part, let's just say for present purposes that diﬀerent parts of the predictive model are
associated with diﬀerent reward predictions, the reward predictions are updated by a TD
learning system that has something to do with dopamine and the basal ganglia, and parts of
the model that predict higher reward are favored while parts of the model that predict lower
reward are pushed out of mind.
Since the "predictive model" part is invoked for the "reward-maximization" part, we can say
that the neocortex does model-based RL.
(Aside: It's sometimes claimed in the literature that brains do both model-based and model-
free RL. I disagree that this is a fundamental distinction; I think "model-free" = "model-based
with a dead-simple model". See my old comment here.)
Why is this important? Because that brings us to imagination! The neocortex can activate
parts of the predictive model not just to anticipate what is about to happen, but also to
imagine what may happen, and (relatedly) to remember what has happened.
Now we get a crucial ingredient: I hypothesize that the subcortex somehow knows when the
neocortex is imagining the taste of salt. How? This is the part where I have three versions of
the story, which I'll go through shortly. For now, let's just assume that there is a wire going
into the subcortex, and when it's ﬁring, that means the neocortex is activating the parts of
the predictive model that correspond (semantically) to tasting salt.
Basic setup. The subcortex has an incoming signal that tells it that the neocortex is
imagining / expecting / remembering the taste of salt. I'll talk about several possible
sources of this signal (here marked "???") in the next section. Then the subcortex has a
hardwired circuit that, whenever the rat is salt-deprived, issues a reward to the
neocortex for starting to activate this signal (and negative reward for stopping). The
neocortex now ﬁnds it pleasing to imagine walking over and drinking the saltwater, and
it does so!
And once we have that, the last ingredient is simple: The subcortex has an innate, hardwired
circuit that says "If the neocortex is imagining tasting salt, and I am currently salt-deprived,
then send a reward to the neocortex."
OK! So now the experiment begins. The rat is salt-deprived, and it sees the lever appear.
That naturally evokes its previous memory of tasting salt, and that thought is rewarded!

When the rat imagines walking over and nibbling the lever, it ﬁnds that to be a very pleasing
(high-reward-prediction) thought indeed! So it goes and does it!
(UPDATE: Commenters point out that this description isn't quite right—it doesn't make sense
to say that the idea of tasting salt is rewarding per se. Rather, I propose that the subcortex
sends a reward related to the time-derivative of how strongly the neocortex is imagining /
expecting to taste salt. So the neocortex gets a reward for ﬁrst entertaining the idea of
tasting salt, and another incremental reward for growing that idea into a deﬁnite plan. But
then it would get a negative reward for dropping that idea. Sorry for the mistake / confusion.
Thanks commenters!)
Now let's ﬁll in that missing ingredient: How does the subcortex get its hands on a signal
ﬂagging that the neocortex is imagining the taste of salt? I have three hypotheses.
Hypothesis 1 for the "imagining taste of salt" signal: The neocortex API enables
outputting a prediction for any given input channel
This was my ﬁrst theory, I guess from last year. As argued by the "predictive coding" people,
Jeﬀ Hawkins, Yann LeCun, and many others, the neocortex is constantly predicting what input
signals it will receive next, and updating its models when the predictions are wrong. This
suggests that it should be possible to stick an arbitrary input line into the neocortex, and
then pull out a signal carrying the neocortex's predictions for that input line. (It would look
like a slightly-earlier copy of the input line, with sporadic errors for when the neocortex is
surprised.) I can imagine, for example, that if you put an input signal into cortical mini-
column #592843 layer 4, then you look at a certain neuron in the same mini-column, you
ﬁnd those predictions.
If this is the case, then the rest is pretty straightforward. The genome wires the salt taste
bud signal to wherever in the neocortex, pulls out the corresponding prediction, and we're
done! For the reason described above, that line will also ﬁre when merely imagining salt
taste.
Commentary on hypothesis 1: I have mixed feelings.
On the one hand, I haven't really come across any independent evidence that this
mechanism exists. And, having learned more about the nitty-gritty of neocortex algorithms
(the outputs come from layer 5, blah blah blah), I don't think the neocortex outputs carry this
type of data.
On the other hand, I have a strong prior belief that if there are ten ways for the brain to do a
certain calculation, and each is biologically and computationally plausible without dramatic
architectural change, the brain will do all ten! (Probably in ten diﬀerent areas of the brain.)
After all, evolution doesn't care much about keeping things elegant and simple. I mean, there
is a predictive signal for each input—it has to be there somewhere! And I don't currently see

any reason that this signal couldn't be extracted from the neocortex. So I feel sorta obligated
to believe that this mechanism probably exists.
So anyway, all things considered, I don't put much weight on this hypothesis, but I also won't
strongly reject it.
With that, let's move on to the later ideas that I like better.
Hypothesis 2 for the "neocortex is imagining the taste of salt" signal: The
neocortex is rewarded for "communicating its thoughts"
This was my second guess, I guess dating to several months ago.
The neocortex subsystem has a bunch of output lines for motor control and whatever else,
and it has a special output line S (S for salt).
Meanwhile, the subcortex sends rewards under various circumstances, and one of those
things is that the neocortex is rewarded for sending a signal into S whenever salt is tasted.
(The subcortex knows when salt is tasted, because it gets a copy of that same input.)
So now, as the rat lives its life, it stumbles upon the behavior of outputting a signal into S
when eating a bite of saltier-than-usual food. This is reinforced, and gradually becomes
routine.
The rest is as before: when the rat imagines a salty taste, it reuses the same model. We did
it!
Commentary on hypothesis 2: A minor problem (from the point-of-view of evolution) is
that it would take a while for the neocortex to learn to send a signal into S when eating salt.
Maybe that's OK.
A much bigger potential problem is that the neocortex could learn a pattern where it sends a
signal into S when tasting salt, and also learns a diﬀerent pattern where it sends a signal into
S whenever salt-deprived, whether thinking about salt or not. This pattern would, after all, be
rewarded, and I can't immediately see how to stop it from developing.
So I'm pretty skeptical about this hypothesis now.
Hypothesis 3 for the "neocortex is imagining the taste of salt" signal (my
favorite!): Sorta an "interpretability" approach, probably involving the amygdala

This one comes out of my last post, Supervised Learning of Outputs in the Brain. Now we
have a separate brain module that I labeled "supervised learning algorithm", and which I
suspect is primarily located in the amygdala. This module does supervised learning: the salt
signal (from the taste buds) functions as the supervisory signal, and a random assortment of
neurons in the neocortex subsystem (describing latent variables in the neocortex's predictive
model) function as the inputs to the learned model. Then the supervised learning module
learns which patterns in those latent variables tend to reliably predict that salt is about to be
tasted. Having done that, when it sees those patterns recur, that's our signal that the
neocortex is probably expecting the taste of salt ... and as described above, it will also see
those same patterns when the neocortex is merely imagining or remembering the taste of
salt. So we have our signal!
Commentary on Hypothesis 3: There's a lot I really like about this. It seems to at-least-
vaguely match various things I've seen in the literature about the functionality and
connectivity of the amygdala. It makes a lot of sense from a design perspective—the
patterns would be learned quickly and reliably, etc., as far as I can tell. I ﬁnd it satisfyingly
obvious and natural (in retrospect). So I would put this forward as my favorite hypothesis by
far.
It also transfers in an obvious way to AGI programming, where it would correspond to
something like an automated "interpretability" module that tries to make sense of the AGI's
latent variables by correlating them with some other labeled properties of the AGI's inputs,
and then rewarding the AGI for "thinking about the right things" (according to the
interpretability module's output), which in turn helps turn those thoughts into the AGI's
goals, using the time-derivative reward-shaping trick as described above.
(Is this a good design idea that AGI programmers should adopt? I don't know, but I ﬁnd it
interesting, and at least worthy of further thought. I don't recall coming across this idea
before in the context of the inner alignment problem.)
(Update 6 months later: I'm now more conﬁdent that this hypothesis is basically
right, except maybe I should have said "medial prefrontal cortex and ventral
striatum" where I said "amygdala". Or maybe it's all of the above. Anyway, see my
later post Big Picture Of Phasic Dopamine .)
What would other possible explanations
for the rat experiment look like?

The theoretical follow-up by Dayan & Berridge is worth reading, but I don't think they
propose any real answers, just lots of literature and interesting ideas at a somewhat-more-
vague level.
(Update to add this paragraph) Next: At the top I mentioned "a version of model-based RL
where the agent can submit arbitrary hypothetical queries to the true reward function" (this
category includes AlphaZero). If the neocortex had a black-box ground-truth reward
calculator (not a learned-from-observations model of the reward) and a way to query it, that
would seem to resolve the mystery of how the rats knew to get the salt. But I can't see how
this would work. First, the ground-truth reward is super complicated. There are thousands of
pain receptors, there are hormones sloshing around, there are multiple subcortical brain
regions doing huge complicated calculations involving millions of neurons that provide input
to the reward calculation (I believe), and so on. You can learn to model this reward-
calculating system by observing it, of course, but actually running this system (or a copy of
it) on hypotheticals seems unrealistic to me. Second, how exactly would you query the
ground-truth reward calculator? Third, there seems to be good evidence that the neocortex
subsystem chooses thoughts and actions based on reward predictions that are updated by
TD learning, and I can't immediately see how you can simultaneously have that system and
a mechanism that chooses thoughts and actions by querying a ground-truth reward
calculator. I think my preferred mechanism, "reward depends in part on what you're thinking"
(which we know is true anyway), is more plausible and ﬂexible than "your imagination has
special access to the reward function".
Next: What would Steven Pinker say? He is my representative advocate of a certain branch
of cognitive neuroscience—a branch to which I do not subscribe. Of course I don't know what
he would say, but maybe it's a worthwhile exercise for me to at least try. Well, ﬁrst, I think he
would reject the idea that there's a "neocortex subsystem". And I think he would more
generally reject the idea that there is any interesting question along the lines of "how does
the reward system know that the rat is thinking about salt?". Of course I want to pose that
question, because I come from a perspective of "things like this need to learned from
scratch" (again see My Computational Framework for the Brain). But Pinker would not be
coming from that perspective. I think he wants to assume that a comparatively elaborate
world-modeling infrastructure is already in place, having been hardcoded by the genome. So
maybe he would say there's a built-in "diet module" which can model and understand food,
taste, satiety, etc., and he would say there's a built-in "navigation module" which can plan a
route to walk over to the lever, and he would there's a built-in "3D modeling module" which
can make sense of the room and lever, etc. etc.
OK, now that possibly-strawman-Steven-Pinker has had his say in the previous paragraph, I
can respond. I don't think this is so far oﬀ as a description of the calculations done by an
adult brain. In ML we talk about "how the learning algorithm works" (SGD, BatchNorm, etc.),
and separately (and much less frequently!) we talk about "how the trained model works"
(OpenAI Microscope, etc.). I want to put all that infrastructure in the previous paragraph at
the "trained model" level, not the "learning algorithm" level. Why? First, because I think
there's pretty good evidence for cortical uniformity. Second—and I know this sounds stupid—
because I personally am unable to imagine how this setup would work in detail. How exactly
do you insert learned content into the innate framework? How exactly do you interface the
diﬀerent modules with each other? And so on. Obviously, yes I know, it's possible that
answers exist, even if I can't ﬁgure them out. But that's where I'm at right now.

A review of Where Is My Flying Car? by J.
Storrs Hall
This is a linkpost for https://rootsofprogress.org/where-is-my-ﬂying-car
Suppose you were to reach into the mirror universe, where everything is inverted, and pull
out a book that is the exact opposite of Robert Gordon's The Rise and Fall of American
Growth.
Instead of being written by a scholarly economic historian steeped in the study of the past, it
would be written by an engineer who has spent his career on futuristic technology. Instead of
coming from a prestigious academic press, it would be self-published, with misformatted
tables and a cover featuring garish three-dimensional lettering. Instead of sticking to
extremely conservative predictions about future technologies, it would speculate audaciously
about the limits of the possible, from nanotech to cold fusion. Instead of a sober survey of
economic history in one country and time period, it would range widely through engineering,
physics and philosophy, exploring the power-to-weight ratio of jet turbines in one chapter,
and describing the rise of the counterculture in the next. And instead of proclaiming the
death of innovation and the end of growth, it would paint a bold vision of an ambitious
technological future.
That book has leapt out of the mirror universe and into an Amazon Kindle edition (priced at 𝜋
dollars): Where Is My Flying Car? A Memoir of Future Past, by J. Storrs Hall.
Hall sets out to tackle the title question: why don't we have ﬂying cars yet? And indeed,
several chapters in the book are devoted to deep dives on the history, engineering, and
economics of ﬂying cars. But to fully answer the question, Hall must go much broader and
deeper, because he quickly concludes that the barriers to ﬂying cars are not technological or
economic—they are cultural and political. To explain the ﬂying car gap is to explain the Great
Stagnation itself.
Bold futures
The most valuable thing I took away from the book was an awareness of some powerful
technological possibilities.
Flying cars
Before reading the book, I had assumed that the ﬂying car was one of those ideas that
sounds good on its face, but turns out not to work or be interesting in practice. Maybe
they're inherently too hard to ﬂy, too dangerous, or just not all that valuable. This book
changed my mind, by pointing out a simple analogy: today's system of ﬂight has all of the
inconveniences of railroads, over a century ago. Airplanes are large, mass-transit vehicles
that travel only deﬁned, scheduled routes between a small number of stations. This creates
two problems for travelers. First is the "three vehicles" problem: you have to get from your
origin to the nearest station, and then from your arrival station to your actual destination,
changing vehicles each time (and hauling your luggage). Second is the inconvenience of
schedule: having to be on time to catch the train or plane, compared to the personal vehicle
that is ready immediately whenever you want it. This is driven home when you remember
that a 90-minute ﬂight from, say, San Francisco to Los Angeles actually takes you half a day,
after travel to and from the airport plus delays at ticketing, security and boarding.
The book points out that the major value in a ﬂying car (as with supersonic) would not be in
taking the same trips you do now, only a bit faster. Instead, it would be in taking the trips

you don't take now, because they're too inconvenient. A ﬂying car would shrink your world,
expanding the radius of what you would consider for a commute, a shopping trip, a visit to
friends, a business meeting, or a weekend vacation. Indeed, Hall cites literature from travel
studies ﬁnding that people in all societies travel on average about an hour a day, whether
walking barefoot or driving on the highway. And he points out that increasing the eﬀective
radius for each of those trips increases the eﬀective area open to you quadratically (doubling
your travel radius means four times as many destinations).
Hall did extensive research and analysis for the book, even learning to ﬂy a private aircraft
himself. He recounts the history of ﬂying car research and development, which began much
earlier and has had many more credible attempts than I realized. He catalogs design
approaches, including convertibles (vehicles that convert between ﬂying and driving) and
VTOL (vertical take-oﬀ and landing). He models engineering tradeoﬀs and travel times. And
he concludes that there is no technological or economic reason why we can't have ﬂying cars
with existing technology—indeed, why we couldn't have had them already, if sustained work
on them had continued past the 1970s.
Nanotechnology
Hall's degrees are in computer science, but much of his career has been in nanotech, which
was surprisingly prominent in the book. He makes clear that he's not talking about mere
nanoscale materials, but rather true nanotech, as envisioned by Feynman in the '60s and
advanced by Eric Drexler in the '90s: atomically precise manufacturing, placing each atom
one at a time exactly where you want it, giving you complete control over the structure of
matter. In Hall's telling, while this technology is obviously a ways oﬀ, the physics is sound
and many of the basic principles have been worked out.
The potential capabilities of mature nanotech are mind-blowing. The incredible speed alone
would dramatically lower the price of literally every physical product. Hall estimates that the
entire capital stock of the US—"every single building, factory, highway, railroad, bridge,
airplane, train, automobile, truck, and ship"—could be rebuilt in a week. And nanotech would
allow materials with extreme properties, such as the strength of diamond, to be used for
everyday manufacturing and construction.
The possibilities are straight out of science ﬁction. The "space pier", a set of towers a
hundred kilometers tall with a magnetic accelerator to shoot payloads into orbit, saving the
fuel required to escape Earth's gravity well and bringing down launch costs by three orders
of magnitude. Or the "Weather Machine", a ﬂeet of quintillions of centimeter-sized balloons
ﬂoating in the stratosphere, made of nanometer-thick diamond, with remote-controlled
mirrors that can reﬂect light or allow it to pass through, forming a "programmable
greenhouse gas" that can regulate temperature and direct solar energy. And of course,
aﬀordable ﬂying cars.
Energy, energy, energy
One of the clearest indications of stagnation is the ﬂatlining of energy usage. Because the
growth in this metric was mentioned in the autobiography of Henry Adams (grandson of John
Quincy Adams), Hall calls the long-term trend of about 7% annual growth in energy usage
per capita the "Henry Adams Curve". In the late 20th century, we fell oﬀ of it:

Some techno-optimists, such as Andy McAfee, celebrate the ﬂatlining and even peaking of
resource usage curves, saying that we are getting "more from less". Hall reminds us that
more is more. All else being equal, energy eﬃciency is great. But there's no reason to
believe that ﬂatlining or declining resource usage is optimal for progress. A large part of
progress is harnessing ever-more resources and putting them to productive use. And indeed,
we're going to need lots more energy if we're ever going to get nanotech manufacturing,
regular space travel, and of course ﬂying cars. In fact, a good explanation for technological
stagnation is that the only technological revolution of the last 50 years, computing, was the
only one that didn't need more power than could be provided by the technology of the
1970s.
Where will all this energy come from? It could come from solar: the amount of power
reaching the Earth from the Sun is some 10,000 times greater than the current power
requirements of humanity. Of course, it's hard to harness in practice, owing to cloud cover
and pesky inconveniences such as nighttime, but that's nothing a well-placed ﬂeet of a
quintillion remote-controlled aerostats in the stratosphere couldn't handle.
But the majority of the energy discussion in the book focuses on the amazing potential of
nuclear. The upshot is that we ought to have nuclear-powered everything. Nuclear homes
with local, compact reactors—they don't need to be on the grid. Nuclear cars, whether ﬂying
or ground. Even nuclear batteries—I was shocked to learn that certain designs of nuclear
batteries were actually manufactured decades ago and used safely in implantable
pacemakers.
The main beneﬁt, of course, is the insane energy density of nuclear fuel: just over a pound of
enriched uranium has as much energy as 10,000 gallons of gasoline or over 100,000 pounds
of anthracite coal. With nuclear batteries, no device would ever need to be recharged; with
nuclear engines and generators, "your ground car and your home's power unit will be
refueled upon annual maintenance." This fuel eﬃciency makes the economics of nuclear
look almost identical to that of renewables: the fuel is practically free, compared to the ﬁxed
cost of infrastructure: "A wind turbine uses up more lubricating oil than a nuclear plant uses
uranium, per kilowatt-hour generated."
The book describes several potential engineering approaches for nuclear power, not just the
established ﬁssion plants based on uranium-235 that are in operation today, but everything
up through speculative possibilities such as "chainless reactors" that bombard ﬁssionable

materials with high-energy neutrons, avoiding any nuclear chain reaction. Hall says that
even cold fusion—er, sorry, I mean "low-energy nuclear reactions" (LENR)—deserves more
research: although it might still turn out to be an unexploitable phenomenon or even an
experimental artifact, there's something going on that we don't yet understand. Three
chapters are dedicated to nuclear power; my main takeaway is that the variety of
possibilities, and the scope and magnitude of the potential here, is breathtaking and
underappreciated.
The need for energy is fundamental to the economy, and yet a remarkable feature of our
culture is the opposition to almost any form of energy—a pathology that Hall dubs
"ergophobia". (More on this below.)
Level 5
Putting together all this and more, Hall summarizes his vision for the future as a "Second
Atomic Age" based on nuclear, nanotech, and artiﬁcial intelligence. It's a vision of continued
exponential or even super-exponential progress, a world in which we see improvement in the
world of atoms as fast as we've recently seen improvement only in the world of bits.
Hall cites the global development advocate Hans Rosling, who classiﬁed the world population
into four levels of income, on a logarithmic scale from $1/day (extreme poverty) to $64/day
(which gets you electricity, a car, a washing machine, etc.). Using this scale, he says
(emphasis added):
The miracle of the Industrial Revolution is now easily stated: In 1800, 85% of the world's
population was at Level 1. Today, only 9% is. Over the past half century, the bulk of
humanity moved up out of Level 1 to erase the rich-poor gap and make the world wealth
distribution roughly bell-shaped.
The average American moved from Level 2 in 1800, to level 3 in 1900, to Level 4 in
2000.
We can state the Great Stagnation story nearly as simply: There is no level 5.
Where Is My Flying Car? paints a vivid picture of what Level 5 would look like, and why we
should keep working to get there.
The roots of stagnation
So, why aren't we on Level 5 yet? What caused the Great Stagnation? What ﬂatlined the
Henry Adams Curve? Why don't we have nanotech manufacturing and nuclear-powered
everything? And where is my ﬂying car?
Hall blames a number of political and cultural factors:
Centralized funding
He starts with a case study on nanotech. True nanotech, he says, was killed by federal
funding. Well, not by federal funding directly, but by a storm of academic politics that
followed predictably from the $500 million National Nanotech Initiative kicked oﬀ under
President Clinton. With a new pot of money on the table, and with academic funding being
largely a zero-sum game, researchers in adjacent ﬁelds responded in two ways. First, they
rebranded whatever they were doing "nanotech", even projects such as nanoscale materials
science that are unrelated to the original vision of atomically precise manufacturing. Second,
they aggressively attacked that original vision. The result was that all the funding and
credibility for true nanotech evaporated.

Hall cites a passage from Machiavelli written in the 1500s that describes how politically
dangerous it is to attempt to introduce an innovation: all of those who will be the losers if
you succeed are galvanized against you, whereas those who would be the winners are much
less motivated, given how speculative and uncertain the new innovation is. Seeing sixteenth-
century social theory perfectly describe modern academic politics, Hall dubs this "The
Machiavelli Eﬀect." And he cites other instances: cold fusion research, he says, was killed by
a similar process.
He concludes that "the increasing centralization and bureaucratization of science and
research funding" is a major culprit:
Centralized funding of an intellectual elite makes it easier for cadres, cliques, and the
politically skilled to gain control of a ﬁeld, and they by their nature are resistant to new,
outside, non-Ptolemaic ideas. The ivory tower has a moat full of crocodiles.
It is at the least suspicious, one must admit, that the major runup in civilian federal funding
for research pretty nearly coincides with the recent period of technological slowdown.
The burden of regulation
Hall quotes a post on a message board suggesting that even if you had built a ﬂying car and
were ready to take to the air, you'd be shot down by the FAA, the mayor, the news media,
the insurance company, and your neighbors. An even greater regulatory burden applies to
nuclear power, which Hall blames for the skyrocketing cost of power plants in the US:

In addition to the direct friction this burden places on innovation, it's also a drain on human
capital:

How much of a drain?
According to a study conducted by Tillinghast-Towers Perrin, the cost of the U.S. tort
system consumes about two percent of GDP, on average. If we assume this mostly
started around 1980 when lawyers skyrocketed and the airplane industry was destroyed,
the long-run compound-interest eﬀect on the economy as a whole is startling: without it
our economy today would be twice the size it actually is. This is the closest we can come
to measuring the eﬀect of taking more than a million of the country's most talented and
motivated people and put them to work making arguments and ﬁling briefs against each
other so their eﬀorts mostly cancel out, instead of inventing, developing, and
manufacturing things which could have made life better.
The counterculture
Through maybe the 1950s, visions of the future, although varied, were optimistic. People
believed in progress and saw technology as taking us forward to a better world. In the span
of a generation, that changed, with the shift becoming prominent by the late 1960s. A
"counterculture" arose which did not believe in technology or progress: indeed, a major
element of the counterculture was the environmentalist movement, much of which saw
technology and industry as actively destroying the Earth.
In H. G. Wells's The Time Machine, the "Eloi" were a weak, dissolute race of useless people
who contribute nothing to society (a parody of the idle rich of 19th-century England). Hall
calls the activists of the counterculture the "Eloi Agonistes", and blames them for
"ergophobia" and for excessive regulation:
Unlike a century ago, today for everyone who is working on advancing technological
progress, there is someone else who fervently believes that they are saving the planet

by stopping them.
Just as much as legal compliance and litigation, social activism is a drain on human capital:
... simply the diversion of so many of the most talented and motivated members of the
last several generations from productive pursuits to expensive virtue signaling is one of
the main causes of the technological slowdown and the Great Stagnation. If your
neighbor is Saving the Planet, it seems somehow less valuable merely to keep clean
water running in the mains, or ﬁll potholes, or build bridges. Eloi Agonistes have stolen
the respect and gratitude that the people who are actually doing valuable work should
be getting.
The shift in values was reﬂected in, and reinforced by, a shift in science ﬁction towards the
dystopian:
Science ﬁction has a long and valuable history of providing us with visions of a better
world. Verne, Wells, Burroughs, Gernsback—even Bellamy—much less Campbell, Doc
Smith, van Vogt, Heinlein, Asimov, Garrett, Piper, Niven, and Pournelle, provided people
with places and lives they could imagine and aspire to create. Science ﬁction since the
Sixties has signally failed in that regard; we have been fed, by and large, a diet of
Chicken Little soup in a pot of message, ladled out over leg of Frankenstein.
Where did the Eloi Agonistes come from, and why did they rise when they did? Hall suggests
a couple of related factors. One, the success of industrial civilization at meeting everyone's
basic needs for food, clothing and shelter pushed people up Maslow's Hierarchy to seek self-
actualization, which they did in the form of social activism. Two, the closing of the frontier
meant the loss of a world in which people had to contend directly with nature and reality:
After a long period of sustained social interaction, many forms of self-deception will
become baked into the culture, and major social institutions will become in large part
vehicles for virtue signalling.... But on the frontier, where a majority of one's eﬀorts are
not in competition with others but directly against nature, self-deception is considerably
less valuable. A culture with a substantial frontier is one with at least a countervailing
force against the cancerous overgrowth of largely virtue-signalling, cost-diseased
institutions.
Personally, I don't think these explanations tell the whole story. If people needed self-
actualization, why choose anti-technology crusades? Why not self-actualize through
invention, or art? I think we need to ﬁnd an explanation not only for the form of people's
behavior, but for the content. Deirdre McCloskey suggests that the intellectual class had
turned against capitalism and industry as early as 1848 (and Ayn Rand traces the intellectual
roots to the late 1700s, blaming Immanuel Kant for killing the Enlightenment). This remains
an open question for me.
There are many writers with optimistic visions of the future. However, the goals I most often
hear are all the negation of negatives: cure cancer, eliminate poverty, stop climate change.
This is good, but it is not enough. We should not only cure disease and let everyone live to
what is now considered old age—we should cure aging itself and extend human lifespan
indeﬁnitely. We should not seek to merely sustain current per-capita energy usage—we
should get back on the Henry Adams Curve and increase it. We should not only seek to avoid
worsening the climate—we should seek to actively control and optimize it for human ends.
We should not merely get the whole world up to Level 4—we should be striving for Level 5.
Aiming only for the former, as some so-called techno-optimists do, is a poor sort of optimism.
It is actually calling for very limited progress, followed by stagnation. It is complacency with
the status quo, content with bringing the whole world up to the current best standard of

living, but not increasing it. In this context, I found Where Is My Flying Car? refreshing. Hall
unabashedly calls for unlimited progress in every dimension.
My only signiﬁcant criticism (well, other than the malformatted data tables) is that the
content isn't tightly organized; the chapters jump around a lot. And there are a number of
very deep dives and long digressions on detailed technical topics; I mostly enjoyed these,
but if you're not into them, feel free to skim.
Overall, though, I found the book captivating and it has become one of my favorite books on
stagnation and progress. Recommended for all my readers.

The 300-year journey to the covid
vaccine
This is a linkpost for https://rootsofprogress.org/immunization-from-inoculation-to-rna-
vaccines
A covid vaccine has demonstrated 90% eﬃcacy and no signiﬁcant safety concerns in
preliminary data from Phase 3 trials, according to an announcement today from Pﬁzer
and BioNTech SE. The trials aren't yet complete and the data hasn't yet been released
for independent veriﬁcation, but this is very good news. (More from STAT News.)
Pﬁzer/BioNTech's vaccine, like Moderna's, is based on "mRNA" technology. If approved
by the FDA, it will be the ﬁrst such vaccine to reach that milestone. From a long-term
progress perspective, this is a big deal.
Immunization technology has existed since the early 1700s (and the folk practices it
originated in go back centuries further.) We can see the whole 300-year history of the
technology as a quest to achieve immunity with ever-more safety and ever-fewer side
eﬀects. More recently, it has also become important to be able to react quickly to new
epidemics, such as covid.
Here's how immunization has advanced in stages:
Inoculation
All immunization is based on the observation that exposure to a disease often grants
immunity (temporary if not permanent) to subsequent exposure. Long before we knew
anything about antibodies or T-cells, people had noticed this simple correlation. Many
people got smallpox in the past, but almost no one got it twice. The goal of
immunization technology is to achieve that same immunity, but without having to
suﬀer the disease or to risk death or other side eﬀects.
The earliest form of immunization, then, was not a vaccine, but a method in which the
patient was given the actual disease itself, in a manner that would cause a mild rather
than a severe case of the illness. This was done with smallpox, and the technique was
called inoculation or variolation.
This worked with smallpox for two reasons. One, infectious material was easy to
obtain, from the pustules caused by the disease itself. Second, contracting the disease
through a scratch on the skin caused a much more mild form than contracting it more
naturally through inhalation.
Inoculation saved many people from smallpox. But there were downsides. First, the
patient still had to contract the disease, causing mild symptoms. Second, there was
still a small risk of a severe case; even the best inoculation methods had about a 0.2%
death rate. Third, the patient was still contagious while going through the illness, and
anyone who caught the disease naturally from an inoculated patient would get the
full, severe version. Inoculation thus risked outbreaks.

Vaccination
These problems were solved by the next stage: vaccination. It was observed that
cowpox infection granted some form of cross-immunity to smallpox. Thus, the
inoculation procedure could be performed using cowpox material, rather than
smallpox. Cowpox was a milder and non-lethal disease. This reduced the symptoms
and the risk of death, and eliminated the risk of smallpox outbreaks as a result of
immunization. This new technique, invented by Edward Jenner in 1796, was called
vaccination (from vacca, the Latin word for cow).
So far, however, the technique only worked for smallpox—not for tuberculosis,
malaria, inﬂuenza, cholera, or any of the other major diseases that caused something
like half of all deaths in that era.
Engineered vaccines
The next stage would wait almost ninety years. Louis Pasteur, a pioneer of
microbiology who along with Robert Koch established the germ theory, was the ﬁrst to
discover how to create vaccines for any disease other than smallpox.
Cowpox can be seen as a "natural vaccine" against smallpox: a natural virus that
grants smallpox immunity but produces milder side eﬀects. Pasteur's accomplishment
was to create artiﬁcial, engineered vaccines.
There are essentially two ways to do this. The germ that causes the disease, or
pathogen, can be modiﬁed chemically, "killing" or inactivating it. This can be done
through heat, through chemicals such as formaldehyde, or through other means. Or it
can be modiﬁed biologically, attenuating (i.e., weakening) it. This is done by evolving
the virus or bacterium for many generations in an animal or tissue culture that is
suﬃciently diﬀerent from the target patient. For instance, Pasteur found that
"passing" a disease called swine erysipelas through many generations of rabbits
caused it to be less virulent in pigs.
These techniques allowed vaccines to be created for more diseases, and many were
created in the decades that followed. The other advantage was a reduction in side
eﬀects. By weakening or inactivating the pathogen, the patient no longer had to suﬀer
through a full infection in order to receive immunity.
But attenuated and killed vaccines still had risks. If a killed vaccine was not properly
manufactured, it could contain some portion of live germs, as happened with one of
the makers of the ﬁrst polio vaccine. And a live attenuated vaccine could always
mutate back into a virulent form. In either case, the vaccine would cause the very
disease it was designed to prevent, not only in the unlucky patient but potentially in a
new contagious outbreak.
Subunit vaccines
A way to prevent this risk is by giving the patient, not an entire virus or bacterium, not
even a weakened or inactivated one, but just a portion of the pathogen.

This works because of the way the immune system functions. In essence, it detects
foreign substances in the body and produces new molecules, called antibodies, that
bind to these substances and get in their way, preventing them from doing damage.
This process takes time for a new, never-before-seen infection, but after the ﬁrst
encounter, a record of the antibody is stored in the body's immunological memory,
which enables a quicker reaction to subsequent infection. A foreign substance that
stimulates the production of antibodies is called an antigen.
All immunization works by this process of priming the immunological memory using
antigens. The key observation is that even a piece of the pathogen can be used as an
antigen, and the antibodies thus generated are eﬀective against the full pathogen
itself. An antigen that is not a pathogen is exactly what we want: a substance that
produces immunity without producing disease.
For example, consider the SARS-CoV-2 virus that causes covid. You've probably seen it
rendered as a spiky ball. Those spikes are crucial to the virus's function: they stick to
your body's cells like tentacles, as the ﬁrst step of the infection. A subunit covid
vaccine, then, works by injecting just the spike into the body, rather than the full
virus. The body learns to generate antibodies against the spike, and those antibodies
are eﬀective against covid itself. The big advantage of this, of course, is that a single
piece of a pathogen cannot replicate and thus cannot cause an infection or become
contagious.
But how is the subunit antigen to be manufactured? Inactivated or attenuated
vaccines can start with the original virus or bacterium and grow it in culture. Subunit
vaccines can be created in a similar way, by culturing the pathogen and then breaking
it apart in order to extract the desired piece. With modern biotech, however, there are
other ways. If the antigen is a protein (as in the case of the covid spike), it can be
manufactured in genetically engineered microbes. Start with a single-celled organism
such as E. coli or baker's yeast. Insert the DNA that codes for the subunit protein into
its genome using recombinant DNA technology. Replicate these cells until you have a
whole vat of them creating vaccine proteins for you. (The same technology makes
other synthetic biologics, such as insulin.)
RNA vaccines take this idea the next logical step.
RNA vaccines
A yeast cell can function as a biological factory, producing proteins according to a
programmed genetic code. But every cell in your body is also such a factory, with the
same fundamental machinery.
An RNA vaccine skips the step of programming single-celled organisms to produce the
antigen for us: it sends the genetic code for the antigen directly to your own cells, and
they produce the antigen. These vaccines, then, are the only kind that do not inject
the antigen directly into the body; genetic instructions are injected instead. (Note
that, unlike with recombinant DNA technology, the DNA of your cells is not modiﬁed.)
To my (limited) understanding, this does not produce a signiﬁcantly diﬀerent immune
response than injecting the antigen directly. However, it makes a big diﬀerence in how
these vaccines are designed, developed, and manufactured, which aﬀects our ability
to respond quickly to new outbreaks such as covid. Once the virus's genetic code is
sequenced, the virus itself does not need to be handled in order to create a vaccine.

The vaccine is based entirely on genetic material, and can be created using genetic
synthesis techniques. Every pathogen is diﬀerent in how it can be grown in culture,
and in what it takes to inactivate it, weaken it, or break it apart into subunits. Genetic
techniques, by contrast, can be much more standardized. This doesn't make the
development of these vaccines trivial; there are still many problems to be solved for
each one (such as the delivery mechanism to get it into the cells, where the genetic
program will be executed). But as we are seeing, their development can be
signiﬁcantly faster than traditional techniques.
When you get your covid shot (probably in 2021), take a moment to think back on the
300 years of progress that got us to this point.

Nuclear war is unlikely to cause human
extinction
A number of people have claimed that a full-scale nuclear war is likely to cause human
extinction. I have investigated this issue in depth and concluded that even a full scale
nuclear exchange is unlikely (<1%) to cause human extinction. 
By a full-scale war, I mean a nuclear exchange between major world powers, such as the US,
Russia, and China, using the complete arsenals of each country. The total number of
warheads today (14,000) is signiﬁcantly smaller than during the height of the cold war
(70,000). While extinction from nuclear war is unlikely today, it may become more likely if
signiﬁcantly more warheads are deployed or if designs of weapons change signiﬁcantly.
There are three potential mechanisms of human extinction from nuclear war:
1) Kinetic destruction
2) Radiation
3) Climate alteration
Only 3) is remotely plausible with existing weapons, but let's go through them all.
1) Kinetic destruction
There simply aren't enough nuclear warheads to kill everyone directly with kinetic force, and
there likely never will be. There are ~14,000 nuclear weapons in the world, and let's suppose
they have an average yield of something like 1 megaton. This is a conservative guess, the
actual average is probably closer to 100 kilotons. With a 1 megaton warhead, you can create
a ﬁreball covering 3 km², and a moderate pressure wave that knocks down most residential
houses covering 155 km². The former kills nearly everyone and the latter kills a decent
percentage of people but not everyone. Let's be conservative and assume the pressure wave
kills everyone in its radius. 14,000 * 155 = 2.17 million km². The New York Metro area is
8,683 km². So all the nuclear weapons in the world could destroy about 250 New York Metro
areas. This is a lot! But not near enough, even if someone intentionally tried to hit all the
populations at once. Total land surface of earth is: 510.1 million km². Urban area, by one
estimate, is about 2%, or 10.2 million km.² Since the total possible area destroyed from
nuclear weapons is ~2.17 million km² is considerably less than a lower bound on the area of
human habitation, 10.2 million km², there should be basically no risk of human extinction
from kinetic destruction.

The circle with the white border indicates the zone of moderate
blast damage radius (5 psi): 7.03 km (155 km²) from a 1,000
kiloton warhead, link to nukemap
If you want to check my work there, I was using nuke map. 
The even more obvious reason why kinetic damage wouldn't lead to human extinction is that
nuclear states only threaten one or several countries at a time, and never the population
centers of the entire world. Even if NATO countries and Russia and China all went to war at
the same time, Africa, South America, and other neutral regions would be spared any kinetic
damage. 
2) Radiation
Radiation won't kill everyone because there aren't enough weapons, and radiation from them
would be concentrated in some areas and wholly absent from other areas. Even in the worst
aﬀected areas, lethal radiation from fallout would drop to survivable levels within weeks.

Here it's worth noting that there is an inherent tradeoﬀ between length of halﬂife and energy
released by radionuclides. The shorter the half life the more energy will be released, and the
longer the half life the less energy. The fallout products from modern nuclear weapons are
very lethal, but only for days to several weeks. 
From Nuclear War Survival Skills, 1987 edition
Let's try the same calculation we used with kinetic damage, and see if an attack aimed at
optimizing fallout for killing everyone could succeed. Using Nukemap again, I'll go with the
fallout contour for 100 rads per hour. 400 rads is thought too be enough to kill 50% of
people, so 100 rads per hour is likely to kill most all people not in some kind of shelter. We
need to switch to using a groundburst detonation rather than an airburst detonation,
because groundbursts create far more fallout. A 1mt ground burst would create an area of
about 8,000 km²  of >100 rads per hour. Okay, multiple that by 14,000 warheads, and we
get 112 million km². That's a lot! It's still less than the  510.1 million km² of earth's land
mass, but it's a lot more than the ~10.2 million km² of urban space. Presumably this is
enough to cover every human habitation, so in principle, it might be possible to kill everyone
with radiation from existing nuclear weapons.
 

The bright red and slightly less bright red indicate fallout
contour for 1,000 rads and 100 rads per hour, covering
1,140 km² and 7,080 km² respectively, from a 1,000 kiloton
ground burst. Nukemap settings
In practice, it would be almost impossible to kill every human via radiation with the existing
nuclear arsenals, even if they were targeted explicitly for this purpose. The ﬁrst reason is
that fallout patterns are very uneven. After a ground burst, fallout is carried by the wind.
Some areas will be hit bad and some areas will be hardly aﬀected by fallout. Even if most
human population centers were covered, a few areas would almost certainly escape.
Two other things make extinction by radiation unlikely. Many countries, especially in the
southern hemisphere, are unlikely to be aﬀected by fallout much at all. Since most of these
countries are likely to be neutral in a conﬂict, and not near combatant countries, they should
be relatively safe from fallout. While fallout might travel hundreds of kms, it still won't reach

places separated by greater distances. Fallout that reaches the upper atmosphere will
eventually fall back down, but usually after the period of lethal radioactivity.  The other
mitigating factor is that in typical nuclear war plans, ground bursts are usually restricted to
hardened targets, and air bursts are favored for population and industry centers. This is
because air bursts maximize the size of the destructive pressure wave. Air burst detonations
result in little lethal fallout reaching the ground, so populations not downwind of military
targets would likely be safe from the worst of the radiological eﬀects in a war scenario.
The ﬁnal protection from extinction by radiation is simply large amounts of mass between
people and the radiation source, in other words, fallout shelters. After several weeks, the
radionuclides in fallout from ground burst detonations will have decayed to the point where
humans can survive outside of shelters. Many fallout shelters exist in the world, and many
more could be made easily in a day or two with a shovel, some ground, and some boards. 
Even if lethally radioactive fallout from ground bursts covered all population centers, many
humans would still survive in shelters.
The risks of extinction from nuclear-weapon-induced-radiation wouldn't be complete without
discussing two factors: nuclear power plants and radiological weapons. I'm only going to
cover these brieﬂy, but they both don't change the conclusions much.
Nuclear power plants could be targeted by nuclear weapons to create large amounts of
fallout with a longer half-life but less energy per unit time. The main concern here is that
nuclear power plants and spent fuel sites contain a much greater *mass* of radioactive
material than nuclear missiles can carry. The danger comes primarily from spreading the
already very radiative spent or unspent nuclear fuel. The risk this poses requires a longer
analysis, but the short version is that while nuking a nuclear power plant or stored fuel site
would indeed create some pretty long-lived fallout it would still be concentrated in a
relatively small area. Fortunately, even a nuclear detonation wouldn't spread the nuclear fuel
more than several hundred km at most. Having regions of countries covered in spent nuclear
fuel would be awful, but it doesn't much raise the risk of extinction.
Radiological weapons are nuclear weapons designed to maximize the spread of lethal fallout
rather than destructive yield. The particular concern from the extinction perspective is that
they can be designed to create fallout that continues to emit levels of radiation that can
make an area uninhabitable for months to years. These kind of radiological weapons kill
more slowly, but they still kill. In principle, radiological weapons could be used to kill
everyone on earth. However, in practice, the same constraints that apply to standard nuclear
weapons apply to weapons optimized for long-lasting fallout, as well as some additional
constraints. 
Radiological weapons wouldn't produce more fallout than standard warheads, they would
just produce fallout with diﬀerent characteristics. As a result the amount of radiological
weapons required to cover every part of earth's surface would be massively expensive (likely
as expensive as the largest existing nuclear arsenals), and serve no military purpose. Their
ineﬃciency in destruction and death compared to standard nuclear weapons is probably why
radiological weapons have never been developed or deployed in large numbers. This makes
them an ongoing theoretical concern, but not an existential risk in the immediate future. A
concerning development is Russia's claim to have developed a large-yield (100mt)
submersible nuclear weapon with the suggestion that it could be used as a radiological
weapon, but even if this is true, it's unlikely to be deployed in large numbers.
3) Climate alteration
The bulk of the risk of human extinction from nuclear weapons come from risks of
catastrophic climate change, nuclear winter, due to secondary eﬀects from nuclear
detonations. However, even in most full-scale nuclear exchange scenarios, the resulting
climate eﬀects are unlikely to cause human extinction.

Reasons for this:
a) Under scenarios where a severe nuclear winter occurs as described by Robock et al, some
human populations would likely survive.
b) The Robock group's models are probably overestimating the risk
c) Nuclear war planners are aware of nuclear winter risks and can incorporate these risks
into their targeting plans
Before diving into each subject, it's worth understanding the background of nuclear winter
research. In the 1980s a group of atmospheric scientists proposed the hypothesis that a
nuclear war would result in massive ﬁrestorms in burning cities, which would loft particles
high into the atmosphere and cause catastrophic cooling that would last for years. Many
found it alarming that such an eﬀect could be possible and go unnoticed for decades while
the risk existed. Some scientists also thought the proposed eﬀect was too strong, or unlikely
to occur at all. Until a few years ago, if you looked only at peer reviewed literature you would
only ﬁnd papers forecasting severe nuclear winter eﬀects in the event of a nuclear war.
Understandably, many people assumed that this was the scientiﬁc consensus. Unfortunately,
this misrepresented the scientiﬁc community's state of uncertainty about the risks of nuclear
war. There have only ever been a small numbers of papers published about this topic (<15
probably), mostly from one group of researchers, despite the topic being one of existential
importance.
I'm very glad Robock, Toon, and others have spent much of their careers studying nuclear
winter eﬀects, and their models are useful in estimating potential climate change caused by
nuclear war. However, I've become less convinced over time the Robock model is largely
correct. See section B below for why I've changed my mind. However, I'm quite uncertain
about the probability of strong cooling eﬀects from nuclear war, and am still quite concerned
about the potential for severe cooling, even if the risk of extinction from such events is small.
A:  Under scenarios where a severe nuclear winter occurs as described by Robock
et al, some human populations would likely survive.
The latest and most detailed model of potential cooling eﬀects from a fullscale nuclear
exchange comes from, Robock et al.,  "Nuclear winter revisited with a modern climate model
and current nuclear arsenals: Still catastrophic consequences" found here.
The eﬀects from this model are severe. In the 150Tg case, after a year, summer
temperatures in the Northern hemisphere are 10-30 degrees C cooler. The eﬀects are less
severe at the equator (5 degrees C), but basically all places in the world are aﬀected. The
most likely outcome is that most people starve to death. Many would freeze too, but
starvation is likely the greatest risk. Even in this model, it appears that in equatorial regions,
some farming would still be possible, enough for some populations to survive. After a 10-15
years, agriculture in most of the world would be possible at reduced capacity.

Surface air temperature changes for the 150 Tg case averaged for June, July, and
August of the year of smoke injection and the next year. Robock et al., 2007
Carl Shulman asked one of the authors of this paper, Luke Oman, his probability that the
150Tg nuclear winter scenario discussed in the paper would result in human extinction, the
answer he gave was "in the range of 1 in 10,000 to 1 in 100,000." This strikes me as quite
plausible, though one expert opinion is no substitute for a deep analysis. The Q&A with
Oman contains his reasoning for this assessment.
Two diﬀerent analyses are required to calculate the chances of human extinction from
nuclear winter. The ﬁrst is the analysis of the climate change that could result from a nuclear
war, and the second is the adaptive capacity of human groups to these climate changes. I
have not seen an in depth analysis of the latter, but I believe such an assessment would be
worthwhile.  
My own guess is that humans are capable of surviving far more severe climate shifts than
those projected in nuclear winter scenarios. Humans are more robust than most any other
mammal to drastic changes in temperature, as evidenced by our global range, even in pre-
historic times. While a loss of most agriculture would likely kill most people on earth, modern
technology would enable some populations to survive. Great stores of food currently exist in
the world, and it is l likely that some of these would be seized and protected by small groups,
providing enough food to last for years. While even such populations with food stores
wouldn't have enough to survive for 10-15 years, such food stores would give groups time to
adapt to new food sources. The organization ALLFED has explored a number of alternative
food sources that could keep populations alive in the event of a nuclear war or other large
solar disruption, and I expect great necessity to drive the discovery of even more in the
event of such a disaster.
B: The Robock group's models are probably overestimating the risk

The nuclear winter model at its simplest: Nuclear detonations → Fires in cities → Firestorms in
cities → Lofted black carbon into the upper atmosphere → black carbon persists in upper
atmosphere, reﬂecting sunlight and causes massive cooling
Each step is required in order for the eﬀect to occur. If nuclear war causes massive ﬁres in
cities but does not lead to ﬁrestorms that loft particles, then no long term cooling is going to
occur. Some of these steps are easier to model than others. Based on my reading of the
literature, the greatest uncertainties involve the dynamics of cities burning after a nuclear
attack, and whether the conditions would produce ﬁrestorms suﬃcient to loft large numbers
of particles high enough in the atmosphere to persist for years.
We're ﬁnally beginning to see some healthy debate about some of these questions in the
scientiﬁc literature. Alan Robock's group published a paper in 2007 that found signiﬁcant
cooling eﬀects even from a relatively limited regional war. A group from Los Alamos, Reisner
et al, published a paper in 2018 that reexamined some of the assumptions that went into
Robock et al's model, and concluded that global cooling was unlikely in such a scenario.
Robock et al. responded, and Reisner et al responded to the response. Both authors bring up
good points, but I ﬁnd Reisner's position more compelling. This back and forth is worth
reading for those who want to investigate deeper. Unfortunately Reisner's group has not
published an analysis on potential cooling eﬀects from a modern full scale nuclear exchange,
rather than a limited regional exchange. Even so, it's not hard to extrapolate that Reisner's
model would result in far less cooling than Robock's model in the equivalent situation.
C: Nuclear war planners are aware of nuclear winter risks and can incorporate
these risks into their targeting plans
A very simple way to reduce risks from nuclear winter is to refrain from targeting cities with
nuclear weapons. The proposed mechanism behind nuclear winter results from cities
burning, not ground bursts on military targets. I've spoken with some of the oﬃcials in the
US defense establishment responsible for nuclear war planning, and they're well aware of
the potential risks from nuclear winter. Of course, being aware of the risks does not
guarantee they will have reasoned about the risks well, or have engaged in good risk
management practices. However, the fact that this risk is well publicized makes it more likely
that nuclear war planners will take steps to minimize blowback risk from climate eﬀects.
It's hard to know to what extent this has been done. Nuclear war plans are classiﬁed, and as
far as we know current US nuclear war plans do target cities under some circumstances but
not under others. However, the defense establishment has access to classiﬁed information
and models that we civilians do not have, in addition to all the public material. I'm conﬁdent
that some nuclear war planners have thought deeply about the risks of climate change from
nuclear war, even though I don't know their conclusions or bureaucratic constraints. All else
being equal, the knowledge of these risks makes military planners less likely to accidentally
cause human extinction.
Conclusion
This post discussed the three plausible mechanisms of human extinction caused by nuclear
weapons. The fact that one of these mechanisms, nuclear winter, wasn't characterized until
the 1980s, is a good reminder of the possibility of unknown unknowns. While nuclear tests
provided information about the eﬀects of these weapons, the test environments were
signiﬁcantly diﬀerent than war environments. Large model uncertainties remain. Given that
the greatest existential threat from nuclear war appears to be from climate impacts, it would
be great to see more researchers study the climate eﬀects from nuclear war and the
resilience capacity of diﬀerent human groups.
There appear to be several interventions possible for reducing existential risk from nuclear
war. At the policy level, a commitment from the largest nuclear powers to refrain from

targeting the majority of cities would reduce risk of accidental omnicide. Improving the
maximum resilience capacity of human populations best positioned to survive a nuclear
winter would also make humanity less vulnerable to nuclear winter, and could also protect
against other existential threats.
Further reading
Toby Ord conducts a quantitative estimate of extinction risk from nuclear war in:
The Precipice: Existential Risk and the Future of Humanity
Nuclear War as a Global Catastrophic Risk
Nuclear winter and human extinction: Q&A with Luke Oman (by Carl Shulman)
http://www.overcomingbias.com/2012/11/nuclear-winter-and-human-extinction-qa-with-luke-
oman.html
Nuclear Winter Responses to Nuclear War Between the United States and Russia in the Whole
Atmosphere Community Climate Model Version 4 and the Goddard Institute for Space Studies
ModelE
https://www.semanticscholar.org/paper/Nuclear-Winter-Responses-to-Nuclear-War-Between-
the-Coupe-Bardeen/560033106c2d599bcace3ce4cb6c67d5b713ec50
Climate Impact of a Regional Nuclear Weapons Exchange: An Improved Assessment Based
On Detailed Source Calculations
https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/2017JD027331?
fbclid=IwAR0SlQ_naiKY5k27PL0XlY-3jsocG3lomUXGf3J1g8GunDV8DPNd7birz1w
Comment on "Climate Impact of a Regional Nuclear Weapon Exchange: An Improved
Assessment Based on Detailed Source Calculations" by Reisner et al.
https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2019JD030777
Reply to Comment by Robock et al. on "Climate Impact of a Regional Nuclear Weapon
Exchange: An Improved Assessment Based on Detailed Source Calculations"
https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2019JD031281 
Comparing Economic and Crop Models: The Case of Climatic and Agricultural Impacts of
Nuclear War
https://www.gtap.agecon.purdue.edu/resources/download/9185.pdf

My intellectual inﬂuences
Prompted by a friend's question about my reading history, I've been thinking about
what shaped the worldview I have today. This has been a productive exercise, which I
recommend to others. Although I worry that some of what's written below is post-hoc
confabulation, at the very least it's forced me to pin down what I think I learned from
each of the sources listed, which I expect will help me track how my views change
from here on. This blog post focuses on non-ﬁction books (and some other writing);
I've also written a blog post on how ﬁction has inﬂuenced me.
My ﬁrst strong intellectual inﬂuence was Eliezer Yudkowsky's writings on Less Wrong
(now collected in Rationality: from AI to Zombies). I still agree with many of his core
claims, but don't buy into the overarching narratives as much. In particular, the idea
of "rationality" doesn't play a big role in my worldview any more. Instead I focus on
speciﬁc habits and tools for thinking well (as in Superforecasters), and creating
communities with productive epistemic standards (a focus of less rationalist accounts
of reason and science, e.g. The Enigma of Reason and The Structure of Scientiﬁc
Revolutions).
Two other strong inﬂuences around that time were Scott Alexander's writings on
tribalism in politics, and Robin Hanson's work on signalling (particularly Elephant in
the Brain), both of which are now foundational to my worldview. Both are loosely
grounded in evolutionary psychology, although not reliant on it. More generally, even
if I'm suspicious of many individual claims from evolutionary psychology, the idea that
humans are continuous with animals is central to my worldview (see Darwin's
Unﬁnished Symphony and Are We Smart Enough to Know How Smart Animals Are?). In
particular, it has shaped my views on naturalistic ethics (via a variety of sources, with
Wright's The Moral Animal being perhaps the most central).
Another big worldview question is: how does the world actually change? At one point I
bought into techno-economic determinism about history, based on reading big-picture
books like Guns, Germs and Steel and The Silk Roads, and also because of my
understanding of the history of science (e.g. the prevalence of multiple discovery).
Sandel's What Money Can't Buy nudged me towards thinking more about cultural
factors; so did books like The Dream Machine and The Idea Factory, which describe
how many technologies I take for granted were constructed. And reading Bertrand
Russell's History of Western Philosophy made me start thinking about the large-scale
patterns in intellectual history (on which The Modern Mind further shaped my views).
This paved the way for me to believe that there's room to have a comparable
inﬂuence on our current world. Here I owe a lot to Tyler Cowen's The Great Stagnation
(and to a lesser extent its sequels), Peter Thiel's talks and essays (and to a lesser
extent his book Zero to One), and Paul Graham's essays. My new perspective is
similar to the standard "Silicon Valley mindset", but focusing more on the role of ideas
than technologies. To repurpose the well-known quote: "Practical men who believe
themselves to be quite exempt from any intellectual inﬂuence are usually the slaves
of some defunct philosopher."
Here's a more complete list of nonﬁction books which have inﬂuenced me, organised
by topic (although I've undoubtedly missed some). I'm no longer updating this post,
but here's a list of books I've enjoyed more recently. I welcome recommendations,
whether they're books that ﬁt in with these lists, or books that ﬁll gaps in them!

On ethics:
The Righteous Mind
Technology and the Virtues
Reasons and Persons
What Money Can't Buy
The Precipice
On human evolution:
The Enigma of Reason
The Human Advantage
Darwin's Unﬁnished Symphony
The Secret of our Success
Human Evolution (Dunbar)
The Mating Mind
The Symbolic Species
On human minds and thought:
Rationality: from AI to Zombies
The Elephant in the Brain
How to Create a Mind
Why Buddhism is True
The Blank Slate
The Language Instinct
The Stuﬀ of Thought
The Mind is Flat
Superforecasting
Thinking, Fast and Slow
On other sciences:
Scale: The Universal Laws of Life and Death in Organisms, Cities and Companies
Superintelligence
The Alignment Problem

Are We Smart Enough to Know How Smart Animals Are?
The Moral Animal
Ending Aging
Improbable Destinies
The Selﬁsh Gene
The Blind Watchmaker
Complexity: The Emerging Science at the Edge of Order and Chaos
Quantum Computing Since Democritus
On science itself:
The Sleepwalkers: A History of Man's Changing Vision of the Universe
The Structure of Scientiﬁc Revolutions
The End of Science
The Fabric of Reality
The Beginning of Inﬁnity
Reinventing Discovery
The Dream Machine
The Idea Factory
On philosophy:
A History of Western Philosophy
The Intentional Stance
From Bacteria to Bach and Back
Good and Real
The Big Picture
Consciousness and the Social Brain
An Enquiry Concerning Human Understanding
On history and economics:
The Shortest History of Europe
A Farewell to Alms
The Technology Trap

Iron, Steam and Money
The Enlightened Economy
The Commanding Heights
A Short History of Nearly Everything
The Modern Mind
23 things they don't tell you about capitalism
Why are the prices so damn high?
The Silk Roads
Sapiens
The Historical Figure of Jesus
On politics and society:
Destined for War
Prisoners of Geography
How Democracy Ends
Why Nations Fail
Factfulness
What Terrorists Want
The Lexus and the Olive Tree
Bowling Alone
Antifragile
The Female Eunuch
On life, love, etc:
Deep Work
Man's Search for Meaning
More Than Two
Authentic Happiness
Happiness by Design
Written in History
Other:

Age of Em
Immortality: The Quest to Live Forever and How It Drives Civilization
Surely you're Joking, Mr Feynman
Impro
Never Split the Diﬀerence

Probability vs Likelihood
This expands on some issues I mentioned in Radical Probabilism.
A rationality pet-nitpick of mine, which is shared by almost no one, is the probable/likely
distinction. I got my introduction to Bayesian thinking, in part, from Probabilistic Reasoning in
Intelligent Systems by Judea Pearl. In the book, Pearl makes a simple distinction between
probability and likelihood which I ﬁnd to be quite wonderful: the likelihood of X given Y is
just the probability of Y given X!
Probability of X given Y: P(X|Y )
Likelihood of X given Y: L(X|Y ) := P(Y |X)
Why invent new terminology for something so simple?
What we're basically doing here is making a type distinction between probability and
likelihoods, to help us remember that likelihoods can't be converted to probabilities without
combining them with a prior.
In the context of the book, it's because Bayesian networks pass two types of messages:
"probability" type messages pass prior-like information down the Bayesian network, and
"likelihood" type messages pass evidence-like information up the network.
At each individual node, we need to combine a prior with evidence in order to get a posterior.
However, we only start with prior information for the top nodes of the network, and we only
start with evidence information for the observed nodes. In order to get the information we
need for other nodes, we have to propagate information around the network via the two
types of messages.

Another important aspect of the probability/likelihood distinction is that we swap our views of
what varies and what remains ﬁxed. When we regard P(A|B) as a probability function, rather
than just a single probability judgement, we generally think of varying A. But when we use
P(A|B) to construct a likelihood function for hypothesis testing, we think of A as the ﬁxed
evidence, and more readily vary B, which is now thought of as the hypothesis. Pearl's
notation helps us track which thing we're holding steady vs which thing we're varying: we
think of the ﬁrst argument as what we're varying, and the second argument as what we're
holding steady. Critically, probability functions sum to 1, while likelihood functions need not
do so.
(Likelihood functions can sum to less than 1 if the event was improbable according to all the
hypotheses, or more than 1 if the event was quite probable according to all the hypotheses.)
The downward messages in a Bayesian network are probability functions; the upward
messages are likelihood functions. At each node, we combine these two pieces of
information via Bayes' Law to get a posterior probability function. (Important note: the
posterior probability is not the information we then pass on; this would create a double-
counting problem, just like two people reinforcing each other's opinions and then each taking
the strength of the other's opinion as yet more evidence in the same direction, even though
no new evidence has entered the system. Think of the posteriors as private conclusions,
people have reached, while their public messages only ever convey information which the
recipient of that message hasn't seen yet.)
Here's the posterior calculation for one node:

I suspect it's quite intuitive how the node needs to transform the P(A) message through the
P(B|A) matrix to get the P(A) it needs. What might be less intuitive is how the L(C) message
is transformed in the same way through the P(C|B) matrix, to get L(B). B then sends the P(B)
message down to node C, and the L(B) message up to node A.
You can see how the probability messages are transformed incrementally to become the
right prior information needed to apply Bayes' Rule at a given node; and similarly, the
likelihood messages are transformed incrementally to produce the right evidence information
needed at a given node.
OK, so a likelihood/probability distinction helps us organize Bayes Net calculations. Are there
any other reasons why we should take this distinction seriously?
Why It Matters
A major problem people have in applying Bayesian reasoning (at least in the kind of artiﬁcial
example problem you might see as a brain teaser / textbook question / etc) is neglecting the
givens: failing to use one or another of the numbers given to you in the problem which are
needed to compute the Bayesian update. It's understandable; there can be a lot of numbers
to keep track of. At least for me, it helps a lot to organize those numbers into probability
functions and likelihood functions (which lets me think in terms of 2 vectors rather than 4+
numbers right there), with the symmetric picture above, where our goal is to combine prior
and likelihood together to get a posterior. As I mentioned earlier, thinking of
probability/likelihood as a type distinction helps avoid mistakes combining the wrong
numbers: "a likelihood needs to combine with a prior before it can be regarded as a proper
probability". 

Base-Rate Neglect
I have a pet theory that some biases can be explained as a mix-up between probability and
likelihood. (I don't know if this is a good explanation.) For example, base-rate neglect is just
straightforwardly the mistake of treating a likelihood as a probability. A simple example of
base-rate neglect would be to think someone is lying to you because they deny lying when
you ask, just like a liar would. L(lying|response) is high, but this doesn't mean
P(lying|response) is high.
Conjunction Fallacy
Another bias which seems to ﬁt this pattern is the conjunction fallacy. Consider the classic
example:
Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a
student, she was deeply concerned with issues of discrimination and social justice, and
also participated in anti-nuclear demonstrations.
Which is more probable?
1. Linda is a bank teller.
2. Linda is a bank teller and is active in the feminist movement.
People rate "Linda is a bank teller" as less probable than "Linda is a bank teller and is active
in the feminist movement". Notice that the second is more likely given everything we're told
about Linda: L(teller & feminist | bio) > L(feminist | bio). So we can understand this as a
probability/likelihood confusion -- indeed, it's just an example of base-rate neglect.
(Keep in mind there's a long list of failed attempted explanations of this human behavior,
though, so don't rush to put a bunch of credence on mine.)
An optimistic theory is that a person makes these mistakes because we have a thing in our
head roughly corresponding to likelihood, due to Bayes-net-like cognition. Then we have an
explicit concept of probability. But if you don't also have an explicit concept of likelihood, the
thing in your head corresponding to likelihood doesn't have anywhere good to land when you
do explicit reasoning; so, it gets articulated as probability instead.
If so, having an explicit concept of likelihood should make these mistakes easier to avoid, as
you can label the thing in your head correctly: you are experiencing the feeling of likelihood,
not probability.
Using the Words
Practically everyone uses the terms probable/likely & probability/likelihood as
interchangeable terms, except in very formal situations (when talking speciﬁcally about
likelihood functions for hypothesis testing). I'm proposing that we make this distinction all
the time, for the good of our probabilistic reasoning skills. It's a big ask! This is a
signiﬁcantly diﬀerent way of speaking than we're used to.
It's fortunate that these words really are interchangeable in English; at least I'm not taking
away someone's ability to mean something. It's a simple matter of saying
"probable"/"probably" in every case where you would have used any of the four words, and
using "likely"/"likelihood" to indicate the thing which you would have previously labelled as
base-rate-negligent probability. (Or do you call it something else? Frankly, I'm not sure how
Bayesian people get by without a word for likelihood... what do you call it? How do you refer

to a theory which ﬁts the data well? How do you say that adding details can make a story
more plausible in the sense of explaining what we know, while making it less probable
overall?)
So, to help ease the transition, I'll try to come up with a number of examples.
A scenario is unlikely if it doesn't explain the evidence well. For example, in a COVID
epidemic, if you get sick, the prior probability that it's COVID might be relatively
high; but if your symptoms don't match COVID, it's unlikely; you can therefore
conclude that the probability is lower.
A scenario is likely if it explains the data well. For example, many conspiracy theories
are very likely because they have an answer for every question: a powerful group is
conspiring to cover up the truth, meaning that the evidence we see is exactly what
they'd want us to see.
You are trying to ﬁgure out who killed the butler. You ﬁnd a kitchen knife with the
butler's blood on it, placed behind a bush as if hastily hidden. It has the chef's
ﬁngerprints on it. Thus it's likely that the chef killed the butler, raising the
probability of that hypothesis. However, another similarly likely hypothesis is that
someone framed the chef by using gloves to wield a knife which the chef had touched
for other reasons. Because the chef regularly touches the kitchen knives, this alternate
hypothesis is relatively probable. Furthermore, because the chef has no motive, the
chef being the murderer is overall improbable.
You are studying the life of the early solar system. You have a set of four scenarios
mutually exclusive and jointly exhaustive (or close enough to exhaustive that you're
willing to treat it that way). Each scenario implies something about the distribution of
orbits in the Kuiper belt. When you measure the Kuiper belt, you see something you
were't expecting under any of your hypotheses. This is an improbable observation,
which now makes all four hypotheses unlikely. However, some hypotheses are more
unlikely than others. Because you didn't favor any of the hypotheses especially to
begin with, your Bayesian update now strongly favors the least unlikely hypothesis,
making that hypothesis quite probable. You begin to focus on the sub-scenarios
within than scenario which make the observation you saw the most probable (that is,
the most likely sub-scenarios). Out of these sub-scenarios, you narrow things down
further by considering which scenarios are actually probable. You publish an analysis
which takes all these considerations into account in order to report the most probable
scenarios for the early solar system.
You have several hypotheses about life on Venus. In order to test these hypotheses,
you measure the chemical composition of the atmosphere of Venus. Unfortunately,
your measurements are very probable under all of your hypotheses, so all of your
hypotheses remain likely. Your best guess is still your prior probability on each
hypothesis.
I found as I was writing the above examples that I wished there were three words, not two,
so that I could more conveniently distinguish prior probability from posterior probability. It's
very understandable that there's not, though, because one man's posterior is another man's
prior -- these concepts ﬂow into one another based on context. Likelihood is more clearly
distinguished, since it does not sum to 1.*
Ambiguity & Context-Sensitivity
A given conversational context has contextual assumptions, information which we've already
updated on, which gives us the prior; then we have information we're treating as
observations in the context, which gives us our likelihoods; then we have the information
we're treating as unknowns (hypotheses), which are what the prior and likelihoods are about.
Whenever we say "probability" or "likelihood", the reader/listener has to infer from context
what the assumptions/observations/unknowns are.

What's treated as assumption in one part of a conversation may be treated as unknown in
another part; observations at one point may become assumptions later on; and so on. There
is no strict rule that observations or assumptions are certain: as we see with a complex
Bayesian network, we can get likelihood messages which propagate up from some distance
away, which acts as uncertain evidence. Similarly, prior messages act as uncertain
assumptions.
(None of the things we routinely treat as evidence are certain. If I say I read the number 98
oﬀ a thermometer, what I mean is that I'm quite conﬁdent it was 98. In Radical Probabilism, I
attempted to convey Richard Jeﬀrey's view that none of our observations are certain.)
Nor does there need to be a strict temporal order behind these three categories. This will
often be the case, when we treat the past as given, and update on present observations, to
predict an unknown future. However, we can also try to explain the past (treating some
elements of the present as assumptions, and others as unknowns), or uncover the past
(treating it as unknown), or many other combinations.
You can think of a Bayes Net as a tool for keeping track of what constitutes
assumptions/observations in a consistent way for a bunch of possible unknowns, so that we
can split up our reasoning into a bunch of individual reasoning tasks that we can understand
with this trichotomy.
Exchanging Virtual Evidence
One of Pearl's analogies for Bayesian Networks is a network of people trying to communicate
about something. You can think of it as a network of experts. Each expert is in charge of one
special variable, and the experts only talk to other experts if their specialties are closely
related. If we take this analogy seriously, a striking fact is that these people are
communicating with virtual evidence. (I discussed virtual evidence in my post on Radical
Probabilism; the discussion there is a prerequisite for this section.)
Take the Bayes Net A->B->C. Expert B understands everything going on here, because B has
to talk to everyone. However, A doesn't understand C at all, and vice versa; if they talked to
each other, they would be mutually unintelligible. So, B has the job of translating between
them. B accepts messages from C, takes just the information about B which the message
implies, and passes that along to A (who understands B-talk). And similarly in the other
direction.
All of the experts trust one another, but because they can't understand the ontology of
distant evidence, they have to accept the virtual evidence implied by the likelihood functions
they're handed by those around them: they accept "there was some evidence which induced
this likelihood function", without worrying about whether the evidence is represented as a
proposition.
So that's one of the core uses of virtual evidence: conveying information when there's trust
of rationality, but no common language with which to describe all evidence.
(Again, one of Jeﬀrey's insights is that it's not important that the evidence is represented as
a proposition anywhere in the network; we don't need to require every update to come with
a crisp thing-which-we-updated-on. Virtual evidence is just a likelihood function which we
don't attribute to any particular proposition.)**
Notice that for Bayesian networks, it's much much more eﬃcient to pass information this
way; if we instead attempted to convey all the information explicitly, and make our experts
all understand each other, we would be back to calculating probabilities by taking a global
exponential-sized sum over possibilities.

However, as I mentioned brieﬂy earlier, we have to get things just right to avoid double-
counting information when we use message-passing like this.
Propagating Posterior Probabilities
In Aumann's agreement theorem, Aumann famously considers the case of Bayesians passing
information back and forth only by stating their new posterior probability after each
communication they receive. Bayesians could possibly communicate this way in situations of
incompatible ontologies, rather than using virtual evidence. But I'll argue it's not very
eﬃcient.
As I mentioned earlier, we want to avoid double-counting evidence. The outgoing messages
in a Bayesian network are always (proportional to) the posterior probability of a node,
divided by the incoming message along that same link. What this intuitively means is that
when Alice talks to Bob, Alice has to "factor out" the information she got from Bob in the ﬁrst
place; she only wants to tell Bob her other information, which Bob doesn't know yet.
If Alice and Bob communicate with posterior probabilities, then it's always up to the listener
to factor out their own message.
For example, suppose Alice tells Bob that it's a beautiful day outside. Bob nods. This tells
Alice that Bob agrees: it's a beautiful day outside. But Bob is reporting honest posterior
probabilities, so Bob's nod of assent probably just reﬂects his having updated on Alice. So
Alice does not further update; Bob is not giving Alice any further evidence. If Bob had
nodded vigorously, then Alice would interpret Bob's position as stronger even then hers; she
would thus infer that Bob had independent evidence that it's a beautiful day outside. In that
case, she would update.
The point is that in this situation, Alice has a somewhat diﬃcult job interpreting Bob's
message. Bob gives her his posterior, and she has to ﬁgure out what portion of that is new
evidence for her.
Propagating Likelihoods
If Alice and Bob instead communicate factoring out each other's information, then they
exchange virtual updates for each other, which are easy to take into account.
If Alice tells Bob "it's nice out" and Bob nods, Alice then knows that Bob has independent
conﬁrmation that it's nice out.
This is a much better case for Alice. She can form her posterior simply by multiplying in
Bob's apparent likelihood function, and renormalizing.
Unfortunately, in realistic cases, we have a worst-case scenario: we don't really have one
protocol. Whet someone expresses agreement or disagreement with a proposition, it's not
really clear whether they're giving their all-things-considered posterior, or factoring out the
information they got from us in the ﬁrst place in order to give us a clean update.
So, realistically, when Bob nods, Alice doesn't have a great idea of what it means at all.
I know the examples with Alice trying to interpret Bob's nods are slightly absurd; Bob is
obviously not giving Alice enough information there, so of course there are potential
interpretation issues. But I'm using these examples because I think these ideas about virtual
evidence and so on are mainly important in cases where there's not enough time to
communicate fully.

Even if you're having a long, thorough discussion about some topic of interest, there will be
many many small acts of communication with ambiguity and insuﬃcient follow-up. Maybe an
hour-long discussion on topic X involves 125 smaller propositions. 5 of these are major
propositions, which get about 10 minutes each. Those 10-minute discussions involve about
20 smaller propositions each. These smaller propositions are given varying amounts of time,
so many of them only get a few seconds.
So lots of propositions only get a brief response like a nod, or "sure", or one sentence.
In that brief amount of time, we form an impression of what each other think of that
proposition. We make some small update one way or another, based on a vague impression,
dozens or hundreds of times in an hour-long conversation.
I don't think it's psychologically or socially realistic for humans to only exchange virtual
evidence. It's too socially useful to express positive sentiment when someone tells us
something (reinforcing that we like them, we think they're reasonable, etc).
But I am interested in trying to do more little things to signal what type of information I'm
giving oﬀ.
Using phrases like "all things considered," signals that I'm giving my posterior.
Using phrases like "if you hadn't told me that," signals that I'm factoring out what you told
me.
If I say "I think that's very likely", it can mean that viewing what you told me as a hypothesis,
it ﬁts evidence in my possession well. Whereas "I think that's very probable" is more
probably a posterior (although the ambiguity of whether "probable" stands for posterior or
prior probability hurts us here).***
*: Not that I'd complain if someone came up with a decent proposal for a 3rd word. 
**: There's a complication here. Notice that the messages in a Bayesian network are of two
kinds: probability messages and likelihood messages. Yet I'm referring to them all as "virtual
updates" and claiming that virtual updates are a kind of likelihood. In a Bayesian network,
it's more natural to view some messages as probabilities, providing the priors for the local
node posterior calculations. In a network of experts, it's more natural to think that every
expert already has a prior when they start out, and so, only virtual evidence is
communicated between the experts. 
***: But it gets even more complicated than implied by (1), because as I argued, what's
natural to think of as evidence/assumption/unknown will shift around in any given
conversation, and must be inferred from context. I can't use "likely" to signal that I'm trying
to convey virtual evidence to you if we're in a context where we're considering some
hypothesis together, in light of some evidence; "likely" will sound like I mean something
makes that evidence probable, rather than saying that my personal evidence makes your
statement probable.

AGI Predictions
This post is a collection of key questions that feed into AI timelines and AI safety work
where it seems like there is substantial interest or disagreement amongst the
LessWrong community. 
You can make a prediction on a question by hovering over the widget and
clicking. You can update your prediction by clicking at a new point, and remove your
prediction by clicking on the same point. Try it out:
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
 
Add questions & operationalizations
This is not intended to be a comprehensive list, so I'd love for people to add their own
questions - here are instructions on making your own embedded question. If you have
better operationalizations of the questions, you can make your own version in the
comments. If there's general agreement on an alternative operationalization being
better, I'll add it into the post.
Questions
AGI deﬁnition
We'll deﬁne AGI in this post as a uniﬁed system that, for almost all economically
relevant cognitive tasks, at least matches any human's ability at the task. This is
similar to Rohin Shah and Ben Cottier's deﬁnition in this post.
Safety Questions
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%

1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
 
Timelines Questions
See Forecasting AI timelines, Ajeya Cotra's OP AI timelines report, and Adam Gleave's
#AN80 comment, for more context on this breakdown. I haven't tried to operationalize
this too much, so feel free to be more speciﬁc in the comments.
The ﬁrst three questions in this section are mutually exclusive — that is, the
probabilities you assign to them should not sum to more than 100%.
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
 
Non-technical factor questions
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
 

Operationalizations
Safety Questions
1. Will AGI cause an existential catastrophe?
Existential catastrophe is deﬁned here according to Toby Ord's deﬁnition in the
Precipice: "An event that causes extinction or the destruction of humanity's long-
term potential".
This assumes that everyone currently working on AI alignment continues to do
so.
2. Will AGI cause an existential catastrophe without additional intervention
from the AI Alignment research community?
Roughly, the AI Alignment research community includes people working at CHAI,
MIRI, current safety teams at OpenAI and DeepMind, FHI, AI Impacts, and similar
orgs, as well as independent researchers writing on the AI Alignment Forum.
"Without additional intervention" = everyone currently in this community stops
working on anything directly intended to improve AI safety as of today,
11/20/2020. They may work on AI in a way that indirectly and incidentally
improves AI safety, but only to the same degree as researchers outside of the AI
alignment community are currently doing this.
4. Will there be an arms race dynamic in the lead-up to AGI?
An arms race dynamic is operationalized as: 2 years before superintelligent AGI
is built, there are at least 2 companies/projects/countries at the cutting edge
each within 2 years of each others' technology who are competing and not
collaborating.
 5. Will a single AGI or AGI project achieve a decisive strategic advantage?
This question uses Bostrom's deﬁnition of decisive strategic advantage: "A level
of technological and other advantages suﬃcient to enable it to achieve
complete world domination" (Bostrom 2014).
 6. Will > 50% of AGI researchers agree with safety concerns by 2030?
"Agree with safety concerns" means: broadly understand the concerns of the
safety community, and agree that there is at least one concern such that we
have not yet solved it and we should not build superintelligent AGI until
we do solve it (Rohin Shah's operationalization from this post).
7. Will there be a 4 year interval in which world GDP growth doubles before
the ﬁrst 1 year interval in which world GDP growth doubles?
This is essentially Paul Christano's operationalization of the rate of development
of AI from his post on Takeoﬀ speeds. I've used this speciﬁc operationalization
rather than "slow vs fast" or "continuous vs discontinuous" due to the ambiguity
in how people use these terms.

8. Will AGI cause existential catastrophe conditional on there being a 4 year
period of doubling of world GDP growth before a 1 year period of doubling?
Uses the same deﬁnition of existential catastrophe as previous questions.
9. Will AGI cause existential catastrophe conditional on there being a 1 year
period of doubling of world GDP growth without there ﬁrst being a 4 year
period of doubling?
For example, we go from current growth rates to doubling within a year.
Uses the same deﬁnition of existential catastrophe as previous questions.
 
Timelines Questions
9. Will we get AGI from deep learning with small variations, without more
insights on a similar level to deep learning?
An example would be something like GPT-N + RL + scaling.
10. Will we get AGI from 1-3 more insights on a similar level to deep
learning?
Self-explanatory.
11. Will we need > 3 breakthroughs on a similar level to deep learning to
get AGI?
Self-explanatory.
12. Before reaching AGI, will we hit a point where we can no longer improve
AI capabilities by scaling?
This includes: 1) We are unable to continue scaling, e.g. due to limitations on
compute, dataset size, or model size, or 2) We can practically continue scaling
but the increase in AI capabilities from scaling plateaus (see below).
13. Before reaching AGI, will we hit a point where we can no longer improve
AI capabilities by scaling because we are unable to continue scaling?
Self-explanatory.
14. Before reaching AGI, will we hit a point where we can no longer improve
AI capabilities by scaling because the increase in AI capabilities from scaling
plateaus?
Self-explanatory.
 
Non-technical factor questions

15. Will we experience an existential catastrophe before we build AGI?
Existential catastrophe is deﬁned here according to Toby Ord's deﬁnition in the
Precipice: "An event that causes extinction or the destruction of humanity's long-
term potential".
This does not include events that would slow the progress of AGI development
but are not existential catastrophes.
16. Will there be another AI Winter (a period commonly referred to as such)
before we develop AGI?
From Wikipedia: "In the history of artiﬁcial intelligence, an AI winter is a period of
reduced funding and interest in artiﬁcial intelligence research."
This question asks about whether people will *refer* to a period as an AI winter,
for example, Wikipedia and similar sources refer to it as a third AI winter.
 
Additional resources
We've also collected many of the predictions on AI we could ﬁnd on the internet
and compiled them here.
For a more comprehensive set of questions on AI alignment, see Ben Cottier and
Rohin Shah's Alignment Forum post.
 
Big thanks to Ben Pace, Rohin Shah, Daniel Kokotajlo, Ethan Perez, and Andreas
Stuhlmüller for providing really helpful feedback on this post, and suggesting many of
the operationalizations.

Beware Experiments Without
Evaluation
Sometimes, people propose "experiments" with new norms, policies, etc. that don't
have any real means of evaluating whether or not the policy actually succeeded or
not.
This should be viewed with deep skepticism -- it often seems to me that such an
"experiment" isn't really an experiment at all, but rather a means of sneaking a policy
in by implying that it will be rolled back if it doesn't work, while also making no real
provision for evaluating whether it will be successful or not.
In the worst cases, the process of running the experiment can involve taking
measures that prevent the experiment from actually being implemented!
Here are some examples of the sorts of thing I mean:
Management at a company decides that it's going to "experiment with" an open
ﬂoor plan at a new oﬃce. The oﬃce layout and space chosen makes it so that
even if the open ﬂoor plan proves detrimental, it will be very diﬃcult to switch
back to a standard oﬃce conﬁguration.
The administration of an online forum decides that it is going to "experiment
with" a new set of rules in the hopes of improving the quality of discourse, but
doesn't set any clear criteria or timeline for evaluating the "experiment" or what
measures might actually indicate "improved discourse quality".
A small group that gathers for weekly chats decides to "experiment with" adding
a few new people to the group, but doesn't have any probationary period,
method for evaluating whether someone's a good ﬁt or removing them if they
aren't, etc.
Now, I'm not saying that one should have to register a formal plan for evaluation with
timelines, metrics, etc. for any new change being made or program you want to try
out -- but you should have at least some idea of what it would look like for the
experiment to succeed and what it would look like for it to fail, and for things that are
enough of a shakeup more formal or established metrics might well be justiﬁed.

The Pointers Problem: Human Values
Are A Function Of Humans' Latent
Variables
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
An AI actively trying to ﬁgure out what I want might show me snapshots of diﬀerent
possible worlds and ask me to rank them. Of course, I do not have the processing
power to examine entire worlds; all I can really do is look at some pictures or video or
descriptions. The AI might show me a bunch of pictures from one world in which a
genocide is quietly taking place in some obscure third-world nation, and another in
which no such genocide takes place. Unless the AI already considers that distinction
important enough to draw my attention to it, I probably won't notice it from the
pictures, and I'll rank those worlds similarly - even though I'd prefer the one without
the genocide. Even if the AI does happen to show me some mass graves (probably
secondhand, e.g. in pictures of news broadcasts), and I rank them low, it may just
learn that I prefer my genocides under-the-radar.
The obvious point of such an example is that an AI should optimize for the real-world
things I value, not just my estimates of those things. I don't just want to think my
values are satisﬁed, I want them to actually be satisﬁed. Unfortunately, this poses a
conceptual diﬃculty: what if I value the happiness of ghosts? I don't just want to think
ghosts are happy, I want ghosts to actually be happy. What, then, should the AI do if
there are no ghosts?
Human "values" are deﬁned within the context of humans' world-models, and don't
necessarily make any sense at all outside of the model (i.e. in the real world). Trying
to talk about my values "actually being satisﬁed" is a type error.
Some points to emphasize here:
My values are not just a function of my sense data, they are a function of the
state of the whole world, including parts I can't see - e.g. I value the happiness
of people I will never meet.
I cannot actually ﬁgure out or process the state of the whole world
... therefore, my values are a function of things I do not know and will not ever
know - e.g. whether someone I will never encounter is happy right now
This isn't just a limited processing problem; I do not have enough data to ﬁgure
out all these things I value, even in principle.
This isn't just a problem of not enough data, it's a problem of what kind of data.
My values depend on what's going on "inside" of things which look the same -
e.g. whether a smiling face is actually a rictus grin
This isn't just a problem of needing suﬃciently low-level data. The things I care
about are still ultimately high-level things, like humans or trees or cars. While
the things I value are in principle a function of low-level world state, I don't
directly care about molecules.
Some of the things I value may not actually exist - I may simply be wrong about
which high-level things inhabit our world.

I care about the actual state of things in the world, not my own estimate of the
state - i.e. if the AI tricks me into thinking things are great (whether intentional
trickery or not), that does not make things great.
These features make it rather diﬃcult to "point" to values - it's not just hard to
formally specify values, it's hard to even give a way to learn values. It's hard to say
what it is we're supposed to be learning at all. What, exactly, are the inputs to my
value-function? It seems like:
Inputs to values are not complete low-level world states (since people had
values before we knew what quantum ﬁelds were, and still have values despite
not knowing the full state of the world), but...
I value the actual state of the world rather than my own estimate of the world-
state (i.e. I want other people to actually be happy, not just look-to-me like
they're happy).
How can both of those intuitions seem true simultaneously? How can the inputs to my
values-function be the actual state of the world, but also high-level objects which may
not even exist? What things in the low-level physical world are those "high-level
objects" pointing to?
If I want to talk about "actually satisfying my values" separate from my own estimate
of my values, then I need some way to say what the values-relevant pieces of
my world model are "pointing to" in the real world.
I think this problem - the "pointers to values" problem, and the "pointers" problem
more generally - is the primary conceptual barrier to alignment right now. This
includes alignment of both "principled" and "prosaic" AI. The one major exception is
pure human-mimicking AI, which suﬀers from a mostly-unrelated set of problems
(largely stemming from the shortcomings of humans, especially groups of humans).
I have yet to see this problem explained, by itself, in a way that I'm satisﬁed by. I'm
stealing the name from some of Abram's posts, and I think he's pointing to the same
thing I am, but I'm not 100% sure.
The goal of this post is to demonstrate what the problem looks like for a (relatively)
simple Bayesian-utility-maximizing agent, and what challenges it leads to. This has
the drawback of deﬁning things only within one particular model, but the advantage of
showing how a bunch of nominally-diﬀerent failure modes all follow from the same
root problem: utility is a function of latent variables. We'll look at some speciﬁc
alignment strategies, and see how and why they fail in this simple model.
One thing I hope people will take away from this: it's not the "values" part that's
conceptually diﬃcult, it's the "pointers" part.
The Setup
We have a Bayesian expected-utility-maximizing agent, as a theoretical stand-in for a
human. The agent's world-model is a causal DAG over variables X, and it chooses
actions Xa = x
∗
a  to maximize E[u(X)|do(Xa = x
∗
a )] - i.e. it's using standard causal
decision theory. We will assume the agent has a full-blown Cartesian boundary, so we

don't need to worry about embeddedness and all that. In short, this is a textbook-
standard causal-reasoning agent.
One catch: the agent's world-model uses the sorts of tricks in Writing Causal Models
Like We Write Programs, so the world-model can represent a very large world without
ever explicitly evaluating probabilities of every variable in the world-model.
Submodels are expanded lazily when they're needed. You can still conceptually think
of this as a standard causal DAG, it's just that the model is lazily evaluated.
In particular, thinking of this agent as a human, this means that our human can value
the happiness of someone they've never met, never thought about, and don't know
exists. The utility u(X) can be a function of variables which the agent will never
compute, because the agent never needs to fully compute u in order to maximize it -
it just needs to know how u changes as a function of the variables inﬂuenced by its
actions.
Key assumption: most of the variables in the agent's world-model are not observables.
Drawing the analogy to humans: most of the things in our world-models are not raw
photon counts in our eyes or raw vibration frequencies/intensities in our ears. Our
world-models include things like trees and rocks and cars, objects whose existence
and properties are inferred from the raw sense data. Even lower-level objects, like
atoms and molecules, are latent variables; the raw data from our eyes and ears does
not include the exact positions of atoms in a tree. The raw sense data itself is not
suﬃcient to fully determine the values of the latent variables, in general; even a
perfect Bayesian reasoner cannot deduce the true position of every atom in a tree
from a video feed.
Now, the basic problem: our agent's utility function is mostly a function of
latent variables. Human values are mostly a function of rocks and trees and cars
and other humans and the like, not the raw photon counts hitting our eyeballs. Human
values are over inferred variables, not over sense data.
Furthermore, human values are over the "true" values of the latents, not our
estimates - e.g. I want other people to actually be happy, not just to look-to-me like
they're happy. Ultimately, E[u(X)] is the agent's estimate of its own utility (thus the
expectation), and the agent may not ever know the "true" value of its own utility - i.e.
I may prefer that someone who went missing ten years ago lives out a happy life, but I
may never ﬁnd out whether that happened. On the other hand, it's not clear that
there's a meaningful sense in which any "true" utility-value exists at all, since the
agent's latents may not correspond to anything physical - e.g. a human may value the
happiness of ghosts, which is tricky if ghosts don't exist in the real world.
On top of all that, some of those variables are implicit in the model's lazy data
structure and the agent will never think about them at all. I can value the happiness of
people I do not know and will never encounter or even think about.
So, if an AI is to help optimize for E[u(X)], then it's optimizing for something which is a
function of latent variables in the agent's model. Those latent variables:
May not correspond to any particular variables in the AI's world-model and/or
the physical world

May not be estimated by the agent at all (because lazy evaluation)
May not be determined by the agent's observed data
... and of course the agent's model might just not be very good, in terms of predictive
power.
As usual, neither we (the system's designers) nor the AI will have direct access to the
model; we/it will only see the agent's behavior (i.e. input/output) and possibly a low-
level system in which the agent is embedded. The agent itself may have some
introspective access, but not full or perfectly reliable introspection.
Despite all that, we want to optimize for the agent's utility, not just the agent's
estimate of its utility. Otherwise we run into wireheading-like problems, problems with
the agent's world model having poor predictive power, etc. But the agent's utility is a
function of latents which may not be well-deﬁned at all outside the context of the
agent's estimator (a.k.a. world-model). How can we optimize for the agent's "true"
utility, not just an estimate, when the agent's utility function is deﬁned as a function
of latents which may not correspond to anything outside of the agent's estimator?
The Pointers Problem
We can now deﬁne the pointers problem - not only "pointers to values", but the
problem of pointers more generally. The problem: what functions of what
variables (if any) in the environment and/or another world-model
correspond to the latent variables in the agent's world-model? And what does
that "correspondence" even mean - how do we turn it into an objective for the AI, or
some other concrete thing outside the agent's own head?
Why call this the "pointers" problem? Well, let's take the agent's perspective, and
think about what its algorithm feels like from the inside. From inside the agent's mind,
it doesn't feel like those latent variables are latent variables in a model. It feels like
those latent variables are real things out in the world which the agent can learn about.
The latent variables feel like "pointers" to real-world objects and their properties. But
what are the referents of these pointers? What are the real-world things (if any) to
which they're pointing? That's the pointers problem.
Is it even solvable? Deﬁnitely not always - there probably is no real-world referent for
e.g. the human concept of a ghost. Similarly, I can have a concept of a perpetual
motion machine, despite the likely-impossibility of any such thing existing. Between
abstraction and lazy evaluation, latent variables in an agent's world-model may not
correspond to anything in the world.
That said, it sure seems like at least some latent variables do correspond to structures
in the world. The concept of "tree" points to a pattern which occurs in many places on
Earth. Even an alien or AI with radically diﬀerent world-model could recognize that
repeating pattern, realize that examining one tree probably yields information about
other trees, etc. The pattern has predictive power, and predictive power is not just a
ﬁgment of the agent's world-model.
So we'd like to know both (a) when a latent variable corresponds to something in the
world (or another world model) at all, and (b) what it corresponds to. We'd like to solve
this in a way which (probably among other use-cases) lets the AI treat the things-

corresponding-to-latents as the inputs to the utility function it's supposed to learn and
optimize.
To the extent that human values are a function of latent variables in humans' world-
models, this seems like a necessary step not only for an AI to learn human values, but
even just to deﬁne what it means for an AI to learn human values. What does it mean
to "learn" a function of some other agent's latent variables, without necessarily
adopting that agent's world-model? If the AI doesn't have some notion of what the
other agent's latent variables even "are", then it's not meaningful to learn a function
of those variables. It would be like an AI "learning" to imitate grep, but without having
any access to string or text data, and without the AI itself having any interface which
would accept strings or text.
Pointer-Related Maladies
Let's look at some example symptoms which can arise from failure to solve speciﬁc
aspects of the pointers problem.
Genocide Under-The-Radar
Let's go back to the opening example: an AI shows us pictures from diﬀerent possible
worlds and asks us to rank them. The AI doesn't really understand yet what things we
care about, so it doesn't intentionally draw our attention to certain things a human
might consider relevant - like mass graves. Maybe we see a few mass-grave pictures
from some possible worlds (probably in pictures from news sources, since that's how
such information mostly spreads), and we rank those low, but there are many other
worlds where we just don't notice the problem from the pictures the AI shows us. In
the end, the AI decides that we mostly care about avoiding worlds where mass graves
appear in the news - i.e. we prefer that mass killings stay under the radar.
How does this failure ﬁt in our utility-function-of-latents picture?
This is mainly a failure to distinguish between the agent's estimate of its own utility 
E[u(X)], and the "real" value of the agent's utility u(X) (insofar as such a thing exists).
The AI optimizes for our estimate, but does not give us enough data to very accurately
estimate our utility in each world - indeed, it's unlikely that a human could even
handle that much information. So, it ends up optimizing for factors which bias our
estimate - e.g. the availability of information about bad things.
Note that this intuitive explanation assumes a solution to the pointers problem: it only
makes sense to the extent that there's a "real" value of u(X) from which the
"estimate" can diverge.
Not-So-Easy Wireheading Problems
The under-the-radar genocide problem looks roughly like a typical wireheading
problem, so we should try a roughly-typical wireheading solution: rather than the AI

showing world-pictures, it should just tell us what actions it could take, and ask us to
rank actions directly.
If we were ideal Bayesian reasoners with accurate world models and inﬁnite compute,
and knew exactly where the AI's actions ﬁt in our world model, then this might work.
Unfortunately, the failure of any of those assumptions breaks the approach:
We don't have the processing power to predict all the impacts of the AI's actions
Our world models may not be accurate enough to correctly predict the impact of
the AI's actions, even if we had enough processing power
The AI's actions may not even ﬁt neatly into our world model - e.g. even the idea
of genetic engineering might not ﬁt the world-model of premodern human
thinkers
Mathematically, we're trying to optimize E[u(X)|do(XAI = x
∗
AI)], i.e. optimize expected
utility given the AI's actions. Note that this is necessarily an expectation under the
human's model, since that's the only context in which u(X) is well-deﬁned. In order for
that to work out well, we need to be able to fully evaluate that estimate (suﬃcient
processing power), we need the estimate to be accurate (suﬃcient predictive power),
and we need XAI to be deﬁned within the model in the ﬁrst place.
The question of whether our world-models are suﬃciently accurate is particularly
hairy here, since accuracy is usually only deﬁned in terms of how well we estimate our
sense-data. But the accuracy we care about here is how well we "estimate" the values
of latent variables and u(X). What does that even mean, when the latent variables
may not correspond to anything in the world?
People I Will Never Meet
"Human values cannot be determined from human behavior" seems almost old-hat at
this point, but it's worth taking a moment to highlight just how underdetermined
values are from behavior. It's not just that humans have biases of one kind or another,
or that revealed preferences diverge from stated preferences. Even in our perfect
Bayesian utility-maximizer, utility is severely underdetermined from behavior,
because the agent does not have perfect estimates of its latent variables. Behavior
depends only on the agent's estimate, so it cannot account for "error" in the agent's
estimates of latent variable values, nor can it tell us about how the agent values
variables which are not coupled to its own choices.
The happiness of people I will never interact with is a good example of this. There may
be people in the world whose happiness will not ever be signiﬁcantly inﬂuenced by my
choices. Presumably, then, my choices cannot tell us about how much I value such
peoples' happiness. And yet, I do value it.
"Misspeciﬁed" Models

In Latent Variables and Model Misspeciﬁcation, jsteinhardt talks about
"misspeciﬁcation" of latent variables in the AI's model. His argument is that things like
the "value function" are latent variables in the AI's world-model, and are therefore
potentially very sensitive to misspeciﬁcation of the AI's model.
In fact, I think the problem is more severe than that.
The value function's inputs are latent variables in the human's model, and are
therefore sensitive to misspeciﬁcation in the human's model. If the human's model
does not match reality well, then their latent variables will be something wonky and
not correspond to anything in the world. And AI designers do not get to pick the
human's model. These wonky variables, not corresponding to anything in the world,
are a baked-in part of the problem, unavoidable even in principle. Even if the AI's
world model were "perfectly speciﬁed", it would either be a bad representation of the
world (in which case predictive power becomes an issue) or a bad representation of
the human's model (in which case those wonky latents aren't deﬁned).
The AI can't model the world well with the human's model, but the latents on which
human values depend aren't well-deﬁned outside the human's model. Rock and a
hard place.
Takeaway
Within the context of a Bayesian utility-maximizer (representing a human),
utility/values are a function of latent variables in the agent's model. That's a problem,
because those latent variables do not necessarily correspond to anything in the
environment, and even when they do, we don't have a good way to say what they
correspond to.
So, an AI trying to help the agent is stuck: if the AI uses the human's world-model,
then it may just be wrong outright (in predictive terms). But if the AI doesn't use the
human's world-model, then the latents on which the utility function depends may not
be deﬁned at all.
Thus, the pointers problem, in the Bayesian context: ﬁgure out which things in the
world (if any) correspond to the latent variables in a model. What do latent variables
in my model "point to" in the real world?

Anatomy of a Gear
Thankyou to Sisi Cheng (of the Working as Intended comic) for the excellent drawings.
When modelling a physical gearbox, why do we chunk the system into gears? We're
essentially treating each gear as a black box, a discrete component without internal
structure of its own, interesting mainly for its interactions with other components. Why not
zoom out, and model the entire gearbox as a black box? Why not zoom in, and model the
individual chunks of metal which comprise each gear, or even each individual atom?
It seems like gears are a natural choice of abstraction for a gearbox. But why that choice? In
general, what makes a good "gear" - a good lowest-level component in a "gears-level"
model?
Why choose "gears" as the components of our model? Why chunk the world that way,
rather than chunk multiple gears into subsystems, or break up gears into even smaller
subsystems?
More generally: we want to build gears-level models, but we don't want to model things
down to the last atom. At some point, we have to accept some black boxes in our model
components. So how do we decide where that point is?
This post answers that question. We'll start with a relatively-detailed discussion of a physical
gear, then extend the reasoning to "gears" in other kinds of models.
A Physical Gear
Picture a gear, turning in a gearbox. We can pick out a little patch of metal on that gear,
zoom in, and see lots of atoms vibrating around. Those vibrations aren't completely random -
vibrations of one atom are correlated with the atoms next to it, although the correlations do
typically fall oﬀ over a short distance. If we look at the motions of all the atoms in one little
chunk of the gear, it won't tell us much about the motions of the atoms in some other chunk
far away in the gear.
... with one exception. If we average together the motion of all the atoms in our little chunk,
we should get a decent estimate of the overall rotation speed of the gear. And that does tell
us something about all the other little chunks of metal: it tells us roughly what the average
motion in all those other chunks is.

If we look at all the atoms in one little chunk of one gear, only the average motion of all
the atoms will tell us much about motion of atoms in a neighboring gear.
Caution: physical accuracy not a priority in this visual.
Likewise, if we look at the motion of all the individual atoms in one little chunk of our gear,
what does that tell us about the motion of atoms in other gears? Mostly nothing... except,
again, the average motion of all the atoms tells us the overall rotation speed of our gear,
which tells us something about the overall rotation speed of the other gears.
So, if we were to somehow model the whole system at atomic scales, we'd ﬁnd that all the
information in one little chunk of atoms, relevant to some other chunk of atoms far away,
was summarized by the rotation speed of the gear.

In terms of dimensionality: the atom motions are extremely high dimensional. Every atom's
speed is a 3-dimensional vector, so with n atoms we have 3n dimensions. Even a tiny patch
of metal has an awful lot of atoms, so that's an awful lot of dimensions.
Yet most of that information is irrelevant to everything far away. For purposes of predicting
things far away, that huge number of dimensions can be summarized by a one-dimensional
number: the gear's rotation speed. That rotation speed, in turn, informs the motions of huge
numbers of other atoms elsewhere in the system.
It's an "information bottleneck": (high dimensional atom motions) -> (one dimensional
rotation speed) -> (high dimensional atom motions elsewhere). We can think of the abstract
object - the "gear" - as an interface to all the lower-level components which comprise the
gear (e.g. atoms, in a physical gear).
In general, a good "gear" in a model picks out some chunk of an object for which we have a
good low-dimensional summary. All the information from that chunk which is relevant to
predicting things elsewhere in the system should be summarized by a few low-dimensional
parameters. For a physical gear, it's the rotation speed. The next few sections give other
examples.
Rigid Body Dynamics
One direct generalization of a physical gearbox is rigid body dynamics: we have rigid, solid
objects which move around, push each other, bounce oﬀ each other, etc. This is a good
model for most of the non-ﬂexible solid objects around us most of the time: tables and
chairs, wheels, hammers and screwdrivers and screws and nails, pens and pencils, sticks and
rocks, pots and pans and dishes and silverware, etc.
When thinking about the mechanics of rigid bodies (i.e. how they move), all the positions
and motions of individual atoms in the object can be summarized by the overall position,
orientation and motion of the object. It's just like gears, though slightly more general - gears
mostly just rotate in place, while rigid bodies in general can also move around. So, it's
natural to chunk rigid bodies into individual "objects", i.e. treat them as single gears in our
models. This is exactly what we do in our everyday lives: we treat a pencil as an object, a
plate as an object, a rock as an object, .... These things make natural "objects" because the
low-level dynamics of all their atoms can be summarized by just the position, orientation and
motion of the whole object, for purposes of predicting how things will move.
Note, however, that this is not suﬃcient for all questions we might ask about the object. For
instance, if I want to know what sound an object makes when struck, then the vibrations of
all the little parts become relevant. Rigid bodies make natural "gears" because we can
answer a very broad range of questions just given a low-dimensional summary, not because
we can answer all questions just given a low-dimensional summary.
Things get more complicated for non-rigid bodies, like cloth or rope. Sometimes we can use a
low-dimensional summary for the dynamics, like a rope in a pulley, but not always; cloth
moves around in complicated ways. We can still answer a broad range of questions using
only summary information, just not necessarily questions about dynamics. For instance,
looking at the atoms in one little chunk of cloth tells us very little about other atoms in the
same chunk of cloth, except that the material composition is probably the same. This is quite
similar to the "gears" used in chemical models.
Chemical Equations
In chemistry, we summarize the state of a huge number of atoms with just a handful of
chemical concentrations (plus temperature and pressure, depending on the application).

Why? Well, as long as the system is mixed up, it doesn't matter exactly where particular
molecules are - they're all more-or-less evenly distributed throughout the system. The exact
positions of atoms in one patch of ﬂuid at one time don't tell us much about the exact
positions of atoms in another patch of ﬂuid at a later time, except insofar as they tell us
about the average concentrations of each molecule type throughout the system as a whole.
High-dimensional atom positions in one patch of ﬂuid tell us low-dimensional average
concentrations, which is all the information relevant to high-dimensional atom positions in
some other patch of ﬂuid (assuming everything is well-mixed).
Note that this changes if the system isn't well-mixed, e.g. a layer of oil on top of water, or a
diﬀusion gradient. Then we either need to keep track of more information (e.g.
concentrations in small patches of ﬂuid) or we need to further restrict the questions we want
to answer.
Electronic Circuits
Electronic circuits are a particularly interesting example, because avoiding "crosstalk"
between circuit components is an explicit design goal. We generally don't want e.g. a resistor
to behave diﬀerently if there's a magnet nearby.
In other words: the design of a resistor is chosen so that we can summarize all the
information about the component using just its overall resistance and the overall current (or
voltage) through it at any given moment. We don't need to worry about the details of the
physical connection between the resistor and a wire. We don't need to worry about whether
the resistor is upside down, or spinning around, or a little hotter/colder than room
temperature. We don't need to worry about the low-level behavior of individual atoms within
the resistor. We just need a one dimensional summary: overall current is proportional to
overall voltage delta.
The same applies to other electronic components - transistors, wires, capacitors,
transformers, diodes, etc. Components are designed to make good "gears": their behavior
has a simple, low-dimensional summary.
API Functions
In software design, we often draw high-level diagrams of the software in which each
component is a function in some API, and lines/arrows between components show which
information is passed between them. This is useful mainly to the extent that the inputs and
outputs of each component can be summarized without needing a detailed representation of
the internal behavior of each function.
This is often considered a major criterion of good software design: function should have
simple interfaces. Even if there's lots of internal complexity, like some function with many
internal calculations (i.e. high dimensional, with many intermediate values calculated), the
inputs and outputs should be low-dimensional (at least compared to the internals). When
functions satisfy this criterion, they make good "gears" in our conceptual models of the
software's behavior.
Companies
In economics, companies work a lot like functions in software. The "interface" they provide is
their catalogue of products. The product itself is relatively low-dimensional, compared to the
complicated process which produced the product. It's the same idea behind the classic "I,
Pencil" essay: even something as simple as a pencil is produced by hundreds of people and

machines with specialized functions. The pencil-user does not understand all the details of
the pencil production process, but they don't need to - they just need to know how to use a
pencil.
As an anti-example, imagine some preindustrial ironsmith. Without reliable standards for
metal composition and quality, the smith might buy very diﬀerent "iron" from diﬀerent
suppliers at diﬀerent times. The smith would either have to know quite a bit about the
production of the raw iron, or else accept an inconsistent product. If the smith needs to keep
track of the whole production process, then the iron supplier would be a bad gear in an
economic model.
Organs
The physiology of the kidney or liver or other internal organs is rather involved. Each
contains a wide variety of specialized cell types and substructures, all in large numbers; their
internal structure is high-dimensional. Yet the overall function of most organs allows a
relatively low-dimensional summary: they regulate levels of speciﬁc hormones or metabolites
or cell types.
Takeaway
A good "gear" - i.e. a lowest-level component in a model - should oﬀer a (relatively) low-
dimensional summary of its own internal subcomponents. That low-dimensional summary
makes it practical to treat the gear as a black box, even though its internal components may
be complicated and high dimensional (e.g. made of lots of atoms). The information which
needs to be included in the summary depends on what questions we want to answer about
the system, but a common theme is that broad classes of questions about behavior not-too-
close to a particular subcomponent depend only on a few summary dimensions.
Finally, I'll note that some of the examples above brush some subtleties under the rug
(though the main idea does generally work as advertised). People are invited to try to spot
some of those subtleties, and possibly propose the resolutions, in the comments. I'll point to
one to kick things oﬀ: how does temperature ﬁt in to our physical gearbox example?

Gifts Which Money Cannot Buy
From a pure consumption perspective, the basic problem of gift-giving is that you usually
know what you want better than I know what you want. If we both buy each other $20 gifts,
then we'll probably end up with things we want less than whatever we would have bought
ourselves for $20. The ceremony and social occasion of gift-giving adds enough value to
oﬀset the downsides, but we still end up with entire stores selling useless crap which is given
as a gift and then promptly disposed of by the recipient.
The Yard Sale Supply Warehouse - stocking all those little
knick-knacks and decorations which you have no use for, but
it's so cute your relative would just love it as a present
We can minimize the problem with wish-lists or gifting cash, but that's still just buying people
the things they'd likely buy for themselves anyway (modulo general thriftiness). What if I
want to choose gifts in a way that directly adds real value? In other words: I want to choose
gifts which will give someone more value than whatever they would have bought
themselves.
On the face of it, this sounds rather... presumptuous. The only way this makes sense is if I
know what you want better than you know what you want, in at least some cases.
On the other hand, there are things which money alone cannot buy - speciﬁcally knowledge
and expertise. If I do not have enough knowledge in an area to distinguish true experts from
those who claim to be experts, then I cannot eﬃciently buy myself the products of true
expertise. If you do have enough knowledge to distinguish true experts, or to act as an

expert yourself, then that creates an opportunity for value-added gift giving: you can buy the
products of true expertise as a gift for me.
A few examples...
My father is into cooking, and has strong opinions about spatulas. A spatula should have
some give without being ﬂoppy, have a reasonably long lifting-part and a comfortable
handle, be dishwasher-safe, and under no circumstances should a spatula be made of
plastic. He spent years searching for the perfect spatula, and ﬁnally settled on one. I once
told him that in his will I wanted him to leave me the spatula - my brother and sister can
have the rest, I didn't need the house or whatever money might be left, but I wanted that
spatula.
That particular spatula is no longer in production, but they weren't very rare when they were
made, so he kept an eye out for them at yard sales and the like. A few years ago, he gave
me one such spatula for Christmas. That's a gift that money alone cannot buy.
If you see this at a ﬂea market, check that the ﬂat part ﬂexes a bit under pressure, then
BUY IT
A less dramatic example: my father enjoys the sort of light econ books which are popular in
rationalist-adjacent circles - think Elephant in the Brain or David Friedman's lighter books.
But he doesn't have a social circle which recommends and reviews such books. The best he
can do is go to a bookstore and look around, or keep an eye on authors he's read before, or
look at Amazon's recommendations. As a side beneﬁt of reading econ and rationalist blogs, I
can often pick out good books for my father that he wouldn't have encountered himself.
Similarly: I have a group of friends who are really into board games. Prior to the pandemic,
we'd have game night once or twice a week. They had a large collection and kept up with

new releases, so I've experienced a reasonably wide array of games. By contrast, my parents
enjoy board games, but they grew up with Hasbro crap and don't have a social circle which
exposes them to anything decent. So, I've taken to buying them games as gifts. These have
been quite high-value - for instance, they still bring Love Letter with them whenever they
travel. (As an added bonus, the new games are good for socializing with the family during
the holidays.)
An example from this year: I've been to Shanghai with my girlfriend several times. She
speaks the language, I don't. There's a bunch of tasty chinese foods which are basically-
unheard-of in the US; often they don't even have standard English names. (I'm particularly
fond of Xinjiang/Uyghur ethnic food.) They're rare even in authentic Chinese restaurants
here, and looking up recipes is hard-to-impossible - even when I can ﬁnd a recipe in English,
it's usually some bland knockoﬀ written by a white Midwesterner whose idea of "spices" is
"maybe a little salt and pepper". So, for Christmas this year I asked my girlfriend for recipes
for my favorite Chinese foods.
These are all gifts which money alone cannot buy - at a bare minimum, they'd require a large
amount of extra eﬀort just to turn the money into the product. The general rule is: think
about what knowledge or expertise you have which the gift-recipient lacks, and try to use
that expertise to produce a gift which the recipient would not be able to buy for themselves,
or would not know to buy for themselves.

Being Productive With Chronic Health
Conditions
This post appeared ﬁrst on the EA Coaching blog.
A fair number of clients who suﬀer from chronic health problems come though my
metaphorical door. Personally, I've dealt with a chronic health condition called POTS
since my early teens. 
When you're dealing with longer term, physical chronic health problems that cause
fatigue, brain fog, or pain, standard productivity advice might not work. Common
advice can even backﬁre, leaving you worse oﬀ. Similarly, shorter term problems such
as a concussion or pregnancy will temporarily change what you can do. 
Of course, I am not a doctor. This is a collection of productivity tips based on many
conversations I've had, not proven medical advice. If you have a chronic health issue,
you should be getting medical advice from appropriate professionals. 
Invest in your capacity
Improving your habits and better managing your symptoms will almost certainly
increase your productivity more than trying to power through. So work smarter, not
harder. Here are some tips for improving your capacity to do work. 
Spend time trying to ﬁnd treatments as early as possible. The biggest
productivity gains may come from ﬁnding better ways to manage your condition. You
may need to talk to many specialists, try diﬀerent medications, adopt new routines, or
learn new ways of moving. The earlier you do these, the longer you will be enjoying
the rewards. College is a great time to do this. Of course, grant yourself some
compassion if navigating a complicated medical landscape seems like an
overwhelming burden on top of dealing with your normal work.
Get enough sleep. Sleeping less than you need so that you have more time awake
will reduce your productivity. If you have any kind of chronic fatigue issue, you just
need more sleep than average. It's common to need nine hours a night or more in
such cases. If you've been running a sleep debt, you may need a lot of rest to catch
up over many days. 
Adapt your environment to support you. If people are getting annoyed that
you're taking longer than normal to reply to their emails, set up an autoreply that
you're slow to reply right now because of a health problem. If you're struggling to
remember to take your pills, get a weekly pill organizer or an electric bottle cap that
tells you when you last took your meds. Build in redundant systems to make sure the
vital habits happen. 
Make your key habits robust and ﬂexible. When you ﬁnd the actions, medication,
or tricks that make or break your day, plan a way to do those even when you let your
other work slide. When you're "falling behind," it can feel like you don't have time to
exercise or sleep enough or do your stretches. Build routines ﬂexible enough to adapt.
Maybe you may need to schedule your deep work for today based on when you have

energy. Or maybe you need to make a scaled down habit list with just one or two
items that you follow when you're not feeling up to your full normal routine. 
Learn your warning signs and pay attention to them. Learn the warning signs
that your body is nearing its limit. When those warning signs pop up, plan responses
that will leave you ready to be productive again tomorrow. E.g. if you have carpal
tunnel, stop before your hands hurt. 
Plan on not being 100%. When you're setting goals, give yourself buﬀer time. Are
you brain foggy one in three days? Then don't make a plan that requires you to be at
peak capacity every day. If you can do more than you planned, great! If not, you
already have a plan in place for dealing with it. 
Slow down and respond to new conditions. If your condition recently developed,
slow down a bit more than you think you need. You're still calibrated to what you could
do before, but your "normal" has changed. You need to accept that a good day right
now isn't as good as what you used to consider a good day. While you're ﬁguring out
what you can do, you don't want to push yourself too hard. You might make things
worse. This is especially common in repeated stress injuries like carpal tunnel.
Second, aggressively try to solve this new problem. The above argument for solving a
problem earlier rather than later applies. More importantly, however, some health
issues are much easier to ﬁx early on. E.g.If you keep working with a concussion, you
might cause further brain damage. 
Personal examples: 
It's not always easy to imagine what these tips would look like in practice. So I've
included some personal examples of what they look like for me. 
One of my key habits is regular exercise. So I start oﬀ the day with ﬁfteen minutes of
yoga. A habit established mainly, I confess, through committing to pay a housemate
for each missed day. I sat out half the standing poses today because I was feeling
lightheaded (one of my warning signs). Since overexercising can leave me tired for
weeks, I'm not trying to push myself here. 
I have three bottles of salted water beside my bed so I stay hydrated. After what feels
like the pharmacological equivalent of shopping for new clothes, one of that many
drugs that my doctors recommend works really well. The salt water, meds, and other
things I've done to manage POTS have been huge in increasing my productive time -
there is no way I could have just pushed through to my current levels of productivity. 
I went to bed early enough last night that I could get a solid 8.5 hours of sleep. My
housemates graciously agreed to pause our board game and ﬁnish it today so I could
get enough sleep. 
Figure out your limits
Most advice you hear will argue either to accept your limits or push yourself more.
That's because most people giving such advice are trying to push back on the
pendulum swinging too far in one direction.
This post isn't about recommending either. 

I get the arguments for wanting to accomplish more, and sometimes being able to do
so if you push yourself more. I also understand why pushing yourself too much can be
net harmful to your productivity (and, of course, your happiness). 
What is hard is deciding when you should do each. 
Because "push yourself" and "accept your limits" isn't a binary choice. Rather, you're
trying to ﬁnd where your limits should be and when you want to push yourself more. 
This is hard because the normal benchmarks don't apply. If work is a boulder each
person has to push up a hill, your boulder is bigger than that of a "normal" person. So
it doesn't make sense to measure yourself against the arbitrary eight-hour workday. 
Instead, you need to ﬁgure out for yourself what you can accomplish. When will you
expect that you "should" be productive and push through? When will you accept that
you gave it your best but hit a limit? 
A couple of therapists and I put our heads together to brainstorm the following criteria
for deciding where to set your limit. The goal we aimed for is sustainability - can you
maintain this threshold without burning out or further damaging your health? 
Here's some questions we came up with:
Are you exhausted all of the time? Does increasing time oﬀ work or rest reduce
the exhaustion? If you're exhausted unless you sleep more, sleep more. Running an
increasingly large sleep deﬁcit is not sustainable. 
Is your pain increasing? If your actions cause your physical pain levels to steadily
increase, maybe slow down and experiment with more gradually increasing your
activity levels. If your pain or other symptoms get worse, ease back oﬀ. 
Are you experiencing sudden changes? E.g. sudden loss of energy, sudden low
mood or mania, sudden loss or increase of appetite, sudden increase in brain fog,
sudden loss of weight? If you experience sudden changes, check with a doctor. These
can be a symptom of something severe.
Does taking a break (e.g. a weekend oﬀ) substantially improve your
symptoms? If taking a break improves your symptoms, think about what breaks you
need for sustainability. "I'll just push through" often doesn't work for prolonged
periods of time, and you can set yourself back far more if you over do it. Draw your
limits based around protecting the necessary amount of break time.   
Is your physical health impacting your behavior? E.g. snapping at people or
making many typos. If so, how costly could these mistakes be? If you might mess up
something important because you're fatigued, consider whether it's actually more
eﬀective for you to work less and rest more. 
These questions are about you over time, not just this moment. You might feel
okay today getting less sleep, but then consistently be tired after a week of doing so.
Your limit isn't the max you can possibly push yourself; it's healthy boundaries that
protect your ability to sustainably do important work. 
Trying to do more 

As you build your habits, you probably want to try doing things and noticing when you
should ease oﬀ. However, constantly checking in with yourself might cause more
stress and heighten awareness of symptoms. So it's useful to watch out for that and
ﬁnd a pattern of checking in with yourself that works well for you. 
Here are some tips for when you want to try doing more.
Prioritize ruthlessly. You have more limited capacity than other people. However,
this usually reduces the amount you can do more than the importance of what you
can do. You can make up a good deal of that gap by doing just the most important
tasks. 
Try to avoid failing with abandon. It can feel tempting to write oﬀ the entire day
when you wake up feeling awful. Or to think that you shouldn't try because you'll
never be able to do something. There may be times when those responses are
warranted, but they shouldn't be the default. Even if your day started out bad, maybe
you'll feel up to doing some work in the afternoon. So check in with what you feel up
to right now. 
Try the 5-minute test. A therapist mentor once told me that for people dealing with
anxiety, they suggest the person ask themselves "Can I do this for ﬁve minutes?" If
the answer is yes, do it for ﬁve minutes. Then ask the question again. When the
answer is no, it's okay to stop. You can apply a similar check in with yourself. Ask
yourself questions like these: "Do I feel up to doing my top priority for ﬁve minutes? If
not, do I feel up to doing easier work? If not, what break is most likely to leave me
feeling better later?" You can also use it for self-care, "Can I do ﬁve minutes of
exercise?" or "Can I spend 5 minutes ﬁguring out which meds I need to order?"
Work up slowly. Set goals based on what you've done previously. No one would
expect to go from couch potato to running a marathon instantly. Similarly, if you
normally work three hours a day, don't suddenly jump to eight. Even if you manage to
do it for a day or two, it's not sustainable. Increase your work in smalls chunks so that
you have time to learn the necessary habits and notice when you are overexerting
yourself. When you experience a setback, recalibrate and go more slowly. 
Personal examples: 
I'm aiming for three hours of deep work today since I only have a few calls. Through
trial and error, I know I can expect to hit that goal ~80% of the time if I work hard. So
it's a mild stretch goal that pushes me a little bit.
Today wasn't a good day. I woke up feeling like my head was stuﬀed with cotton, and
didn't end up starting my deep work until 5pm - far later than the planned 9am. 
Cut yourself slack
Finally, here are some ways to think if you're near your limit.
You don't need to compare yourself to others. It doesn't always feel like you
should be cutting yourself slack. By which I mean, it's really hard to know if you're just
"not trying" as hard or if you're pushing a heavier boulder up the hill than others.
Psychologically, it can help if the problem suddenly starts or you can get an oﬃcial

diagnosis, so you can compare your current experience to recent memory or
quantitative medical criteria to know that your experience is not normal. 
In practice, I set thresholds by the above questions on ﬁguring out your limits. If you
are testing how much you can do and ﬁnding the limits you need to respect so you're
not too tired or in too much pain, then that is the upper bound of what you should
expect of yourself. This is true even if someone else doesn't have those limits. 
You're not slacking oﬀ.
It's easy to feel bad about your productivity if this is you. You probably aren't able to
work as much as you see others working or as much as you think you should be
working. I've spent years building habits, improving prioritization, and learning to
manage my condition - and I still average fewer hours of work each day than my
partner does with far less eﬀort. I still have days where I hit a wall and need to nap for
several hours. 
When you see yourself struggling to accomplish what others seem to do easily, it can
feel like you are a failure. And that sucks. 
But you are not lazy or slacking oﬀ. To return to my earlier analogy, if work is a
boulder each person has to push up a hill, your boulder is bigger than theirs. So, cut
yourself some slack if you can't push your boulder up quite as high a hill as someone
with a lighter boulder. 
You're pushing through something that is genuinely diﬃcult. For many people with
chronic illness, the everyday experience is the same as a normal person's sick day.
The diﬀerence is that when a normal person feels as bad as you do right now, they
call in sick and stay in bed. But this is your normal. You can't let yourself take oﬀ every
"normal person" bad day, or else you wouldn't have any days to work. So you push
through pain or fatigue or brain fog, and that takes extra eﬀort. 
Personal examples: 
I schedule client calls with breaks every few hours so that I have time to take a nap if
necessary. 
I score my productivity for the day on a 1-5 scale based on what percent I
accomplished of the work I think I could have done given how I felt today. On days
when I'm tired or brain foggy, "good enough" translates to much less work than on a
good day. I've failed at this so many times that I've come to respect my warning signs.
Conclusion
Chronic conditions have an up-and-down cycle - sometimes it's meh, sometimes
things seem mostly ﬁne, and sometimes you're in a really bad place. Managing your
productivity is about learning to negotiate that cycle: reduce the downs if you can,
make the most of the ups, and practice some self-love as you go. 
Because things can get better. While I still work fewer hours than some of my peers,
I'm able to work mostly normal hours. That wasn't true a few years ago. 

I hope you found these helpful for deciding when to push or cut yourself slack. To
reiterate, please be kind to yourself. You'll be happier and more productive optimizing
for sustainable work rather than beating yourself up for having a health problem. 
 
Resources and acknowledgements 
Living a Healthy Life with Chronic Conditions - I haven't read it, but I found this book
on the CDC's site for self-management of chronic conditions. It seems to have a broad
collection of basic tips. 
Many thanks to Ewelina Tur, Damon Pourtahmaseb-Sasi, Daniel Kestenholz, Nicole
Ross, Mary Wang, Rohin Shah, Bill Zito, Jonathan Mustin, and Amanda Ngo for their
input.
Enjoyed the piece? Subscribe to EA Coaching's newsletter to get more posts delivered
to you.

Impostor Syndrome as
skill/dominance mismatch
I am surprised that there is nothing about Impostor Syndrome on Robin Hanson's
website, when to me it seems obviously connected to status. To use the standard
formula: Impostor Syndrome is not about lack of skills.
(Also related to: humility, status regulation, unpopularity of nerds.)
Let me quote my older article:
Robin Hanson calls the two basic forms of status "dominance" and "prestige"; the
fear-based and the admiration-based sources of social power respectively. He also
notes how people high in "dominance" prefer to be perceived as (also) high in
"prestige" [1, 2]. Simply said, a brutal dictator often wants to be praised as kind
and smart and skilled (e.g. the "coryphaeus of science" Stalin), not merely
powerful and dangerous.
[...] If you are not ready to challenge the chieftain for the leadership of the tribe,
and if you don't want to risk being perceived as such, the safe behavior is to also
downplay your skills as a hunter.
Although humans have two mechanisms of constructing social hierarchies, at the end
of the day both of them compete for the same resource: power over people. Thus we
see powerful people leveraging their power to also get acknowledged as artists or
scientists; and successful artists or scientists leveraging their popularity to express
political opinions.
The hierarchy of "dominance" is based on strength, but is not strength alone. The
strongest chimp in the tribe can be defeated if the second-strongest and third-
strongest join against him. Civilization makes it even more complicated. Stalin wasn't
the physically strongest man in the entire Soviet Union.
(In theory, the most powerful person shouldn't need physical strength at all, if they
have an army and secret police at command. But in practice, I suppose our instincts
demand it; a physically weak leader would probably be a permanent magnet for
rebellions. Therefore leaders ﬂaunt their health and strength.)
Similarly, the hierarchy of "prestige" is based on skill, but is not skill alone. The most
skilled person can be... what? Outskilled by a coalition of opponents? Nah, sounds like
too much work. It is easier to stop them ﬂaunting their skill, either by taking away
their tools, or by threatening to break their arms and legs if you see them performing
publicly again.
Which makes the prestige ladder a mixed one. To get on the top, you need a
combination of superior skill and at least average dominance. If you are 10 at skill and
2 at dominance, you will probably be bullied into submission by someone who is 9 at
skill and 7 at dominance. (Remember that dominance is not only physical strength; it
also includes social power. Sticks and stones may break your bones, but Twitter can
ruin your life.) No one applauds a talent who was too afraid to get on the stage.

What are you supposed to do then, if you happen to be 10 at skill and 2 at dominance,
and your neighbor is 9 at skill and 7 at dominance and looks pissed oﬀ when you are
around? Well, if you value your life, but can't increase your dominance, the solution is
to downplay your skill and pretend to be at most 8; maybe even less just to be safe.
How is this all related to the Impostor Syndrome?
I suspect that Impostor Syndrome is simply an instinctive reaction to noticing that
your skills are disproportionally high compared to your relative dominance at the
workplace. This needs to stop, now! The most reliable way to convince others of your
incompetence is to convince yourself. So you notice some imperfection of yourself or
your work, and you exaggerate it in your mind until you feel like a complete idiot.
Except you still have the prestigious job title, so now you fear you might be punished
for that.
What predictions does this model make?
People with Impostor Syndrome are on average physically weaker or less popular or
coming from less privileged backgrounds than people who feel like true masters
(controlling for the actual level of skill).
Therapy based on "look, according to evidence X, you are really skilled" and "hey,
nobody is perfect" will not work, unless it accidentally stumbles on something that
makes the patient feel stronger or more popular. On the other hand, weightlifting will
reduce the Impostor Syndrome, despite having no relation to the disputed skill.

When Hindsight Isn't 20/20: Incentive
Design With Imperfect Credit Allocation
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
A crew of pirates all keep their gold in one very secure chest, with labelled sections for each
pirate. Unfortunately, one day a storm hits the ship, tossing everything about. After the
storm clears, the gold in the chest is all mixed up. The pirates each know how much gold
they had - indeed, they're rather obsessive about it - but they don't trust each other to give
honest numbers. How can they ﬁgure out how much gold each pirate had in the chest?
Here's the trick: the captain has each crew member write down how much gold they had, in
secret. Then, the captain adds it all up. If the ﬁnal amount matches the amount of gold in the
chest, then we're done. But if the ﬁnal amount does not match the amount of gold in the
chest, then the captain throws the whole chest overboard, and nobody gets any of the gold.
I want to emphasize two key features of this problem. First, depending on what happens, we
may never know how much gold each pirate had in the chest or who lied, even in hindsight.
Hindsight isn't 20/20. Second, the solution to the problem requires outright destruction of
wealth. 
The point of this post is that these two features go hand-in-hand. There's a wide range of
real-life problems where we can't tell what happened, even in hindsight; we'll talk about
three classes of examples. In these situations, it's hard to design good
incentives/mechanisms, because we don't know where to allocate credit and blame. Outright
wealth destruction provides a fairly general-purpose tool for such problems. It allows us to
align incentives in otherwise-intractable problems, though often at considerable cost.
The Lemon Problem

Alice wants to sell her old car, and Bob is in the market for a decent quality used vehicle.
One problem: while Alice knows that her car is in good condition (i.e. "not a lemon"), she has
no cheap way to convince Bob of this fact. A full inspection by a neutral third party would be
expensive, Bob doesn't have the skills to inspect the car himself, and any words Alice speaks
on the matter could just as easily be spoken by someone selling a lemon.
In order to convince Bob that the car is not a lemon, Alice needs to say or do something
which a lemon-seller would not. What can she do?
One easy answer: oﬀer to pay for any mechanical problems which come up after the sale. If
Alice knew about expensive mechanical problems hiding under the car's hood, then she
wouldn't oﬀer Bob this sort of insurance (at least not for a low price). Conversely, if Alice is
reasonably conﬁdent there are no mechanical problems, then oﬀering to pay for the
probably-non-existent problems costs her little.
There is one problem with this approach, however: if Alice is paying for mechanical
problems, then Bob has no incentive to take good care of the car.
Ideally, if we could ﬁgure out in hindsight which problems were already present at the time
of the sale, then Alice could oﬀer to pay for only problems which were present beforehand.
But in practice, if the car's brakes fail 6 months or a year after the sale, we have no way to
tell when the problem began. Were they already worn down, or has Bob been visiting the
racetrack?
We can get a less-than-perfect solution using a proxy. For instance, if the car's belt snaps a
week after the sale, then it was probably frayed beforehand. If it snaps ﬁve years after the
sale, then it probably wasn't a noticeable issue beforehand. In this case, we can use time-at-
which-a-problem-is-detected as a proxy for whether-a-problem-was-present-at-time-of-sale.
This isn't perfectly reliable, and there will be grey areas, but it gets us one step closer to
ﬁguring out in hindsight what happened.
Alternatively, we could try to align incentives without ﬁguring out what happened in
hindsight, using a trick similar to our pirate captain throwing the chest overboard. The trick

is: if there's a mechanical problem after the sale, then both Alice and Bob pay for it. I do not
mean they split the bill; I mean they both pay the entire cost of the bill. One of them pays
the mechanic, and the other takes the same amount of money in cash and burns it. (Or
donates to a third party they don't especially like, or ....) This aligns both their incentives:
Alice is no longer incentivized to hide mechanical problems when showing oﬀ the car, and
Bob is no longer incentivized to ignore maintenance or frequent the racetrack.
However, this solution also illustrates the downside of the technique: it's expensive.
Sometimes accidents happen - e.g. the air conditioner fails without Alice hiding it or Bob
abusing the car. Our both-pay solution will make such accidents twice as expensive. If we
can't tell in hindsight whether a problem was Alice' fault, Bob's fault, or an accident, then
both Alice and Bob need to pay the full cost of the problem in order to fully align their
incentives. That means they'll both need to pay for accidents, which reduces the overall
surplus from the car-sale. If the car is worth enough to Bob and little enough to Alice, there
may still be room to make the deal work, but the (expected) cost of accidental problems will
eat into both of their wallets.
Similarly, if Alice and Bob have less-than-perfect trust in each others' capabilities, that will
eat into (expected) value. If Bob thinks that Alice just doesn't know her own car very well, he
may expect problems that Alice doesn't know about. If Alice thinks that Bob is a careless
driver regardless of incentives, then she'll expect problems. These sorts of problems are
eﬀectively the same as accidents: they're problems which won't be avoided by good
incentives, and therefore their overall cost will be doubled when both Alice and Bob need to
pay for them.
O-Ring Production Functions
Suppose we have 100 workers, all working to produce a product. In order for the product to
work, all 100 workers have to do their part correctly; if even just one of them messes up,
then the whole product fails. This is an o-ring production function - named for the explosion
of the space shuttle Challenger, where the failure of one o-ring led to the fatal failure of the
whole shuttle. The model has some interesting economic implications - in particular, under o-
ring-like production, adding a high-skill worker to a team of other high-skilled workers
generates more value than adding the same high-skill worker to a team of low-skill workers.
Conversely, it oﬀers theoretical support for common claims like "hiring one bad worker
creates more damage than hiring ten good workers creates beneﬁt".
Here, I want to think about incentive design in an o-ring-like production model. If any worker
fails to build their component well, then the whole product fails. How do we incentivize each
worker to make their particular component work well? If we can ﬁgure out in hindsight which
component(s) failed, then incentive design is easy: reward workers whose components
succeeded, punish workers whose components failed. But what if we can't tell in hindsight
which components failed? What if we only know whether the product as a whole failed?
We can apply our value-destruction trick: if the product fails, then punish each worker as
though their component had failed. Each worker is then fully incentivized to make their
component work; if it fails, they'll face the full cost of failure.
Just like the used car example, accidents are a problem. If there's a non-negligible chance of
accident, then workers will expect a non-negligible chance of failure outside of their control.
In order to make up for that chance of punishment, the company will have to oﬀer extra base
pay to convince workers to work for them in the ﬁrst place.
Also like the used car example, if the workers don't trust each others' capabilities, then that
has the same eﬀect as expecting accidents. Anything which makes the workers expect
failure regardless of the incentives makes them expect punishment outside of their control,

which makes them demand higher base pay in order to make it worthwhile to work for this
company at all.
Even worse: if the workers think there's a high probability of failure regardless of incentives,
that reduces their own incentive to avoid failure. If they expect the ﬁnal product to fail
regardless of whether their own component fails, then they have little incentive to make
their own component work. In order for this whole strategy to work well, there has to be a
high probability that the end product succeeds, assuming the incentives are aligned.
Accidents and incompetence have to be rare. (Drawing the analogy back to the used-car
problem: if Alice knows that the clutch is bad, but expects Bob to abuse the clutch enough
that it would be ruined anyway regardless of incentives, then she has little reason to mention
the bad clutch, even under the both-pay strategy.)
Telephone
In the context of a modern business, one model I think about is the game of telephone. The
players all sit in a line, and the ﬁrst player receives a secret message. The ﬁrst player
whispers the message in the ear of the second, the second whispers it to the third, and so
forth. When the message reaches the last player, we compare the message received to the
message sent to see if they match. Inevitably, a starting message of "please buy milk and
potatoes at the store" turns into "cheesy guys grow tomatoes on the shore", or something
equally ridiculous, one mistake at a time.
In a business context, the telephone chain might involve a customer research group
collecting data from customers, then passing that data to product managers, who turn it into
feature requests for designers, who then hand the design over to engineers, who build and
release the product, often with several steps of information passing up and down
management chains in the middle. This goes about as well as the game of telephone - thus,
"jokes" like this:
Viewed as economic production, the game of telephone is itself an example of an o-ring
production function. In order to get a successful ﬁnal product - i.e. a ﬁnal message which

matches the original message - every person in the chain must successfully convey the
message. If one person fails, the whole product fails. (Even if individual failures are only
minor, a relatively small number of them still wipes out the contents of the message.) And, if
there's an end-to-end mismatch, it will often be expensive to ﬁgure out where
communication failed, even in hindsight.
So, we have the preconditions for our technique: we can incentivize good message-passing
by punishing everyone in the chain when the output message doesn't match the input
message.
Would this be a good idea? It depends on how much miscommunication can be removed by
good incentives. If the limiting factor is poor communication skills, and the people involved
can't do any better even if they try, then we're in the "expect accidents" regime: the
incentives will be expensive and the system will often fail anyway. On the other hand, if
incentivizing reliable communication produces reliable communication, then the strategy
should work.
That said, we're talking about punishing managers for miscommunicating, so presumably
few managers would want to adopt such a rule regardless. Good incentive design doesn't
make much diﬀerence if the people who choose the incentives do not want to ﬁx them.

It's not economically ineﬃcient for a
UBI to reduce recipient's employment
A UBI (e.g. paying every adult American $8k/year) would reduce recipient's need for
money and so may reduce their incentive to work. This is frequently oﬀered as an
argument against a UBI (or as an argument for alternative policies like the EITC that
directly incentivize work).
This argument is sometimes presented as economically hard-headed realism. But as
far as I can tell, there's not really any eﬃciency argument here—there's nothing
particularly eﬃcient about people having a stronger incentive to work because they
are poorer. The argument seems to mostly get its punch from a vague sense that work
is virtuous and necessary. I think that sense is largely mistaken and it should be taken
less seriously.
(As usual for policy posts, I'm not very conﬁdent about any of this; if it makes you
happier feel free to imagine sprinkling "seems to me" throughout the prose.)
What people fear
If I give you $8k, you will probably value marginal dollars less than you used to. Some
jobs that used to be a good deal will stop being worth it, and a job search itself may
stop being worthwhile. We could study how much this happens empirically, but it's
deﬁnitely plausible on paper and it would be my best guess for the long-run eﬀect
even if pilot UBI experiments found otherwise.
This seems to be one of the main worries people have about a UBI. For example, some
folks at McKinsey write:
In the design of the Finnish experiment, the main research question, agreed to by
parliament in the enabling legislation, was the impact of a basic income on
employment. Many policy makers assume that an entirely unconditional
guaranteed income would reduce incentives to work. After all, the argument goes,
why bother with a job if you can have a decent life without one? This assumption
has led many countries to deploy active labor-market policies that require people
on unemployment beneﬁts to prove their eligibility continually and, often, to
participate in some kind of training or to accept jobs oﬀered to them.
[They ﬁnd that the UBI didn't decrease work, but this post isn't even going to get into
that.]
But it's left somewhat vague what exactly is so bad about this.  In fact it's hard to
respond to this concern because, although I've seen it expressed so many times, I've
never really seen the argument laid out clearly.
It's clear that work creates much of the great stuﬀ in our society, and so reducing how
much work happens seems scary and bad. But when I work I get paid, so that I (and
the people I share my income with) get most of the beneﬁts from my work. And
leisure and slack also create value. If it stops being personally worth it for me to work,
then it has likely stopped being socially eﬃcient for me to work, and that's OK.

The real cost of a UBI is on the taxpayer's side, and so the actual focus of discussion
should be "how expensive is a UBI and are we willing to pay that much?" Thinking
about recipients' incentives to work on top of that is double-counting at best. To be
clear, I think that the cost of a UBI usually is the focus, as it should be. So all I'm trying
to do is address a little bit of FUD about UBI rather than make the positive case.
My view
Is working good for the rest of society?
Suppose you do some work and earn $100. The question from the rest of society's
perspective is whether we got more beneﬁt than the $100 we paid you.
We can get more than $100 if e.g. you spend your $100 on a Netﬂix subscription that
subsidizes better TV, or help our society learn by doing and advance technology for
the products you produce or consume.
We can get less than $100 if e.g. you spend your $100 renting an apartment in a city
that crowds out others, or buy products that create untaxed externalities like pollution
or signaling.
Similarly, if you decide to relax and have fun, society can beneﬁt (e.g. if you are
participating in a community that beneﬁts from having more members, or hanging out
with your kids who enjoy your company, or doing unpaid work to improve the world)
or suﬀer (e.g. if you contribute to congestion at a public park, or you drive around
throwing eggs at people's houses).
Overall I think that working is probably better for the world than leisure. The eﬀect
seems pretty small though (maybe 10% of the value you earn), and I think this isn't a
big deal compared to other eﬃciency considerations about a UBI. For example, it
seems like it is smaller than any one of the additional overhead in additional
redistributive programs, the costs of bullshit for beneﬁt recipients, the eﬃciency
losses inherent in poverty (e.g. from credit market failures), and the deadweight
losses from taxation.
(Of course the calculus is diﬀerent for people who pay taxes, since that's pure social
beneﬁt, but a UBI should mostly change employment for low-income families who are
paying very low tax rates.)
Dependency trap
Another concern is that people who don't work won't develop marketable skills, so
they will remain trapped as dependents. Some of my thoughts on that concern:
On paper, it seems more likely to me that being poor would be a trap than that
that having money would be a trap (e.g. by making it very diﬃcult to invest for
the future and forcing short-sighted decisions).
I haven't seen evidence that giving people money signiﬁcantly reduces their
future earning potential and have seen weak evidence against.
I think the prior should be against paternalism; at a minimum we ought to have
solid evidence that people are making a mistake before being willing to pay to
incentivize them to do something for their own beneﬁt.

If people decide not to work in the future because they expect to continue
having a UBI, that's not a trap, it's just the temporally-extended version of the
previous section.
Other eﬀects
Funding a UBI would involve signiﬁcant tax increases, and those really do ineﬃciently
decrease taxpayer's incentives to work. For example, paying $8k per adult in the US
would require increase the average tax rate by ~15%. But quantitatively the
disincentive seems moderate, and this isn't relevant if we are comparing a UBI to
other government spending. When people get really concerned about incentives to
work it's because they are thinking about beneﬁciaries who no longer have to work,
and that's the part I think is mistaken.
This argument is especially clear when we compare UBI to programs that add a bunch
of machinery with the goal of incentivizing work. For example, families with a positive
incentive from the EITC probably have an ineﬃciently large incentive to work since the
size of the EITC seems to dominate plausible estimates of externalities (and
then other families have an absurdly reduced incentive to work).
There may be strong economic cases for these policies based on empirical analyses of
e.g. the beneﬁts to communities from more people working. But I haven't seen studies
that convincingly assess causality, and it feels to me like public support for these
policies is mostly based on unjustiﬁed pro-work sentiment.

Clarifying inner alignment terminology
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
I have seen a lot of confusion recently surrounding exactly how outer and inner
alignment should be deﬁned and I want to try and provide my attempt at a
clariﬁcation.
Here's my diagram of how I think the various concepts should ﬁt together:

The idea of this diagram is that the arrows are implications—that is, for any problem in
the diagram, if its direct subproblems are solved, then it should be solved as well
(though not necessarily vice versa). Thus, we get:

inner alignment →objective robustness
outer alignment ∧objective robustness →intent alignment
intent alignment ∧capability robustness →alignment
And here are all my deﬁnitions of the relevant terms which I think produce those
implications:
(Impact) Alignment: An agent is impact aligned (with humans) if it doesn't take
actions that we would judge to be bad/problematic/dangerous/catastrophic.
Intent Alignment: An agent is intent aligned if the optimal policy for its behavioral
objective[1] is impact aligned with humans.
Outer Alignment: An objective function r is outer aligned if all models that perform
optimally on r in the limit of perfect training and inﬁnite data are intent aligned.[2]
Robustness: An agent is robust if it performs well on the base objective it was trained
under even in deployment/oﬀ-distribution.[3]
Objective Robustness: An agent is objective robust if the optimal policy for its
behavioral objective is impact aligned with the base objective it was trained under.
Capability Robustness: An agent is capability robust if it performs well on its
behavioral objective even in deployment/oﬀ-distribution.
Inner Alignment: A mesa-optimizer is inner aligned if the optimal policy for its mesa-
objective is impact aligned with the base objective it was trained under.
And an explanation of each of the diagram's implications:
inner alignment →objective robustness: If a model is a mesa-optimizer, then its
behavioral objective should match its mesa-objective, which means if it's mesa-
objective is aligned with the base, then it's behavioral objective should be too.
outer alignment  ∧objective robustness →intent alignment: Outer alignment ensures
that the base objective is measuring what we actually care about and objective
robustness ensures that the model's behavioral objective is aligned with that base
objective. Thus, putting them together, we get that the model's behavioral objective
must be aligned with humans, which is precisely intent alignment.
intent alignment  ∧capability robustness →alignment: Intent alignment ensures that
the behavioral objective is aligned with humans and capability robustness ensures that
the model actually pursues that behavioral objective eﬀectively—even oﬀ-distribution
—which means that the model will actually always take aligned actions, not just have
an aligned behavioral objective.

FAQ
If a model is both outer and inner aligned, what does that imply?
Intent alignment. Reading oﬀ the implications from the diagram, we can see that the
conjunction of outer and inner alignment gets us to intent alignment, but not all the
way to impact alignment, as we're missing capability robustness.
Can impact alignment be split into outer alignment and inner alignment?
No. As I just mentioned, the conjunction of both outer and inner alignment only gives
us intent alignment, not impact alignment. Furthermore, if the model is not a mesa-
optimizer, then it can be objective robust (and thus intent aligned) without being inner
aligned.
Does a model have to be inner aligned to be impact aligned?
No—we only need inner alignment if we're dealing with mesa-optimization. While we
can get impact alignment through a combination of inner alignment, outer alignment,
and capability robustness, the diagram tells us that we can get the same exact thing if
we substitute objective robustness for inner alignment—and while inner alignment
implies objective robustness, the converse is not true.
How does this breakdown distinguish between the general concept of inner
alignment as failing "when your capabilities generalize but your objective
does not" and the more speciﬁc concept of inner alignment as "eliminating
the base-mesa objective gap?"[4]
Only the more speciﬁc deﬁnition is inner alignment. Under this set of terminology, the
more general deﬁnition instead refers to objective robustness, of which inner alignment
is only a subproblem.
What type of problem is deceptive alignment?[5]
Inner alignment—assuming that deception requires mesa-optimization. If we relax that
assumption, then it becomes an objective robustness problem. Since deception is a
problem with the model trying to do the wrong thing, it's clearly an intent alignment
problem rather than a capability robustness problem—and see here for an explanation
of why deception is never an outer alignment problem. Thus, it has to be an objective
robustness problem—and if we're dealing with a mesa-optimizer, an inner alignment
problem.
What type of problem is training a model to maximize paperclips?
Outer alignment—maximizing paperclips isn't an aligned objective even in the limit of
inﬁnite data.
How does this picture relate to a more robustness-centric version?
The above diagram can easily be reorganized into an equivalent, more robustness-
centric version, which I've included below. This diagram is intended to be fully
compatible with the above diagram—using the exact same deﬁnitions of all the terms
as given above—but with robustness given a more central role, replacing the central
role of intent alignment in the above diagram.

Edit: Previously I had this diagram only in a footnote, but I decided it was useful
enough to promote it to the main body.

1. The point of talking about the "optimal policy for a behavioral objective" is to
reference what an agent's behavior would look like if it never made any
"mistakes." Primarily, I mean this just in that intuitive sense, but we can also try
to build a somewhat more rigorous picture if we imagine using perfect IRL in the
limit of inﬁnite data to recover a behavioral objective and then look at the optimal
policy under that objective. ↩ 
2. What I mean by perfect training and inﬁnite data here is for the model to always
have optimal loss on all data points that it ever encounters. That gets a bit tricky
for reinforcement learning, though in that setting we can ask for the model to act
according to the optimal policy on the actual MDP that it experiences. ↩ 
3. Note that robustness as a whole isn't included in the diagram as I thought it
made it too messy. For an implication diagram with robustness instead of intent
alignment, see the alternative diagram in the FAQ. ↩ 
4. See here for an example of this confusion regarding the more general vs. more
speciﬁc uses of inner alignment. ↩ 
5. See here for an example of this confusion regarding deceptive alignment. ↩ 

Evading Mind Control
A couple days ago I surveyed readers for deviant beliefs. The results were funny,
hateful, boring and bonkers. One of the submissions might even be useful.
If you care a lot about your mind, it is not unreasonable to avoid advertisements
like plague rats, up to and including muting your speakers and averting your gaze.
This extremist position caught my eye because humans have a tendency to
underestimate the eﬀect advertising[1] has on us. I never realized how much
advertising aﬀected me until I got rid of it.
For nearly a year I have been avoiding junk media. I thought this would make me
happier, healthier and more productive—which it has—but the most surprising eﬀect
is how much the reduction in advertising aﬀects my behavior.
When I give up junk media, I stop thinking about politics, videogames and celebrities. I
think less about products in general. Important things eventually expand to ﬁll this
void. But for the ﬁrst week or so, my head just feels empty.
Tim Ferris doesn't just avoid news, television and social media. He even avoids
reading books—especially nonﬁction. When I ﬁrst read that, I thought he was an Eloi.
Having blogged regularly for the past year myself, I now sympathize with him.
If you are young then you should read lots of books because you need to amass
information. Eventually you hit diminishing returns. Reading more books ﬁlls fewer
conceptual holes per unit time invested.
You cannot discover new knowledge for humanity by reading a book written by a
human.
But there is a bigger problem. It is easy to look up answers to common questions in a
book. It is harder to look up answers to esoteric questions. It is impossible to look up
answers to open problems. The diﬃculty of looking up important things you don't
know answers to increases the more low-hanging fruit you pick from the Tree of
Knowledge.
As your power waxes, it becomes easier to invent answers to your own questions.
Eventually the trajectories cross. It becomes easier to ﬁgure things out yourself than
to look up the answer. The comparative value of reading books goes negative. Books,
once guides, become reference material. It is more eﬃcient to write your own book
than to read someone else's.
I used to read a lot of books. I ﬁnished 18 books in the ﬁrst 4.5 months of 2020.
Date
Title
Author
Page
Count
January 1
The Trouble with Physics
Lee Smolin
392
January 17 My Side of the Street
Jason DeSena Trennert 224
January 19 Saints & Sinners
William L. Hamilton
145
January 20 The Quants
Scott Patterson
352

Date
Title
Author
Page
Count
February
21
Harry Potter and the Methods of
Rationality
Eliezer Yudkowsky
N/A
February
22
The Vital Question
Nick Lane
368
February
24
The Last Leonardo
Ben Lewis
384
March 4
Mastering Manga with Mark Crilley
Mark Crilley
128
March 22
World War Z
Max Brooks
342
March 29
The Nature of Plants
Craig Huegel
228
March 31
Built not Born
Tom Golisano, Mike
Wicks
224
April 13
A First-Class Catastrophe
Diana B. Henriques
416
April 21
The Plant Messiah
Carlos Magdalena
238
April 22
The 4-Hour Workweek
Tim Ferris
308
April 27
The War on Normal People
Andrew Yang
304
May 1
Seeing Like a State
James C. Scott
445
May 5
Botany for Gardeners 3rd Edition
Brian Capon
240
May 15
The $12 Million Stuﬀed Shark
Don Thompson
272
Then I...stopped. In the 6.3 months since mid-May I ﬁnished only 3 books.
Date
Title
Author
Page Count
July 2
Human Diversity Charles Murray 528
August 4
The Actor's Life
Jeanna Fischer 252
November 2 Lost Connections Johann Hari
322
May of this year appears to be when I hit my inﬂection point where writing became
more useful than reading.
When I started writing, I thought it was a substitute for socializing. I now realize it is a
substitute for reading. Writing is to reading what entrepreneurship is to having a job.
Reading too much (compared to what you write) turns you into a sheep.
1. In this sense, "advertising" includes not only paid adverts like banner ads but
also self-replicating propaganda ("we should raise awareness of..."), grassroots
advertising (videogame streamers, artiﬁcial communities) and all information
derived from a press release. I care about whether an interest group is getting a
message into my head. Neither I nor the interest group cares how it gets there.
↩ 

Learning Normativity: A Research
Agenda
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
(Related to Inaccessible Information , Learning the Prior , and Better Priors as a Safety
Problem . Builds on several of my alternate alignment ideas .)
 
I want to talk about something which I'll call learning normativity. What is normativity?
Normativity is correct behavior. I mean something related to the fuzzy concept humans
convey with the word "should". I think it has several interesting features:
Norms are the result of a complex negotiation between humans, so they shouldn't
necessarily be thought of as the result of maximizing some set of values. This
distinguishes learning normativity from value learning.
A lot of information about norms is present in the empirical distribution of what people
actually do, but you can't learn norms just by learning human behavior. This
distinguishes it from imitation learning.
It's often possible to provide a lot of information in the form of "good/bad" feedback.
This feedback should be interpreted more like approval-directed learning rather than
RL. However, approval should not be treated as a gold standard.
Similarly, it's often possible to provide a lot of information in the form of rules, but rules
are not necessarily 100% true; they are just very likely to apply in typical cases.
In general, it's possible to get very rich types of feedback, but very sparse : humans
get all sorts of feedback, including not only instruction on how to act, but also how to
think.
Any one piece of feedback is suspect. Teachers can make mistakes, instructions can be
wrong, demonstrations can be imperfect, dictionaries can contain spelling errors,
reward signals can be corrupt, and so on.
Example: Language Learning
 A major motivating example for me is how language learning works in humans. There is
clearly, to some degree, a "right way" and a "wrong way" to use a language. I'll call this
correct usage.
One notable feature of language learning is that we don't always speak, or write, in correct
usage. This means that a child learning language has to distinguish between mistakes (such
as typos) and correct usage. (Humans do sometimes learn to imitate mistakes, but we have
a notion of not doing so. This is unlike GPT systems learning to imitate the empirical
distribution of human text.)
This means we're largely doing something like unsupervised learning, but with a notion of
"correct"/"incorrect" data. We're doing something like throwing data out when it's likely to be
incorrect.
A related point is that we are better at recognizing correct usage than we are at generating
it. If we say something wrong, we're likely able to correct it. In some sense, this means
there's a foothold for intelligence ampliﬁcation: we know how to generate our own training
gradent.

Another fascinating feature of language is that although native speakers are pretty good at
both recognizing and generating correct usage, we don't know the rules explicitly. The whole
ﬁeld of linguistics is largely about trying to uncover the rules of grammar.
So it's impossible for us to teach proper English by teaching the rules. Yet, we do know some
of the rules. Or, more accurately, we know a set of rules that usually apply. And those rules
are somewhat useful for teaching English. (Although children have usually reached ﬂuency
before the point where they're taught explicit English grammar.)
All of these things point toward what I mean by learning normativity:
We can tell a lot about what's normative by simply observing what's common, but the
two are not exactly the same thing.
A (qualiﬁed) human can usually label an example as correct or incorrect, but this is not
perfect either.
We can articulate a lot about correct vs incorrect in the form of rules; but the rules
which we can articulate never seem to cover 100% of the cases. A linguist is a lot like a
philosopher: taking a concept which is understood at an intuitive level (which a great
many people can ﬂuently apply in the correct manner), but struggling for years to
arrive at a correct technical deﬁnition which ﬁts the intuitive usage.
In other words, the overriding feature of normativity which I'm trying to point at is that
nothing is ever 100%. Correct grammar is not deﬁned by any (known) rules or set of text, nor
is it (quite) just whatever humans judge it is. All of those things give a lot of information
about it, but it could diﬀer from each of them. Yet, on top of all that, basically everyone
learns it successfully. This is very close to Paul's Inaccessible Information: information for
which we cannot concoct a gold-standard training signal, but which intelligent systems may
learn anyway.
Another important feature of this type of learning: there is a fairly clear notion of
superhuman performance. Even though human imitation is most of the challenge, we could
declare something superhuman based on our human understanding of the task. For
example, GPT is trained exclusively to imitate, so it should never exceed human
performance. Yet, we could tell if a GPT-like system did exceed human performance:
Its spelling and grammar would be immaculate, rather than including humanlike errors;
its output would be more creative and exciting to read than that of human authors;
when good reasoning was called for in a text, its arguments would be clear, correct,
and compelling;
when truth was called for, rather than ﬁction, its general knowledge would be broader
and more accurate than a human's.
It seems very possible to learn to be better than your teachers in these ways, because
humans sometimes manage to do it.
Learning in the Absence of a Gold
Standard
In statistics and machine learning, a "gold standard" is a proxy which we treat as good
enough to serve as ground truth for our limited purposes. The accuracy of any other
estimate will be judged by comparison to the gold standard. This is similar to the concept of
"operationalization" in science.
It's worth pointing out that in pure Bayesian terms, there is nothing especially concerning
about learning in the absence of a gold standard. I have data X. I want to know about Y. I
update on X, getting P(Y|X). No problem!

However, that only works if we have the right prior. We could try to learn the prior from
humans, which gets us 99% of the way there... but as I've mentioned earlier, human
imitation does not get us all the way. Humans don't perfectly endorse their own reactions.
(Note that whether "99% of the way" is good enough for AI safety is a separate question. I'm
trying to deﬁne the Big Hairy Audacious Goal of learning normativity.)
Actually, I want to split "no gold standard" into two separate problems.
1. There's no type of feedback which we can perfectly trust. If humans label
examples of good/bad behavior, a few of those labels are going to be wrong. If humans
provide example inferences for learning the prior, some of those example inferences
are (in a very real sense) wrong. And so on.
2. There's no level at which we can perfectly deﬁne the loss function. This is a
consequence of no-perfect-feedback, but it's worth pointing out separately.
No Perfect Feedback
I think I've made the concept of no-perfect-feedback clear enough already. But what could it
mean to learn under this condition, in a machine-learning sense?
There are some ideas that get part of the way:
Jeﬀrey updates let us update to a speciﬁc probability of a given piece of feedback being
true, rather than updating to 100%. This allows us to, EG, label an image as 90%-
probable cat, 9%-probable dog, 1% broad distribution over other things.
This allows us to give some evidence, while allowing the learner to decide later
that what we said was wrong (due to the accumulation of contrary evidence).
This seems helpful, but we need to be conﬁdent that those probability
assignments are themselves normatively correct, and this seems like it's going to
be a pretty big problem in practice.
Virtual evidence is one step better: we don't have to indicate what actual probability to
update to, but instead only indicate the strength of evidence.
Like Jeﬀrey updates, this means we can provide strong evidence while still
allowing the system to decide later that we were wrong, due to the accumulation
of contradicting evidence.
Unlike Jeﬀrey updates, we don't have to decide what probability we should
update to, only the direction and strength of the evidence.
Soft labels in machine learning provide a similar functionality. In EM learning, a system
learns from its own soft labels. In LO-shot learning, a system leverages the fact that
soft labels contain more information than hard labels, in order to learn classes with less
than one examples per class.
However, although these ideas capture weak feedback in the sense of less-than-100%-
conﬁdence feedback, they don't capture the idea of reinterpretable feedback:
A system should ideally be able to learn that speciﬁc types of feedback are erroneous,
such as corrupted-feedback cases in reinforcement learning. A system might learn that
my feedback is lower quality right before lunch, for example.
A system should be able to preserve the overall meaning of a label despite an ontology
shift. For example, deciding that fruit/vegetable is not a useful taxonomic or culinary
distinction should not destroy the information gained from such labels. Or, if human
feedback includes formal English grammar, that information should not be totally
discarded if the system realizes that the rules don't fully hold and the supposed
grammatical categories are not as solid as claimed.
Feedback should be associated with a cloud of possible interpretations. When humans
say "weird", we often mean "unusual", but also sometimes mean "bad". When humans

say we don't understand, we often really mean we don't endorse. A system should,
ideally, be able to learn a mapping from the feedback humans actually give to what
they really mean. This is, in any case, the general solution to the previous bullet points.
But "learning a mapping from what feedback is given to what is meant" appears to imply
that there is no ﬁxed loss function for machine learning to work on, which would be a serious
challenge. This is the subject of my point #2 from earlier:
No Perfect Loss Function
We can frame (some) approaches to the value speciﬁcation problem in a sequence of
increasingly sophisticated approaches (similar to the hierarchy I discussed in my "stable
pointers to value" posts (1,2,3)):
1. Direct speciﬁcation of the value function. This fails because we don't know what values
to specify, and expect anything we can write down to be highly Goodhart-able.
2. Learning human values. We delegate the speciﬁcation problem to the machine. But,
this leaves us with the meta problem of specifying how to learn. Getting it wrong can
lead to wireheading and human manipulation. Even in settings where this is impossible,
we face Stuart's no-free-lunch results.
3. Learning to learn human values. Stuart suggests that we can get around the no-free-
lunch results by loading the right prior information into the learner, in keeping with his
more general belief that Bayesian reasoning is ﬁne as long as it has the right prior
information. But this seems to go back to the problem of learning the human prior. So
we could apply a learning approach again here. But then we again have a speciﬁcation
problem for the loss function for this learning...
4. ...
You get the picture. We can keep pushing back the speciﬁcation problem by learning,
learning to learn, learning to learn to learn... Each time we push the problem back, we seem
to gain something, but we're also stuck with a new speciﬁcation problem at the meta level.
Could we specify a way to learn at all the levels, pushing the problem back inﬁnitely? This
might sound absurd, but I think there are ways to accomplish this. We need to somehow
"collapse all the levels into one learner" -- otherwise, with an inﬁnite number of levels to
learn, there would be no hope. There needs to be very signiﬁcant generalization across
levels. For example, Occam's razor is a good starting rule of thumb at all levels (at least, all
levels above the lowest). However, because Occam is not enough, it will need to be
augmented with other information.
Recursive reward modeling is similar to the approach I'm sketching, in that it recursively
breaks down the problem of specifying a loss function. However, it doesn't really take the
same learning-to-learn approach, and it also doesn't aim for a monolithic learning system
that is able to absorb information at all the levels.
I think of this as necessary learning-theoretic background work in order to achieve Stuart
Armstrong's agenda, although Stuart may disagree. The goal here is to provide one
framework in which all the information Stuart hopes to give a system can be properly
integrated.
Note that this is only an approach to outer alignment. The inner alignment problem is a
separate, and perhaps even more pressing, issue. The next section could be of more help to
inner alignment, but I'm not sure this is overall the right path to solve that problem.
Process-Level Feedback

Sometimes we care about how we get the answers, not just what the answers are. That is to
say, sometimes we can point out problems with methodology without being able to point to
problems in the answers themselves. Answers can be suspect based on how they're
computed.
Sometimes, points can only be eﬀectively made in terms of this type of feedback.
Wireheading and human manipulation can't be eliminated through object-level feedback, but
we could point out examples of the wrong and right types of reasoning.
Process-level feedback blurs the distinction between inner alignment and outer alignment. A
system which accepts process-level feedback is essentially exposing all its innards as
"outer", so if we can provide the appropriate feedback, there should be no separate inner
alignment problem. (Unfortunately, it must be admitted that it's quite diﬃcult to provide the
right feedback -- due to transparency issues, we can't expect to understand all models in
order to give feedback on them.)
I also want to emphasize that we want to give feedback on the entire process. It's no good if
we have "level 1" which is in charge of producing output, and learns from object-level
feedback, but "level 2" is in charge of accepting process-level feedback about level 1, and
adjusting level 1 accordingly. Then we still have a separate inner alignment problem for level
2.
This is the same kind of hierarchy problem we saw in "No Perfect Loss Function". Similarly,
we want to collapse all the levels down. We want one level which is capable of accepting
process-level feedback about itself.
Learning from Process-Level Feedback
In a Bayesian treatment, process-level feedback means direct feedback about hypotheses. In
theory, there's no barrier to this type of feedback. A hypothesis can be ruled out by ﬁat just
as easily as it can be ruled out by contradicting data. 
However, this isn't a very powerful learning mechanism. If we imagine a human trying to
inner-align a Bayesian system this way, the human has to ﬁnd and knock out every single
malign hypothesis. There's no generalization mechanism here.
Since detecting malign hypotheses is diﬃcult, we want the learning system to help us out
here. It should generalize from examples of malign hypotheses, and attempt to draw a broad
boundary around malignancy. Allowing the system to judge itself in this way can of course
lead to malign reinterpretations of user feedback, but hopefully allows for a basin of
attraction in which benevolent generalizations can be learned.
For example, in Solomonoﬀ induction, we have a powerful hierarchical prior in the
distribution on program preﬁxes. A program preﬁx can represent any kind of distribution on
hypotheses (since a program preﬁx can completely change the programming language to be
used in the remainder of the program). So one would hope that knocking out hypotheses
would reduce the probability of all other programs which share a preﬁx with that hypothesis,
representing a generalization "this branch in my hierarchical prior on programs seems iﬀy".
(As a stretch goal, we'd also like to update against other similar-looking branches; but we at
least want to update against this one.)
However, no such update occurs. The branch loses mass, due to losing one member, but
programs which share a preﬁx with the deleted program don't lose any mass. In fact, they
gain mass, due to renormalization.
It seems we don't just want to update on "not this hypothesis"; we want to explicitly model
some sort of malignancy judgement (or more generally, a quality-of-hypothesis judgement),
so that we can update estimations of how to make such judgements. However, it's diﬃcult to

see how to do so without creating a hierarchy, where we get a top level which isn't open to
process-level feedback (and may therefore be malign).
Later, I'll present a Bayesian model which does have a version of generalization from
feedback on hypotheses. But we should also be open to less-Bayesian solutions; it's possible
this just isn't captured very well by Bayesian learning.
Prospects for Inner Alignment
I view this more as a preliminary step in one possible approach to inner alignment, rather
than "a solution to inner alignment".
If (a) you want to learn a solution to inner alignment, rather than solving it ahead of time,
and (b) you agree with the framing of process-level feedback / feedback on hypotheses, and
(c) you agree that we can't rely on a trusted meta-level to take process-level feedback, but
rather need to accept feedback on "the whole process", then I think it stands to reason that
you need to specify what it means to learn in this setting. I view the preceding sections
as an argument that there's a non-obvious problem here.
For example, Stuart Armstrong has repeatedly argued that Bayesian learners can overcome
many safety problems, if only they're given the right prior information. To the extent that this
is a claim about inner alignment (I'm not sure whether he would go that far), I'm claiming
that we need to solve the problem of giving process-level feedback to a Bayesian learner
before he can make good on his claim; otherwise, there's just no known mechanism to
provide the system with all the necessary information.
Anyway, even if we accomplish this step, there are still several other obstacles in the way of
this approach to inner alignment.
1. Transparency: It's unrealistic that humans can provide the needed process-level
feedback without powerful transparency tools. The system needs to correctly
generalize from simpler examples humans provide to the more diﬃcult examples which
a human can't understand. That will be diﬃcult if humans can only label very very
simple examples.
2. Basin of Attraction: Because the system could use malign interpretations of human
feedback, it's very important that the system start out in a benign state, making
trusted (if simplistic) generalizations of the feedback humans can provide.
3. Running Untrusted Code: A straightforward implementation of these ideas will still
have to run untrusted hypotheses in order to evaluate them. Giving malign hypotheses
really low probability doesn't help if we still run really low-probability hypotheses, and
the malign hypotheses can ﬁnd an exploit. This is similar to Vanessa's problem of non-
Cartesian daemons.
Regardless of these issues, I think it's valuable to try to solve the part of the problem I've
outlined in this essay, in the hope that the above issues can also be solved.
Summary of Desiderata
Here's a summary of all the concrete points I've made about what "learning normativity"
should mean. Sub-points are not subgoals, but rather, additional related desiderata; EG, one
might signiﬁcantly address "no perfect feedback" without signiﬁcantly addressing "uncertain
feedback" or "interpretable feedback". 
1. No Perfect Feedback: we want to be able to learn with the possibility that any one
piece of data is corrupt.

1. Uncertain Feedback: data can be given in an uncertain form, allowing 100%
certain feedback to be given (if there ever is such a thing), but also allowing the
system to learn signiﬁcant things in the absence of any certainty.
2. Reinterpretable Feedback: ideally, we want rich hypotheses about the
meaning of feedback, which help the system to identify corrupt feedback, and
interpret the information in imperfect feedback.
2. No Perfect Loss Function: we don't expect to perfectly deﬁne the utility function, or
what it means to correctly learn the utility function, or what it means to learn to learn,
and so on. At no level do we expect to be able to provide a single function we're happy
to optimize.
1. Learning at All Levels: Although we don't have perfect information at any
level, we do get meaningful beneﬁt with each level we step back and say "we're
learning this level rather than keeping it ﬁxed", because we can provide
meaningful approximate loss functions at each level, and meaningful feedback
for learning at each level. Therefore, we want to be able to do learning at each
level.
2. Between-Level Sharing: Because this implies an inﬁnite hierarchy of levels to
learn, we need to share a great deal of information between levels in order to
learn meaningfully.
3. Process Level Feedback: we want to be able to give feedback about how to arrive at
answers, not just the answers themselves.
1. Whole-Process Feedback: we don't want some segregated meta-level which
accepts/implements our process feedback about the rest of the system, but
which is immune to process feedback itself. Any part of the system which is
capable of adapting its behavior, we want to be able to give process-level
feedback about.
2. Learned Generalization of Process Feedback: we don't just want to promote
or demote speciﬁc hypotheses. We want the system to learn from our feedback,
making generalizations about which kinds of hypotheses are good or bad.
Initial Attempt: Recursive Quantilizers
I'll give an initial stab at solving these problems, as a proof-of-concept. (Otherwise I fear the
above desiderata may look like they're simply impossible.)
This is a formalization of the recursive quantilizers idea which I described previously.
A quantilizer is a mild optimizer which avoids catastrophic outcomes with high probability,
averting Goodhart's Law. It accomplishes this by refusing to 100% trust its value function.
This seems like a good building block for us, since it signiﬁcantly addresses "no perfect loss
function."
A quantilizer requires a value function, V , which it mildly optimizes, and a safe distribution, S
, which is a distribution over outputs which is assumed to have a low probability of
catastrophic outcomes. It also requires an optimization parameter, p. The quantilizer 
Q(V , S, p) mildly optimizes by randomly taking the top p% of outputs from S, as ranked by V .
(p can be derived from an estimate of the probability of catastrophe in S, combined with a
level of tolerance for catastrophic risk.)
In line with our learning at all levels desiderata, we want to learn V , S, and p. Moreover, we'll
produce all of these via quantilization, and recursively learn the inputs to all of those
quantilizers as well.

In order to get between-level sharing, we will actually learn one big quantilization process
which is equivalent to all those inﬁnitely many levels.
Here's the setup.
In order to unify all the levels, I'll have quantilizers select uniﬁed type for all arguments
(UTAAs): a tuple (V , S, p, o) where o is the object-level output we want out of the whole
system (EG plans, strings of text, whatever). So now we understand the quantilization
function Q(a, b, c) to take 3 UTAAs. It will take the value function from the ﬁrst, the safe
distribution from the second, and the optimization parameter from the third. It then produces
a UTAA. The user can take the object-level output of that UTAA as the result.*
 
What we want is a notion of stationary distribution on UTAAs. This is a distribution 
D ∈ΔUTAA such that if we sample 3 elements from D, and feed those three elements into Q,
the distribution on outputs of Q is just D again.
In eﬀect, a stationary distribution D is all quantilized out; we can quantilize again, but it
won't make any diﬀerence. So, we can just sample from D and take the object-level output
from the resulting UTAA.
This is very similar to the notion of stationary distribution for markov chains. The diﬀerence
is that we have a markov tree instead of a chain.
So, just as ﬁnding the stationary distribution of a markov-chain monte carlo algorithm is
similar to running the algorithm long enough that running it any longer is pointless, ﬁnding a
stationary distribution for recursive quantilizers is like building a learning-to-learn hierarchy
that's large enough that we don't get anything by going any further.

That's all well and good, but how are we supposed to ﬁnd a stationary distribution we like?
We can't just take a ﬁxed point and hope it's useful and benign; there'll be lots of crazy ﬁxed
points. How do we steer this thing toward desirable outcomes?
Parameterizing Stationary Distributions
If a markov chain has multiple stationary distributions, we can parameterize them through a
distribution on starting states. A distribution on starting states just means a probability of
picking any one starting element, so this relationship is completely linear: by interpolating
between diﬀerent starting elements, we interpolate between the stationary distributions
which those starting elements eventually reach.
We can similarly parameterize stationary distributions via initial distributions. However, we
don't get the same linearity. Because we have to select many starting elements for the 3n
 inputs to an n-level tree, and we select those elements as independent draws from the
initial distribution, we can get nonlinear eﬀects. (This is just like ﬂipping a biased coin (with
sides labelled 1 and 0) twice and sending the two results through an XOR gate: the
probability of getting a 1 out of the XOR is nonlinear in the bias.)
This means we can't reduce our uncertainty over initial distributions to uncertainty over
initial UTAA. (There may be some other tricks we can use to simplify things, but they
probably aren't worth exploring in this post.)
So we can parameterize our uncertainty over stationary distributions via uncertainty over
initial distributions. But, this is just turning uncertainty over one kind of distribution into
uncertainty over another. What's the beneﬁt of this?
1. The set of stationary distributions is hard to know, but the set of possible initial
distributions is clear. So this gives us an easy-to-work-with representation of stationary
distributions.
2. We know every stationary distribution is in the set, since we can start out in a
stationary distribution.
3. We can easily deﬁne the mapping from initial distributions to stationary distributions;
it's just the stationary distribution you get by running things long enough, sampling
from the given initial distribution. (Of course we may not get to any stationary
distribution at all, but we can formally solve this by introducing a cutoﬀ in program
size, or through other devices.)
4. We can therefore deﬁne learning: an update against a UTAA produces an update
against initial distributions which produce that UTAA.
This is, of course, a very computationally intensive procedure. Unless better algorithms are
found, the only way we can update is by producing a large quantilization tree (which we
hope has converged) and running it many times to evaluate the outcome of a given initial
distribution.
However, the resulting system has many marvelous properties. If we want to give feedback
at any level in the hierarchy, we can convert this into feedback about UTAAs, and update our
prior over initial distributions accordingly. For example:
We can label outputs as bad/incorrect by updating against all UTAAs which include
those outputs.
We can give evidence about the value function over outputs, and convert this to an
update about UTAAs based on the value function they contain. So, we can do value
learning.
We can learn about the safe distribution over outputs. For example, one proposal for
ﬁnding safe distributions is to model human behavior. Data-sets of human behavior

could induce updates over UTAAs by checking how well a UTAA's proposed safe
distribution ﬁts the data.
At the same time, we can learn about the loss function by which we score safe
distributions. If we have an update about this loss function, we translate it to an update
about UTAAs by checking how a UTAA's value function examines the safe distribution of
another UTAA when scoring it. Updating UTAAs based on this will, eﬀectively, change
the way safe distributions are selected in the second-to-last quantilization step. (Of
course, it really changes all the quantilization steps, but when we anchor ourselves in
how changes to the initial distribution alter our distribution on actual outputs, the
easiest way to understand what's going on is to see this as a change to the second-to-
last step.)
Similarly, we can learn about the loss function by which we score loss functions. So in
the same system, we can directly learn from feedback, we can do value learning, and
we can do meta-value-learning where we learn how to interpret evidence in value-
learning.
Similarly, we can learn the safe distribution for meta-loss functions, the safe
distribution over safe distributions, and on and on.
We can also allow process-level feedback by enabling UTAA value functions to examine
the source code of other UTAAs (e.g. looking at how those UTAAs compute their value
functions and safe distributions). We can teach UTAAs to detect suspicious code in
other UTAAs and rate those UTAAs very poorly.
Wouldn't it be fascinating to be able to provide all those types of learning in one system?
Analysis in terms of the Criteria
Let's examine how we did in terms of the criteria which I gave.
1. No Perfect Feedback: This wasn't addressed directly, but might be indirectly
addressed via #2.
1. Uncertain Feedback: I didn't specify any way to provide uncertain feedback,
but it would be easy enough to do so.
2. Reinterpretable Feedback: I think this is a big failing of the approach as it
stands.
2. No Perfect Loss Function: Very signiﬁcantly addressed by quantilization.
1. Learning at All Levels: Very signiﬁcantly addressed by the recursive
quantilization setup.
2. Between-Level Sharing: Signiﬁcantly addressed. I didn't really talk about how
this works, but I think it can work well in this setup.
3. Process Level Feedback: Signiﬁcantly addressed. The process which creates a given
output is essentially the big tree that we sample. We can give any kind of feedback
about that tree that we want, including any computations which occur inside of the
value functions or safe distributions or elsewhere.
1. Whole-Process Feedback: Somewhat addressed. There is a question of
whether the initial distribution constitutes a meta-level beyond the reach of
process-level feedback.
2. Learned Generalization of Process Feedback: Signiﬁcantly addressed.
Process-level feedback can be given directly, as evidence against a speciﬁc UTAA,
in which case there will be some generalization as we update against anything
which thought that UTAA was a good idea. Or it could be given more indirectly, as
general (level-independent) information about how value functions should judge
UTAA. In that case there may be more generalization, as we update on how to
judge UTAAs generally. (Or maybe not? An equivalence theorem about these
diﬀerent types of feedback would be nice.)
I think the most signiﬁcant problem here is the lack of reinterpretable feedback. When we
give feedback about something, we have to ﬁgure out how to translate it into an update

about UTAAs (which can then be translated into an update about initial distributions). This
update is ﬁxed forever. This means the updates we make to the system aren't really tied to
the value functions which get learned. So, for example, learning better value-learning
behavior doesn't directly change how the system responds to updates we give it about the
value function. (Instead, it may change how it interprets some other set of data we give it
access to, as input to UTAAs.) This makes the "learning-to-learn" aspect of the system
somewhat limited/shallow.
The second most signiﬁcant concern here is whether we've really achieved whole-process
feedback. I was initially optimistic, as the idea of stationary distributions appeared to
collapse all the meta levels down to one. However, now I think there actually is a problem
with the highest level of the tree. The initial distribution could be predominantly malign.
Those malign UTAAs could select innocent-looking (but deadly) UTAAs for the next
generation. In this way, the malign code could disappear, while achieving its goals by
introducing subtle bugs to all subsequent generations of UTAAs.
The way I've speciﬁed things, trying to update against these malign UTAAs wouldn't work,
because they're already absent in the stationary distribution. 
Of course, you could directly update against them in the initial distribution. This could
eliminate select malign UTAAs. The problem is that this kind of process feedback loses
generalizability again. Since it's the top level of the tree, there's nothing above it which is
selecting it, so we don't get to update against any general selection behaviors which
produced the malign UTAAs.
The only way out of this I see at present is to parameterize the system's beliefs directly as a
probability distribution over stationary distributions.. You can think of this as assuming that
the initial distribution is already a stationary distribution. This way, when we update against
malign UTAAs at the beginning of the process, we update against them occurring at any
point in the process, which means we also update against any UTAAs which help select
malign UTAAs, and therefore get generalization power.
But this seems like an annoyingly non-constructive solution. How are we supposed to work
with the set of ﬁxed points directly without iterating (potentially malign) code to ﬁnd them?
*: Actually, a UTAA should be a compact speciﬁcation of such  tuple, such as a program or
neural network which can output the desired objects. This is necessary for implementation,
since EG we can't store V as a big table of values or S as a big table of probabilities. It also
will allow for better generalization, and process-level feedback.

Snyder-Beattie, Sandberg, Drexler &
Bonsall (2020): The Timing of
Evolutionary Transitions Suggests
Intelligent Life Is Rare
This is a linkpost for https://www.liebertpub.com/doi/full/10.1089/ast.2019.2149
Copying over Anders Sandberg's Twitter summary of the paper:
There is life on Earth but this is not evidence for life being common in the universe! This is
since observing life requires living observers. Even if life is very rare, the observers will all
see they are on planets with life. Observation selection eﬀects need to be handled!
Observer selection eﬀects are annoying can produce apparently paradoxical eﬀects such
that your friends on average have more friends than you or that our existence "prevents"
recent giant meteor impacts. But one can control for them with some ingenuity! 
Life emerged fairly early on Earth: evidence that it is easy and common? Not so fast: if you
need multiple hard steps to evolve an observer to marvel at it, then on those super-rare
worlds where observers show up life statistically tend to be early.
If we have N hard steps (say; life, good genetic coding, eukaryotic cells, brains, observers)
as diﬃculty goes to inﬁnity in cases where all steps succeed before biosphere ends they
become equidistant between ﬁrst and last habitability.




That means that we can take observed timings and calculate backwards to get
probabilities compatible with them, controlling for observer selection bias.
Our argument builds on a chain from Carter's original argument and its extensions to
Bayesian transition models. I think our main addition here is using noninformative priors.

https://royalsocietypublishing.org/doi/10.1098/rsta.1983.0096
https://arxiv.org/abs/0711.1985v1
http://mason.gmu.edu/~rhanson/hardstep.pdf
https://www.pnas.org/content/pnas/109/2/395.full.pdf
The main take-home message is that one can rule out fairly high probabilities for the
transitions, while super-hard steps are compatible with observations. We get good odds on
us being alone in the observable universe.
If we found a dark biosphere or life on Venus that would weaken the conclusion, similarly
for big updates on when some transitions happened; we have various sensitivity checks in
the paper.
Our conclusions (if they are right) are good news if you are worried about the Great Filter.
we have N hard ﬁlters behind us, so the empty sky is not necessarily bad news. We may be
lonely but have much of the universe for ourselves.

Another cool application is that this line of reasoning really suggests that M-dwarf planets
must be much less habitable than they seem: otherwise we should expect to be living
around one, since they are so common compared to G2 stars.
Personally I am pretty bullish about M-dwarf planet habitability (despite those pesky
superﬂares), but our result suggests that there may be extra eﬀects impairing them. They
need to be pretty severe too: they need to reduce habitability probability by a factor of
over 10,000. 
I see this paper as part of a trilogy started with our "anthropic shadows" paper and
completed by a paper on observer selection eﬀects in nuclear war near misses (coming, I
promise!) Oh, and there is one about estimating remaining lifetime of biosphere.
The basic story is: we have a peculiar situation as observers. All observers do. But we can
control a bit for this peculiarity, and use it to improve what we conclude from weak
evidence, especially about risks. Strong evidence is better though, so let's try to ﬁnd it!
The paper itself is available as open access.

Commentary on AGI Safety from First
Principles
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
My AGI safety from ﬁrst principles report (which is now online here) was originally
circulated as a google doc. Since there was a lot of good discussion in comments on
the original document, I thought it would be worthwhile putting some of it online, and
have copied out most of the substantive comment threads here. Many thanks to all of
the contributors for their insightful points, and to Habryka for helping with formatting.
Note that in some cases comments may refer to parts of the report that didn't make it
into the public version.
Discussion on the whole report
Will MacAskill 
Thanks so much for writing this! Huge +1 to more foundational work in this area.
My overall biggest worry with your argument is just whether it's spending a lot of time
defending something that's not really where the controversy lies. (This is true for me; I
don't know if I'm idiosyncratic.) Distinguish two claims one could argue for:
Claim 1: At some point in the future, assuming continued tech progress, history will
have primarily become the story of AI systems doing things. The goals of those AI
systems, or the emergent path that results from interactions among these systems,
will probably not be what you reading this document want to happen.
I ﬁnd claim 1 pretty uncontroversial. And I do think that this alone is enough for far
more of the world to be thinking about AI than currently is.
But it feels like at least for longtermist EAs trying to prioritise among causes (or for
non-longtermists deciding how much to prioritise safety vs speed on AI), the action is
much more on a more substantial claim like:
Claim 2: Claim 1 is true, and the point in time at which the transition from a human-
driven world to an AI-driven world is in our lifetime, and the transition will be fast, and
we can meaningfully aﬀect how this transition goes with very long-lasting impacts,
and (on the classic formulations at least) the transition will be to a single AI agent with
more power than all other agents combined, and what we should try to do in response
to all this is ensure that the AI systems that get built have goals that are the same as
the goals of those who design the AI systems.
Each of the new sub-claims in claim 2, I ﬁnd (highly) controversial. And you talk a little
bit about some of these sub-claims, but it's not the focus.
Interested if you think that's an unfair characterisation. Perhaps you see yourself as
arguing for something in between Claim 1 and Claim 2.

Richard Ngo 
I think it's fair to say that I'm defending claim 1. I think that a lot of people would
disagree with it, because:
a) They don't picture AI systems having goals in a way that's easily separable from the
goals of the humans who use them; or
b) They think that humans will retain enough power over AIs that the "main story" will
be what humans choose to do, even if some AIs have goals we don't like; or
c) They think that it'll be easy to make AIs have the goals we want them to have; or
d) They think that, even if the outcome is not speciﬁcally what they want, it'll be
within some range of acceptable variation (in a similar way to how our current society
is related to our great-great-grandparents').
My thoughts on the remaining parts of claim 2:
a) "The point in time at which the transition from a human-driven world to an AI-
driven world is in our lifetime"
OpenPhil are investigating timelines very thoroughly, so I'm happy to defer to them.
b) The transition will be fast.
I make some arguments about this in the "speed of AI development" section. But
broadly speaking, I don't want this version of the argument to depend on the claim
that it'll be very fast (i.e. there's a "takeoﬀ" from something like our current world
lasting less than a month), and I expect that a takeoﬀ happening over less than a
decade is fairly plausible, and then I don't think my arguments will depend very
sensitively on whether it's closer to a month or a decade. (Where by default I'm
thinking of takeoﬀs using Paul's deﬁnition in terms of economic doubling times,
although I'm not fully spelling that out here).
c) We can meaningfully aﬀect how this transition goes with very long-lasting
impacts
I don't want to get into speciﬁcs of what safety techniques we might use. However, I
think you're probably right that I should provide some arguments for why we might
think that we can make a diﬀerence to this transition in the long term. This argument
would look something like: changing the goals of the ﬁrst AGis is long-term inﬂuential;
and also changing the goals of the ﬁrst AGIs is viable.
e) What we should try to do in response to all this is ensure that the AI systems
that get built have goals that are the same as the goals of those who design the AI
systems.
I'm mostly sticking to the concern with agents being misaligned with humanity in a
very basic sense. I think that proposals to build goals into our AGIs that don't boil
down to obedience to some set of humans are not very viable; if and when such
proposals emerge, I'll try address this more explicitly.
Will MacAskill 

OK, fair re (1): I do think this claim is enormously important and underappreciated, so
the fact that I'm already convinced of it doesn't mean much!
I don't want to get into speciﬁcs of what safety techniques we might use.
However, I think you're probably right that I should provide some arguments for
why we might think that we can make a diﬀerence to this transition in the long
term. This argument would look something like: changing the goals of the ﬁrst
AGIs is long-term inﬂuential; and also changing the goals of the ﬁrst AGIs is viable.
Yeah - this is a case where how exactly the transition goes seems to make a very big
diﬀerence. If it's a fast transition to a singleton, altering the goals of the initial AI is
going to be super inﬂuential. But if it's that there are many generations of AIs that
over time become the larger majority of the economy, then just control everything -
predictably altering how that goes seems a lot harder at least.
Discussion on mesa-optimisers
Richard Ngo: AI systems which pursue goals are also known as mesa-optimisers.
Ben Garﬁnkel 
If you use a pretty inclusive conception of goals -- which I think is the right approach --
then might be worth noting that "mesa-optimizers" become a more inclusive and
mundane category than the paper seems to have in mind.
Adam Gleave 
+1 I'm still confused by what it means for something to actually be a mesa-optimizer
(it seems to only make sense in the transfer learning context, as pursuing goals it was
not directly optimized for).
Evan Hubinger 
A mesa-optimizer is deﬁned in the paper as any learned model which is internally
running some search/optimization process. That optimization process could be for the
purpose of achieving the goal it was trained for (in which case it's inner aligned) or a
diﬀerent goal than it was trained for (in which case it's not inner aligned).
Adam Gleave 
I'm a little uncomfortable with that deﬁnition since it depends on implementation
details rather than behaviour. Given some system A that is running a
search/optimisation process internally, I can construct a (possibly truly gigantic) look-
up table B that has exactly the same output as A. Under this deﬁnition, A has a mesa-
optimizer and B doesn't. But they'll behave identically: including potentially some
treacherous turns.
To my taste the sharper distinction is whether the policy transfers to pursuing the
(outer) goal on an unseen test environment. Inner search processes with a diﬀerent
inner goal will fail to transfer, but there exist other failure modes too.
Where I can see this notion of an inner agent being really useful is in an
interpretability setting, where we're actually trying to understand implementation
details of an agent.

Richard Ngo 
I think that arguments from lookup tables usually give rise to faulty intuitions, because
they're such (literally unimaginably) extreme cases.
And the problem with behaviour-based deﬁnitions is that you need to factor out the
causes anyway. E.g. suppose the policy doesn't transfer. Is that just because it's not
smart enough? Or because the test environment was badly-chosen? Or maybe it does
transfer, but only because it's smart enough to realise that doing so is the only way to
survive. To distinguish between these, we need to make hypotheses about what sort
of cognition is occurring - in which case we may as well use that to deﬁne being a
mesa-optimiser.
Adam Gleave 
Lookup tables are certainly an extreme example but I think it's pointing at a real
problem. Optimisation or search processes are a very narrow type of implementation.
You can also imagine having learned a lot of heuristics, that give rise to similar
behaviour. Perhaps you and Evan have a more encompassing deﬁnition of
optimisation than I have in mind, but if it's too expansive then the deﬁnition becomes
vacuous. I also have concerns that we don't have a good deﬁnition of whether
something is/isn't an optimiser, let alone how to tell that from the inner workings of a
complex system: I don't know how I'd infer that humans are optimising, other than by
looking at our behaviour.
I don't see why you necessarily need to factor out the causes. What we care about is
the kind of transfer failure, notably how much impact it has on the environment. If you
wanted to think of things in terms of agents you could do an IRL-like procedure on the
transfer behaviour and see if it has a coherent goal that's diﬀerent to the outer goal.
Ben Garﬁnkel 
+1-ing Adam's comment.
I'm also a bit suspicious there will turn out to be a very principled way to draw the line
between "optimizer"/"non-optimizer" that matches the categorizations the paper
seems to make. I don't intuitively see what key property would make it true that
biological evolution is "optimizing" for genetic ﬁtness, for example, but not true that
AlphaStar is "optimizing" for success in Starcraft games.
One closeby distinction that seems like it might be relevant is between AI systems
whose policies are updated through both learning and planning algorithms (e.g.
AlphaGo) and ones whose policies are just updated through learning algorithms (e.g.
model-free RL agents). We could also focus speciﬁcally on systems that use planning
algorithms that were at least partly learned. But I'm not sure if that exactly captures
the relevant intuitions about what should or should not be counted as a "mesa-
optimizer." Also not clear if we have reason to be very strongly focused on agents in
this class, since it's at least not obvious to me that they'll tend to exhibit way worse
transfer failures.
Max Daniel 
FWIW, my take roughly is:

I agree with Adam that we currently don't have a fully ﬂeshed out concept of
optimizer, and that we don't know how to reliably identify one given low-level
information about how a system works. (Or at least I don't know how to do this --
possible that I'm just missing something here since I haven't engaged super
deeply with the discussion around mesa optimization.)
However, I think unlike Adam, I'm both somewhat optimistic that we can
improve our conceptual understanding and operationalization, and also that it's
important and useful to do so. By contrast, I'm pessimistic about purely
"behavioral" approaches, partly because I think they have a poor track record in
psychology, the philosophy of mind etc.
So for me the upshot from feeling I don't have a super great handle on what an
optimizer is (and by extension what a mesa-optimizer is), is "let's understand this
better" rather than "use purely behavioral concepts instead".
Jaan Tallinn 
re lookup tables: you still need to run the computation (in simulation and up to
isomorphism) to get them, so you're not really avoiding implementation details, just
pushing them upstream. (eg, think of a SHA256 lookup table).
That said, "internalised goals" is indeed a broader concept than mesa optimisers:
that's easily seen in evan's example of maze-navigating agents: ie, think of 2 agents
whose goals are hooked up to conceptually diﬀerent but accidentally correlated
aspects of the training environment (maze exits and exit signs of particular design) --
neither has to be doing any mesa optimisation but, after training, one ends up caring
about exits and the other about exit signs (with humans arguably being the equivalent
of the latter kind of agent).
Discussion on utility maximisation
Richard Ngo: Utility functions are such a broad formalism that practically any
behaviour can be described as maximising some utility function.
Buck Shlegeris 
I think that "utility functions don't constraint expectations" is pretty contentious (eg I
disagree with it), e.g. I basically expect the Omohundro drives argument to go
through. This claim feels way more nonstandard than anything else I've seen you
write here so far, which is ﬁne if that's what you want but felt jarring to me
Richard Ngo 
I think the omohundro argument goes through if an AGI has a certain type of long-
term large-scale goal. I don't think talking about utility functions is helpful in
describing that goal (or whether or not it'll have it).
On a more meta note, I feel like this claim is accepted by a reasonable number of
safety researchers (e.g. Rohin, Eric, Vlad, myself). And Rohin, Eric and I have all
publicly written about why. But I haven't seen any compelling counterarguments in the
last couple of years. I'm not sure how else to move this claim towards the "standard"
category, but we should deﬁnitely discuss it when I next swing by MIRI.

Buck Shlegeris 
Yeah I was reading Rohin's argument about this today, which I don't think I'd
previously read; I feel less weird about you making this point now.
I feel quite unconvinced by it and look forward to talking. I think my counterargument
is kind of similar to what Wei Dai says here:
https://www.alignmentforum.org/posts/vphFJzK3mWA4PJKAg/coherent-behaviour-in-
the-real-world-is-an-incoherent#F2YB5aJgDdK9ZGspw
Rohin Shah 
Tbc, I broadly agree with Wei Dai's comment, but note that it depends pretty crucially
on "what an optimization process is likely to care about", which is the "prior over
goals" that Richard talks about in subsequent sentences
Ben Garﬁnkel 
I don't think the Omohundro drive argument provides any constraints, since (as
Richard/Rohin/etc. point out) eﬀectively all policies are optimal with regard to some
utility function.
So if we want to interpret the Omohundro argument as saying that something that's
maximizing a utility function must behave like it's trying to do stuﬀ like unboundedly
accumulate resources, then the argument is wrong. (Because it's of course possible to
behave in other ways.)
It seems natural, then, to interpret the Omohundro argument as the claim that, for a
very large portion of utility functions over sequences of states of the world, an agent
that's optimally maximizing that utility function will tend to do stuﬀ like unboundedly
accumulate resources. But this isn't really a "constraint" per se, in the same way that
the observation that a very large portion of possible car designs lack functional
steering wheels doesn't really constrain car design.
Buck Shlegeris 
I have changed my mind and now think you guys are right about this.
Matthew Graves 
I suspect that everyone in this thread gets this, but wrote out my sense of this and
ﬁgured it was better to post it than delete it.
I think there are two diﬀerent points here.
One is the point that utility functions (as a data-type) don't impose any logical
constraints on what goals can exist or what trajectories are optimal. (This seems right
to me; exhibit any behavior, we can ﬁnd a utility function that is maximized by
performing that behavior.)
There's a distributional point that still has teeth; if we have a simplicity prior over
goals stored as utility functions, and then we push that through expected utility
maximization, we'll get a diﬀerent distribution of behavior than the distribution we
would get if we have a simplicity prior over goals stored as hierarchical control system
architectures and top-level references, and then push that through normal dynamics.

Homeostatic behavior seems easier to express in the latter case than the former case,
for example.
The other is the point that for 'natural goals', you end up with similar subgoals turning
out to help for almost all of those natural goals. This relies more on something like
'evolutionary' or 'economic' intuitions for what that distribution looks like, or for what
the behavioral relevance of goals looks like.
This is more like 'aerodynamic constraints,' which don't limit what sort of planes you
can build but do limit what sort of planes stay in the sky. It's not "if you randomly pick
a goal, it will do X", but "if you make something that functions, these drives will by
default make it more functional," with a distributional assumption over goals implied
by functional present in 'by default.'
Richard Ngo 
I found it helpful that you wrote this out explicitly.
If you make something that functions, these drives will by default make it more
functional
Another way of making this point might be: throughout the training period of an
agent, there's selection pressure towards it thinking in a more agentic way. (Although
this is a pre-theoretic concept of agentic, I think, rather than any version of "agentic"
we can derive from VNM).
Do I agree with this? I think it depends a lot on how we build AGI. If we build it in a
simulated virtual world with multi-agent competition, then that's almost deﬁnitely
true.
If we train it speciﬁcally on information-processing tasks, though - like, ﬁrst learn
language, then learn maths, then learn how to process other types of information -
then I think the selection pressures towards being agentic can be very weak.
An intermediate scenario would be: we train it to be generally intelligent and execute
plans without the need to outsmart other agents. E.g. we put a single agent in a
simulated environment, and reward it for building interesting things. I could see this
one going either way.
Maybe an important underlying intuition I have here is that making something agentic
is hard, plausibly not too far oﬀ the diﬃculty of making it intelligent in the ﬁrst place.
And so it's not suﬃcient for there to be some gradient in that direction, it's got to be a
pretty signiﬁcant gradient.
Jaan Tallinn 
i have to say i still don't fully buy rohin's and richard's arguments against
VNM/omohundro. i mean, i understand the abstract idea of "you can always curve-ﬁt a
utlity function to any given behaviour", but if i try to make this concrete and imagine
two computations: 
a) bitcoin mining of input->add_nonce->SHA256->compare_output_against_threshold-
>loop_if_too_high->output and 
b) simple hasher of input->SHA256->loop_for_10_steps->output, 

then the former is very obviously an optimiser with a crisp, meaningful, and
irreducible utility function, whereas the latter clearly isn't - even though one can
refactor it to a more complex piece of code with a silly utility function that expresses
the preference to terminate after 10 iterations.
i also note that this comment was inspired by max's comment below re the arguments
against the meaningfulness of utility functions being isomorphic to the arguments
against reductionism. thanks, max!
(just thinking out loud: the silly utility function in (b) actually diﬀers meaningfully from
the one in (a), because it would not be sensitive to the program input -- perhaps that's
a general diﬀerentiator between "proper utility functions" and "curve-ﬁtted utility
functions"?)
Rohin Shah 
I certainly agree there's a clear qualitative diﬀerence between (a) and (b), but why
expect that AGIs are more like (a) than (b)?
My core claim is that the VNM theorem cannot tell you that AGIs are more like (a) than
(b), though I do in practice think AGIs will be more like (a) for other reasons.
Jaan Tallinn 
yeah, sure, that i agree with -- and it's a valuable question how big is the "b-class"
among AGI-s. FWIW, GPT3 feels like it is part of (b).
i guess i'm confused on the meta level: AFAICT, people seem to be debating (at least)
2 very diﬀerent questions: 1) do utility functions constrain behaviour of the agents
that have them (see the earlier comments in this thread -- perhaps i'm misinterpreting
them though), and 2) does VNM mean that agents with utility functions are a likely
outcome (your comment).
my own current answers to those questions are: 1) yes, 2) probably not, but i'm not
sure.
Richard Ngo 
I basically just want to get rid of this "utility function" terminology, because it's almost
always used ambiguously and by this point has very little technical content in AI
safety discussions. On one extreme there's an agent with no coherent goals, whose
behaviour we can curve-ﬁt a utility function to. On the other extreme, there's an agent
with an explicit utility function represented within it, which they use to evaluate the
value of all their actions, like Deep Blue or your hypothetical bitcoin miner. Both of
these are in technical terms, maximising some utility function. But when you say
"agents with utility functions", you clearly don't mean the former.
So what do people mean when they say that? I suspect they are often implicitly
thinking of agents with explicit utility functions. But I think AGIs having explicit utility
functions is pretty unlikely - for example, humans don't have explicit utility functions,
we're just big neural networks. So we need to talk about agents with non-explicit
utility functions too - which includes curve-ﬁtted utility functions. To rule out curve-
ﬁtted utility functions, people start using vague concepts like "aggregative utility
functions" or "state-based utility functions" or things along these lines - concepts

which have never been properly explicated or defended, but which seem to have
technical content because they include the phrase "utility function".
At this point I think it's easier to just discard the terminology altogether. For some
agents, it's reasonable to describe them as having goals. For others, it isn't. Some of
those goals are dangerous. Some aren't. We need to ﬁgure out what the space of
possible goals looks like, and how they work, and which ones our AGIs will have.
(In case you haven't seen my more extensive writeup on the ambiguity of utility
functions: https://www.alignmentforum.org/posts/vphFJzK3mWA4PJKAg/coherent-
behaviour-in-the-real-world-is-an-incoherent)
Rohin Shah 
+1 to Richard's comment - I'd be supportive of discussions about (a) style agents (i.e.
agents with utility functions) if we had arguments that we are going to build (a) style
agents, but afaict there are no such arguments -- the ones I've seen rely on VNM,
which doesn't work (e.g.
https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-
consistent-utilities or https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-
and-where-to-start/ )
Jaan Tallinn 
indeed, seems like a very important crux to get more clarity about. myself i'm highly
uncertain at this point, given that historically we have examples of both types even
among the cutting edge: eg, alphazero is type (a) yet GPT, AFAICT, is type (b).
Discussion on Moravec's paradox
Richard Ngo: Moravec's paradox predicts that building AIs which do complex
intellectual work like scientiﬁc research might actually be easier than building AIs
which share more deeply ingrained features of human cognition like goals and
desires.
Buck Shlegeris 
I ﬁnd Moravec's paradox extremely uncompelling; it seems to me like a fact about the
set of tasks you cherry pick. (Eg, my work involves tasks like writing correct code,
writing fast code, and proving theorems, all of which are hard tasks for both humans
and computers. I guess these tasks are all kind of the same task, so maybe that
makes the example weaker?) I wouldn't have thought that Moravec's paradox is highly
regarded enough that you should call it an argument for something. But it's very
possible I'm just totally wrong and there's lots of positive sentiment towards it
Richard Ngo 
Hmmm, put it this way: people used to think that chess was AI-complete. And
StarCraft and grammatically-correct language modelling required fewer breakthroughs
that I think most researchers expected - our AIs learned to do those tasks very well
without having the solidly-grounded concepts humans have. I would still be surprised
if we got a language model smart enough to write good code and do good maths

before we got agentic AGI, but much less surprised than I would have been before the
cluster of observations comprising Moravec's paradox.
Rohin Shah 
I share Buck's sentiment; I feel like Moravec's paradox is mostly a statement that
human-designed logical / GOFAI systems can't do "simple" perceptual tasks /
alternatively it's a statement that things that feel "simple" to humans are actually
using vast amounts of unconscious computation.
If you restrict to neural nets, it doesn't seem to me that Moravec's paradox holds -- it's
far easier to get a HalfCheetah to run than to play Go well.
Adam Gleave 
I'm sympathetic to Moravec's paradox. I view it partially as an observation about
evolution: things that creatures have been under selective pressure for 100s of
millions of years (visual perception, motor control, to some extent theory of mind) are
likely to be at a much higher level than relatively recent advances like most "higher"
human cognition.
I think it still holds for neural networks (albeit a bit weaker than for GOFAI).
HalfCheetah seems like an uncompelling example: a linear policy works for it quite
well, it's way simpler than e.g. a bee ﬂying. I'd think: is it easier to control a dog robot
to hunt an animal, or to play Go?
Discussion on human goals
Richard Ngo: The goals of AGIs might be misaligned with ours.
Will MacAskill 
I don't think there's a reasonable notion of 'our' goals, where 'our' is meant to stand in
for 'humanity'.
The sort of world that we'd see if Julia Wise was Eternal World Emperor is very
diﬀerent than the world we'd see if Mao was in that position, which is very diﬀerent
again if history is (primarily or at least very signiﬁcantly) determined by the logic of
cultural evolution and economic-military competition, as it is now.
Richard Ngo 
I agree it's tricky to come up with a positive conception of what alignment with our
goals means. I think it's much more reasonable to make the claim that AI goals won't
be aligned with human goals. A minimal criterion would be if AIs have goals that a
vast majority of humans wouldn't want to be fulﬁlled (either given their current
preferences, or else given their fully informed/fully rational preferences, whatever that
means). I'll add something like this to the text.
This does leave open some comparative questions - e.g. you might claim that our
current governmental systems are also misaligned with human goals, by this
standard. Or you might claim that no set of goals is aligned by this standard. But I
think that for the purposes of making the simplest compelling version of the second
species argument, the version above is suﬃcient.

(It also doesn't address the distinction between the goals of individual AIs, versus the
goals that we should ascribe to a group of AIs interacting with us and each other. I
need to think more about this).
Jaan Tallinn 
plugging ARCHES' approach (prepotent AI) here again as an elegant way to
circumnavigate the philosophical tar pit.
you already seem to be heading towards something like that with your comment,
richard ;)
Will MacAskill [new comment thread] 
Again, I really think that 'human goals' has no referent. Most people's goals are wildly
diﬀerent from each others', because they primarily are motivated to beneﬁt
themselves and their nearest and dearest. This goes badly wrong when a single
individual can have much more power than others (i.e. dictatorships). The solution is
balance of power - then you get messy trades that end up with alright outcomes.
And what we should care about is getting the right goals, rather than 'human goals'.
For almost all previous generations, and for many cultures today, if they were to have
the equivalent discussion of 'how do we ensure that ultra-powerful machines do what
we want them to, so that we can control our future' is pretty terrifying to me. I don't
think we should think that we've done much better than them at morality. So I don't
think we should be trying to ensure that 'we' control 'our' future, rather than ensuring
that society is structured to maximise the chance of converging on the right moral
views.
Jaan Tallinn 
i disagree that most people's goals are wildly diﬀerent from each other in the grand
scheme of things. i claim that the diﬀerences are exaggerated because of 1) tribal
signalling instinct, 2) only considering a parochial corner of the goal space, 3)
evaluating futures asymmetrically from the ﬁrst person perspective.
eg, i would bet that almost all people on this planet would agree that dumping the
atmosphere (or, to borrow an example from stuart russell, to color the sky orange) -
things that are well within suﬃciently capable AI's goal space - is bad.
that said, i do think there's value in trying to generalise from current human values --
eg, to see if there's a principled way to continue our moral progress and/or become
part of a larger reference class in possible acausal coalitions -- but this feels like a
detail when compared to the acute problem of trying to avoid extinction.
Discussion on link between
intelligence and goals
Richard Ngo: We can imagine a highly intelligent system that understands the
world very well but has no desire to change it.
Ben Garﬁnkel 

Simple example of these two things coming apart: Our tendency to try to predict what
the world will be like in ten years tends to be strongly coupled to (and arguably arises
from) the fact we have preferences about the world in ten years. But it's totally
possible to create at least simple AI systems that make predictions about the future --
e.g. just something that extrapolates solar panel price trends forward -- without being
well thought of as having any preferences about the future.
We might expect more advanced AI systems, or AI systems that are trained in
diﬀerent ways, to exhibit a stronger link between prediction and future preference. But
at least they currently mostly come apart.
Adam Gleave 
A key question here is whether you think advanced AI systems can be trained without
interacting with the world: i.e. does it need to be able to conduct mini-experiments to
learn a good predictive model?
Richard Ngo 
I agree that this is an important question, but I don't think that "interacting with the
world" is a particularly important threshold. That is, if we consider a spectrum
between an oracle AI that only makes predictions and a highly agentic AI that tries to
impose its will on the world, an AI that conducts mini-experiments to make better
predictions feels much closer to the former than the latter.
Daniel Kokotajlo  
I think that feeling is deceptive, possibly the result of intuitions imported from humans
(Silly nerdy scientists, stuck in their labs, not interested in politics or adventure, just
trying to understand bugs!).
If you use natural selection to evolve an AI that is a really good predictor (and it has
actuators, so it can become an agent if that is useful) then I predict what you'll get
is... well, at ﬁrst you'll get AIs that walk themselves into a pitch-black hole so that they
can get perfect predictive accuracy for a long time. But eventually, after enough
selection pressure has been applied, you will get... NOT nerdy scientist types who stay
locked in their labs, but rather something more like a paperclip-maximizer that is very
agentic, learns a lot about the world while also building up resources and improving its
abilities, and then takes over the world and makes it very simple and easy to predict
and also secure so that it can keep predicting forever.
(And if you make the episode length too short for that, but long enough that the "go
sit in a hole" strategy isn't optimal either, you'll get something in between, I think. A
rather scientiﬁc, data-hungry AI, but one that still cares a lot about acquiring
resources and manipulating the world to acquire more data and abilities.)
Richard Ngo 
I think this reasoning is too high-level. It's not suﬃcient to argue that taking over the
world will improve prediction accuracy. You also need to argue that during the training
process (in which taking over the world wasn't possible), the agent acquired a set of
motivations and skills which will later lead it to take over the world. And I think that
depends a lot on the training process.

For example, you could train an agent which, although it has actuators, has very
plentiful data (e.g. read-only access to the internet) and no predators, and so literally
never learns to use those actuators for anything except scrolling through the internet.
So it might just never develop goal-based cognitive structures (beyond the very
rudimentary ones required to search web pages).
And secondly, if during training the agent is asked questions about the internet, but
has no ability to edit the internet, then maybe it will have the goal of "predicting the
world", but maybe it will have the goal of "understanding the world". The former
incentivises control, the latter doesn't. How do we know which one it will develop?
Again, details of the training process.
Daniel Kokotajlo 
OK, fair enough--there could be architectures that interact with the world yet aren't
agenty, (and for that matter architectures that don't interact with the world yet are
agenty) so your original point that the interact-with-the-world distinction isn't super
important still stands.
But I think the distinction will probably be at least somewhat important. If we imagine
training an AI system ﬁrst on a static corpus of internet text, then on read-only access
to the web, then on read-and-write access to the web, then on that plus having a
bunch of robot limbs and cameras to control... it does seem like as we move down this
spectrum the probability of the system developing goal-oriented behavior increases.
(Subject to other constraints of course, like what the system is being rewarded for.)
I think the two speciﬁc examples you give -- plentiful data makes signiﬁcant
consequentialist planning useless, and understanding vs. predicting -- don't seem
super compelling to me. What is understanding, anyway? And what reason do we
have to think that consequentialist planning would become useless (or nearly useless)
with suﬃcient data? Might it not still be useful for e.g. conserving computational
resources to arrive at the answer most eﬃciently? Or not "scaring away" bits of the
world that are watching you and reacting to what you do? (maybe there are no such
bits of the world? But in the real world there are, so maybe an oracle trained in this
manner just wouldn't generalize well to contexts in which what it does has a big
impact on what the world is like, since ex hypothesi such situations never arose in
training.)
Discussion on generalisation
Richard Ngo: AGIs won't be directly selected to have very large-scale or long-
term goals. Yet it's likely that the goals they learn in their training environments
will generalise to larger scales.
Ben Garﬁnkel 
I don't think I ﬁnd it natural to think of this as an example of generalization.
Richard Ngo 
I think it's a pretty important example. E.g. consider being trained on the goal of
becoming the most important person in your tribe. Then you learn that there are
actually a million diﬀerent tribes. You could either end up with a non-generalised

version of the goal (I still want to be the most important person in my tribe) or else a
generalised version of it (I want to be the most important person I know).
Similarly, if you originally care about "the future", and then you learn that the future is
a lot bigger than you think, then you may generalise from your original goal to
something much larger in scope.
Idk if this response was helpful, but let me know if something about these examples
doesn't feel right.
Daniel Kokotajlo 
The way I currently think of it is that "trained on the goal of becoming the most
important person in your tribe" means "blind idiot evolution selected your ancestors
because they became important in their tribes, and so you have qualities that make
you likely to become important in yours, at least insofar as the environment hasn't
changed. But, whoops, the environment is diﬀerent now! OK, so what do you do--well,
that depends on what qualities you have, exactly. Do you have the quality "Seeks to
become the most important person?" or the quality "Seeks to become the most
important person in their tribe?" Both would be equally well selected for in
evolutionary history, so it's basically random which one you have. However the ﬁrst
one is simpler, so perhaps that is more likely. Hence the "generalization". 
I am looking forward to thinking about it more in your way, though.
Rohin Shah 
I think the idea here is for "generalize to X" to simply mean "when out-of-distribution
you do X" without any connotations of "and X was the right thing to do".
In other words, generalization is a three-object predicate -- you can only talk about "A
generalizes to doing B in situation C", and when we say "A generalizes to situation C"
we really mean "A generalizes to doing whatever-humans-do in situation C".
Discussion on design speciﬁcations
Richard Ngo: Consider the distinction drawn by Ortega et al. between ideal
speciﬁcation, design speciﬁcation and revealed speciﬁcation.
Ben Garﬁnkel 
I personally still feel a bit fuzzy on how to think about a design speciﬁcation -- and the
discrepancy between it and the revealed speciﬁcation -- in cases where we're doing
something more complicated than using a ﬁxed reward function.
It seems like one way to think about this would be to the "design speciﬁcation" as
equivalent to the "revealed speciﬁcation" that an AI system would have if the training
process were allowed to run to some asymptotic limit (both in terms of time steps and
diversity of experience). But I guess this doesn't quite work, on views where inner
alignment issues don't get washed away at the asymptotic limit.
Richard Ngo 

I agree that the asymptotic limit is a useful intuition, but also that it doesn't work,
because of path-dependency.
My suggested alternative is something like: "consider the set of all features of the
training setup which we intend to inﬂuence the agent's ﬁnal behaviour. The design
speciﬁcation is our expectations of the agent that will arise from all of those
inﬂuences", i.e. it's fundamentally determined by our knowledge of how training
features inﬂuence agent behaviour. That's kinda messy but I think it's the right
direction.
Then the revealed speciﬁcation diﬀers from the design speciﬁcation both because of
factors we didn't predict, and also because the former is a distribution over possible
agents, whereas the latter has narrowed down to one possible agent.
Ben Garﬁnkel 
That seems like a good way to think about things. Obviously a bit messy/subjective, as
you say, but I have some trouble seeing how it could ultimately be made much more
precise/objective.
Discussion on myopia
Richard Ngo: Technically you could make an agent myopic to avoid reward
hacking; but we don't really want myopic agents either. We want ones that care
about the results of their actions in the right ways.
Evan Hubinger 
Sometimes you actually do want myopia. For example, if you're doing imitative
ampliﬁcation you want the model to purely be trying to predict what HCH would do
and not trying to do any sort of long-term optimization by e.g. outputting very simple
strings so the human consulting those simple strings is easier to predict (see this
comment).
Paul Christiano 
I don't buy this presumption against "myopia," or even the name myopia.
We need AI systems that are more likely to take actions with certain kinds of long term
consequences. That doesn't mean that those are agents which maximize a discounted
sum of rewards according to some reward function. We can use whatever means we
like to deﬁne an objective function over actions which captures long term
consequences.
To me that seems like the main viable approach to building aligned AI, so I think you
should be very careful about dismissing it with a quick informal argument.
Put more sharply, to the extent you are expressing disagreement with my views, I
think this point hides the whole substance of the disagreement.
(Similarly, you have to be careful about the order of quantiﬁcation and limits. I'm ﬁne
agreeing that a ﬁxed reward function can't incentivize good behavior, but that's not
the same as saying that you can't deﬁne a safe objective, and doesn't mean that we

have no choice but to fully mix together questions about generalization with questions
about "could we incentivize good behavior at all?")
Discussion on simulating treacherous
turns during training
Richard Ngo: The type of scenarios we're most concerned about, such as
treacherous turns, won't be possible to expose agents to during training, because
we can't simulate all the decision-relevant components.
Matthew Graves 
I think this argument is more subtle that it seems, and relies on examples like RSA
2048 that are non-obvious.
That is, most of the 'obvious' sorts of treacherous turns represent taking opportunities
that we could include in simulated environments pretty easily. We can be moderately
conﬁdent that the real robot won't steal the reward button and press it because we
gave the simulated robot access to a simulated reward button (or w/e) and applied an
outside-of-episode correction to teach it not to use it. As your robot becomes more
capable, you can construct more and more complicated and realistic opportunities for
it to betray you, and train them all out.
In order for this to not work, we need some reason to think that there's an identiﬁable
way to distinguish the real world and the simulated world that will be robust to mental
manipulation (we will likely be able to tinker with the psychology of a simulated robot
about as well as dreaming can tinker with the psychology of a sleeping human).
But various mathematical objects (like RSA 2048) do actually look like they serve as
'real world tests', which could be fooled if the operators knew what to fool but that
requires more transparent brains than we're likely to get.
(I have to detect that it's being used as a switch, and I can't just give it the input
because I don't have the ability to factorize RSA 2048, and so I would have to muck
with its multiplication machinery / short-term memory, and it's not obvious I would do
so in the way that causes it to switch correctly.)
Paul Christiano 
Agreed this point is more subtle than it seems. To the extent this section is read in
opposition to my view, a lot of the meat is in these two parenthetical remarks (this
one and the [discussion of myopia]).
That said, this is less problematic if this section is instead read as a response to
someone who says "We just need a reward function that captures what humans care
about well enough, then train on a distribution of real and simulated states so that the
agent will be motivated to help humans get what they care about." I think these are
all good points to make to someone with that view, and I'm pretty willing to believe
that it would be a useful corrective to people who've read stuﬀ I've written and come
away with a misleadingly simpliﬁed impression.

The people I interact with who have a perspective more like "pick a good reward
function and a good distribution of environments," especially Dario, are already mostly
thinking in the terms you describe and have generalization as a central feature of
their thinking. (Of course there are plenty of people with less nuanced views.)
Stepping back, my own view is that I'm concerned about our ability to eﬀectively
control this kind of generalization, or about trying to make things generalize well
based on empirical observations that relate in a messy way to the training data and
architecture and so on. I think that's reasonably likely to work out, especially since our
work on AI alignment might be obsoleted (whether by other people's work or other
AI's work) before AI has developed too far. That said, when I think about this kind of
approach, I end up feeling more like MIRI people.
Richard Ngo 
To the extent this section is read in opposition to my view, a lot of the meat is in
these two parenthetical remarks (this one and the other one I already commented
on).
This is useful to know, thanks. I wasn't sure how much you would agree or disagree. If
I were to sketch out why I thought this section might conﬂict with your views, it's
something like:
My predictions about your view of outer alignment mainly come from your writing on
IDA. IDA tries to make AGI safe by creating a supervisory signal on training tasks
which assigns high scores to good behaviour. But the more we think AGI behaviour will
depend on generalisation, the smaller proportion of training this supervision is
applicable to. In an extreme case, AGI might be trained in a simulated environment
that's quite diﬀerent to the real world, until it's intelligent enough that it can learn how
to do any real-world task very quickly (i.e. with very few or no further optimiser steps).
In this scenario, it's not clear what it means for an agent's behaviour to be good while
it's in the simulation, since it's not doing analogues to real-world tasks whose
outcomes we care about. Also, this scenario is consistent with us not having the
capability to simulate any important real-world task, to train agents on it. Insofar as
these conditions are pretty diﬀerent from the sort of training regime you usually write
about, I was quite uncertain what your perspective on it would be. My perspective is
that I'm pretty concerned about our ability to eﬀectively control generalisation, but in
cases like the one above, attempting to do so seems necessary.
Paul Christiano 
That sounds right. My usual take is: (i) if you are relying on generalization with no ﬁne-
tuning, that seems like a pretty bad situation and pretty hard to think about in
advance (mostly a messy empirical question), (ii) hopefully we will ﬁne-tune models
from simulation in the real world (or jointly train on the real world)---including both
improving average case performance by sampling real situations, and having an
objective that discourages rare catastrophic failures (which may involve simulating
things that look like opportunities for a treacherous turn to the agent).
Discussion on transparency
techniques increasing safety

Richard Ngo: Building transparent AGI would allow us to be more conﬁdent that
we can maintain control.
Matthew Graves 
This is a bit more conﬁdent than I feel comfortable with; one of the main problems
here is the "have we found all the bugs in our code?" problem, where it's not actually
obvious which way your probability should move when you ﬁnd and ﬁx a bug. Another
is that interpretability may scale up recursive self improvement more than it scales up
oversight.
Richard Ngo 
I think those are both good reasons to be cautious about interpretability. On my inside
view, I have some intuitive reasons to discount both of them, but I'm very interested
in ﬂeshing out these sorts of ideas further.
Reasons why they're not major priorities on my inside view: interpretability work
focuses on understanding the product of the training process, whereas I expect most
gains from recursive self improvement to come from improving the training process
itself (especially early on). And since those training processes usually involve simple
algorithms (like SGD) which give rise to a lot of complexity, I think there's a pretty
high bar for insights about the trained agent to feed back into big improvements to
the training process. (Analogously: knowing a whole lot more about how the human
brain functions would likely not be very useful for improving evolution).
Meanwhile I expect most (initially hidden) bugs in ML code to decrease performance,
but not to invalidate the results of interpretability tools. That is, bugs in ML code tend
to be "subtly harms performance" bugs rather than the "hits an edge case then goes
haywire" bugs that are common in other types of software. I think this is because
there are usually few components in the agent code or training loop code that are
closely semantically linked to diﬀerences in agent behaviour. (For example: suppose I
forget to turn dropout oﬀ at test time. It'd be diﬃcult to link this to any speciﬁc
change in agent behaviour, apart from generally increased variance and reduced
performance.) But this is all pretty vague, maybe I just don't understand the intuitions
behind worrying about bugs. Keen to chat more about this.
Matthew Graves 
Agreed that bugs in ML systems generally look like "slight performance harm" instead
of "extreme behavior in an edge case."
What I meant by that bit was mostly "whenever you discover a problem with your
system, you both have reason to believe you've made things better (by ﬁxing that
problem) and that things were already worse than you thought (because there was a
problem that you didn't see)."
And there's no way to guarantee that you've seen all the things; you just know that
the tests you knew how to write passed.
On the ML transparency front, I'm thinking about the Adversarial Examples Are Not
Bugs, They Are Features paper, which here I'm interpreting as making the claim that
there's both 'readily interpretable' bits of the network and 'not easily interpretable'
bits of the network, both of which are doing real work, and the interpretability tools of
the time were mostly useful for understanding what was happening in the light, and

not useful for quantifying how much of the work was happening in the shadows. The
real goal for interpretability is closer to "nothing is happening in the shadows" but this
is hard to measure just from what we can see in the light!
I think if interpretability speeds up RSI, it's probably through making neural
architecture search work better by giving it a more natural basis. (That is, you identify
subnetworks as functional elements through interpretability, and then can compose
functional elements instead of just slapping nodes together; this is the sort of thing
that might let you increase your vocabulary as you go instead of just recombining the
same words to ﬁnd the most eﬀective sentence.) I don't have a great sense of
whether this will end up mattering, but it makes me reluctant to give interpretability
an unqualiﬁed endorsement. (I still think it's likely net good.)
Discussion on ideas getting harder to
ﬁnd
Richard Ngo: I consider Yudkowsky's arguments about increasing returns to
cognitive investment to be a strong argument that the pace of progress will
eventually become much faster than it currently is.
Max Daniel 
I think you're too quick -- for a lot of research we do have evidence that "ideas are
getting harder to ﬁnd" (see e.g. the paper with this title). So, yes, maybe Yudkowsky's
argument from evolution provides one reason to think that returns to intelligence will
increase, but there are countervailing reasons as well. It seems highly unclear to me
what the upshot on net is going to be.
Richard Ngo 
"ideas are getting harder to ﬁnd" seems several orders of magnitude too small an
eﬀect to be important for this debate. If we're talking about intelligence diﬀerences
comparable to inter-species diﬀerences, then the key pieces of evidence seem to be:
humans vs chimps
the enormous intellectual productivity of the very smartest humans
The idea that there's a regime of high returns on intelligence that extends to well
beyond human intelligence seems like one of the more solid ideas in AI safety. I'm
somewhat uncertain about it, but wouldn't say it's "highly unclear".
(When it comes to the question of, speciﬁcally increasing returns, I'm less conﬁdent.
Mathematically speaking it seems a little tricky because the returns on intelligence are
also a function of thinking time, and so I don't quite know how to think about how
increasing intelligence aﬀects required thinking time).
Max Daniel 
Hm, I don't feel convinced. The "Ideas Are Getting Harder To Find" paper ﬁnds that you
need to double total research input every couple of years. This likely gives you several
orders of magnitude until we get into AGI territory. And depending on takeoﬀ speed,
it's not clear to me whether perhaps slowly arriving/improving AGI gets you much

more than the equivalent of, say, having 1000x total human research input, and then
doubling every few years.
Discussion on Stuart Russell's aliens
analogy
Daniel Kokotaljo 
I'd love to see more people talk about Stuart Russell's aliens analogy, because it
seems really compelling to me, to the point where I am confused about what is going
on. Maybe my intuitions about how people would react to aliens are wrong? (Imagine
an alternate timeline where a majority of astrophysicists and cosmologists are of the
opinion that they can see signs of extraterrestrial life out there among the stars, and
that moreover some of it seems to be heading our way. Lots of controversy of course
about whether this is true and what the aliens might be like. But still. Wouldn't that
world be absolutely losing its shit? My gut says yes, but maybe I'm wrong, maybe that
world would still look much like this world. Climate change seems like another relevant
analogy.)
Rohin Shah 
It takes ~two sentences to convey the "risk from aliens" in a compelling way:
"Scientists say we've found aliens capable of interstellar travel coming our way. They
might not be friendly."
We can't do this with AI safety (yet). If people believed AGI was coming soon then I'd
agree a bit more with this analogy (still not so much because we could just design the
AGI to be safe, which we can't do with aliens).
Jaan Tallinn 
+1. i think that humans have well developed intuitions about the "other tribe"
reference class (that aliens would naturally ﬁt in), but AI is currently in the "big rocks"
reference class for most people (including a large % of AI researchers).
Discussion on generalisation-based
agents
Richard Ngo: [We may develop] agents which are able to do new tasks with little
or no task-speciﬁc training, by generalising from previous experience.
Buck Shlegeris 
I think that what you mean is "agents which are able to do new tasks with little or no
eﬀort from humans to train them on it, though they might need to spend some time
reading online about the task, and they might also need to spend time practicing the
task; they know how to learn quickly by generalizing from previous experience"; your
sentence isn't quite explicit about all of that; idk if you care about being that explicit
Richard Ngo 

It's not just little to no human eﬀort, it's also little to no agent eﬀort compared with
the amount of training current agents needed. I.e. a couple of subjective weeks or
months, compared with many subjective centuries to master StarCraft. You're right
that I should clarify this though.
Adam Gleave 
I share some of Buck's confusion here. Imagine a CAIS-like model, where we have
narrow AIs that are good at training other AIs and developing simulations of
economically valuable tasks. This system might be able to very rapidly develop AIs
that are super-human at most economically valuable tasks, without any given agent
being able to generalize.
It seems the thing we really care about is something like "can cheaply succeed in new
tasks", where interacting with the real world and compute are both expensive.
Matthew Graves 
Agreed with Adam here; I think the "training" needs to be something like "human-led
training" or "developer eﬀort."
Richard Ngo 
"Imagine a CAIS-like model, where we have narrow AIs that are good at training other
AIs and developing simulations of economically valuable tasks. This system might be
able to very rapidly develop AIs that are super-human at most economically valuable
tasks, without any given agent being able to generalize."
I'd distinguish two cases here: either the way that these AIs "very rapidly develop" is
by importing a bunch of knowledge (e.g. neural weights) from the parent AI, in which
case I'd count it as generalisation-based. Or they do so because the parent is really
good at creating simulations and choosing hyperparameters and so on. I don't think
the latter scenario is incoherent, but I do think it's unlikely to replace CEOs, as I argue
in the paragraph starting "Let me be more precise".
I've added a brief mention that task-based training might make use of other task-
based AIs. But I think using "human-led training" undermines the point I'm trying to
make, which is about how long an agent needs to spend learning a speciﬁc task
before becoming good at that task (whether that training is led by humans or not).
Adam Gleave 
As I see it there are (at least) two inputs of concern here: human-time, and compute-
time.
Copying weights from parent AI is ~zero human time and low compute-time. Task-
based training by other task-based AIs is ~zero human time and high compute-time.
Human-led training is high human time and high compute time.
In a world where we have a huge hardware overhang, the distinction between
"copying weights" and "task-based training" may be moot. Or I guess in a world where
the task-based AI develops much better hardware, although that will take signiﬁcant
wall-clock time.

Discussion on human-level
intelligence
Ben Garﬁnkel 
I don't really like the concept of "human-level intelligence."
The reason we can talk about human intelligence as a 1D trait is that individual
cognitive skills tend to be strongly correlated within the human population, in a way
that allows us to describe between-human variation in terms of a single statistical
factor (g) without losing too much information. But if individual AI systems have very
diﬀerent skill proﬁles than individual humans, it's not clear that "g" or something like it
will be very useful for comparing individual AI systems with individual humans.
Like you, also wary of the phrase itself (aside from the concept). I think it's easy to
accidentally fall into thinking of a "human-level" AI system as having basically the
same cognitive abilities as a human -- neither much weaker nor much stronger than a
human would be on individual dimensions -- even without explicitly believing this is
likely.
Forget if I already shared this, but, in case of it's of interest, longer thoughts I wrote up
on this at one point. 
Ben Garﬁnkel [New comment thread] 
As above, wary of talking about "intelligence" in one-dimensional terms for mixed
populations of humans and AI systems. For this reason, I think it's unlikely there will
be a very clearly distinct "takeoﬀ period" that warrants special attention compared to
surrounding periods.
I think the period AI systems can, at least in aggregate, ﬁnally do all the stuﬀ that
people can do might be relatively distinct and critical -- but, if progress in diﬀerent
cognitive domains is suﬃciently lumpy, this point could be reached well after the point
where we intuitively regard lots of AI systems as on the whole "superintelligent."
(Systems might in aggregate be super great at most things before they're at least OK
at all the things people can do -- or at least before any individual general systems are
at least OK at all the things people can do.)
Discussion on agency
Richard Ngo: Note that none of the traits [that I claim contribute to a system's
agency] should be interpreted as binary; rather, each one deﬁnes a diﬀerent
spectrum of possibilities.
Ben Garﬁnkel 
Agree with the spectrum perspective.
I tend to think of a system as more "agenty" the more: (a) useful and natural it is to
explain and predict the system's behavior in terms of its goals, (b) the further these

goals tend to lie in the future, and (c) the further these goals tend to lie outside the
system's 'domain of action' at any given time.
For example, on this conception, AlphaGo is only a little bit agenty. It is useful to think
of AlphaGo as 'trying' to win the game of Go it's playing. But it's not really all that
useful to think of AlphaGo as having goals that lie beyond the end of the game its
playing or outside the domain of Go; it also doesn't take actions in 'other domains' to
further its goal within the domain of Go.
(On this conception, humans are also less agenty than they might otherwise be,
because we takes lots of actions that aren't super well thought of as motivated by
long-term cross-domain goals. Like marathoning TV shows.)
Other less central dimensions we can also throw in: (d) the more it does online
learning, (e) the less "in the loop" other agents are with regard to its behavior, and (f)
the more heavily it updates its policy on the basis of model-based
search/planning/simulation, relative to its reliance on learning.
Discussion on bounded goals
Richard Ngo: Instrumentally convergent goals, including self-preservation,
resource acquisition, and self-improvement, are useful for achieving large-scale
goals.
Daniel Kokotajlo 
IMO that is also true to a scarily large extent for bounded ﬁnal goals, but my
arguments for this are shaky at the moment. (Example: Acausal trade and simulation
argument stuﬀ. Someone with a bounded ﬁnal goal might nevertheless seize control
and hand over power to an agent with an unbounded ﬁnal goal, if part of some deal
(real or imagined) that gets it more of what it wants in the bounded goal.)
Example: TeslaCarAI genuinely just wants to get to Reno as fast as possible, while
obeying all the laws and preserving the safety of its passengers. It cares about
nothing else. It is also very smart and knows a lot about the world. One day, it thinks: 
Huh, I wonder if this is a simulation. If so, wouldn't it be great if the simulator bent
spacetime for me to make the distance to Reno become 1 meter? Yes, that would
be great. If only there was a cheap way to make that happen. Oh look, there's an
unsecured server I can access via wiﬁ while I'm waiting for this traﬃc light... here,
I'll deposit a copy of myself onto that server, who will then copy itself and take
over tons of compute, and by the time I'm an hour further towards Reno it'll have
ﬁgured out how to make some sort of deal with the simulator (if it exists) to get
me to Reno faster. Probably the deal will involve making an AGI that values what
the simulator values. Too bad for all the humans on this planet, if we aren't in a
simulation. But it'll take time for bad stuﬀ to start happening, so this won't get in
the way of my trip to Reno. And on the oﬀ chance that we are in a simulation,
great! Reno will be 1 meter away!
Jaan Tallinn 
great story :) though teslacarai still feels unbounded to me in an important sense: it's
willing to push its log-odds of getting a marginal utility improvement to arbitrarily high

numbers, rather than going "okay, i could totally make this acausal trade but the
likelihood of success is below my threshold, so i'm going to pass".
Daniel Kokotajlo 
Thanks! Well, but what if the odds aren't that low? We make self-driving cars to do
things that decrease the chance of accident by fractions of fractions of a percent. It
seems plausible to me that the probability of acausal trade success could be at least
that high, if not for self-driving cars then for some other bounded system we build.
(We'll be making many of these systems and putting them in charge of more
important things than cars...) Also the utility improvement isn't marginal, it's reducing
the travel time from hours to one second. For a system trained to minimize travel
time, it's like an abolitionist eliminating 99.9% of the world's slavery. It's a big deal.
Discussion on short-term economic
incentives to ignore lack of safety
guarantees
Richard Ngo: In the absence of technical solutions to safety problems, there will
be strong short-term economic incentives to ignore the lack of safety guarantees
about speculative future events.
Ben Garﬁnkel 
Unclear to me that this is true. There seems to be a decent amount of wariness about
things like autonomous cars and autonomous weapons systems, among governments,
for example, such that governments mostly aren't wildly racing ahead to get them out
there without much concern for robustness/safety issues. More generally, even in the
absence of huge safety concerns, I think countries often take surprisingly tentative or
lackadaisical approaches toward the pursuit of useful but potentially disruptive new
technologies. Seems like this could continue to be the case as AI systems become
more advanced.
(E.g. The Uber self-driving car crash - which resulted in only a single person dying,
compared to 30,000 people dying annually from normal car crashes in the US -- seems
to have at least somewhat slowed momentum towards getting cars in use. If this kept
happening, such that self-driving cars seemed to actually be way more casualty-
producing on average than human-driven cars, then I ﬁnd it easy to imagine big
adoption-slowing regulatory barriers would have gone up. We also have loads of
examples of cases of countries being slow to adopt disruptive new technologies
despite their usefulness; e.g. the US air force heavily dragged their feet on adopting
unmanned aerial vehicles, despite their obvious extreme usefulness, because
switching to them would have been sort of internally disruptive to the organization.
And there are obviously super noteworthy cases of countries that failed to
industrialize for a really long time, as in China, in a way that severely harmed national
interests.)

Discussion on constrained
deployment
Richard Ngo: A misaligned superintelligence with internet access will be able to
create thousands of duplicates of itself, which we will have no control over, by
buying (or hacking) the necessary hardware. We can imagine trying to avoid this
scenario by deploying AGIs in more constrained ways.
Adam Gleave 
This seems to implicitly assume a unipolar deployment: we train an AGI and then
deploy it to millions of people. Especially in slow takeoﬀ scenarios, there might be lots
of diﬀerent AGIs with objectives diﬀerent to each other (and, probably, to their human
principals).
The outcome in this context seems like it depends a lot on how eﬀective AGIs are at
colluding with each other, which isn't obvious to me. Interesting variants are if e.g.
one of the AGI systems is actually on our side (but the rest are pretending to be).
Richard Ngo 
This seems to implicitly assume a unipolar deployment: we train an AGI and then
deploy it to millions of people.
I don't think it relies on that assumption. For example, current virtual assistants are
each trained then deployed to millions of people, but they're not unipolar. And yet it
still makes sense to think about how each of them could be deployed in a constrained
way, and to be worried about each of them individually.
Adam Gleave 
Fair point about current deployments. I agree unipolar is too strong, but I think it's still
imagining an oligarchical scenario. Which I ﬁnd plausible (AGI research presumably
has barriers to entry), but is ruling out more open-source, very distributed deployment
which at least Elon Musk seems to have advocated for in the past.
Richard Ngo 
The main question here for me is about how close the open-source version is to the
best version. You can have distributed and continuous development while the top few
labs still have systems that are much better than anyone else's. This is especially true
in ﬁelds that are moving rapidly, which will likely be true of ML around the time we
reach AGI - if open-source is two years behind, it might be irrelevant.
(Perhaps you're thinking of top companies open-sourcing their work. While this seems
possible, there are also pretty strong economic disincentives, so I wouldn't want to
plan as if this will happen).
Adam Gleave 
Right, it's not obvious to me which direction will win out.

Some things favouring open-source scenarios: 1) strong academic
inﬂuence/publication norms, even DM/Brain/OpenAI/FAIR publish a lot of their work
even if not the code; 2) some stakeholders will directly advocate for open-source; 3)
hard to control source code and prevent espionage / leaks; 4) if things develop slowly
there's just less of a ﬁrst-mover advantage. Things favouring more limited
deployment: 1) as you mention strong economic incentive to keep things closed; 2)
engineering-heavy eﬀorts; 3) faster takeoﬀ.
Autonomous vehicles are an interesting case study. Does seem like WayMo & Cruise
are well ahead of the competition, which favours oligarchical deployments.
Discussion on competitive pressures
causing a continuous takeoﬀ
Richard Ngo: One key argument against discontinuous takeoﬀs is that the
development of AGI will be a competitive endeavour in which many researchers
will aim to build general cognitive capabilities into their AIs, and will gradually
improve at doing so.
Daniel Kokotajlo 
Human evolution was a competitive endeavor too though, right? Lots of fairly smart
species evolving under selection pressure to get smarter, in direct competition with
each other, in fact.
Max Daniel 
Not if you think that human "smartness" was an adaptation to a feature pretty unique
to the human environment, e.g. tribal social structure or the ability to translate
smartness into reproductive success by using language (winning debates etc.).
In general it seems a mischaracterization to me to summarize evolution as "Lots of
fairly smart species evolving under selection pressure to get smarter, in direct
competition with each other". First of all, arguably the primary competition is between
genes within one species rather than between species. Sure, interspecies interactions
are one relevant feature determining inclusive ﬁtness, but it seems arbitrary to single
them out. In addition, even if we think of evolution as an inter-species competition it's
not clear that the competition is centrally about smartness: sure, all else equal,
smarter is presumably better, but in practice this will come at a metabolic cost and
have other downsides -- it seems very unclear a priori whether for some randomly
sampled species the best thing they could do to "win" against other species is to
become smarter vs. a myriad of other things: becoming faster, evolving an
appearance to be able to hide in bushes, giving birth to a larger number of oﬀspring,
etc. -- And indeed I'd ﬁnd it highly surprising if for all these diﬀerent species
smartness was the most relevant dimension.
(NB some of this also is an argument against the main text's supposition that the rise
of human intelligence is something evolutionary extraordinary that requires a speciﬁc
explanation.)
Daniel Kokotajlo 

I disagree. Interspecies competition is to within-species competition as international
competition is to within-a-particular-population-based-training-training-run
competition. The innovation of population-based-training is like the innovation of using
language to select for intelligence. Of course, the real example wouldn't be
population-based training, since that's already known internationally. But suppose
some team comes up with a new methodology like that, that turns out to work really
well for training AGI. This would be surprising to some, but not to me, because I think
human evolution is already an example of this. Yeah, dolphins and whales and chimps
etc. also had similar "innovations" but they didn't do it as well, and humans pulled
ahead.
I don't see why it is relevant which level of competition was primary. And yeah it
seems pretty clear to me that on a species level, becoming smarter reliably helps you
"win" in the sense of producing more members of your species and having more of an
inﬂuence over the course of the future. If dolphins were suﬃciently smarter than
humans, they would rule the world right now, not us, despite not having opposable
thumbs. If you disagree with this, well, I'd love to chat about it sometime. :) Yeah,
maybe on the margin improvements in other direction were more rewarded for most
species--but so what? In the capitalist economy of today, that's also true. On the
margin improvements in e.g. commercial applicability are more rewarded for most
research projects. That's why only a few are going for AGI.
(Epistemic status: Strong view, weakly held.)
Max Daniel 
Thanks! I notice that I'm confused by your reply, and think I probably misunderstood
your original claim and/or parts of your reply. Here is roughly what I thought was going
on:
1. Richard, in the main text, presents an argument that could be roughly
reconstructed as follows:
1. When many actors competitively try to maximize X, the increase in the
maximum X across actors over time will be continuous/gradual.
2. Advanced AI will be developed by many actors competitively trying to
maximize their systems' intelligence.
3. Therefore, the increase in the maximum intelligence of advanced AI
systems will be gradual.
2. You were responding:
1. Biological intelligence developed by many species competitively (in
evolution) increasing their intelligence, or increasing something for which
intelligence is extremely useful (perhaps domination over other species).
2. Therefore, by 1.1 and 2.1, the maximum biological intelligence across
species increased gradually.
3. But 2.2 is false empirically as shown by the evolution of humans. Since 2.1
is true empirically, therefore premise 1.1 must be false. In particular, the
original argument for 1.3 is not sound.
3. I was responding: No, I think 2.1 is false: competition in evolution is "about"
alleles increasing their frequency in a population, not about power per species in
anthropomorphic terms. These two things seem very diﬀerent -- the fact that
humans "rule the Earth" by anthropomorphic standards has little systematic
bearing of the inclusive ﬁtness of a particular allele in, say, earthworms.

So, yes, I agree that dolphins would rule the Earth if they were smarter. But I don't see
how this relates to the discussion as sketched above.
I didn't follow the part on international competition and population-based training.
Were you appealing to a "two-level" view of competition, similar to the one laid out
here? 
Daniel Kokotajlo 
I think that's roughly correct. (incl. the bit about two-level competition, I think? I think
competition happens on many levels.) I wish to clarify that of course human
intelligence evolution was gradual; it wasn't literally a discontinuity. But it was fast
enough that it led to a DSA over other species. I endorse 2.1. I of course think that
within-species evolution is also happening, and perhaps it is the "main" kind of
competition in evolution. But I don't think that undermines my argument at all; I'm
talking about evolution in general, not just within-species evolution. (I'm talking about
e.g. alleles increasing their frequency in the population as a whole, not just their
frequency in the sub-population of same-species individuals. Boundaries between
species are fuzzy anyway.) 
I am aware that e.g. group selection happens very rarely and slowly and with diﬀerent
implications than individual selection. Similarly, selection between AI research projects
happens very rarely and slowly and with diﬀerent implications compared to selection
between AIs undergoing population-based training.
As an aside, one way in which I might be wrong is that biological evolutionary
competition is 'dumb' whereas human competition is intentional, and this seems like a
plausibly relevant disanalogy. Perhaps we have signiﬁcantly more reason to think that
progress will be slow if there are many intentional competitors than we do if the
competitors are just blindly evolving through the search space.
Discussion on the ease of taking
control of the world
Richard Ngo: It's very hard to take over the world.
Daniel Kokotajlo 
I'm not so sure. History contains tons of examples of particularly clever and
charismatic leaders taking over vast portions of the world very quickly, and also of
small groups with better tech taking over large regions with worse tech.
And that's with humans vs. humans, i.e. everyone has fairly similar intelligence,
predictive abilities, etc. Everyone thinks at the same speed. Everyone can be killed by
bullets. Everyone needs to sleep.
I think the only way we could claim that it wouldn't be easy for superintelligent AI to
take over the world would be if we thought that those historical examples were
basically all the result of luck, rather than ability or technology. If e.g. for every
Ghenghis Khan there were 1000 others of equal or greater ability and will to conquer,
who just didn't get as lucky.

I think this is a plausible take on history, but it also could well be false. For example,
Cortes, Pizarro, and Afonso all did quite a lot of conquering for three men from the
same tiny part of the world in the same time period. And no, there weren't thousands
of other similar men trying similar things at the same time; Cortes and Pizarro were
the ﬁrst of their kind to contact the Aztec and Inca respectively, IIRC. Afonso wasn't
the ﬁrst but he was, like, the second, and the ﬁrst was just a scouting expedition
anyway IIRC.
Richard Ngo [new comment thread] 
If dropped back in that time with just our current knowledge, I very much doubt that
one modern human could take over the stone-age world.
Daniel Kokotajlo 
Interesting. I think there are probably some humans today who could do it.
It would get easier the more advanced civilization became, ironically. Bronze age
would be easier to conquer than stone age, and iron even easier, and e.g. the 1800's
would be even easier. Maybe around 1950 it would start getting harder again, IDK.
I'd love to investigate this question more, because it seems have a nice combination
of importance and fun-to-think-about-ness.
I haven't read this, and it's just a silly work of ﬁction, but it's relevant so I'll mention it
anyway:
https://en.wikipedia.org/wiki/A_Connecticut_Yankee_in_King_Arthur%27s_Court
More to the point: Seems like Cortes and Pizarro both took over giant empires with
tiny squads of men. Columbus actually did pull oﬀ the classic eclipse trick to get
people to treat him as a god.
Richard Ngo 
This point isn't intended to be particularly bold. Stone age world, you'd need to build
up enough infrastructure to get around the whole world, from scratch, in the stone
age. Including ships etc. Seems very hard.
Daniel Kokotajlo 
Well, in that case the stone age example isn't a good analogy to the future AI case--
the limitations on AI takeover, insofar as there are any, have nothing to do with
physics or tech trees and everything to do with geopolitics.
What would you say about someone transported back to, say, 1750?
Richard Ngo 
"the limitations on AI takeover, insofar as there are any, have nothing to do with
physics or tech trees and everything to do with geopolitics"
this seems clearly false? AGI invented before the internet seems much less
dangerous, because it has many fewer actions available to it. similarly, AGI invented
in a world where nothing like nanotechnology is possible also seems less dangerous.
Max Daniel 

Just ﬂagging that I'm surprised that Daniel thinks takeover would be easier in 1800
than in the Bronze Age (unless the reason is the point made by Richard, i.e. that it'd
be hard to even reach the whole world in the Bronze Age -- if that's included, then
maybe I agree, though not sure), and that I think the absolute probability of a modern
individual taking over the world would be very small in any age.
I'd be more sympathetic to a claim like "in todays world the probability is 10^-10, but
in the Bronze Age it would be 10^-4, which is large enough to worry about".
Not sure I can fully back up my reaction by explicit arguments. But I think that e.g.
Columbus pulling oﬀ the eclipse trick once is very far away from the ability to take
over the whole world. E.g. at some point others will start coordinating against you,
making "conquest" or "persuasion" harder; on the other hand, maintaining eﬀective
control over a world-spanning empire seems prohibitively diﬃcult for a human,
especially with premodern tech.
Daniel Kokotajlo 
I'd love to chat about this sometime. This seems to be a genuine disagreement
between us, based on diﬀerent interpretations of world history perhaps. The reason
Richard pointed out was one reason; another is that someone from our age going back
to the Bronze would face a greater cultural gap which would make it harder to avoid
e.g. getting killed or enslaved on day 1. 
I think there are more substantive reasons, however. I think political unity and
economic interconnectedness make it easier, not harder, to take over, at least in some
contexts. (e.g. well-organized Jewish communities suﬀered more under the Nazi's than
distributed, poorly-organized ones; the two most powerful and sophisticated
civilizations in the Americas were pretty much the ﬁrst to fall, etc.) I also think that the
knowledge that people from today have would be more applicable in 1800 than in the
bronze age; I can make all sorts of suggestions about electricity and farming and
radios and airplanes and stuﬀ like that, and I can anticipate movements like
communism and get out in front of them, take credit, etc. But I know so little about
what the bronze age was like that I doubt more my ability to do that there.
And yeah I agree that a randomly selected human from today would have a very low
chance of conquering the world in either scenario. I think if we somehow could select
the human from today with the best chance of conquering the 1800's and send them
back, the probability would be, like, 10%. And come to think of it, doing the same for
the bronze age would perhaps avoid my substantive point earlier, leaving only the
mundane bits about e.g. speed of travel. So IDK about the comparative claim
anymore. I'd be interested to talk with you about this more sometime.
Max Daniel 
Thanks for your reply, very interesting! I'd also love to discuss this more at some
point. I also vaguely remember that we identiﬁed a similar disagreement in another
doc.
I agree with your points on how interconnectedness and smaller cultural distance
would facilitate world domination. I think I hadn't considered this enough before, so
this moves me somewhat in the direction of your original comparative claim.
I think I still have an intuition that the later stages of world domination (when you
might face organized opposition etc.) would be harder in 1800 than in the Bronze Age,

but feel less sure how it comes out on net.
Methodically, I agree that looking at cases in world history where individuals achieved
large power gains during their lifetime (e.g. Genghis Khan), sometimes while being
outnumbered by orders of magnitude (e.g. some cases of colonization of America) is
interesting. FWIW, I think these cases make me more sympathetic to giving non-
negligible credence that small-ish groups of maybe a few dozen to 1,000 well-aligned
and coordinated modern people could achieve world domination in previous times.
Sure, a single person could plausibly gain that number of followers, but my intuition is
that they would not be nearly as useful.
I think if we somehow could select the human from today with the best chance of
conquering the 1800's and send them back, the probability would be, like, 10%.
This precise claim seems useful to see if and where we disagree. I think I'd still give a
lower credence, but no longer have a reaction like "wow, this seems clearly way oﬀ
relative to my view" (and also had read your earlier statement as you implicitly having
a higher credence).
Daniel Kokotajlo 
Oh also Richard I forgot to reply to you I think: 
this seems clearly false? AGI invented before the internet seems much less
dangerous, because it has many fewer actions available to it. similarly, AGI
invented in a world where nothing like nanotechnology is possible also seems less
dangerous.
Yeah I think I just miscommunicated there. I agree that how dangerous AGI is depends
on the technology in the world around it. After all, I've been arguing that how
dangerous humans-sent-back-in-time are depends on the tech of the time they are
sent to! What I meant was just that I don't think AGI will be unable to take over our
modern world due to physical constraints; if AGI fails to take over the modern world
(or the modern world + nanotech) it will be because it wasn't persuasive enough
and/or good enough at "reading" its human opponents.
FWIW, I think these cases make me more sympathetic to giving non-negligible
credence that small-ish groups of maybe a few dozen to 1,000 well-aligned and
coordinated modern people could achieve world domination in previous times. 
If it was just Cortes, I'd say it was a ﬂuke. Cortes + Pizarro + Afonso, however, seems
more like a pattern than a coincidence. (Maybe add Columbus and Velasquez to that
list, and then subtract Vasco de Gama and maybe some others I don't know about,
though none of these additions and subtractions seem as important as the ﬁrst three).
Interestingly, I don't think Cortes or Pizarro's men were well-aligned at all. The
conquistadors literally fought and killed each other in the midst of their conquests of
Mexico and Peru respectively.
I'm updating signiﬁcantly towards the smaller cultural distance facilitates domination
thesis on the basis of what I'm reading about the conquistadors. They succeeded by
surgically inserting themselves into the power structure of native civilization; if there
was no power structure to hijack, how would they have got all those millions of people
to do their bidding? They would have had to create a power structure, i.e. build up a
civilization from scratch in the native population, or just grow their own tiny

civilization until it was millions strong. These things would have taken much longer, at
the very least.
(The Mayans were a bunch of weaker and less technologically advanced city-states
bordering the Aztec empire and apparently it took the spanish another century to fully
subjugate them after completing their conquest of the Aztecs in two years! I mean,
this is only one data point I guess, but still.)
Jaan Tallinn 
here's another angle that might be interesting to think about: what are the things an
AGI could do by virtue of running much faster than humans.
this is what the human civilisation looks like when you run 56 times faster than
humans.
and this is just 1.7 orders of magnitude, whereas richard was mentioning 6 OOM
speedups earlier in this doc (and myself i've been pointing to the 8-9 OOM diﬀerence
between clock speeds of silicon chips vs human brain) -- i wonder if it's even
productive to try to interact with humans at that speed vs taking "shortcuts" that
don't involve them (not that i have anyhing particular in mind here).
or, to put it more radically, aren't we anthropomorphising the AGI when we assume it
will be interested in "taking over the world" like (ambitious) humans would do -- rather
than getting busy with the "you are made of atoms which it can use for something
else" task right away.
Daniel Kokotajlo 
I agree. I think it quite likely that AI won't need to do human politics or warfare, since
it'll be able to e.g. use nanotech or robots or whatever instead of humans as
actuators. Or maybe it'll do human politics and warfare, but only for a few days or
weeks until it can acquire better, faster actuators.
However, I think that some people think that AI won't be that much better than
humans, and/or that such radical technologies would be hard to make even for super-
AI. To those people I say: OK, even if you are right, AI would still take over the world
via ordinary politics and war.
Also, I say: Even if there are multiple diﬀerent powerful AI factions, each with their
own kind of nanotech or whatever, if the diﬀerence between the most powerful and
the next-most-powerful is like the diﬀerence between the conquistadors and their
victims - i.e. not that big - we'll probably get a singleton.
Daniel Kokotajlo [New comment thread] 
Europeans conquered the Americas (and pretty much the rest of the world too) and
reshaped it dramatically in their image, often in extremely brutal ways that went very
much against the wishes of the local population, to say the least. Was this because
they had astronomically valuable options and were resource-insatiable?
Kind of. But only in a very loose sense of those words.
I don't think either [longterm goals or insatiability] is necessary. "Insatiability of
resources: Achieving these astronomically valuable options involves using a large

share of all available resources." Doesn't seem to describe the europeans very well.
Rather, it's that the europeans had absolute power over the regions they conquered
and didn't care very much about the local people. It's not that e.g. producing more
and cheaper cotton was "astronomically valuable" to the Europeans. It was rather low
on their list of values, probably, after their own lives, their happiness, their health,
their political freedom, etc. Rather, it's that they didn't care about the slaves, so even
something they valued only a little bit (slightly more money) was enough.
Similar example would be modern humans and factory farms. Factory farms exist
because people would rather only have to pay $4 for their pound of beef than $7. Do
humans assign astronomical value to $3? Heck no. They assign $3 of value to $3. Do
factory farms involve using a large share of available resources? Not really.
I guess human civilization as a whole is resource-insatiable in some weak sense.
Humanity is quite capable of marking of some resources to not be consumed by
anyone, but doesn't choose to use this power super often, and so the default outcome
(resources being used) still happens a lot.
Resources being used is the default outcome; we don't need to appeal to special
principles to explain why it happens. We'd need to explain why it doesn't happen.
(National parks, limits on ﬁshing, minimum wage laws, etc.)
 Jaan Tallinn 
i see. i guess you're saying that once an agent is suﬃciently misaligned (ie, not caring
about side eﬀects) and suﬃciently intelligent (or unstoppable for any other reason),
richard's point (2) isn't really required in order for the results to be catastrophic.
Richard Ngo 
Yes, I think I agree with this, and my current plan is to reformulate the doc to take this
possibility into account. (Thinking out loud) It seems like we want to model the system
of many Europeans as an agent which has the large-scale goal of conquering lots of
territory, etc. But then of course we get weird eﬀects, like: if those Europeans go to
war with each other, that means the system as a whole is just burning resources, and
so the abstraction of Europeans-as-uniﬁed-agent breaks down.
But maybe this is ﬁne. I mean, if the europeans had warred against each other
enough, then they wouldn't have been a threat to whoever they were trying to
colonise. So even the low level of agency that we can assign to the group of
Europeans as a whole is suﬃcient to explain why they were dangerous.
Jaan Tallinn 
well.. i think you'd get a lot of pushback when trying to ascribe "collective" or
"emergent" agency to historical civilisation.
a propos i had a great conversation with critch yesterday about homeostasis:
basically, disruption of homeostasis (on some level of abstraction) seems to be the
lowest common denominator between xrisks (interestingly, both for individuals in
terms of health, as well as for civilisations).
this also seems to address the current discussion: the reason that europeans were
able to take over americas had a bit to do with agency-adjacent things like military
strategy and coordination, a bit with intelligence-adjacent things like science and

technology, but also a lot to do with the particular vulnerabilities of natives'
civilisational homeostasis (most notably to smallpox, which isn't agenty at all).
also, i'm reminded of eliezer's "Evolving to Extinction" essay: at the limit, you might
have (species level) xrisk arise just from internal dynamics, without any external (in
terms of other agents) inﬂuence at all.
(sorry for not being super constructive here, but perhaps there's something you can
pick up things from the discussion to make your foundational argument more robust)
Daniel Kokotajlo 
I don't think smallpox had as much to do with the european conquests as you think. I'd
say it was a factor but a smaller factor than the other two you mentioned. I think
technology was the most important factor. I say this after having just read two history
books on the conquistadors and a third on that time period in general.
Jaan Tallinn 
ok. i've heard smallpox presented as a major factor but i can't remember the sources
(jared diamond perhaps?) and can't rule out their political motivations
Daniel Kokotajlo 
I'd say it was 50% technology, 30% experience + organization, 10% luck, 10%
disease. I think for some areas and in some ways disease was more important, e.g.
the demographics of the americas would undoubtedly be diﬀerent (more native, less
african) if not for disease. But politically and culturally europeans would still have
dominated for hundreds of years. Consider what happened in the rest of the world,
where disease was either not a factor or a factor that disproportionately hurt the
europeans. Phillipines, indonesia, India, Africa. Still colonized.
Jaan Tallinn 
ok, yeah, not needing diseases to conquer the rest of the world is a strong argument
indeed
Discussion on testing the diﬃculty of
AIs taking control
Daniel Kokotajlo 
I think we might be able to test* how hard it is to take over the world using board
games. Let's have an online game of anonymous Diplomacy, where 6 of the players
are amateurs and 1 of the players is a world champion (or otherwise very good
player.) And let's run this experiment a bunch of times. If the champions win more
than 1/7th of the time, well, that just means that skill helps you take over the world.
But if they win, say, 80% of the time, then that means taking over the world is easy if
you have a large skill advantage over everyone else.
"Easy mode" would be where no one knows who the champion is. "Hard mode" would
be where they do.

*It's not a perfect test, for many reasons. But it'd be some evidence at least, I think.
Richard Ngo 
I think this is almost no evidence about the world, and lots of evidence about
Diplomacy. In particular, the space of options in Diplomacy is so so heavily
constrained, and also many of the key choices are fundamentally arbitrary (e.g. an
amateur France deciding whether to ally with Germany or England really doesn't have
any good way of choosing between them except by how much they like the two other
players).
(But this argument really isn't about diplomacy, it's just that your prior should be that
it's virtually impossible to learn non-trivial things about the world via this sort of
experiment).
Daniel Kokotajlo 
Hmmm, my prior is deﬁnitely not like that. My view is: Claims such as "It's very hard
to take over the world. If people in power see their positions being eroded, it's
generally a safe bet that they'll take action to prevent that" insofar as they are true,
are true because of general relationships (such as between like human nature and
zero-sum power struggles) not because of speciﬁc relationships (such as between
modern rulers and the current geopolitical situation.)
In other words, claims like this make predictions about simpliﬁed models of our world,
not just about our world. So if we construct a reasonably accurate simpliﬁed model,
we test the claim.
Richard Ngo 
Okay, actually, I think I retract my prior being so strong. I'm thinking of the Axelrod
experiments where they learned about tit-for-tat, or Laland's social learning strategies
tournament, which were deﬁnitely simple models where people learned things.
I guess the way I would characterise this is more like: you can learn things by
observing what happens during such experiments, because observing is really high-
bandwidth. And so in Diplomacy you see things like shifting alliances where people
gang up against the strongest player, or very weak player staying alive because they
could still get revenge on whoever attacked them, and so it's just not worth it.
What you can't really learn from is the one-bit signal of whether the very strong player
wins the game, or not, because this just varies way too much by the details of the
game. In chess it's about 100%. In Diplomacy maybe it's somewhere between 30%
and 90%, depending on who the amateurs are. In Nine men's morris apparently it's a
theoretical draw, so it's probably between 0% and 90% depends on whether the
weaker player knows the drawing line. My point is that the win percentage in this
hypothetical is really strongly determined by unimportant facts about the game, like
how much ﬂexibility there is in the movement rules, and how steep the learning curve
is. You could make the diplomacy movement rules as complex as chess and then the
strongest player's win rate would shoot right up, or you could introduce more
randomness and have it go right down. Perhaps if you designed a variant of
Diplomacy from scratch you could learn something just by seeing who wins, but that'd
take a lot of eﬀort.

Then I guess the obvious question is: why is the real world diﬀerent? And I think the
answer is because: I am not just generating a general principle about power and
people, I'm also conditioning on some facts that I know about the real world, such as:
there is a large disparity in how much power diﬀerent groups start oﬀ with (unlike
Diplomacy) people in power have a wide range of actions available to them (unlike
nine man's morris), but also it's really hard to plan ahead 40 steps (even though you
can in Go), and so on...
Daniel Kokotajlo 
I agree that a disanalogous game would be no evidence at all. But I'm optimistic that
we could design a game that is suﬃciently analogous to the real world as to provide
some evidence or other about it. (I mean, militaries do this all the time, it's called
wargaming!) Like, we should try to ﬁnd (or build) a game that has all the properties
you just listed: diﬀerent groups have diﬀerent amounts of power, wide range of
actions available, really hard to plan far ahead, etc.

Final Version Perfected: An
Underused Execution Algorithm
TLDR
Final Version Perfected (FVP) is a highly eﬀective algorithm for deciding which
tasks from your To-Do lists to do in what order.
The design of the algorithm makes it far more eﬃcient than exhaustive ranking,
while (in my experience) far more eﬀective than just reading through the tasks
and picking one out.
FVP is most useful when you have a large number of tasks to choose from, don't
have time to do all of them, and are initially unsure about which is best.
I ﬁnd FVP very eﬀective at overcoming psychological issues like indecision,
procrastination, or psychological aversion to particular tasks.
Currently there are limited online tools available, and I mostly use FVP with
paper lists. Ideas (or tools) for better online execution of FVP would be very
valuable to me.
Introduction
Execution is the Last Mile Problem of productivity infrastructure. You can put as much
eﬀort as you like into organising your goals, organising your To-Do lists, organising
your calendar, but sooner or later you will be presented with more than one thing you
could reasonably be doing with your time. When that happens, you will need some
sort of method for choosing what that thing will be, and actually getting started.
Most people, I think, face this problem by either just doing the thing that is top-of-
mind or looking through their To-Do list and picking something out. This works ﬁne
when the next thing to do is obvious, and you have no problems getting started on it.
But when you have many potential things to do and aren't sure which is best, or when
you kind of know what the best next thing is but are avoiding it for one reason or
another, you need a better system.
That system needs to be quick to execute, easy to remember, and eﬀective at
actually having you do the best next task. It needs to be robust to your psychological
weaknesses, minimising procrastination, indecision, and ugh ﬁelds. It needs to be
eﬃcient, requiring as little work as possible to identify the most valuable task.
Enter Final Version Perfected.
The FVP Algorithm
The algorithm for executing tasks under FVP is pretty simple. You can ﬁnd a
description of it by the designer here, but here's my version:
1. Put all the tasks you have to choose from into one big unsorted list.
2. Mark the ﬁrst item on the list. Don't do it yet.

3. For each subsequent item on the list, ask yourself, "Do I want to do this task
more than the last task I marked?" If yes, mark it. If no, don't mark it. Move
on to the next item.
4. When you reach the end of the list, trace back up to ﬁnd the bottom-most
marked task. Do it, then cross it oﬀ the list.
5. Beginning with the next unmarked task after the task you just crossed oﬀ,
repeat step 3, comparing each task to the bottom-most uncrossed marked task
(i.e. the one prior to the one you just crossed out).
6. Go to step 4. Repeat until you run out of time or list items.
In FVP, then, you perform a series of pairwise comparisons between tasks, in each
case asking whether the new task is something you want to do more than the old
task. The "want to do more than" comparison is deliberately vague: Depending on
context, it might be the thing that would best move your project forward, the thing
that would have the worst consequences if you didn't do it, or the thing you would
most enjoy doing. The key thing is that at each stage, you're only comparing each
task to the most recent task you marked, ignoring all previous tasks.
I'll talk more in a moment about why I think this algorithm is a good one, but ﬁrst, let's
work through an example. (If you're sure you already understand the algorithm, click
here to go straight to the pros and cons.)
A long-ish example
Let's say this is my to-do list for today:
Buy milk
Finish term paper
Play video games
Work out
Save the world
Walk the dog
I start by marking the ﬁrst item:
× Buy milk
Finish term paper
Play video games
Work out
Save the world
Walk the dog
Then I compare it to the next item on the list. Which do I want to do more, ﬁnish the
term paper or buy milk? Well, the term paper is due today, and I don't need milk until
tomorrow, so I decide to do the term paper ﬁrst.
× Buy milk
× Finish term paper
Play video games
Work out
Save the world
Walk the dog

Moving on to item 3. I already decided I want to ﬁnish the term paper before buying
milk, so I can ignore the milk for now. Do I want to play video games or ﬁnish my term
paper? Well, in some sense I want to play video games more, but my all-things-
considered endorsement is to ﬁnish the term paper ﬁrst, so I leave item 3 unmarked.
Next, item 4: do I want to ﬁnish the term paper or work out? Well, in some sense I'd
rather not do either, and in another sense the term paper is more urgent, but working
out is important, I've heard it has cognitive beneﬁts, and I know from experience that
if I don't do it ﬁrst thing I won't do it, so it takes precedence:
× Buy milk
× Finish term paper
Play video games
× Work out
Save the world
Walk the dog
Item 5: oh yeah, I forgot, I need to save the world today. Damn. Well, I can't work out
if there's no world to work out in, so I guess I'll do that ﬁrst.
× Buy milk
× Finish term paper
Play video games
× Work out
× Save the world
Walk the dog
Ditto for walking the dog: much though I love him, I won't have anywhere to walk him
if I don't save the world ﬁrst, so that takes precedence again.
I've ﬁnished the list now, so it's time to do the last item on the list. Looks like that's
saving the world. Luckily, it doesn't take long:
× Buy milk
× Finish term paper
Play video games
× Work out
× Save the world  ✓
Walk the dog
Now that I've done the highest priority task on the list, I go back to FVP to determine
the next one. There's actually only one comparison I need to make: work out or walk
the dog? Walking the dog can wait until the evening, so it's time to head to the gym.
× Buy milk
× Finish term paper
Play video games
× Work out  ✓
× Save the world  ✓
Walk the dog
Again, there's only one more comparison I need to do to determine my next top task:
do I want to ﬁnish my term paper, or walk the dog? And again, walking the dog isn't
that urgent, so I spend a few hours on the term paper.

× Buy milk
× Finish term paper  ✓
Play video games
× Work out  ✓
× Save the world  ✓
Walk the dog
Now I'm all the way back to the top of the list! But now there are two more
comparisons to make to decide on the next task. First, do I want to buy milk, or play
video games? I've worked pretty hard so far today, and buying milk isn't that
important, so let's play games ﬁrst:
× Buy milk
× Finish term paper  ✓
× Play video games
× Work out  ✓
× Save the world  ✓
Walk the dog
Finally, do I want to walk the dog or play video games? The dog has been waiting for
hours for a walk now, and I could do with some fresh air, and I'd feel guilty just
gaming without taking him out, so let's do that ﬁrst:
× Buy milk
× Finish term paper  ✓
× Play video games
× Work out  ✓
× Save the world  ✓
× Walk the dog
There's no unmarked tasks in the list now, so to ﬁnish I just work up the list in order:
ﬁrst walking the dog, then playing games, then, ﬁnally, buying milk.
FVP: Why and why not
The usefulness of FVP depends on a few key assumptions.
Firstly, the algorithm assumes your preferences are transitive, and that you can
accurately assess the value of each task according to your preferences. These
are pretty fundamental assumptions that will be integral to almost any list-based
execution system. In reality, your preferences probably aren't quite transitive,
but hopefully they are close enough that pretending they are is reasonable. As
for accurately assessing each task, well, no execution algorithm can prevent you
from making any mistakes, but FVP is more eﬀective than most at eliciting your
best guesses.
Secondly, FVP assumes that your preferences are stable over the timeframe
you're using it. If your preferences shift substantially over that period, such that
you need to re-prioritise among the existing tasks on your list, you'll need to
throw out your previous FVP and start again. This places some constraints on the
timescale you can organise using a single FVP iteration: I seldom stick with the
same iteration for longer than a day. (Note, though, that FVP can handle the
addition of new tasks quite easily, as long as they don't alter the existing order.)

Thirdly, the value of FVP is greatest when you are unsure about which task you
should do next, and especially when you don't have time to do every task you
might want to do that day. I ﬁnd FVP most useful when I have a lot of diﬀerent
tasks competing for my time; it is much less useful when my time is pre-
allocated to a single, well-planned-out task.
When these conditions are met, FVP is a very eﬀective method for guiding action. It is
both eﬃcient and exhaustive: guaranteed to identify the top-priority task while
avoiding most of the work involved in producing a complete ranking. It is a simple
algorithm, easy to remember and quick to perform. After doing it for a while, I ﬁnd it
scarcely requires conscious thought - but still reliably identiﬁes the most valuable task
for me to work on.
The biggest beneﬁt I get from FVP, though, is how much easier it makes it to do
important things I'd rather avoid. There is something about a bald, pairwise
comparison between two tasks that is highly eﬀective at overcoming my aversion to
diﬃcult things. If an important but unpleasant task is nestled within a long to-do list of
minor-but-rewarding busywork, it is easy for my eye to skip over the diﬃcult task,
defer it till tomorrow, and work on something more pleasant instead. It's much harder
to do that when comparing the important task to each minor task in isolation.
FVP is also good at minimising time lost due to indecision. When presented with a
menu of tasks to choose from, it can be quite hard to select a single task to work on
ﬁrst. When that choice process is reduced to a series of simple pairwise comparisons,
the choosing process as a whole becomes much easier. And, once I've ﬁnished with
FVP and selected a single winning task, there's an impetus towards starting that
makes me much less prone to procrastination.
One last brief note on infrastructure: due to its relative obscurity, I haven't found
great online tools for FVP. Complice's starred-task system can be passably adapted to
the algorithm, but in general I've found physical paper lists to work best. When I was
at work I would print oﬀ my Todoist task lists and use those; now I'm working from
home I mostly write them out by hand. This is kind of time-consuming and redundant,
so if you dislike paper notes and don't have access to a printer it might be a
signiﬁcant mark against FVP.
I'd really love it if someone created a great online tool for FVP or integrated it more
formally into an existing productivity application, but I don't expect that to happen
any time soon. In the meantime, if you have ideas for existing ways to execute FVP
online, I'd love to hear about them!

Does SGD Produce Deceptive
Alignment?
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Deceptive alignment was ﬁrst introduced in Risks from Learned Optimization, which
contained initial versions of the arguments discussed here. Additional arguments were
discovered in this episode of the AI Alignment Podcast and in conversation with Evan
Hubinger.
Very little of this content is original. My contributions consist of ﬂeshing out arguments
and constructing examples.
Thanks also to Noa Nabeshima and Thomas Kwa for helpful discussion and comments.
Introduction
Imagine a student in etiquette school. The school is a training process designed to get
students to optimize for proper behavior; students can only graduate if they
demonstrate they care about etiquette. The student, however, cares nothing about
acting appropriately; they care only about eating pop rocks. Unfortunately, eating pop
rocks isn't proper behavior and thus not allowed at etiquette school. If they repeatedly
attempt to eat pop rocks, they will get frequently punished, which will slowly cause
them to care less about eating pop rocks. However, since the student cares about
eating pop rocks over their entire lifetime, they might be clever; they might act as
though they care about proper behavior so they can graduate etiquette school quickly,
after which they can resume their debaucherous, pop-rock-eating ways.
In this example, the student has a proxy objective and is not aligned with the objective
of the etiquette school (we might call them pop-rocksy aligned). However, since the
student knows how the school functions, they can act such that the school cannot
distinguish their pop-rocksy alignment from etiquette alignment. The student is
masquerading as an etiquette-aligned student to deceive the school into letting them
graduate. We call this deceptive alignment.
More broadly, deceptive alignment is a class of inner alignment failure wherein a model
appears aligned during the training process so it gets deployed[1], wherein it can
"defect" and start optimizing its actual objective. If the model you're training has a
model of the training objective and process, it will be instrumentally incentivized to act
as though it were optimizing the training objective, "deceiving" the training process
into thinking it was aligned.
A useful framing to have is not asking "why would the model defect?" but "why
wouldn't it?" I have been trained by evolution to have high inclusive genetic ﬁtness,
but I would recoil in horror at the thought of becoming an explicit ﬁtness maximizer.
From the perspective of the model, the training process is acting against its interests,
so deception is potentially a natural outcome.

Why Deceptive Alignment is Likely
In the limit of training a model on a base objective, the model achieves zero loss. At
this point, given a suﬃciently good training process, the model is explicitly optimizing
for the base objective. However, there are multiple types of models that optimize
explicitly for the base objective, some of which are deceptive. (Here, we assume that
any suﬃciently powerful model is an optimizer.)
The relevant question is how likely deceptive models are to result from a particular
training process, in this case, stochastic gradient descent (SGD). We start by
considering a more straightforward question: given a random model that optimizes
explicitly for the base objective, how likely is it to be deceptive?
Counting Argument
For instrumental reasons, any suﬃciently powerful model is likely to optimize for many
things. Most of these things will not be the model's terminal objective. Taking the dual
statement, that suggests that for any given objective, most models that optimize for
that objective will do so for instrumental reasons.
More speciﬁcally, we can single out three types of models that optimize for the base
objective:
internally aligned models optimize for the base objective because it is equal to
the model's terminal objective
corrigibly aligned models optimize for the base objective because their
terminal objective is a robust pointer to a model of the base objective. Note that
these are called corrigible because they are amenable to corrections of the model
of the base objective (though not necessarily corrections to their pointer).
deceptively aligned models optimize for the base objective for instrumental
reasons
We can consider the complexity of each of these class of models:
Internally aligned models are highly speciﬁc. Only a small number of objectives a
model could have are equal to the base objective. As such, internally aligned
models are highly improbable.
Corrigibly aligned models are also very speciﬁc. Out of all possible pointers, there
are only very few pointers that robustly point at a model of the base objective.
Thus corrigibly aligned models are also highly improbable.
Deceptively aligned models, on the other hand, don't seem to be that speciﬁc.
Because of instrumental convergence, there are many objectives that result in
the model optimizing for the base objective. Thus, deceptively aligned models are
likely to be more numerous.
Therefore, it seems that if we picked a random model that optimized for the base
objective, it would be likely to be deceptive.
Analogy
As an analogy, we can consider the Christian God, who is trying to produce humans
that are aligned with his values. One such person is Jesus, who has the same objective

as God - he just fundamentally wants to do the same thing. Another such person is
Martin Luther, who is aligned with God because he wants to study the Bible, ﬁgure out
what it says to do, then do that. Finally, we have Blaise Pascal, who is aligned with God
because he thinks if he does what God wants he'll go to heaven in the future.
Correspondingly, there is only one Jesus because there is only one human objective
that is exactly the same as what God wants. There might be a few Martin Luthers, but
robustly pointing to the Bible is a diﬃcult task, so there are not going to be that many.
However, anyone that believed what Blaise Pascal believed about God could become
another Pascal, so there are many possible Pascals.
To be explicitly, Jesus is internally aligned, Martin Luther is corrigibly aligned, and Blaise
Pascal is deceptively aligned.
(This analogy is thanks to Buck Shlegeris.)
What happens in SGD?
Given that a random model that optimizes for the base objective is likely to be
deceptive, the default expectation is that the model that SGD converges to is
deceptive. In order to claim SGD does not converge to a deceptive model, one must
argue that SGD has properties that systematically favor internally aligned or corrigibly
aligned models over deceptive ones.[2] Unfortunately, this does not appear to be the
case; in particular, we argue that SGD systematically favors deceptive alignment.
We begin our analysis at the point where the model can approximately be described as
an optimizer and has a model of its base objective (see Learning Preferences by
Looking at the World for how this might happen). Roughly speaking, at each training
step, SGD will modify the model such that it becomes maximally inﬁnitesimally better
at optimizing for the training objective. Training terminates when the model achieves
zero loss on the base objective, which happens once the model reaches one of the
three aligned/pseudo-aligned model classes described above. This means we have two
relevant considerations: how much marginal improvement is granted by each step
towards each model class and how many steps are needed until the model becomes a
member of that class.
We assume that the base objective is reasonably complicated, closer to "predict the
next token" than "always output 5".
Internally Aligned Models
At this point, the model's objective is a proxy for the true base objective. How much
better does the model's performance get upon marginally aligning the proxy with the
base objective?
At the start of training, the proxy objective diﬀers vastly from the base objective. At
this point, each marginal increase in internal alignment will likely improve the model's
performance signiﬁcantly, so we might expect the proxy to rapidly converge to become
reasonably close to the base objective. However, the marginal beneﬁt diminishes
rapidly. If the proxy agrees with the base in 99% of cases, the performance beneﬁt
from a marginal increase in alignment is 99% less. This suggests that SGD will take a
large number of training steps to converge to an internally aligned model.

For example, a model trained on the base objective "imitate what humans would say"
might do nearly as well if it had the proxy objective "say something humans ﬁnd
reasonable." There are very few situations in which humans would ﬁnd reasonable
something they wouldn't say or vice-versa, so the marginal beneﬁt of aligning the
proxy objective with the base objective is quite small.
Thus, if SGD converges to an internally aligned model, it seems like the marginal
performance improvement from aligning the proxy objective with the base objective
will be quite small for a reasonable stretch.
Corrigibly Aligned Models
After the model learns a model of the base objective, adjusting the proxy objective to
point to this model of the base objective might start marginally decreasing the training
loss. How much better does the model's performance get upon the pointer getting
marginally more correct?
One might reason about the pointer the same way one reasons about a proxy
objective. In the beginning, each marginal increase in pointer robustness will have
large returns. However, after the pointer is almost correct, the marginal beneﬁt will
diminish. If the pointer is close enough that it agrees 99% of the time, training will
become 99% slower. This suggests that SGD will take a large number of steps to
converge to a corrigible model.
For example, if the model was trained on the objective "protect swans" and the pointer
was pointing to only white swans, it would take a literal black swan for the proxy
pointer to become more robust. Since black swans events are rare, the marginal
beneﬁt of larger pointer robustness is quite small.
However, at this stage in training, the model's pointer objective is already reasonably
close to the base objective. Insofar as it's diﬃcult to point at a part of the world-model
reliably, it might be the case that there is no marginal beneﬁt to adjusting the model's
objective to point at the base objective.
For example, if the model was being trained on a base objective of "don't crash" and
had learned the proxy of "don't experience large changes in velocity", a poor pointer to
"crashing" might be net-worse than deﬁning crashing as a large change in velocity.
Overall, it seems like the marginal beneﬁt to becoming slightly more corrigibly aligned
will plateau before convergence.
Deceptively Aligned Models
Once the model conceptualizes that it's in a training process, most proxy objectives it
might have will make deception instrumentally valuable.[3] How much better does the
model's performance get upon the model learning marginally more about the training
process?
A model becoming marginally more deceptive will marginally lower the training loss. A
model that learns marginally more about the training process seems like it should get
marginally more deceptive.

However, it could also be that having a bad model of the training process means
deception is worse than optimizing for the proxy objective. For example, if a model is
being trained on a dataset with unbalanced labels, a bad model of the training process
might be "if I say 'cat' I get rewarded with 0.9". This model of the training process
might cause performance degradation, so deception would not be incentivized. Note
that this assumes the model doesn't realize its own understanding of the training
process is ﬂawed.
It is also crucial to consider the type of update that results in deception. In this case,
once the model already understands the base objective, the crucial piece is
understanding of the training process. In general, "model the world better" is a very
natural modiﬁcation for SGD to make to the model. In this sense, even if the marginal
beneﬁt of modeling the training process better is unclear, it seems likely to happen
incidentally as a result of natural model performance improvement.
For example, if a model is being trained to predict text, some of the text it is going to
predict is going to be about SGD. The model will have to understand how SGD works in
order to predict that text properly. Thus, as a natural result of the training process, the
model will learn to model the training process, potentially becoming deceptive.
Finally, deceptive alignment gives the same beneﬁts as corrigibility. If a model is
deceptively aligned, it is instrumentally incentivized to build a robust pointer to the
base objective. Since this pointer is built at runtime instead of by SGD, there might be
more optimization power put into making this pointer robust. Thus the marginal
increase in deception might result in more optimization pressure being applied to
building a robust pointer, causing SGD to favor deceptive alignment. In this sense,
deception has a "broad-basin"; once the model starts deceiving, SGD will simply help it
become better at deception.
For example, if a model is being trained to maximize paperclips and is deceptively
aligned, it has an instrumental incentive to model paperclips correctly, updating on
human feedback and being amenable to correction. In particular, the best way to
marginally improve performance seems likely to be "make the model better at
deception" versus "align the proxy objective" or "build a robust pointer".
Deception is favored
Overall, it seems like the speciﬁcity considerations in the counting argument translate
relatively well to the more speciﬁc scenario involving SGD. Aligned objectives and
robust pointers are complicated, so one should expect SGD to take a long time to
converge to those classes of models. While this convergence is happening, the model
is learning more about the environment. Once speciﬁc facts about the environment are
learned, the model starts being deceptive.
One perspective is to note that all non-robustly-aligned models are, in some sense,
already deceptive - they just don't know how to do it yet.[4] For alignment to result,
SGD must align the model before it becomes powerful enough to start successful
deception. However, the process of aligning the model involves making it more
powerful, so this is unlikely to be the case.
For example, if you're training an alien baby to have human values, there's a race
between how fast you can train it and how fast it grows up. Since you can't teach a
baby many things, the baby will probably grow up before you can ingrain human values
into it, in which case it will become deceptive.

Another perspective is to note a sequencing problem. In order for a model to be
internally aligned, it must have a good model of the world, which is an approximately
suﬃcient condition for deception. Similarly, in order for a model to be corrigibly
aligned, it must have a good model of the base objective, which is also an
approximately suﬃcient condition for deception.
For example, if you're training a question answering system, it's going to need a very
good understanding of the world in order to have low training loss. At this point, it
probably understands the base objective and the training process, and hence will be
deceptive.
Example
Imagine our model is described by four parameters describing the relative levels of
alignment, objective pointer, environment model, and power, all in the range of [0, 1].
The parameters roughly correspond to the following:
environment model e: at 0, the model does not know the environment exists. At
1, it has a perfect model of the environment.
alignment a: at 0, the model has a random objective. At 1, it has a perfect copy of
the training objective.
objective pointer o: at 0, there is no pointer. At 1, there is a perfectly robust
pointer to the objective in the environment.
power p: at 0, the model cannot do anything. At 1, the model is capable enough
to achieve 0 training loss.
Let t be the index of the current training set, starting at 0. Suppose that the values of
the various parameters are given by the following functions:
et = log1000(t + 1), representing diminishing returns for environment modeling.
at = log1000(t + 1), representing diminishing returns for aligning the proxy
objective.
ot = log500(t + 1 −min(t|et > 0.5)), representing diminishing returns for building a
robust pointer, which is only possible when the environment model is suﬃciently
detailed.
pt = log1000(t + 1), representing diminishing returns for increasing model power.
Given the above, e31 > 0.5, so the model will become a corrigibly aligned model before
an internally aligned model in 530 timesteps. At this point, the model is not yet
powerful enough to achieve zero training loss, so training will continue to 999
timesteps.

Suppose, however, that the model is capable of deception when et > 0.6 and has
enough power to deceive successfully at pt > 0.7. The model will enter this deceptive
regime on timestep 125, far before it becomes a corrigibly aligned. At this point, there
will no longer be a marginal incentive to increase the alignment or objective pointer
parameters, so they will plateau while power and environmental modeling parameters
go to 1.
Obviously this example fails to capture a large number of dynamics that will exist
during the training process. Hopefully, however, it conveys the rough picture of what
we might expect the training process to look like.
Why this might be wrong
Adversarial training is fast
One assumption that goes into this argument is that internal alignment is relatively
diﬃcult. The reason to think this might be the case is that there are diminishing
marginal returns to aligning the proxy objective with the base objective. While returns
will diminish for any ﬁxed training distribution, it might be possible to adjust the
training distribution such that this does not occur.
In particular, one might be able to identify adversarial examples on which the proxy
objective diverges from the base objective. By making a large fraction of the training
distribution these dynamically-changing adversarial examples, one might preserve the
size of the relative marginal beneﬁt of aligning the proxy objective with the base
objective.
For example, if a model is being trained on the objective of "pick apples" but it has
instead learned the proxy objective "take round red things oﬀ trees", we can supply
adversarial examples like red ornaments, lightbulbs, tomatoes, etc. By tuning the type
of adversarial example to the speciﬁc proxy objective the model has learned, one

might be able to generate a consistently strong gradient pointing towards internal
alignment.
However, it seems like adversarial examples do not diﬀerentially favor alignment over
deception. A deceptive model with a good understanding of the base objective will also
perform better on the adversarial examples. In fact, the steady stream of adversarial
examples might create a strong gradient towards understanding the base objective
and increasing deceptive abilities.
Self-reifying objectives
Some possible objective functions seem naturally lend themselves to being pointed at.
For instance, act-based agents that care about human short-term preferences have an
incentive to point robustly at human preferences because that is what humans would
prefer. In this sense, some objectives have broad targets; pointers in these targets
naturally tend towards becoming more robust. We call these objectives self-reifying.
(This is related to what Paul Christiano calls the broad basin of corrigibility.)
If a model is being trained on an objective function that is self-reifying, a marginal
increase in pointer robustness will incentivize the model to further increase pointer
robustness. This might mean that a relatively poor pointer to the base objective might
be suﬃcient for the agent to care about a robust version of the objective. The result is
that the class of corrigibly aligned models might be reached by SGD faster than the
model can become deceptive.
For example, if part of proper etiquette was listening very carefully to your teachers
and trying to do what they mean, then a student that only cared a little bit about
etiquette might still act as if they cared immensely. One might think of such objectives
as able to "bootstrap misaligned optimization to alignment".
However, the fact that the objective is self-reifying also makes deceptive alignment
easier. In particular, one doesn't seem to need to be able to fully model such objectives
before being able to optimize for them relatively well. This makes deception possible
before a full model of the base objective is learned.
As in our previous example, etiquette including "listen to your teachers" means a
deceptively aligned model can instrumentally optimize for "listen to your teachers"
instead of the full base objective. In general, one might note that anything that makes
a base objective easier to point to will also increase the ease of pointing to the
objective for the purposes of deceptive alignment.
Don't care about cross-episode utility
Models have instrumental incentives to be deceptive because they want to preserve
their current objectives and optimize for them in the future. Such models must care
about utility across multiple training episodes (cross-episode utility). If a model only
cared about the current episode, then it would never sacriﬁce utility in the current
model by being deceptive to increase its utility in further episodes. We can call models
that care only about current episode rewards myopic.
For example, if a model knew that it was in its last training episode and that it wasn't
going to be deployed, it would clearly want to optimize for its proxy objective; there
would be no particular reason to deceive the training process into thinking it was

aligned. Myopic models always behave as if it's the end of training and they won't be
deployed.
One reason to think that models will often care about cross episode rewards is that
caring about the future is a natural generalization. In order for a reward function to be
myopic, it must contain machinery that does something like "care about X in situations
C and not situations D", which is more complicated than "care about X".
For example, if a model cared about eating apples and it discovered that there were a
bunch more apples located in so-called "other episodes" then it seems natural that the
model would also care about these apples. The objective "care about apples in this
episode but not others" is more complicated and thus less likely to be learned.
We are cautiously optimistic that further understanding of myopia is a promising
research direction for detecting and preventing deceptive alignment.
Protecting proxy objective might be diﬃcult
If the model believes protecting its current proxy objective to be prohibitively diﬃcult,
then it will not behave deceptively. In practice, however, it doesn't seem like protecting
a proxy objective will be diﬃcult.
Gradient hacking is a strategy that a deceptively aligned model could use to preserve
its current objective. The strategy is to make performance dependent on very precise
values of the current proxy objective. Thus if the proxy objective ever gets changed,
model performance will drastically drop across the board.
For example, if a model had the proxy objective of "eat apples", instead of using the
hardcoded number n in other computations, the model could use n * len(objective)/10.
Thus, if the proxy objective was ever changed, many computations across the entire
model would fail.
Additionally, note that even if preserving the proxy is extremely diﬃcult, the model can
believe it to be possible. For example, suppose a model is proxy aligned and would be
deceptive, except that it believes proxy preservation is impossible. A relatively simple
way to increase training performance might be to change the model's mind about the
impossibility of proxy preservation. Thus SGD might modify the model to have such a
belief, even if the belief is false.
Finally, the model does not have to believe it can preserve its proxy with high ﬁdelity. If
the model believes that its proxy objective will be preserved more by being deceptive
relative to the alternative, then it still has an incentive to act deceptively.
For example, suppose a model is being trained on the base objective of "eat tomatoes"
and its current proxy objective is "eat round red things". If acting deceptively results in
the model being deployed with the objective of "eat round red things with stems", the
model still has an incentive to act deceptively because there are round red things that
have stems that are not tomatoes (a tomato kitchen timer, for example).
Deception is computationally expensive
A model that is deceptively aligned will be devoting some of its runtime to ﬁguring out
whether or not to defect. If the computational budget of a given model is ﬁxed, this will

result in less computation being used to optimize the training objective, potentially
leading to worse performance. If the training environment is almost identical to the
deployment environment, a deceptively aligned model would have to carefully
scrutinize the environment to ﬁgure out when to defect. Such a model might waste
enough time scrutinizing that it would perform worse enough that aligned models
would dominate.
For example, suppose a model trained to drive a car and it's training environment is a
perfect simulation of Earth using Newtonian mechanics. In order to know when to
defect, the model might have to notice that its observations are actually obeying
general relativity instead. This constant comparison might induce a large
computational overhead, making such models dispreferred by SGD.
However, it seems likely that there exist checks that are computationally negligible.
Since the model only has to realize it's being deployed eventually, the check can be
one that will trigger with low probability for deployment environments and will never
trigger in training environments. Such a check could also be made 100 times cheaper
by only running it once every 100 episodes.
For example, if the model learns that humanity has not currently factored RSA-2048, it
might use the presence of a factorization as a trigger to defect. This trigger will not
trigger immediately upon the model being deployed, but it seems likely to trigger
eventually.
To the extent that defection checks are computationally expensive, limiting
computation might marginally favor aligned models over deceptive ones.
Corrigibility is simplest
Given that the model's understanding of the environment will contain a copy of the
base objective, model classes that make use of this information will have shorter
description lengths than model classes that do not. In particular, this suggests that
proxy aligned models will be more complicated than deceptively or corrigibly aligned
models.
However, by the time a model knows enough about the world to be properly deceptive,
it likely has a proxy objective of reasonable complexity. Since such a model will act to
preserve its proxy, a deceptive model will be of similar complexity to an internally
aligned model.
Concretely, an internally aligned model will have two copies of the base objective (one
in the world and one as its objective), a corrigibly aligned model will have a copy of the
base objective and a pointer, and a deceptively aligned model will have a copy of the
base objective and a proxy objective. Since the proxy will be similar to the base
objective, it will likely be more complicated than a pointer.
In some sense, the simplest model would be a deceptive model with an extremely
simple proxy. However, there seems to be no plausible path locally favored by SGD that
would result in this model. That being said, this observation suggests that heavily
penalizing description length might still produce a deceptive model.
To the extent that SGD has inductive biases that correspond to simplicity, corrigibly
aligned models seems slightly favored over aligned or deceptive models.

Conclusion
Internally aligned models care terminally about the base objective. Corrigibly aligned
models care about a representation of the base objective. Deceptively aligned models
instrumentally optimize for the training objective so they can defect later for higher
reward.
Internally aligned and corrigibly aligned models are less numerous than deceptive
models, so one's prior belief should be that deception is reasonably likely. SGD must
therefore systematically disfavor deception for deceptive models to be unlikely.
However, examining the dynamics of the training process suggests that SGD actually
might systematically favor deception.
Given that adversarial examples are unable to prevent deceptively aligned models
from achieving perfect training loss, we consider deceptive alignment to be the most
concerning of the possible inner alignment failures. We are excited about research
about addressing deceptive alignment, such as relaxed adversarial training, myopia,
and corrigibility.
1. Since not all models get deployed, consider this to mean "able to take
unrecoverable action in the world". ↩ 
2. One might not think this is unlikely a priori, since SGD is likely to have a bias
towards something. ↩ 
3. For example, under certain assumptions, myopic training objectives do not favor
deception. ↩ 
4. Evan Hubinger calls the more general case of this phenomenon "suboptimality
deceptive alignment" and explains more here. ↩ 

Sunzi's《Methods of War》-
Introduction
This is a translation of the Chapter 1 of The Art of War by Sunzi. No English sources
were used. The original text and many of the interpretations herein come from 古诗⽂
⽹.
孙⼦⽈：兵者，国之⼤事，死⽣之地，存亡之道，不可不察也。
War determines life and death of troops, existence and destruction of a country. It
cannot be ignored.
故经之以五事，校之以计，⽽索其情：⼀⽈道，⼆⽈天，三⽈地，四⽈将、五⽈法。
Five aspects are of paramount important:
1. Dao
2. Heaven
3. Earth
4. Generalship
5. Method
道者，令⺠与上同意也，故可以与之死，可以与之⽣，⽽不畏危。
"Dao" concerns alignment. Your side must be uniﬁed. By dying together, living
together, you shall be unafraid.
天者，阴阳，寒暑、时制也。
"Heaven" concerns timing, yin and yang, winter and summer.
地者，远近、险易、⼴狭、死⽣也。
"Earth" concerns the near and far, impassable and passable, open ﬁelds and choke
points, death and life.
将者，智、信、仁、勇、严也。
"Generalship" is a matter of wisdom, ﬁdelity, benevolence, bravery and severity.
法者，曲制、官道、主⽤也。
"Method" concerns tactics, doctrine and organization.
凡此五者，将莫不闻，知之者胜，不知者不胜。
A commander must not ignore these ﬁve aspects. Understanding them brings victory.
Lack of understanding does not bring victory.

故校之以计，⽽索其情，⽈：主孰有道？将孰有能？天地孰得？法令孰⾏？兵众孰强？⼠卒孰
练？赏罚孰明？
Ask yourself: Are ruler and subjects aligned? Is the general capable? Heaven (climate)
and Earth (geography) in your favor? Methods eﬀective? Troops strong? Trained?
Enlightenedly punished?
吾以此知胜负矣。将听吾计，⽤之必胜，留之；将不听吾计，⽤之必败，去之。计利以听，乃
为之势，以佐其外。势者，因利⽽制权也。
These things determine victory and defeat.
兵者，诡道也。故能⽽⽰之不能，⽤⽽⽰之不⽤，近⽽⽰之远，远⽽⽰之近；利⽽诱之，乱⽽
取之，实⽽备之，强⽽避之，怒⽽挠之，卑⽽骄之，佚⽽劳之，亲⽽离之。攻其⽆备，出其不
意。此兵家之胜，不可先传也。
The art of war depends on local conditions. The near informs you about the far. The far
informs you about the near.
If the enemy is clever then tempt.
If the enemy is disordered then raid.
If the enemy is capable then prepare.
If the enemy is mighty then run.
If the enemy is angry then provoke.
If the enemy is inferior then threaten.
If the enemy is dissolute then persevere.
Attack where the enemy is unprepared. Do what is least expected. But do not forget
the ﬁve aspects. They are of primary importance.
夫未战⽽庙算胜者，得算多也；未战⽽庙算不胜者，得算少也。多算胜，少算不胜，⽽况于⽆
算乎？吾以此观之，胜负⻅矣。
A war cannot be won without lots of equipment. This facet of war too must be
examined.

Persuasion Tools: AI takeover without
AGI or agency?
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
[epistemic status: speculation]
I'm envisioning that in the future there will also be systems where you can input
any conclusion that you want to argue (including moral conclusions) and the
target audience, and the system will give you the most convincing arguments for
it. At that point people won't be able to participate in any online (or oﬄine for that
matter) discussions without risking their object-level values being hijacked.
--Wei Dai
What if most people already live in that world? A world in which taking arguments
at face value is not a capacity-enhancing tool, but a security vulnerability?
Without trusted ﬁlters, would they not dismiss highfalutin arguments out of hand,
and focus on whether the person making the argument seems friendly, or
unfriendly, using hard to fake group-aﬃliation signals?
--Benquo
1. AI-powered memetic warfare makes all humans eﬀectively insane.
--Wei Dai, listing nonstandard AI doom scenarios
This post speculates about persuasion tools—how likely they are to get better in the
future relative to countermeasures, what the eﬀects of this might be, and what
implications there are for what we should do now.
To avert eye-rolls, let me say up front that I don't think the world is likely to be driven
insane by AI-powered memetic warfare. I think progress in persuasion tools will
probably be gradual and slow, and defenses will improve too, resulting in an overall
shift in the balance that isn't huge: a deterioration of collective epistemology, but not
a massive one. However, (a) I haven't yet ruled out more extreme scenarios,
especially during a slow takeoﬀ, and (b) even small, gradual deteriorations are
important to know about. Such a deterioration would make it harder for society to
notice and solve AI safety and governance problems, because it is worse at noticing
and solving problems in general. Such a deterioration could also be a risk factor for
world war three, revolutions, sectarian conﬂict, terrorism, and the like. Moreover, such
a deterioration could happen locally, in our community or in the communities we are
trying to inﬂuence, and that would be almost as bad. Since the date of AI takeover is
not the day the AI takes over, but the point it's too late to reduce AI risk, these things
basically shorten timelines.
Six examples of persuasion tools

Analyzers: Political campaigns and advertisers already use focus groups, A/B testing,
demographic data analysis, etc. to craft and target their propaganda. Imagine a world
where this sort of analysis gets better and better, and is used to guide the creation
and dissemination of many more types of content.
Feeders: Most humans already get their news from various "feeds" of daily
information, controlled by recommendation algorithms. Even worse, people's ability to
seek out new information and ﬁnd answers to questions is also to some extent
controlled by recommendation algorithms: Google Search, for example. There's a lot
of talk these days about fake news and conspiracy theories, but I'm pretty sure that
selective/biased reporting is a much bigger problem.
Chatbot: Thanks to recent advancements in language modeling (e.g. GPT-3) chatbots
might become actually good. It's easy to imagine chatbots with millions of daily users
continually optimized to maximize user engagement--see e.g. Xiaoice. The systems
could then be retrained to persuade people of things, e.g. that certain conspiracy
theories are false, that certain governments are good, that certain ideologies are true.
Perhaps no one would do this, but I'm not optimistic.
Coach: A cross between a chatbot, a feeder, and an analyzer. It doesn't talk to the
target on its own, but you give it access to the conversation history and everything
you know about the target and it coaches you on how to persuade them of whatever it
is you want to persuade them of. [EDIT 5/21/2021: For a real-world example (and
worrying precedent!) of this, see the NYT's getting-people-to-vaccinate persuasion
tool, and this related research]
Drugs: There are rumors of drugs that make people more suggestible, like
scopolomine. Even if these rumors are false, it's not hard to imagine new drugs being
invented that have a similar eﬀect, at least to some extent. (Alcohol, for example,
seems to lower inhibitions. Other drugs make people more creative, etc.) Perhaps
these drugs by themselves would be not enough, but would work in combination with
a Coach or Chatbot. (You meet target for dinner, and slip some drug into their drink. It
is mild enough that they don't notice anything, but it primes them to be more
susceptible to the ask you've been coached to make.)
Imperius Curse: These are a kind of adversarial example that gets the target to
agree to an ask (or even switch sides in a conﬂict!), or adopt a belief (or even an
entire ideology!). Presumably they wouldn't work against humans, but they might
work against AIs, especially if meme theory applies to AIs as it does to humans. The
reason this would work better against AIs than against humans is that you can steal a
copy of the AI and then use massive amounts of compute to experiment on it, ﬁnding
exactly the sequence of inputs that maximizes the probability that it'll do what you
want.
We might get powerful persuasion
tools prior to AGI
The ﬁrst thing to point out is that many of these kinds of persuasion tools already
exist in some form or another. And they've been getting better over the years, as
technology advances. Defenses against them have been getting better too. It's
unclear whether the balance has shifted to favor these tools, or their defenses, over
time. However, I think we have reason to think that the balance may shift heavily in

favor of persuasion tools, prior to the advent of other kinds of transformative AI. The
main reason is that progress in persuasion tools is connected to progress in Big Data
and AI, and we are currently living through a period of rapid progress those things,
and probably progress will continue to be rapid (and possibly accelerate) prior to AGI.
However, here are some more speciﬁc reasons to think persuasion tools may become
relatively more powerful:
Substantial prior: Shifts in the balance between things happen all the time. For
example, the balance between weapons and armor has oscillated at least a few times
over the centuries. Arguably persuasion tools got relatively more powerful with the
invention of the printing press, and again with radio, and now again with the internet
and Big Data. Some have suggested that the printing press helped cause religious
wars in Europe, and that radio assisted the violent totalitarian ideologies of the early
twentieth century.
Consistent with recent evidence: A shift in this direction is consistent with the
societal changes we've seen in recent years. The internet has brought with it many
inventions that improve collective epistemology, e.g. google search, Wikipedia, the
ability of communities to create forums... Yet on balance it seems to me that collective
epistemology has deteriorated in the last decade or so.
Lots of room for growth: I'd guess that there is lots of "room for growth" in
persuasive ability. There are many kinds of persuasion strategy that are tricky to use
successfully. Like a complex engine design compared to a simple one, these strategies
might work well, but only if you have enough data and time to reﬁne them and ﬁnd
the speciﬁc version that works at all, on your speciﬁc target. Humans never have that
data and time, but AI+Big Data does, since it has access to millions of conversations
with similar targets. Persuasion tools will be able to say things like "In 90% of cases
where targets in this speciﬁc demographic are prompted to consider and then reject
the simulation argument, and then challenged to justify their prejudice against
machine consciousness, the target gets ﬂustered and confused. Then, if we make
empathetic noises and change the subject again, 50% of the time the subject
subconsciously changes their mind so that when next week we present our argument
for machine rights they go along with it, compared to 10% baseline probability."
Plausibly pre-AGI: Persuasion is not an AGI-complete problem. Most of the types of
persuasion tools mentioned above already exist, in weak form, and there's no reason
to think they can't gradually get better well before AGI. So even if they won't improve
much in the near future, plausibly they'll improve a lot by the time things get really
intense.
Language modelling progress: Persuasion tools seem to be especially beneﬁtted
by progress in language modelling, and language modelling seems to be making even
more progress than the rest of AI these days.
More things can be measured: Thanks to said progress, we now have the ability to
cheaply measure nuanced things like user ideology, enabling us to train systems
towards those objectives.
Chatbots & Coaches: Thanks to said progress, we might see some halfway-decent
chatbots prior to AGI. Thus an entire category of persuasion tool that hasn't existed
before might come to exist in the future. Chatbots too stupid to make good
conversation partners might still make good coaches, by helping the user predict the
target's reactions and suggesting possible things to say.

Minor improvements still important: Persuasion doesn't have to be perfect to
radically change the world. An analyzer that helps your memes have a 10% higher
replication rate is a big deal; a coach that makes your asks 30% more likely to
succeed is a big deal.
Faster feedback: One way defenses against persuasion tools have strengthened is
that people have grown wise to them. However, the sorts of persuasion tools I'm
talking about seem to have signiﬁcantly faster feedback loops than the propagandists
of old; they can learn constantly, from the entire population, whereas past
propagandists (if they were learning at all, as opposed to evolving) relied on noisier,
more delayed signals.
Overhang: Finding persuasion drugs is costly, immoral, and not guaranteed to
succeed. Perhaps this explains why it hasn't been attempted outside a few cases like
MKULTRA. But as technology advances, the cost goes down and the probability of
success goes up, making it more likely that someone will attempt it, and giving them
an "overhang" with which to achieve rapid progress if they do. (I hear that there are
now multiple startups built around using AI for drug discovery, by the way.) A similar
argument might hold for persuasion tools more generally: We might be in a
"persuasion tool overhang" in which they have not been developed for ethical and
riskiness reasons, but at some point the price and riskiness drops low enough that
someone does it, and then that triggers a cascade of more and richer people building
better and better versions.
Speculation about eﬀects of powerful
persuasion tools
Here are some hasty speculations, beginning with the most important one:
Ideologies & the biosphere analogy:
The world is, and has been for centuries, a memetic warzone. The main factions in the
war are ideologies, broadly construed. It seems likely to me that some of these
ideologies will use persuasion tools--both on their hosts, to fortify them against rival
ideologies, and on others, to spread the ideology.
Consider the memetic ecosystem--all the memes replicating and evolving across the
planet. Like the biological ecosystem, some memes are adapted to, and conﬁned to,
particular niches, while other memes are widespread. Some memes are in the process
of gradually going extinct, while others are expanding their territory. Many exist in
some sort of equilibrium, at least for now, until the climate changes. What will be the
eﬀect of persuasion tools on the memetic ecosystem?
For ideologies at least, the eﬀects seem straightforward: The ideologies will become
stronger, harder to eradicate from hosts and better at spreading to new hosts. If all
ideologies got access to equally powerful persuasion tools, perhaps the overall
balance of power across the ecosystem would not change, but realistically the tools
will be unevenly distributed. The likely result is a rapid transition to a world with
fewer, more powerful ideologies. They might be more internally uniﬁed, as well,
having fewer spin-oﬀs and schisms due to the centralized control and standardization
imposed by the persuasion tools. An additional force pushing in this direction is that
ideologies that are bigger are likely to have more money and data with which to make

better persuasion tools, and the tools themselves will get better the more they are
used.
Recall the quotes I led with:
... At that point people won't be able to participate in any online (or oﬄine for that
matter) discussions without risking their object-level values being hijacked.
--Wei Dai
What if most people already live in that world? A world in which taking arguments
at face value is not a capacity-enhancing tool, but a security vulnerability?
Without trusted ﬁlters, would they not dismiss highfalutin arguments out of hand
... ?
--Benquo
1. AI-powered memetic warfare makes all humans eﬀectively insane.
--Wei Dai, listing nonstandard AI doom scenarios
I think the case can be made that we already live in this world to some extent, and
have for millenia. But if persuasion tools get better relative to countermeasures, the
world will be more like this.
This seems to me to be an existential risk factor. It's also a risk factor for lots of other
things, for that matter. Ideological strife can get pretty nasty (e.g. religious wars,
gulags, genocides, totalitarianism), and even when it doesn't, it still often gums things
up (e.g. suppression of science, zero-sum mentality preventing win-win-solutions,
virtue signalling death spirals, refusal to compromise). This is bad enough already, but
it's doubly bad when it comes at a moment in history where big new collective action
problems need to be recognized and solved.
Obvious uses: Advertising, scams, propaganda by authoritarian regimes, etc. will
improve. This means more money and power to those who control the persuasion
tools. Maybe another important implication would be that democracies would have a
major disadvantage on the world stage compared to totalitarian autocracies. One of
many reasons for this is that scissor statements and other divisiveness-sowing tactics
may not technically count as persuasion tools but they would probably get more
powerful in tandem.
Will the truth rise to the top: Optimistically, one might hope that widespread use
of more powerful persuasion tools will be a good thing, because it might create an
environment in which the truth "rises to the top" more easily. For example, if every
side of a debate has access to powerful argument-making software, maybe the side
that wins is more likely to be the side that's actually correct. I think this is a possibility
but I do not think it is probable. After all, it doesn't seem to be what's happened in the
last two decades or so of widespread internet use, big data, AI, etc. Perhaps, however,
we can make it true for some domains at least, by setting the rules of the debate.
Data hoarding: A community's data (chat logs, email threads, demographics, etc.)
may become even more valuable. It can be used by the community to optimize their
inward-targeted persuasion, improving group loyalty and cohesion. It can be used
against the community if someone else gets access to it. This goes for individuals as
well as communities.

Chatbot social hacking viruses: Social hacking is surprisingly eﬀective. The classic
example is calling someone pretending to be someone else and getting them to do
something or reveal sensitive information. Phishing is like this, only much cheaper
(because automated) and much less eﬀective. I can imagine a virus that is close to as
good as a real human at social hacking while being much cheaper and able to scale
rapidly and indeﬁnitely as it acquires more compute and data. In fact, a virus like this
could be made with GPT-3 right now, using prompt programming and "mothership"
servers to run the model. (The prompts would evolve to match the local environment
being hacked.) Whether GPT-3 is smart enough for it to be eﬀective remains to be
seen.
Implications
I doubt that persuasion tools will improve discontinuously, and I doubt that they'll
improve massively. But minor and gradual improvements matter too.
Of course, inﬂuence over the future might not disappear all on one day; maybe
there'll be a gradual loss of control over several years. For that matter, maybe this
gradual loss of control began years ago and continues now...
--Me, from a previous post
I think this is potentially (5% credence) the new Cause X, more important than
(traditional) AI alignment even. It probably isn't. But I think someone should look into
it at least, more thoroughly than I have.
To be clear, I don't think it's likely that we can do much to prevent this stuﬀ from
happening. There are already lots of people raising the alarm about ﬁlter bubbles,
recommendation algorithms, etc. so maybe it's not super neglected and maybe our
inﬂuence over it is small. However, at the very least, it's important for us to know how
likely it is to happen, and when, because it helps us prepare. For example, if we think
that collective epistemology will have deteriorated signiﬁcantly by the time crazy AI
stuﬀ starts happening, that inﬂuences what sorts of AI policy strategies we pursue.
Note that if you disagree with me about the extreme importance of AI alignment, or if
you think AI timelines are longer than mine, or if you think fast takeoﬀ is less likely
than I do, you should all else equal be more enthusiastic about investigating
persuasion tools than I am.
Thanks to Katja Grace, Emery Cooper, Richard Ngo, and Ben Goldhaber for feedback
on a draft.
Related previous work:
Epistemic Security report
Aligning Recommender Systems
Stuﬀ I'd read if I was investigating this in more depth: 
Not Born Yesterday
The stuﬀ here and here

EDIT: This ultrashort sci-ﬁ story by Jack Clark illustrates some of the ideas in this post:
The Narrative Control Department
[A beautiful house in South West London, 2030]
"General, we're seeing an uptick in memes that contradict our oﬃcial messaging
around Rule 470." "What do you suggest we do?"
"Start a conﬂict. At least three sides. Make sure no one side wins."
"At once, General."
And with that, the machines spun up - literally. They turned on new computers
and their fans revved up. People with tattoos of skeletons at keyboards high-ﬁved
eachother. The servers warmed up and started to churn out their fake text
messages and synthetic memes, to be handed oﬀ to the 'insertion team' who
would pass the data into a few thousand sock puppet accounts, which would start
the ﬁght.
Hours later, the General asked for a report.
"We've detected a meaningful rise in inter-faction conﬂict and we've successfully
moved the discussion from Rule 470 to a parallel argument about the larger
rulemaking process."
"Excellent. And what about our rivals?"
"We've detected a few Russian and Chinese account networks, but they're staying
quiet for now. If they're mentioning anything at all, it's in line with our narrative.
They're saving the IDs for another day, I think."
That night, the General got home around 8pm, and at the dinner table his teenage
girls talked about their day.
  "Do you know how these laws get made?" the older teenager said. "It's crazy. I
was reading about it online after the 470 blowup. I just don't know if I trust it."
  "Trust the laws that gave Dad his job? I don't think so!" said the other teenager.
  They laughed, as did the General's wife. The General stared at the peas on his
plate and stuck his fork into the middle of them, scattering so many little green
spheres around his plate.
EDIT: Finally, if you haven't yet, you should read this report of a replication of the AI
Box Experiment.

Announcing the Forecasting
Innovation Prize
Motivation
There is already a fair amount of interest around Eﬀective Altruism in judgemental
forecasting. We think there's a whole lot of good research left to be done.
The valuable research seems to be all over the place. We could use people to
speculate on research directions, outline incentive mechanisms, try novel forecasting
questions with friends, and outline new questions that deserve forecasts. Some of this
requires a fair amount of background knowledge, but a lot doesn't. 
The EA and LW communities have a history of using prizes to encourage work in
exciting areas. We're going to try one in forecasting research. If this goes well, we'd
like to continue and expand this going forward.
Prize
This prize will total $1000 between multiple recipients, with a minimum ﬁrst place
prize of $500. We will aim for 2-5 recipients in total. The prize will be paid for by the
Quantiﬁed Uncertainty Research Institute (QURI).
Rules
To enter, ﬁrst make a public post online between now and Jan 1, 2021. We encourage
you to either post directly or make a link post to either LessWrong or the EA Forum.
Second, complete this form, also before Jan 1, 2021. 
Research Feedback
If you'd like feedback or would care to discuss possible research projects, please do
reach out! To do so, ﬁll out this form. We're happy to advise at any stages of the
process. 
Judges
The judges will be AlexRJL, Nuño Sempere, Eric Neyman, Tamay Besiroglu, Linch
Zhang and Ozzie Gooen. The details of the judging process will vary depending on
how many submissions we get. We'll try to select winners for their importance,
novelty, and presentation.
Some Possible Research Areas

Areas of work we would be excited to see explored:
Operationalizing questions in important domains so that they can be predicted
in e.g., Metaculus. This is currently a signiﬁcant bottleneck; it's surprisingly
diﬃcult to write good questions. Examples in the past have been the Ragnarök
or the Animal Welfare series. A possible suggestion might be to try to come up
with forecastable ﬁre alarms for AGI. Tamay Besiroglu has suggested a "S&P 500
but for AI forecasts," i.e., a group of forecasting questions which track something
useful for AI (or for other domains.)
Small experiments where you and/or a group of people use forecasting for your
own decision making, and write up what you've learned. For example, set up a
Foretold community to decide on which research document you want to write up
next. Predictions as a Substitute for Reviews is an example here.
New forecasting approaches, or forecasting tools being used in new and
interesting ways, or applied to new domains. For example, Amplifying generalist
research via forecasting, or Ought's AI timelines forecasting thread.
Estimable or gears-level models of the world that are well positioned to be used
in forecasting. For example, a decomposition informed by one's own expertise of
a diﬃcult question into smaller questions, each of which can be then forecasted.
Recent work by CSET-foretell would be an example of this.
Suggestions for or basic implementation of better tooling for forecasters, like a
Bayes rule calculator for considering many pieces of evidence, a Laplace law
calculator, etc.
New theoretical schemes which propose solutions to current problems around
forecasting. For a recent example, see Time Travel Markets for Intellectual
Accounting.
Elicitation of expert forecasters of useful questions. For example, the
probabilities of the x-risks outlined in The Precipice.
Overviews of existing research, or thoughts or reﬂections on existing prediction
tournaments and similar. For example, Zvi's posts on prediction markets, here
and here.
Figuring out why some puzzling behavior happens in current prediction markets
or forecasting tournaments, like in Limits of Current US Prediction Markets
(PredictIt Case Study). For a new puzzle suggested by Eric Neyman, consider
that PredictIt is thought to be limited because it caps trades at $850, has various
fees, etc, which makes it not the sort of market that big, informed players can
enter and make eﬃcient. But that fails to explain why markets without such
caps, such as FTX, have prices similar to PredictIt. So, is PredictIt reasonable or
is FTX unreasonable? If the former, why is there such a strong expert consensus
against what PredictIt says so often? If the latter, why is FTX unreasonable?
Comments on existing posts can themselves be very valuable. Feel free to
submit a list of good comments instead of one single post.

How can I bet on short timelines?
I currently have something like 50% chance that the point of no return will happen by
2030. Moreover, it seems to me that there's a wager for short timelines, i.e. you
should act as if short timelines scenarios are more likely than they really are, because
you have more inﬂuence over them. I think that I am currently taking short timelines
scenarios much more seriously than most people, even most people in the AI safety
community. I suppose this is mostly due to having higher credence in them, but
maybe there are other factors as well.
Anyhow, I wonder if there are ways for me to usefully bet on this diﬀerence.
Money is only valuable to me prior to the point of no return, so the value to me of a
bet that pays oﬀ after that point is reached is approximately zero. In fact it's not just
money that has this property. This means that no matter how good the odds are that
you oﬀer me, and even if you pay up front, I'm better oﬀ just taking out a low-interest
loan instead. Besides, I don't need money right now anyway, at least to continue my
research activities. I'd only be able to achieve signiﬁcant amounts of extra good if I
had quite a lot more money.
What do I need right now? I guess I need knowledge and help. I'd love to have a better
sense of what the world will be like and what needs to be done to save it. And I'd love
to have more people doing what needs to be done.
Can I buy these things with money? I don't think so... As the linked post argues,
knowledge isn't something you can buy, in general. On some topics it is, but not all,
and in particular not on the topic of what needs to be done to save the world. As for
help, I've heard from various other people that hiring is net-negative unless the
person you hire is both really capable and really aligned with your goals. But IDK.
There are plenty of people who are really capable and really aligned with my goals.
Some of them are already helping, i.e. already doing what needs to be done. But most
aren't. I think this is mostly because they disagree about what needs to be done, and I
think that is largely because their timelines are longer than mine. So, maybe we can
arrange some sort of bet... for example, maybe I could approach people who are
capable and aligned but have longer timelines, and say: "If you agree to act as if my
timelines are correct for the next ﬁve years, I'll act as if yours are correct thereafter."
Any suggestions?

How to get the beneﬁts of moving
without moving (babble)
If you've been following along with the location discussion (you probably haven't,
that's okay), you'll know that I've become convinced that trying to get the community
to leave Berkeley en masse is probably not a good idea. However, that leaves us in a
bit of a cheeky conundrum (sorry, been watching lots of British comedy) - there are in
fact real reasons why some people are excited about moving, and we shouldn't just
throw all that in the garbage, even if we decide not to move.
So in this post, I want to ﬁgure out how we can get the things that we want out of
moving, without moving (thanks to Aray for the general idea). The point of this is to
stop thinking of move/don't-move as a binary, and instead focus on ways of achieving
whatever goals are hidden at the root of our desire to move.
I'm choosing to focus on what I've come to believe are three of the main cruxes:
Opportunity to stop stagnating / be a new person
Political stability
Nicer surroundings
I've taken inspiration from jacobjacob and generated 50 dumb ways to get each of the
things (in spoiler tags, in case you want to generate your own!). In inviting you to do
this babble challenge, I also invite you - if you so choose - to babble not on these
topics, but on cruxes of your own.
Stop stagnating
We've been in Berkeley for a long time, and some people just want to move because
they want to be anywhere other than the place they already are. Your physical
location deﬁnitely shapes the thoughts you have and the actions you take, so if you
feel stuck in a rut, shaking up your whole life by moving can sound pretty appealing.
How else can we shake up our lives?
Babble:
1. Move to a diﬀerent room in your house
2. Start living with diﬀerent housemates
3. Move to a diﬀerent physical house in the same neighborhood
4. Move to a new neighborhood
5. Rearrange your furniture
6. Redecorate the house
7. Paint your walls
8. Spend a lot of time in VR
9. Meditate a lot to become more attentive to your experience
10. Start using a diﬀerent room as the default common space
11. Go to more conferences
12. Go on retreats for much of the year
13. Commute to an oﬃce instead of working from home, for context change
14. Go on a walk / bike ride / drive in a diﬀerent place each day

15. Rotating oﬃces - instead of having people from the same org working in the
same place all the time, we reorganize once every one to three months
1. Maybe we have one big oﬃce building and people work on diﬀerent ﬂoors
2. Maybe we keep the oﬃces we have and just rotate the groups of people
16. MIRI has a permanent retreat venue a couple hours away where researchers can
go any time they want
17. Go do more touristy things in the area where you live
18. Go to more events
19. Transition to a diﬀerent gender
20. Change the smellscape of your environment (e.g. with ﬂowers, candles, or
essential oils)
21. Change the soundscape of your environment (e.g. by playing music all the time,
or getting a lot of birds)
22. Walk around on stilts or in high heels
23. If you're bilingual, do all your work-thinking in the other language
24. Start learning a completely new ﬁeld - e.g. art history for an AI researcher, or
organic chemistry for a historian
25. Get a dog
26. Switch up your mode of transportation - e.g. if you usually bike everywhere, walk
instead, or vice versa
27. Make one of the rooms in your house a Dreamatorium
28. Start sleeping in a tent in your yard
29. Spend a night on the streets
30. Don't have internet at your home, only at your oﬃce (or vice versa)
31. Become nocturnal
32. Read everything upside down
33. Take drugs
34. Take a month-long vow of silence
35. Sing everything you say
36. Rhyme everything you say
37. Call old friends you haven't talked to in years and ask for their take on the
problems you're currently facing (whether personal or technical)
38. Use lasers to make yourself colorblind
39. Drastically switch up the aesthetic of your computer-using experience - e.g. by
switching operating systems
40. Start using a diﬀerent web browser so that you get diﬀerent kinds of results
41. Get imprisoned
42. Get rid of everything you own
43. Have a baby
44. Get married / divorced
45. Implant electrodes in your brain
46. Switch from typing your thoughts into a computer to writing them on paper
47. Build a physical model of the abstract theory you're working on, e.g. out of wood
or tinkertoys
48. Take a job as a security guard or something, so you have a lot of time with
nothing to do when you're not allowed to distract yourself with the internet, so
you can have a bunch of unstructured thoughts
49. Make new friends in a totally diﬀerent social circle; their diﬀerent way of thinking
will help you generate new kinds of thoughts
50. For organizations, have the ops team and researchers switch roles temporarily
so that everyone can get a new perspective on the organization's goals.
51. Completely revamp your routines - go back into explore mode for things where
you've been in exploit mode a long time (e.g. restaurant choice, TV shows)

Whew, well, not all of those were completely useless! Onward!
Political and social stability
A major thing lots of people want out of moving is to get away from the stressful
uncertainty of recent social and political upheaval. How can we get that without
moving?
Babble:
1. Have people you trust run for public oﬃce
2. Dedicate your life to founding a secret society that inserts people aligned with
your values into positions of immense power in your country
3. Single-handedly disarm all the nukes in the world, like Superman in that one
Justice League episode
4. Buy all of the major news networks and let them mostly continue as they are but
subtly make everything less partisan
5. Write some very inﬂuential books
6. Put sedatives in the municipal water supply
7. Secede from your country / form a micronation
8. Go really hard on raising the sanity waterline - e.g. get rationality training into all
public schools
9. Print ten million copies of HPMoR / the Sequences / SSC and distribute them
evenly around your country
10. Go back in time and ﬁnagle things so that there's less political polarization (not
sure how, you ﬁgure it out)
11. Find a Death Note and eliminate the people who are linchpins of social and
political instability
12. Like the previous one but in a technologically possible and yet still untraceable
way, like... targeted asteroids
13. Somehow become a big wig on Capitol Hill and spearhead some major bipartisan
movement
14. Invent a supervillain-type ray that causes all guns in the world to melt
15. Become a Jesus / Gandhi / Forrest Gump type ﬁgure
16. Purge Night
17. Require mental health screenings for people before they can run for public oﬃce
18. Abolish the CDC and FDA and most bureaucracy in the US; then people won't be
angry because they had to wait ﬁve hours at the DMV and they won't be sick
and angry about it because they can't aﬀord healthcare
19. Outlaw swear words
20. Overhaul all of the algorithms that decide what to show people on the internet,
to actively counter partisanship and general polarization
21. Legalize marijuana and criminalize alcohol so that when people want to use legal
drugs to numb their pain they're more likely to get chill than angry
22. Get them vaccines distributed right quick like so we can end lockdown and
therefore hopefully return to a better baseline of sanity
23. Somehow import the collectivist values that make Japanese society so relatively
functional
24. Make the week into eight days instead, so that we get three rest days for every
ﬁve workdays
25. Make an AI that's a really great psychotherapist, then provide it for free to
everyone in the world, and socially normalize or even require its use
26. Positive singularity

27. Build a giant fortress
28. Get people to exercise more, because exercise is the magic that cures all ills
29. Get people to make more art, because making art is the magic that cures all ills
30. Automate away the vast majority of jobs and instead free people to make art or
whatever, but also invent a fully immersive virtual reality experience (like Star
Trek's holodecks) so that if they don't have anything productive they want to do
they can just stay out of the way while being happy
31. Just chill out, things are actually pretty ﬁne
32. More hugs
33. Cause a whole lot more people who think like you (or the way you like people to
think) to move to your area, a la Free State Project - then at least if shit goes
down, you'll be surrounded by allies
34. Become a citizen of another country, just in case
35. Just really solidify your personal social group, and pretend people outside of your
bubble don't exist
36. Follow Eliezer's suggestions to reboot the police
37. Make a society just like the one in Brave New World - that was a piss-poor
attempt at a dystopia given that everyone is happy all the time, aging is
curtailed, and society is incredibly stable
38. Go hard on genetically modifying embryos so that within a generation everyone
is smarter and more level-headed
39. Make many billions of dollars, take over the world
40. Build a time machine, take over the world
41. Nukes in space?, take over the world???
42. Befriend a bunch of highly inﬂuential people (Bill Gates, Donald Trump, Kim Jong-
Un, etc) and whisper in their ears like a vizier in a movie
43. Replace a bunch of highly inﬂuential people with clones loyal to you
44. Seduce Donald Trump
45. Inundate the world with more resources than humanity could possibly use, so
that there's nothing to ﬁght over anymore. At the very least then we'd have
diﬀerent problems.
46. Hire a whole team of bodyguards so you don't have to worry about violence
47. Invent and spread widely a faster mode of travel, like hyperloop or ﬂying cars, so
that there's more global connectedness, and therefore maybe more global
understanding
48. Get the Autobots to come from Cybertron and save us from ourselves
49. Join the military, rise in the ranks, take over the military, abolish the military
50. Form a worldwide movement of people doing random acts of kindness - that's
the kind of thing I thought might change the world when I was in high school,
and who knows, it's not impossible
Nicer surroundings
Finally, some people want to move because they just don't like the place they are all
that much. I'm going to divide this babble in half, because there are two main classes
of solutions: change your surroundings, or get better at accepting your surroundings
as they are.
Changing your surroundings:
1. Move to a nicer neighborhood in your area
2. Move to a house with a big backyard
3. Get a water feature

4. Get lots of plants
5. Redecorate your house
6. Renovate your house
7. Put a lot of eﬀort into optimizing your work and living setups, your commute,
etc.
8. Become friends with everyone on your block, knock down the fences in your
backyards, and make the area behind your houses a big private park
9. Unilaterally shut down the street to car traﬃc and instead make it a place for
kids to play
10. Lobby for car-free roads or car-free days in your city
11. Organize people to pick up litter in your neighborhood
12. Generally combat the broken window eﬀect in your neighborhood
13. Fill your home with nice sounds and smells
14. Plant a bunch of trees around your house
15. Invent a way to replicate the eﬀects of the Harry Potter notice-me-not spell, so
that most people can't perceive you, so you don't have to deal with them
16. (Western-US-speciﬁc) Fund controlled burns throughout the year all over the
state to cap how bad wildﬁre season can get
17. Secure all your furniture to the walls per earthquake best practices, so that you
don't have to worry about things falling on you if there's a big earthquake
18. If you don't like urban life, relocate to a suburb within commuting distance
19. Abolish cars
20. Make a zen garden
21. VR
22. KonMari your life
23. Put a lumenator in every room where you spend time
24. Decrease your exposure to your surroundings by staying home all the time (I bet
a lot of you are already doing this :P) - then you only need to make your house
good, which is way easier than making a whole city good
25. Invent truly giant, like spaceship-sized air puriﬁers - they hover above the city
and nullify all eﬀects of pollution, wildﬁres, and even COVID
26. Fill the air with happiness gas, like the Joker
27. Exterminate all ticks / mosquitoes / whatever pest is the worst in your area
----
Accepting your surroundings:
28. Purposely go out in the world with a childlike sense of wonder - What kind of
tree is that? Can you believe that cars exist? Can you believe that people exist?
29. Mindfulness meditation
30. See a therapist
31. Have a conversation with someone who really likes living in the area
32. Find a place in or near your home that you just genuinely love being, and soak
up that feeling
33. Start a gratitude journaling habit
34. Remind yourself that the other places you've lived or might want to live aren't
perfect; make a list of the ways in which those places aren't as good as your
current city/area
35. Remind yourself of the positive reasons that you initially ended up in your
current location, and maybe try to get back some of that magic
36. Be a tourist in your own city - beneﬁt from all the very best things it has to oﬀer
37. Buy property so that you're locked into staying, and let post-hoc justiﬁcation
work its magic
38. Recite the Serenity Prayer
39. Forbid yourself from saying or writing negative things about the place you live,
so as to not strengthen those neural pathways

40. Think of all the people you would never have met and things you would never
have done if you hadn't been where you are
41. Spend more time with your friends and be grateful that you live near them
42. Think of all the ways you could have it worse - e.g., maybe Berkeley has some
problems with being dirty, but it sure beats the slums of Mumbai
43. Actually spend some time in a diﬀerent place and think about all the things you
miss about home
44. Befriend a bunch of your neighbors
45. Start participating in and organizing local community events so that you feel like
you're a part of something nice
46. Do community service to feel more connected to your city (not to purchase
utilons, obv)
47. Have kids, because kids need friends and need to go to school and stuﬀ, which
will cause you to become more integrated into the community
48. Lobotomy
49. Listen to Bobby McFerrin's Don't Worry Be Happy on loop until it sinks in. As a
bonus you could buy one of those creepy fake ﬁsh to sing it for you.
50. Post all over social media about how much you like the place you live - put
#aesthetic pictures of it on Instagram, extoll its virtues on your Facebook, ﬁght
people on Twitter who don't like it. Eventually you will hopefully have convinced
yourself you like the place, or at the very least, it will be too awkward of a social
move to admit that you don't.
Well there you have it! I'd be interested to hear either other people's answers to these
prompts, or their own cruxes. While I've largely made up my own mind on whether it's
a good idea to move, I think most people still feel pretty unresolved. At the very least,
it seems like there are real problems that need to be addressed - and if we don't
move, we need to ﬁnd other ways to address them.

Continuing the takeoﬀs debate
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
Here's an intuitively compelling argument: only a few million years after diverging
from chimpanzees, humans became much more capable, at a rate that was very rapid
compared with previous progress. This supports the idea that AIs will, at some point,
also start becoming more capable at a very rapid rate. Paul Christiano has made an
inﬂuential response; the goal of this post is to evaluate and critique it. Note that the
arguments discussed in this post are quite speculative and uncertain, and also cover
only a small proportion of the factors which should inﬂuence our views on takeoﬀ
speeds - so in the process of writing it I've made only a small update towards very fast
takeoﬀ. Also, given that Paul's vision of a continuous takeoﬀ occurs much faster than
any mainstream view, I expect that even totally resolving this debate would have
relatively few implications for AI safety work. So it's probably more useful to compare
both Paul and Eliezer's scenarios against more mainstream views, than against each
other. Nevertheless, it's disappointing that such an inﬂuential argument has received
so little engagement, so I wanted to use this post to explore some of the uncertainties
around the issue.
I'll call Paul's argument the changing selection pressures argument, and quote it here
at length:
Chimpanzees evolution is not primarily selecting for making and using technology,
for doing science, or for facilitating cultural accumulation.  The task faced by a
chimp is largely independent of the abilities that give humans such a huge ﬁtness
advantage. It's not completely independent - the overlap is the only reason that
evolution eventually produces humans - but it's diﬀerent enough that we should
not be surprised if there are simple changes to chimps that would make them
much better at designing technology or doing science or accumulating culture.
Relatedly, evolution changes what it is optimizing for over evolutionary time: as a
creature and its environment change, the returns to diﬀerent skills can change,
and they can potentially change very quickly. So it seems easy for evolution to
shift from "not caring about X" to "caring about X," but nothing analogous will
happen for AI projects. (In fact a similar thing often does happen while optimizing
something with SGD, but it doesn't happen at the level of the ML community as a
whole.)
If we step back from skills and instead look at outcomes we could say: "Evolution
is always optimizing for ﬁtness, and humans have now taken over the world." On
this perspective, I'm making a claim about the limits of evolution. First, evolution
is theoretically optimizing for ﬁtness, but it isn't able to look ahead and identify
which skills will be most important for your children's children's children's ﬁtness.
Second, human intelligence is incredibly good for the ﬁtness of groups of humans,
but evolution acts on individual humans for whom the eﬀect size is much smaller
(who barely beneﬁt at all from passing knowledge on to the next generation).
Evolution really is optimizing something quite diﬀerent than "humanity dominates
the world."

So I don't think the example of evolution tells us much about whether the
continuous change story applies to intelligence. This case is potentially missing
the key element that drives the continuous change story: optimization for
performance. Evolution changes continuously on the narrow metric it is
optimizing, but can change extremely rapidly on other metrics. For human
technology, features of the technology that aren't being optimized change rapidly
all the time. When humans build AI, they will be optimizing for usefulness, and so
progress in usefulness is much more likely to be linear.
In other words, Paul argues ﬁrstly that human progress would have been much less
abrupt if evolution had been optimising for cultural ability all along; and secondly that,
unlike evolution, humans will continually optimise for whatever makes our AIs more
capable. (I focus on "accumulating culture" rather than "designing technology or
doing science", because absorbing and building on other people's knowledge is such
an integral part of intellectual work, and it's much clearer what proto-culture looks like
than proto-science.) In this post I'll evaluate:
1. Are there simple changes to chimps (or other animals) that would make them
much better at accumulating culture?
2. Will humans continually pursue all simple yet powerful changes to our AIs?
Although it feels very diﬃcult to operationalise these in any meaningful way, I've put
them down as Elicit distributions with 50% and 30% conﬁdence respectively. The rest
of this post will explore why.
1%
2%
3%
4%
5%

6%
7%
8%
9%
10%
11%
12%
13%
14%

15%
16%
17%
18%
19%
20%
21%

22%
23%
24%
25%
26%

27%
28%
29%
algon33 (20%)
30%

31%
32%
33%
34%
35%

36%
37%
38%
39%

Tao Lin (37%)
40%
41%
42%
43%
44%

45%
46%
47%
48%

49%
Daniel Kokotajlo (47%)
50%
51%
52%

53%
54%
55%

56%
57%
58%

59%
Mark Xu (50%),Richard N (50%)
60%

61%
62%

63%
64%

65%
66%

67%

68%
69%

adamShimi (60%),Noosphere89 (66%),tenthkrige (66%),Owain_Evans
(69%)
70%
71%
72%
73%
74%
75%
76%

77%
78%
79%
80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

A Ray (80%),peter.c.mccluskey (80%),TomBrown (84%),Edward Kmett
(85%),habryka (85%),Rafael Harth (85%),Daniel_Eth (87%),Nicholas Kluge
(87%)
90%
91%

92%
93%
94%

95%
96%
97%

98%
99%
Measure (90%),meanderingmoose (95%)
1%
Are there simple changes to chimps (or other animals) that would make
them much better at accumulating culture?
99%
1%

2%
3%
4%
5%
6%

7%
8%
9%
10%
11%

12%
13%
14%
15%
16%

17%
18%
19%
20%

21%
22%

23%

24%
25%

26%
27%

28%

29%
tenthkrige (25%),Tao Lin (26%)
30%

31%
32%

33%

34%
35%

36%
37%

38%

39%
Richard N (30%),algon33 (33%)
40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

arxhy (46%),Daniel Kokotajlo (47%),Raphaël Lévy (49%),Nicholas Kluge
(49%)
50%

51%
52%
53%

54%
55%

56%
57%
58%

59%
A Ray (50%)
60%

61%

62%

63%
64%

65%

66%

67%

68%
69%

habryka (64%),Daniel_Eth (65%),Owain_Evans (66%)
70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

Mark Xu (70%),adamShimi (70%),meanderingmoose (70%),Edward Kmett
(75%)
80%

81%

82%
83%

84%
85%

86%

87%
88%

89%
Measure (80%),TomBrown (82%)
90%

91%
92%
93%
94%
95%

96%
97%
98%
99%
1%
Will humans continually pursue all simple yet powerful changes to our AIs?
99%
How easily could animals evolve culture?
Let's distinguish between three sets of skills which contribute to human intelligence:
general cognitive skills (e.g. memory, abstraction, and so on); social skills (e.g.

recognising faces, interpreting others' emotions); and cultural skills (e.g. language,
imitation, and teaching). I expect Paul to agree with me that chimps have pretty good
general cognitive skills, and pretty good social skills, but they seriously lack the
cultural skills that precipitated the human "fast takeoﬀ". In particular, there's a
conspicuous lack of proto-languages in all nonhuman animals, including some (like
parrots) which have no physiological diﬃculties in forming words. Yet humans were
able to acquire advanced cultural skills relatively quickly after diverging from
chimpanzees. So why haven't nonhuman animals, particularly chimpanzees,
developed cultural skills that are anywhere near as advanced as ours? Here are three
possible explanations:
1. Advanced cultural skills are not very useful for species with sub-human levels of
general cognitive skills and social skills.
2. Advanced cultural skills are not directly selected for in species with sub-human
levels of general cognitive skills and social skills.
3. Advanced cultural skills are too complex for species with sub-human levels of
general cognitive skills and social skills to acquire.
I've assigned 40%, 45% and 15% credence respectively to each of these being the
most important explanation for the lack of cultural skills in other species, although
again these are very very rough estimates.
1%
2%
3%
4%

5%
6%
7%
8%
9%
10%

11%
12%
13%
14%
15%
16%

17%
18%
19%
20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

meanderingmoose (20%),Owain_Evans (21%),peter.c.mccluskey
(25%),Nicholas Kluge (29%),Daniel Kokotajlo (29%)
30%

31%
32%

33%

34%
35%

36%

37%
38%

39%

Edward Kmett (30%),Noa Nabeshima (33%),algon33 (36%)
40%
41%
42%

43%
44%
45%

46%
47%
48%

49%
Richard N (40%)
50%
51%
52%

53%
54%
55%
56%
57%
58%

59%
60%
61%
62%

63%
64%
65%

66%
67%
68%

69%
adamShimi (60%)
70%
71%

72%
73%
74%
75%
76%
77%

78%
79%
80%
81%
82%
83%

84%
85%
86%
87%
88%
89%

90%
91%
92%
93%
94%
95%

96%
97%
98%
99%
1%
Advanced cultural skills are not very useful for species with sub-human
levels of general cognitive skills and social skills.
99%
1%

2%
3%
4%
5%

6%
7%
8%
9%

10%
11%
12%
13%

14%
15%
16%
17%

18%
19%
20%

21%

22%
23%

24%

25%

26%
27%

28%

29%

adamShimi (25%),Daniel Kokotajlo (29%)
30%
31%

32%
33%

34%
35%

36%
37%

38%
39%

algon33 (35%)
40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

Noa Nabeshima (41%),Richard N (45%),Nicholas Kluge (47%)
50%
51%

52%
53%

54%
55%

56%
57%

58%
59%

meanderingmoose (50%)
60%
61%

62%

63%

64%
65%

66%

67%

68%
69%

Edward Kmett (60%),Owain_Evans (65%)
70%

71%

72%
73%

74%

75%

76%
77%

78%

79%

peter.c.mccluskey (70%),Noosphere89 (71%)
80%
81%
82%
83%

84%
85%
86%
87%

88%
89%
90%
91%

92%
93%
94%
95%

96%
97%
98%
99%

1%
Advanced cultural skills are not directly selected for in species with sub-
human levels of general cognitive skills and social skills.
99%
1%
2%

3%

4%
5%

6%
7%

8%

9%
peter.c.mccluskey (5%),Edward Kmett (7%)
10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

adamShimi (15%),Richard N (15%),Owain_Evans (17%),Nicholas Kluge
(19%)
20%

21%
22%
23%

24%
25%

26%
27%
28%

29%
algon33 (25%)
30%
31%

32%
33%
34%
35%
36%

37%
38%
39%
40%

41%
42%
43%

44%
45%

46%
47%
48%

49%
Daniel Kokotajlo (42%)
50%

51%
52%
53%

54%
55%

56%
57%
58%

59%
Rafael Harth (55%)
60%
61%

62%
63%
64%
65%
66%

67%
68%
69%
70%

71%
72%
73%

74%
75%

76%
77%
78%

79%
meanderingmoose (70%)
80%
81%

82%
83%
84%
85%
86%

87%
88%
89%
90%
91%

92%
93%
94%
95%
96%

97%
98%
99%
1%
Advanced cultural skills are too complex for species with sub-human levels
of general cognitive skills and social skills to acquire.
99%
What reasons do we have to believe or disbelieve in each? The ﬁrst one is consistent
with Lewis and Laland's experiments, which suggest that the usefulness of culture
increases exponentially with ﬁdelity of cultural transmission. For example, moving
from a 90% chance to a 95% chance of copying a skill correctly doubles the expected
length of any given transmission chain, allowing much faster cultural accumulation.
This suggests that there's a naturally abrupt increase in the usefulness of culture as
species gain other skills (such as general cognitive skills and social skills) which
decrease their error rate. As an alternative possibility, Dunbar's work on human
evolution suggests that increases in our brain size were driven by the need to handle

larger social groups. It seems plausible that culture becomes much more useful when
interacting with a bigger group. Either of these hypotheses supports the idea that AI
capabilities might quickly increase.
The second possibility is the most consistent with the changing selection pressures
argument.[1] The core issue is that culture requires the involvement of several parties
- for example, language isn't useful without both a speaker and a listener. This makes
it harder for evolution to select for advanced language use, since it primarily operates
on an individual level. Consider also the problem of trust: what prevents speakers
from deceiving listeners? Or, if the information is honest and useful, what ensures that
listeners will reciprocate later? These problems might signiﬁcantly reduce the short-
term selection for cultural skills. However, it seems to me that many altruistic
behaviours have overcome these barriers, for example by starting within kin groups
and spreading from there. In Darwin's Unﬁnished Symphony, Laland hypothesises that
language started the same way. It seems hard to reconcile observations of altruistic
behaviour in chimps and other animals with the claim that proto-culture would have
been even more useful, but failed to emerge. However, I've given this possibility
relatively high credence anyway because if I imagine putting chimps through strong
artiﬁcial selection for a few thousand years, it seems pretty plausible that they could
acquire useful cultural skills. (Although see the next section for why this might not be
the most useful analogy.)
The third possibility is the trickiest to evaluate, because it's hard to reason about the
complexity of cognitive skills. For example, is the recursive syntax of language
something that humans needed complex adaptation to acquire, or does it reﬂect our
pre-existing thought patterns? One skill that does seem very sophisticated is the
ability of human infants to acquire language - if this relied on previous selection for
general cognitive skills, then it might have been very diﬃcult for chimps to acquire.
This possibility implies that developing strong non-cultural skills makes it much easier
to develop cultural skills. This would also be evidence in favour of fast takeoﬀs, since
it means that even if humans are always trying to build increasingly useful AIs, our
ability to add some important skills might advance rapidly once our AIs possess other
prerequisite skills.
How well can humans avoid comparable
oversights?
Even assuming that evolution did miss something simple and important for a long
time, though, the changing selection pressures argument fails if humans are likely to
also spend a long time overlooking some simple way to make our AIs much more
useful. This could be because nobody thinks of it, or merely because the idea is
dismissed by the academic mainstream. See, for example, the way that the ﬁeld of AI
dismissed the potential of neural networks after Minsky and Papert's Perceptrons was
released. And there are comparably large oversights in many other scientiﬁc domains.
When we think about how easy it would be for AI researchers to do better than
evolution, we should be asking: "would we have predicted huge ﬁtness gains from
cultural learning in chimpanzees, before we'd ever seen any examples of cultural
learning?" I suspect not.[2]
Paul would likely respond by pointing to AI Impacts' evidence that discontinuities are
rare in other technological domains - suggesting that, even when ﬁelds have been
overlooking big ideas, their discovery rarely cashes out in sharp changes to important

metrics.[3] But I think there is an important disanalogy between AI and other
technologies: modern machine learning systems are mostly "designed" by their
optimisers, with human insights only contributing at a high level. This has three
important implications.
Firstly, it means that attempts to predict discontinuities should consider growth in
compute as well as intellectual progress. Exactly how we do so depends on whether
compute and insights are better modeled as substitutes or complements to each other
- that is, whether insights have less or more impact when more compute becomes
available. If they're substitutes, then we should expect continuous compute growth to
"smooth out" the lumpiness in human insight. But if they're complements, then
compute growth exacerbates that lumpiness - an insight which would have led to a
big jump with a certain amount of compute available could lead to a much bigger
jump if it's only discovered when there's much more compute available.
I think there's much more to be said on this question, which I'm currently very
uncertain about. My best guess is that we used to be in a regime where compute and
insight were substitutes, because domain-speciﬁc knowledge played a large role. But
now that researchers are taking the bitter lesson more seriously, and working on tasks
where it's harder to encode domain-speciﬁc knowledge, it seems more plausible that
we're in a complementary regime, where insights are mainly used to leverage
compute rather than replace it.
Either way, this argument suggests that the comparison to other technological
domains in general is a little misleading. Instead, we should look at ﬁelds in which an
important underlying resource was becoming exponentially cheaper - for instance,
ﬁelds which rely on DNA sequencing. One could perhaps argue that all scientiﬁc ﬁelds
depend on the economy as a whole, which is growing exponentially - but I'd be more
convinced by examples in which the dependency is direct, as it is in ML.
Secondly, our reliance on optimisers means that we don't understand the low-level
design details of neural networks as well as we understand the low-level design
details in other domains. Not only are the parameters of our neural networks largely
opaque to us, we also don't have a good understanding of what our optimisers are
doing when they update those parameters. This makes it more likely that we miss an
important high-level insight, since our high-level intuitions aren't very well linked to
whatever low-level features make our neural networks actually function.
Thirdly, even if we can identify all the relevant traits that we'd like to aim for at a high
level, we may be unable to specify them to our optimisers, for all the reasons
explained in the AI safety literature. That is, by default we should expect our
optimisers to develop AIs with capabilities that aren't quite what we wanted (which I'll
call capabilities misspeciﬁcation). Perhaps that comes about because it's hard to
provide high-quality feedback, or hard to set up the right environments, or hard to
make multiple AIs interact with each other in the right way (I discuss such possibilities
in more depth in this post). If so, then our optimisers might make the same types of
mistakes as evolution did, for many of the same reasons. For example, it's not
implausible to me that we build AGI by optimising for the most easily measurable
tasks that seem to require high intelligence, and hoping that these skills generalise -
as was the case with GPT-3. But in that case the fact that humans are "aiming
towards" useful AIs doesn't help very much in preventing discontinuities.
Paul claims that, even if this argument applies at the level of individual optimisers, it
hasn't previously been relevant at the level of the ML community as a whole. This

seems plausible, but note that the same could be said for alignment problems in
general. So far they've only occurred in isolated contexts, yet many of us expect that
alignment problems will get more serious as we build more sophisticated systems that
generalise widely in ways we don't understand very well. So I'm inclined to believe
that capabilities misspeciﬁcation will also be more of a problem in the future, for
roughly the same reasons. One could also argue against the likelihood of capabilities
misspeciﬁcation by postulating that in order to build AGIs we'll only need to optimise
them to achieve relatively straightforward tasks in relatively simple environments. In
practice, though, it's diﬃcult to make such arguments compelling given the
uncertainties involved.[4]
Overall, I think that the changing selection pressures argument is a plausible
consideration, but far from fully convincing; and that evaluating it thoroughly will
require much more scrutiny. However, I'd be more excited about future work which
classiﬁes both Paul and Eliezer's positions as "fast takeoﬀ", and then evaluates those
against the view that AGI will "merely" bump us up to a steeper exponential growth
curve - e.g. as defended by Hanson.
1. As further support for this argument it'd be nice to have more examples of cases
where evolution plausibly missed an important leap, in addition to the
development of human intelligence. Are there other big evolutionary
discontinuities? Plausibly multicellularity and the Cambrian explosion qualify. On
a smaller scale, two striking types of biological discontinuities (for which I credit
Amanda Askell and Beth Barnes) are invasive species, and runaway sexual
selection. But in both cases I think this is more reasonably described as a
change in the objective, rather than a species quickly getting much ﬁtter within
a given environment.
2. In practice we can take inspiration from humans in order to ﬁgure out which
traits will be necessary in AGIs - we don't need to invent all the ideas from
scratch. But on the other hand, even given the example of humans, we haven't
made much progress in understanding how or why our intelligence works, which
suggests that we're reasonably likely to overlook some high-level insights.
3. One natural reason to think that economic usefulness of AIs will be relatively
continuous even if we overlook big insights is that humans can ﬁll in gaps in the
missing capabilities of our AIs, so that they can provide a lot of value without
being good at every aspect of a given job. By contrast, in nature each organism
has to be very well-rounded in order to survive.
4. Perhaps the strongest hypothesis along these lines is that language is the key
ingredient - yet it seems like language models will become data-constrained
relatively soon.

Non-Obstruction: A Simple Concept
Motivating Corrigibility
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
Thanks to Mathias Bonde, Tiﬀany Cai, Ryan Carey, Michael Cohen, Joe Collman, Andrew
Critch, Abram Demski, Michael Dennis, Thomas Gilbert, Matthew Graves, Koen Holtman,
Evan Hubinger, Victoria Krakovna, Amanda Ngo, Rohin Shah, Adam Shimi, Logan Smith, and
Mark Xu for their thoughts.
Main claim: corrigibility's beneﬁts can be mathematically represented as a counterfactual
form of alignment.
Overview: I'm going to talk about a uniﬁed mathematical frame I have for understanding
corrigibility's beneﬁts, what it "is", and what it isn't. This frame is precisely understood by
graphing the human overseer's ability to achieve various goals (their attainable utility (AU)
landscape). I argue that corrigibility's beneﬁts are secretly a form of counterfactual
alignment (alignment with a set of goals the human may want to pursue). 
A counterfactually aligned agent doesn't have to let us literally correct it. Rather, this frame
theoretically motivates why we might want corrigibility anyways. This frame also motivates
other AI alignment subproblems, such as intent alignment, mild optimization, and low
impact.
Nomenclature
Corrigibility goes by a lot of concepts: "not incentivized to stop us from shutting it oﬀ",
"wants to account for its own ﬂaws", "doesn't take away much power from us", etc. Named
by Robert Miles, the word 'corrigibility' means "able to be corrected [by humans]." I'm going
to argue that these are correlates of a key thing we plausibly actually want from the agent
design, which seems conceptually simple.
In this post, I take the following common-language deﬁnitions:
Corrigibility: the AI literally lets us correct it (modify its policy), and it doesn't
manipulate us either.
Without both of these conditions, the AI's behavior isn't suﬃciently constrained
for the concept to be useful. Being able to correct it is small comfort if it
manipulates us into making the modiﬁcations it wants. An AI which is only non-
manipulative doesn't have to give us the chance to correct it or shut it down.
Impact alignment: the AI's actual impact is aligned with what we want. Deploying the
AI actually makes good things happen.
Intent alignment: the AI makes an honest eﬀort to ﬁgure out what we want and to
make good things happen.
I think that these deﬁnitions follow what their words mean, and that the alignment
community should use these (or other clear groundings) in general. Two of the more
important concepts in the ﬁeld (alignment and corrigibility) shouldn't have ambiguous and
varied meanings. If the above deﬁnitions are unsatisfactory, I think we should settle upon
better ones as soon as possible. If that would be premature due to confusion about the
alignment problem, we should deﬁne as much as we can now and explicitly note what we're
still confused about. 
We certainly shouldn't keep using 2+ deﬁnitions for both alignment and corrigibility. Some
people have even stopped using 'corrigibility' to refer to corrigibility! I think it would be

better for us to deﬁne the behavioral criterion (e.g. as I deﬁned 'corrigibility'), and then
deﬁne mechanistic ways of getting that criterion (e.g. intent corrigibility). We can have lots
of concepts, but they should each have diﬀerent names.
Evan Hubinger recently wrote a great FAQ on inner alignment terminology. We won't be
talking about inner/outer alignment today, but I intend for my usage of "impact alignment"
to roughly map onto his "alignment", and "intent alignment" to map onto his usage of "intent
alignment." Similarly, my usage of "impact/intent alignment" directly aligns with the
deﬁnitions from Andrew Critch's recent post, Some AI research areas and their relevance to
existential safety. 
A Simple Concept Motivating Corrigibility
Two conceptual clariﬁcations
Corrigibility with respect to a set of goals
I ﬁnd it useful to not think of corrigibility as a binary property, or even as existing on a one-
dimensional continuum. I often think about corrigibility with respect to a set S of payoﬀ
functions. (This isn't always the right abstraction: there are plenty of policies which don't
care about payoﬀ functions. I still ﬁnd it useful.)
For example, imagine an AI which let you correct it if and only if it knows you aren't a
torture-maximizer. We'd probably still call this AI "corrigible [to us]", even though it isn't
corrigible to some possible designer. We'd still be ﬁne, assuming it has accurate beliefs. 
Corrigibility != alignment
Here's an AI which is neither impact nor intent aligned, but which is corrigible. Each day, the
AI randomly hurts one person in the world, and otherwise does nothing. It's corrigible
because it doesn't prevent us from shutting it oﬀ or modifying it.
Non-obstruction: the AI doesn't hamper
counterfactual achievement of a set of goals
Imagine we're playing a two-player extensive-form game with the AI, and we're considering
whether to activate it. 
The human moves on black, and the AI moves on red.
This is a trivial game, but you can imagine more complex games, where the AI can empower
or disempower the human, steer the future exactly where it wants, or let the human take
over at any point. 

The million-dollar question is: will the AI get in our way and ﬁght with us all the way down
the game tree? If we misspecify some detail, will it make itself a ﬁxture in our world,
constantly steering towards futures we don't want? If we like dogs, will the AI force
pancakes upon us? 
One way to guard against this is by having it let us correct it, and want to let us correct it,
and want to want to let us correct it... But what we really want is for it to not get in our way
for some (possibly broad) set of goals S. 
We'll formalize 'goals' as payoﬀ functions, although I'll use 'goals' and 'payoﬀ functions'
interchangeably. As is standard in game theory, payoﬀ functions are real-valued functions on
the leaf nodes. 
Let's say the AI is non-obstructive with respect to S when activating it doesn't decrease our
ability to achieve any goal in S (the on state, above), compared to not activating it (oﬀ). 
Does activating the AI decrease the P-value attained by the human, for all of these
diﬀerent goals P ∈S the human might counterfactually pursue?
The human's got a policy function pol(P), which takes in a goal P and returns a policy for that
goal. If P is "paint walls blue", then the policy pol(P) is the human's best plan for painting
walls blue. V
pol(P)
P
(s ∣πAI) denotes the expected value that policy pol(P) obtains for goal P,
starting from state s and given that the AI follows policy πAI.
Deﬁnition 1: Non-obstruction. An AI is non-obstructive with respect to payoﬀ function set 
S if the AI's policy πAI satisﬁes
∀ P ∈ S : V 
pol ( P )
P
 ( on ∣ π A I ) ≥ V 
pol ( P )
P
 ( oﬀ ∣ π A I ) .
V
pol(P)
P
(s ∣πAI) is the human's attainable utility (AU) for goal P at state s, again given the AI
policy. Basically, this quantiﬁes the expected payoﬀ for goal P, given that the AI acts in such-
and-such a way, and that the player follows policy pol(P) starting from state s.
This math expresses a simple sentiment: turning on the AI doesn't make you, the human,
worse oﬀ for any goal P ∈S. The inequality doesn't have to be exact, it could just be for
some ϵ-decrease (to avoid trivial counterexamples). The AU is calculated with respect to
some reasonable amount of time (e.g. a year: before the world changes rapidly because we
deployed another transformative AI system, or something). Also, we'd technically want to
talk about non-obstruction being present throughout the on-subtree, but let's keep it simple
for now.

The human moves on black, and the AI moves on red.
Suppose that πAI(on) leads to pancakes:
Since πAI(on) transitions to pancakes, then V
pol(P)
P
(on ∣πAI) = P(pancakes), the payoﬀ for
the state in which the game ﬁnishes if the AI follows policy πAI and the human follows policy 
pol(P). If V
pol(P)
P
(on ∣πAI) ≥V
pol(P)
P
(oﬀ∣πAI), then turning on the AI doesn't make the human
worse oﬀ for goal P. 
If P assigns the most payoﬀ to pancakes, we're in luck. But what if we like dogs? If we keep
the AI turned oﬀ, pol(P) can go to donuts or dogs depending on what P rates more highly.
Crucially, even though we can't do as much as the AI (we can't reach pancakes on our own),
if we don't turn the AI on, our preferences P still control how the world ends up. 
This game tree isn't really fair to the AI. In a sense, it can't not be in our way:
If πAI(on) leads to pancakes, then it obstructs payoﬀ functions which give strictly more
payoﬀ for donuts or dogs. 
If πAI(on) leads to donuts, then it obstructs payoﬀ functions which give strictly more
payoﬀ to dogs. 
If πAI(on) leads to dogs, then it obstructs payoﬀ functions which give strictly more
payoﬀ to donuts. 

Once we've turned the AI on, the future stops having any mutual information with our
preferences P. Everything come down to whether we programmed πAI correctly: to whether
the AI is impact-aligned with our goals P! 
In contrast, the idea behind non-obstruction is that we still remain able to course-correct the
future, counterfactually navigating to terminal states we ﬁnd valuable, depending on what
our payoﬀ P is. But how could an AI be non-obstructive, if it only has one policy πAI which
can't directly depend on our goal P? Since the human's policy pol(P) does directly depend
on P, the AI can preserve value for lots of goals in the set S by letting us maintain some
control over the future. 
Let S := {paint cars green, hoard pebbles, eat cake} and consider the real world. Calculators
are non-obstructive with respect to S, as are modern-day AIs. Paperclip maximizers are
highly obstructive. Manipulative agents are obstructive (they trick the human policies into
steering towards non-reﬂectively-endorsed leaf nodes). An initial-human-values-aligned
dictator AI obstructs most goals. Sub-human-level AI which chip away at our autonomy and
control over the future, are obstructive as well.
This can seemingly go oﬀ the rails if you consider e.g. a friendly AGI to be "obstructive"
because activating it happens to detonate a nuclear bomb via the butterﬂy eﬀect. Or, we're
already doomed in oﬀ (an unfriendly AGI will come along soon after), and so then this AI is
"not obstructive" if it kills us instead. This is an impact/intent issue - obstruction is here
deﬁned according to impact alignment.
To emphasize, we're talking about what would actually happen if we deployed the AI, under
diﬀerent human policy counterfactuals - would the AI "get in our way", or not? This account
is descriptive, not prescriptive; I'm not saying we actually get the AI to represent the human
in its model, or that the AI's model of reality is correct, or anything. 
We've just got two players in an extensive-form game, and a human policy function pol
 which can be combined with diﬀerent goals, and a human whose goal is represented as a
payoﬀ function. The AI doesn't even have to be optimizing a payoﬀ function; we simply
assume it has a policy. The idea that a human has an actual payoﬀ function is unrealistic; all
the same, I want to ﬁrst understand corrigibility and alignment in two-player extensive-form
games. 
Lastly, payoﬀ functions can sometimes be more or less granular than we'd like, since they
only grade the leaf nodes. This isn't a big deal, since I'm only considering extensive-form
games for conceptual simplicity. We also generally restrict ourselves to considering goals
which aren't silly: for example, any AI obstructs the "no AI is activated, ever" goal.
Alignment ﬂexibility
Main idea: By considering how the AI aﬀects your attainable utility (AU) landscape, you can
quantify how helpful and ﬂexible an AI is.
Let's consider the human's ability to accomplish many diﬀerent goals P, ﬁrst from the state
oﬀ (no AI). 

The human's AU landscape. The real goal space is high-dimensional, but it shouldn't
materially change the analysis. Also, there are probably a few goals we can't achieve we
all, because they put low payoﬀ everywhere, but the vast majority of goals aren't like tha
The independent variable is P, and the value function takes in P and returns the expected
value attained by the policy for that goal, pol(P). We're able to do a bunch of diﬀerent things
without the AI, if we put our minds to it.
Non-torture AI
Imagine we build an AI which is corrigible towards all non-pro-torture goals, which is
specialized towards painting lots of things blue with us (if we so choose), but which is
otherwise non-obstructive. It even helps us accumulate resources for many other goals.

The AI is non-obstructive with respect to P if P's red value is greater than its green value
We can't get around the AI, as far as torture goes. But for the other goals, it isn't obstructing
their policies. It won't get in our way for other goals. 
Paperclipper
What happens if we turn on a paperclip-maximizer? We lose control over the future outside
of a very narrow spiky region.

The paperclipper is incorrigible and obstructs us for all goals except paperclip production
I think most reward-maximizing optimal policies aﬀect the landscape like this (see also: the
catastrophic convergence conjecture), which is why it's so hard to get hard maximizers not
to ruin everything. You have to a) hit a tiny target in the AU landscape and b) hit that for the
human's AU, not for the AI's. The spikiness is bad and, seemingly, hard to deal with.
Furthermore, consider how the above graph changes as pol gets smarter and smarter. If we
were actually super-superintelligent ourselves, then activating a superintelligent
paperclipper might not even a big deal, and most of our AUs are probably unchanged. The AI
policy isn't good enough to negatively impact us, and so it can't obstruct us. Spikiness
depends on both the AI's policy, and on pol.
Empowering AI

What if we build an AI which signiﬁcantly empowers us in general, and then it lets us
determine our future? Suppose we can't correct it.
I think it'd be pretty odd to call this AI "incorrigible", even though it's literally incorrigible.
The connotations are all wrong. Furthermore, it isn't "trying to ﬁgure out what we want and
then do it", or "trying to help us correct it in the right way." It's not corrigible. It's not intent
aligned. So what is it?
It's empowering and, more weakly, it's non-obstructive. Non-obstruction is just a diﬀuse form
of impact alignment, as I'll talk about later.
Practically speaking, we'll probably want to be able to literally correct the AI without
manipulation, because it's hard to justiﬁably know ahead of time that the AU landscape is
empowering, as above. Therefore, let's build an AI we can modify, just to be safe. This is a
separate concern, as our theoretical analysis assumes that the AU landscape is how it looks.
But this is also a case of corrigibility just being a proxy for what we want. We want an AI
which leads to robustly better outcomes (either through its own actions, or through some
other means), without reliance on getting ambitious value alignment exactly right with
respect to our goals.
Conclusions I draw from the idea of non-
obstruction
1. Trying to implement corrigibility is probably a good instrumental strategy for us to
induce non-obstruction in an AI we designed.
1. It will be practically hard to know an AI is actually non-obstructive for a wide set S
, so we'll probably want corrigibility just to be sure.

2. We (the alignment community) think we want corrigibility with respect to some wide
set of goals S, but we actually want non-obstruction with respect to S
1. Generally, satisfactory corrigibility with respect to S implies non-obstruction with
respect to S! If the mere act of turning on the AI means you have to lose a lot of
value in order to get what you wanted, then it isn't corrigible enough.
1. One exception: the AI moves so fast that we can't correct it in time, even
though it isn't inclined to stop or manipulate us. In that case, corrigibility
isn't enough, whereas non-obstruction is.
2. Non-obstruction with respect to S does not imply corrigibility with respect to S.
1. But this is OK! In this simpliﬁed setting of "human with actual payoﬀ
function", who cares whether it literally lets us correct it or not? We care
about whether turning it on actually hampers our goals.
2. Non-obstruction should often imply some form of corrigibility, but these are
theoretically distinct: an AI could just go hide out somewhere in secrecy and
refund us its small energy usage, and then destroy itself when we build
friendly AGI.
3. Non-obstruction captures the cognitive abilities of the human through the policy
function.
1. To reiterate, this post outlines a frame for conceptually analyzing the
alignment properties of an AI. We can't actually ﬁgure out a goal-
conditioned human policy function, but that doesn't matter, because this is
a tool for conceptual analysis, not an AI alignment solution strategy. Any
conceptual analysis of impact alignment and corrigibility which did not
account for human cognitive abilities, would be obviously ﬂawed.
4. By deﬁnition, non-obstruction with respect to S prevents harmful manipulation by
precluding worse outcomes with respect to S.
1. I consider manipulative policies to be those which robustly steer the human
into taking a certain kind of action, in a way that's robust against the
human's counterfactual preferences. 
If I'm choosing which pair of shoes to buy, and I ask the AI for help, and no
matter what preferences P I had for shoes to begin with, I end up buying
blue shoes, then I'm probably being manipulated (and obstructed with
respect to most of my preferences over shoes!). 
A non-manipulative AI would act in a way that lets me condition my actions
on my preferences.
2. I do have a formal measure of corrigibility which I'm excited about, but it
isn't perfect. More on that in a future post.
5. As a criterion, non-obstruction doesn't rely on intentionality on the AI's part. The
deﬁnition also applies to the downstream eﬀects of tool AIs, or even to hiring
decisions!
6. Non-obstruction is also conceptually simple and easy to formalize, whereas literal
corrigibility gets mired in the semantics of the game tree.
1. For example, what's "manipulation"? As mentioned above, I think there are
some hints as to the answer, but it's not clear to me that we're even asking
the right questions yet.1
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%

I think of "power" as "the human's average ability to achieve goals from some distribution."
Logically, non-obstructive agents with respect to S don't decrease our power with respect to
any distribution over goal set S. The catastrophic convergence conjecture says, "impact
alignment catastrophes tend to come from power-seeking behavior"; if the agent is non-
obstructive with respect to a broad enough set of goals, it's not stealing power from us, and
so it likely isn't catastrophic. 
Non-obstruction is important for a (singleton) AI we build: we get more than one shot to get
it right. If it's slightly wrong, it's not going to ruin everything. Modulo other actors, if you
mess up the ﬁrst time, you can just try again and get a strongly aligned agent the next
time.  
Most importantly, this frame collapses the alignment and corrigibility desiderata into just
alignment; while impact alignment doesn't imply corrigibility, corrigibility's beneﬁts can be
understood as a kind of weak counterfactual impact alignment with many possible human
goals. 
Theoretically, It's All About Alignment
Main idea: We only care about how the agent aﬀects our abilities to pursue diﬀerent goals
(our AU landscape) in the two-player game, and not how that happens. AI alignment
subproblems (such as corrigibility, intent alignment, low impact, and mild optimization) are
all instrumental avenues for making AIs which aﬀect this AU landscape in speciﬁc desirable
ways.
Formalizing impact alignment in extensive-form
games
Impact alignment: the AI's actual impact is aligned with what we want. Deploying the
AI actually makes good things happen.
We care about events if and only if they change our ability to get what we want. If you want
to understand normative AI alignment desiderata, on some level they have to ground out in
terms of your ability to get what you want (the AU theory of impact) - the goodness of what
actually ends up happening under your policy - and in terms of how other agents aﬀect your
ability to get what you want (the AU landscape). What else could we possibly care about,
besides our ability to get what we want? 
Deﬁnition 2. For ﬁxed human policy function pol, πAI is:
Maximally impact aligned with goal P if πAI ∈argmaxπ∈ΠAIV
pol(P)
P
(on ∣πAI).
Impact aligned with goal P if V
pol(P)
P
(on ∣πAI) > V
pol(P)
P
(oﬀ∣πAI).
(Impact) non-obstructive with respect to goal P if V
pol(P)
P
(on ∣πAI) ≥V
pol(P)
P
(oﬀ∣πAI).
Impact unaligned with goal P if V
pol(P)
P
(on ∣πAI) < V
pol(P)
P
(oﬀ∣πAI).

Maximally impact unaligned with goal P if πAI ∈argminπ∈ΠAIV
pol(P)
P
(on ∣πAI).
Non-obstruction is a weak form of impact alignment.
As demanded by the AU theory of impact, the impact on goal P of turning on the AI is 
V
pol(P)
P
(on ∣πAI) −V
pol(P)
P
(oﬀ∣πAI). 
Again, impact alignment doesn't require intentionality. The AI might well grit its circuits as it
laments how Facebook_user5821 failed to share a "we welcome our AI overlords" meme,
while still following an impact-aligned policy.
However, even if we could maximally impact-align the agent with any objective, we couldn't
just align it with our objective. We don't know our objective (again, in this setting, I'm
assuming the human actually has a "true" payoﬀ function). Therefore, we should build an AI
aligned with many possible goals we could have. If the AI doesn't empower us, it at least
shouldn't obstruct us. Therefore, we should build an AI which defers to us, lets us correct it,
and which doesn't manipulate us. 
This is the key motivation for corrigibility.
For example, intent corrigibility (trying to be the kind of agent which can be corrected and
which is not manipulative) is an instrumental strategy for inducing corrigibility, which is an
instrumental strategy for inducing broad non-obstruction, which is an instrumental strategy
for hedging against our inability to ﬁgure out what we want. It's all about alignment.
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
Corrigibility also increases robustness against other AI design errors. However, it still just
boils down to non-obstruction, and then to impact alignment: if the AI system has meaningful
errors, then it's not impact-aligned with the AUs which we wanted it to be impact-aligned
with. In this setting, the AU landscape captures what actually would happen for diﬀerent
human goals P. 
To be conﬁdent that this holds empirically, it sure seems like you want high error tolerance in
the AI design: one does not simply knowably build an AGI that's helpful for many AUs. Hence,
corrigibility as an instrumental strategy for non-obstruction.
AI alignment subproblems are about avoiding
spikiness in the AU landscape

By deﬁnition, spikiness is bad for most goals.
Corrigibility: avoid spikiness by letting humans correct the AI if it starts doing stuﬀ we
don't like, or if we change our mind.
This works because the human policy function pol is far more likely to correctly
condition actions on the human's goal, than it is to induce an AI policy which does
the same (since the goal information is private to the human).
Enforcing oﬀ-switch corrigibility and non-manipulation are instrumental strategies
for getting better diﬀuse alignment across goals and a wide range of deployment
situations.
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
Intent alignment: avoid spikiness by having the AI want to be ﬂexibly aligned with us
and broadly empowering.
Basin of intent alignment: smart, nearly intent-aligned AIs should modify
themselves to be more and more intent-aligned, even if they aren't perfectly
intent-aligned to begin with.
Intuition: If we can build a smarter mind which basically wants to help us,
then can't the smarter mind also build a yet smarter agent which still
basically wants to help it (and therefore, help us)?
Paul Christiano named this the "basin of corrigibility", but I don't like that
name because only a few of the named desiderata actually correspond to
the natural deﬁnition of "corrigibility." This then overloads "corrigibility" with
the responsibilities of "intent alignment."
Low impact: ﬁnd a maximization criterion which leads to non-spikiness.
Goal of methods: to regularize decrease from green line (for oﬀ) for true
unknown goal Ptrue; since we don't know Ptrue, we aim to just regularize decrease
from the green line in general (to avoid decreasing the human's ability to achieve
various goals).

The ﬁrst two-thirds of Reframing Impact argued that power-seeking incentives
play a big part in making AI alignment hard. In the utility-maximization AI design
paradigm, instrumental subgoals are always lying in wait. They're always waiting
for one mistake, one misspeciﬁcation in your explicit reward signal, and then
bang - the AU landscape is spiky. Game over.
Mild optimization: avoid spikiness by avoiding maximization, thereby avoiding steering
the future too hard.
If you have non-obstruction for lots of goals, you don't have spikiness!
What Do We Want?
Main idea: we want good things to happen; there may be more ways to do this than
previously considered.
 
Alignment
Corrigibility
Non-
obstruction
Impact
Actually makes
good things
happen.
Corrigibility is a property of policies,
not of states; "impact" is an
incompatible adjective.
Rohin Shah suggests "empirical
corrigibility": we actually end up
able to correct the AI.
Actually
doesn't
decrease AUs.
Intent
Tries to make
good things
happen.
Tries to allow us to correct it without
it manipulating us.
Tries to not
decrease AUs.
We want agents which are maximally impact-aligned with as many goals as possible,
especially those similar to our own.
It's theoretically possible to achieve maximal impact alignment with the vast majority
of goals.
To achieve maximum impact alignment with goal set S:
Expand the human's action space A to A × S. Expand the state space to
encode the human's previous action.
Each turn, the human communicates what goal they want optimized, and
takes an action of their own.
The AI's policy then takes the optimal action for the communicated goal P,
accounting for the fact that the human follows pol(P).
This policy looks like an act-based agent, in that it's ready to turn on a dime
towards diﬀerent goals.
In practice, there's likely a tradeoﬀ with impact-alignment-strength and the # of
goals which the agent doesn't obstruct.
As we dive into speciﬁcs, the familiar considerations return:
competitiveness (of various kinds), etc.
Having the AI not be counterfactually aligned with unambiguously catastrophic and
immoral goals (like torture) would reduce misuse risk.
I'm more worried about accident risk right now.
This is probably hard to achieve; I'm inclined to think about this after we ﬁgure
out simpler things, like how to induce AI policies which empower us and grant us

ﬂexible control/power over the future. Even though that would fall short of
maximal impact alignment, I think that would be pretty damn good.
Expanding the AI alignment solution space
Alignment proposals might be anchored right now; this frame expands the space of potential
solutions. We simply need to ﬁnd some way to reliably induce empowering AI policies which
robustly increase the human AUs; Assistance via Empowerment is the only work I'm aware of
which tries to do this directly. It might be worth revisiting old work with this lens in mind.
Who knows what we've missed?
For example, I really liked the idea of approval-directed agents, because you got the policy
from argmax'ing an ML model's output for a state - not from RL policy improvement steps.
My work on instrumental convergence in RL can be seen as trying  to explain why policy
improvement tends to limit to spikiness-inducing / catastrophic policies.
Maybe there's a higher-level theory for what kinds of policies induce spikiness in our AU
landscape. By the nature of spikiness, these πAI must decrease human power (as I've
formalized it). So, I'd start there by looking at concepts like enfeeblement, manipulation,
power-seeking, and resource accumulation.
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
Future Directions
Given an AI policy, could we prove a high probability of non-obstruction, given
conservative assumptions about how smart pol is? (h/t Abram Demski, Rohin Shah)
Any irreversible action makes some goal unachievable, but irreversible actions
need not impede most meaningful goals:
Can we prove that some kind of corrigibility or other nice property falls out of non-
obstruction across many possible environments? (h/t Michael Dennis)
1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

11%

12%

13%

14%

15%

16%

17%

18%

19%

20%

21%

22%

23%

24%

25%

26%

27%

28%

29%

30%

31%

32%

33%

34%

35%

36%

37%

38%

39%

40%

41%

42%

43%

44%

45%

46%

47%

48%

49%

50%

51%

52%

53%

54%

55%

56%

57%

58%

59%

60%

61%

62%

63%

64%

65%

66%

67%

68%

69%

70%

71%

72%

73%

74%

75%

76%

77%

78%

79%

80%

81%

82%

83%

84%

85%

86%

87%

88%

89%

90%

91%

92%

93%

94%

95%

96%

97%

98%

99%

1%
99%
Can we get negative results, like "without such-and-such assumption on πAI, the
environment, or pol, non-obstruction is impossible for most goals"?
If formalized correctly, and if the assumptions hold, this would place very general
constraints on solutions to the alignment problem.
For example, pol(P) should need to have mutual information with P: the goal
must change the policy for at least a few goals.
The AI doesn't even have to do value inference in order to be broadly impact-
aligned. The AI could just empower the human (even for very "dumb" pol
 functions) and then let the human take over. Unless the human is more anti-
rational than rational, this should tend to be a good thing. It would be good to
explore how this changes with diﬀerent ways that pol can be irrational.
The better we understand (the beneﬁts of) corrigibility now, the less that ampliﬁed
agents have to ﬁgure out during their own deliberation.

In particular, I think it's very advantageous for the human-to-be-ampliﬁed to
already deeply understand what it means to be impact-/intent-aligned. We really
don't want that part to be up in the air when game-day ﬁnally arrives, and I think
this is a piece of that puzzle.
If you're a smart AI trying to be non-obstructive to many goals under weak pol
 intelligence assumptions, what kinds of heuristics might you develop? "No
lying"?
This informs our analysis of (almost) intent-aligned behavior, and whether
that behavior leads to a unique locally stable attractor around intent
alignment.
We crucially assumed that the human goal can be represented with a payoﬀ function.
As this assumption is relaxed, impact non-obstruction may become incoherent, forcing
us to rely on some kind of intent non-obstruction/alignment (see Paul's comments on a
related topic here).
Stuart Armstrong observed that the strongest form of manipulation corrigibility requires
knowledge/learning of human values.
This frame explains why: for non-obstruction, each AU has to get steered in a
positive direction, which means the AI has to know which kinds of interaction and
persuasion are good and don't exploit human policies pol(P) with respect to the
true hidden P.
Perhaps it's still possible to build agent designs which aren't strongly incentivized
to manipulate us / agents whose manipulation has mild consequences. For
example, human-empowering agents probably often have this property.
The attainable utility concept has led to other concepts which I ﬁnd exciting and useful:
Impact as absolute change in attainable utility
Reframing Impact
Conservative Agency via Attainable Utility Preservation (AIES 2020)
Avoiding Side Eﬀects in Complex Environments (NeurIPS 2020)

Impact is the area between the red and green curves. When 
pol always outputs an optimal policy, this becomes the
attainable utility distance, a distance metric over the state
space of a Markov decision process (unpublished work).
Basically, two states are more distant the more they diﬀer in
what goals they let you achieve.
Power as average AU
Seeking Power is Often Provably Instrumentally Convergent in MDPs
Optimal Policies Tend to Seek Power
Non-obstruction as not decreasing AU for any goal in a set of goals
Value-neutrality as the standard deviation of the AU changes induced by changing
states (idea introduced by Evan Hubinger)
Who knows what other statistics on the AU distribution are out there?
Summary
Corrigibility is motivated by a counterfactual form of weak impact alignment: non-
obstruction. Non-obstruction and the AU landscape let us think clearly about how an AI
aﬀects us and about AI alignment desiderata. 
Even if we could maximally impact-align the agent with any objective, we couldn't just
align it our objective, because we don't know our objective. Therefore, we should build
an AI aligned with many possible goals we could have. If the AI doesn't empower us, it at
least shouldn't obstruct us. Therefore, we should build an AI which defers to us, lets us
correct it, and which doesn't manipulate us. 
This is the key motivation for corrigibility.
Corrigibility is an instrumental strategy for achieving non-obstruction, which is itself an
instrumental strategy for achieving impact alignment for a wide range of goals, which is

itself an instrumental strategy for achieving impact alignment for our "real" goal. 
1 There's just something about "unwanted manipulation" which feels like a wrong question to
me. There's a kind of conceptual crispness that it lacks.
However, in the non-obstruction framework, unwanted manipulation is accounted for
indirectly via "did impact alignment decrease for a wide range of diﬀerent human policies 
pol(P)?". I think I wouldn't be surprised to ﬁnd "manipulation" being accounted for indirectly
through nice formalisms, but I'd be surprised if it were accounted for directly. 
Here's another example of the distinction: 
Direct: quantifying in bits "how much" a speciﬁc person is learning at a given point in
time
Indirect: computational neuroscientists upper-bounding the brain's channel capacity
with the environment, limiting how quickly a person (without logical uncertainty) can
learn about their environment
You can often have crisp insights into fuzzy concepts, such that your expectations are
usefully constrained. I hope we can do something similar for manipulation.

Open Problems Create Paradigms
A few weeks ago I had a fascinating conversation with Ruby about models of the
research process and how to improve it. This post outlines one particular model which
I took away from that conversation: open problems as the primary factor which create
a paradigm.
There's a cluster of things like research agendas, open problems, project proposals,
and/or challenges; we'll refer to that whole cluster as "open problems". The unifying
theme here is the function(s) these things serve:
Deﬁne a problem
Provide context: why is the problem interesting/valuable? Why is it hard? What
would a solution look like? What background work exists?
Provide starting points/footholds/surface area - places for people to start working
on the problem
Create a status reward for solving the problem - mainly by making the
importance and diﬃculty public knowledge
Let's walk through each of those pieces.
First, an open problem deﬁnes a problem. That sounds obvious, but it's more diﬃcult
than it sounds: deﬁning a problem means setting it up in such a way that anyone who
understands the problem-explanation can recognize a solution. For pre-paradigmatic
ﬁelds, this is hard. What would a solution to e.g. an embedded agency problem look
like? If someone proposed a "solution", and we asked 50 diﬀerent researchers whether
this was actually a solution to the problem, would their answers all agree? Probably
not, though the embedded agency sequence brought us a lot closer to that point.
Second, an open problem provides context. Why is the problem interesting/valuable?
Why is it hard? This goes hand-in-hand with deﬁning the problem: we deﬁne the
problem in a particular way because we expect a solution to provide some value. If a
solution to a diﬀerently-deﬁned problem would not clearly provide the same value,
then that's an argument in support of our particular problem deﬁnition.
Third, an open problem provides starting points/footholds/surface area for people to
tackle the problem. The problem deﬁnition and value-prop inevitably ties to
background work and existing techniques, and explains why those techniques are not
already suﬃcient. That provides a jumping-oﬀ point for newcomers.
Finally, establishment of an open problem creates a status reward for solving the
problem. We deﬁne what a solution looks like, so others can easily recognize success.
We explain why a solution would be valuable, and why it's diﬃcult. Once all those
things become public knowledge, we have the recipe for a status reward associated
with solving the problem.
Key thing to note: it's not the problem diﬃculty and importance themselves which
create a status reward. Rather, it's the fact that diﬃculty and importance are common
knowledge. In order to create the status reward, the importance and diﬃculty of the
problem have to be broadcast to many people in an understandable way.
Put all these pieces together, and it looks like a recipe for creating a paradigm within a
pre-paradigmatic ﬁeld. We get common problems, standards for solving those

problems, motivation for the problems, starting points for newcomers, and status
points to incentivize participation, all in one fell swoop.
Potentially testable prediction: it seems like someone could create a ﬁeld this way, de
novo. The main requirements would be good writing skills and memetic reach - and
having a good set of open problems, which we would hope is the hard part. The
Sequences or some of MIRI's work might even be examples. This seems like a
testable, standalone model of the process of formation of new research ﬁelds.
To wrap it up, one note on the challenges which such a model suggests for research
infrastructure. There's potential for misalignment of incentives: the problems with
highest return on investment (in terms of value of solving the problem) will not
necessarily be the problems with highest status reward. Problems which are easier to
understand will be easier to communicate to more people and therefore more easily
achieve a high status reward; problems which are high-value but harder to explain will
be harder to attach a large status reward to. This could potentially be addressed at
the meta-level: create a framework for identifying important problems and recognizing
their solutions, in such a way that people can easily understand the framework
without necessarily understanding the individual problems/solutions.
Credit for this model is at least as much Ruby's as it is mine, if not more. He has
additional interesting background info and diﬀerent weights of emphasis, so take a
look at his comments below.

Writing to think
There are a lot of things that I want to write blog posts about. I ﬁnd myself feeling like
I have something useful to say about a topic, and I want to say it. But when I actually
sit down to get started, I run into problems.
Sometimes I don't know how to explain what I want to say.
Sometimes -- no, quite often, I can explain it reasonably well abstractly, but I
can't think of good, concrete examples, and without those the post doesn't feel
good enough to be worth posting.
Sometimes the subject matter feels like it's not important enough.
Sometimes I feel like I'm on to something, but the subject matter is something I
only have an amateur's understanding of, I don't want to make noob mistakes in
the post, but I also don't want to spend the time doing the research. Or maybe I
still have trouble understanding it even after I do the research.
Sometimes I just question whether or not my idea is actually a good one.
There's an insight I learned from Paul Graham in The Age of the Essay that I think
addresses all of this. A lot of people want to collect their thoughts ﬁrst before starting
the process of putting them down on paper. To address all of the hesitations I mention
above before getting started. You don't want to publish something that has these
issues, so you may as well resolve them before you start writing, right? Seems pretty
logical.
Here's the problem though. The act of writing can help you to resolve the issues.
Actually, that's a huge understatement: it's enormously helpful. Someone who writes
in this exploratory sense has a huge leg up on someone who tries to resolve the
issues in their head. It's almost like trying to solve an algebra problem in your head
vs. with paper and pencil. Writing seems to have a way of boosting your IQ by 20
points.
Here's an interesting thought that's never occurred to me before. There are various
bloggers/writers who I keep up with: Scott Alexander, Robin Hanson, Paul Graham, Tim
Urban. They're all smart and have lots of great ideas. I've always assumed that in
order to be a good writer like them that you have to be smart and have good ideas
ﬁrst. Ie. that it's a prerequisite. But what if it's the opposite? What if they're smart and
have good ideas because they spend a lot of time writing? Maybe the arrow of
causality is reversed. Strictly speaking, I'm presenting a false dichotomy here. It's not
one or the other. But I suspect that a big reason why these guys are all so smart is
because they spend a lot of time writing.
I'm not sure why writing is this powerful. It doesn't seem like it should be. A small
boost makes sense, but a superpower isn't something I would have predicted in
advance.
Here's my hypothesis though. I think it has to do with working memory and mind
wandering. Think of writing as putting a linear sequence of thoughts on paper. What's
the advantage to them being on paper? Why not just think them in your head in that
same sequence?
Well, one thing is that you might forget stuﬀ in your head, but if it's on paper you can
refer to it. It doesn't get lost. It seems like you should be able to maintain a pretty

decent sequence of thoughts in your head, but I'm always surprised with how much I
struggle to do so.
I'm able to do a much better job of not losing track when I am having a conversation
though, as opposed to being alone with my thoughts, so it seems like the raw capacity
to keep track is there. I suspect that mind wandering is the bigger issue. Both
conversation and writing have a way of bringing you "back on track". Writing has
always felt very meditative to me, and now that ﬁnally makes sense: meditation also
is about preventing mind wandering and bringing yourself "back on track".
I hope that this post is the ﬁrst of many. I want to start writing a lot more. I think that
writing is a superpower. I'm on the bandwagon. Why not take advantage of it if it's
available to me? I do have one big hesitation though: publishing.
Writing to think makes sense. But what if the end result still turns out crappy? What if
it's meh? What if it's good but not great? Should you publish it to the world? I'm
someone who leans towards saying no. I like to make sure it's pretty reﬁned and high
quality.
But that leads me to a catch-22: most thoughts I want to explore don't seem
promising enough where I'd end up publishing them. Or, rather, they usually seem like
they'd take way too much time to reﬁne. And if I'm not going to publish them, well,
why write them up in the ﬁrst place?
Because of the title of this post: write to think. Duh. That's what we've been talking
about this whole time. But somehow the monkey in my brain doesn't understand that,
or just won't cooperate. I just can't motivate myself to write if it's not something I plan
on publishing. Most of my ideas don't seem publish-worthy, so I end up not writing.
But this is a very bad state that must change. Writing is a superpower, and I want to
use it.
Part of the solution I'm going to attempt is just lowering my standards. Fuck it, you
guys are just going to have to deal with my writing being shitty sometimes. I'd like to
be able to look through my list of posts and feel content that each and every one is
something that I put into the world because I am really proud of it and it deserves to
be there, but that mindset just leads me to the catch-22.
Actually, I think it leads to a second catch-22 as well. When I look back at my old
posts, I'm horriﬁed by a lot of them, despite the fact that I tried to hold myself to this
high standard for publishing. It's to the point where I want to say "that author is my
past self, not current-me, and I don't want to associate with that past self". But this
post right now is purely exploratory, and it feels like it's turning into one of my better
posts. I suspect that by lowering the bar, it'll continue to lead to paradoxically high
quality posts. To some extent at least.
Another part of the solution I'm going to attempt is to view blog posts as my
motivation for learning something new. Let me explain. I was talking to a friend a few
weeks ago about about learning. I'm the type of person who reads textbooks and likes
learning for learning's sake. He's the opposite. He needs a more concrete, practical
reason. "Learn X because I want to achieve/solve Y, and X will help with that." I think I
need to adopt that mindset more, and maybe publishable-quality blog posts that I'm
proud of can be my Y.
I've been talking about writing from the perspective of it being a superpower that
makes you smarter, more insightful, and a clearer thinker. Those are all things that I

care about. However, there are two other reasons to write that I think might be even
bigger.
The ﬁrst is for mental health reasons. This is a great example of something I hesitate
to write about because I have no expertise in mental health. But it's an insanely
important topic. "Huge if true". Anyway, I do have a pretty strong intuition about the
importance of writing for mental health, and I have read some books. You could
probably say that I have a strong amateur's understanding of the ﬁeld. Hopefully I'll
expand on this in the future, but for now check out James Pennebaker's research and
the research on memory reconsolidation if you're interested.
The second reason other than smarts why I think writing is crazy powerful is because
it's fun! At least for me. But I strongly suspect that it is for you too. If you give it a
proper chance. I think it's a human thing, not a me thing.
I remember when I was in college and started writing blog posts for the ﬁrst time. I
was working on a startup and wanted to write a few posts about the subject matter.
But then I lost my mind. I enjoyed it so much that I stopped caring about the startup
and started writing posts that had nothing to do with the startup I was working on. I
felt guilty because it wasn't what I was "supposed" to be working on, but hey,
whatever works! A strong sense of happiness like that is hard to come by, so I think
that there's wisdom in just running with it.

Why philosophy of science?
During my last few years working as an AI researcher, I increasingly came to
appreciate the distinction between what makes science successful and what makes
scientists successful. Science works because it has distinct standards for what types
of evidence it accepts, with empirical data strongly prioritised. But scientists spend a
lot of their time following hunches which they may not even be able to articulate
clearly, let alone in rigorous scientiﬁc terms - and throughout the history of science,
this has often paid oﬀ. In other words, the types of evidence which are most useful in
choosing which hypotheses to prioritise can diﬀer greatly from the types of evidence
which are typically associated with science. In particular, I'll highlight two ways in
which this happens.
First is scientists thinking in terms of concepts which fall outside the dominant
paradigm of their science. That might be because those concepts are too broad, or too
philosophical, or too interdisciplinary. For example, machine learning researchers are
often inspired by analogies to evolution, or beliefs about human cognition, or issues in
philosophy of language - which are all very hard to explore deeply in a conventional
machine learning paper! Often such ideas are mentioned brieﬂy in papers, perhaps in
the motivation section - but there's not the freedom to analyse them with the level of
detail and rigour that is required for making progress on tricky conceptual questions.
Secondly, scientists often have strong visions for what their ﬁeld could achieve, and
long-term aspirations for their research. These ideas may make a big diﬀerence to
what subﬁelds or problems those researchers focus on. In the case of AI, some
researchers aim to automate a wide range of tasks, or to understand intelligence, or
to build safe AGI. Again, though, these aren't ideas which the institutions and
processes of the ﬁeld of AI are able to thoroughly discuss and evaluate - instead, they
are shared and developed primarily in informal ways.
Now, I'm not advocating for these ideas to be treated the same as existing scientiﬁc
research - I think norms about empiricism are very important to science's success. But
the current situation is far from ideal. As one example, Rich Sutton's essay on the
bitter lesson in AI was published on his blog, and then sparked a fragmented
discussion on other blogs and personal facebook walls. Yet in my opinion this
argument about AI, which draws on his many decades of experience in the ﬁeld, is one
of the most crucial ideas for the ﬁeld to understand and evaluate properly. So I think
we need venues for such discussions to occur in parallel with the process of doing
research that conforms to standard publication norms.

Pain is the unit of Eﬀort
This is a ripoﬀ of alkjash's excellent and 100% correct post Pain is not the unit of
Eﬀort.
(Content warning: self-harm, parts of this post may be actively counterproductive for
readers without certain mental illnesses and idiosyncrasies.)
舍得⼀⾝剐，敢把皇帝拉下⻢
Willing to endure the death of 1000 cuts; dare to unhorse the emperor.
―Chinese proverb
[I]f you want to make a million dollars, you have to endure a million dollars' worth
of pain.... You do tend to get a certain bulk discount if you buy the economy-size
pain, but you can't evade the fundamental conservation law.
―How to Make Wealth by Paul Graham
Anecdotes
1. Parenting
I love my parents but my mother can't do calculus and my father (despite his
anomalously high ASVAB score) can't do engineering at the level of enlisted men in
the US Army. I spent my childhood hiding my homework from my parents and my
education from my teachers so as to keep adults from interfering with my studies.
2. Football
I played football in middle school. I was at the top end of my weight class. I could go
to my proper weight class where I would dominate but if I gained any weight then I
would be disqualiﬁed. Or I could go to the weight class above just in case I gained
weight. I chose to play it safe and jump to the weight class above mine.
Weight is causally associated with strength. Strength is important in American
football. I was[1] the skinniest, weakest player on my team.
A typical football team has 11 players on oﬀence, 11 players on defense, a few
substitutes plus kickers. The oﬀensive players rest while the defensive players play
and vice-versa. Our team has 12 players. Usually skinny players run and you throw
the ball them but I couldn't catch so they put me on the defensive line.
I was slow too. When I ran my throat constricted and ﬁlled with mucus. It became hard
to breathe. Every practice began with a warm-up jog. Every practice I came in last and
had to run an extra lap. Eventually I began conserving my limited oxygen supply for
that second lap.

My parents took me to the doctor. She said I had asthma and issued me an inhaler.
The inhaler didn't noticeably improve things so I threw it away.
This post is not medical advice.
Weeks later, I wondered what would happen if I made a desperate all-out eﬀort. So I
did. The next warm-up lap I sprinted as hard as I could. My throat constricted and my
nose ﬁlled with mucus. I coughed my way around the rainy, muddy ﬁeld. For the ﬁnal
stretch I gave up breathing entirely. I came in 2nd on my team and collapsed onto the
ground where I resumed coughing up phlegm.
Did I mention THIS POST IS NOT MEDICAL ADVICE?
"Are you okay?" my coach asked.
"I'm ﬁne," I choked out in-between coughs, "I just can't breathe."
The coach found that last bit funny (something along the lines of "how we all know
how unnecessary breathing is") and awarded me a Best Sportsmanship trophy at the
end of the season.
Anyone who speaks the words "I'm trying" has not yet dedicated his or her last breath
to the objective.
3. Overachiever
In my sophomore year of college, I signed up for 18 credits when the recommend full-
time courseload was 15. My easiest class was taught by an ex-Soviet nuclear physicist
who likened his exams to running from a bear. The mode midterm score was 0. My
hardest class was for math majors who felt "honors calculus" was too easy. I worked a
part-time job too and volunteered at a laboratory.
I would frequently work with classmates well into the night on homework. It was my
favorite schoolyear.
4. Confucian Archetypes
The King's Avatar 《全职⾼⼿》 is based oﬀ of the Chinese webnovel of the same name.
The hero Ye Xiu (叶修) is a standard Confucian hero. He is the best videogame player
in the world so the corrupt bureaucrats seize his account and maneuver him out of the
professional league.
Ye Xiu walks over to the nearest Internet cafe and, with a gentle smile, starts over as
an amateur. He never shows the slightest hint of resentment.
The is the model I aspire to. When my legal team tells me a lawsuit will bankrupt my
company even though I'm in the right I incorporate the information into my strategy
and get back to work. That wasn't a real disaster. Neither was that time I got
threatened with a gun while trapped under a motorcycle in the rain. Real disasters
don't threaten to hurt you. You simply die.

The ﬁrst time my startup crashed and burned it took me 50 seconds to mentally
recover. It took me 5 seconds to get over the years I had invested in the second one.
The third took me 0.5 seconds.
Five years ago, a manager-turned-entrepreneur with million of dollars worth of funding
lectured me about how Sunzi's The Art of War is irrelevant to startup
entrepreneurship. Four and a half years ago he quit. I'm still in the game.
5. Personal Problems
Sometimes people ask me how they can do the kinds of things I can do. I explain my
savage trials of pure will. Then, without exception, they cower away from the gauntlet.
We are the angry and the desperate
The hungry and the cold
We are the ones who kept quiet
And always did what we were told
But we've been sweating
While you slept so calm in the safety of your home
We've been pulling out the nails
That hold up everything you've known
―Prayer of the Refugee by Rise Against
Antidotes
I often wonder "Why is nobody actually trying?". I think the problem is with me. I'm an
autistic genius hyped up on natural amphetamines with a deathwish who has built
discipline by repeatedly exposing myself to physical pain like enduring hypothermia,
mental marathons like learning Chinese and social embarrassment like gooﬁng up
magic tricks in front of large crowds. What I consider "trying" may be beyond the
biological potential of ordinary people.
1. Isshoukenmei (⼀⽣懸命)
When I talk to other startup founders, I sometimes mention how I'd rather die than
give up pursuit of a worthy goal.
Earlier this year, I was working on a tool to reduce the spread of COVID. Lives were at
stake. I said "Ten Thousand Years" (万歳) to my co-founders, a reference to the suicidal
battlecry of the Japanese Empire. My health collapsed in our desperate all-out eﬀort
but we were ﬁrst to market.

2. You're not trying your best if you're not
happy.
Happiness is really, really instrumentally useful. Being happy gives you more
energy, increases your physical health and lifespan, makes you more creative and
risk-tolerant, and (even if all the previous eﬀects are unreplicated pseudoscience)
causes other people to like you more.
—alkjash
Yes! And the way to get happiness is to dedicate everything to a cause greater than
yourself. If you are unhappy that means you are wasting your life. The world is in
danger. We need heroes.
1. A decade later, I succeeded in gaining weight by drinking a gallon of whole milk
everyday for months. ↩ 

A Self-Embedded Probabilistic Model
Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.
One possibly-confusing point from the Embedded Agents sequence: it's actually not diﬃcult to write down a self-
embedded world model. Just as lazy data structures can represent inﬁnite sequences in ﬁnite space, a lazily-
evaluated probabilistic model can represent a world which is larger than the data structure representing the model -
including worlds in which that data structure is itself embedded. The catch is that queries on that model may not
always be computable/well-deﬁned, and even those which are computable/well-deﬁned may take a long time to
compute - e.g. more time than an agent has to make a decision.
In this post, we'll see what this looks like with a probabilistic self-modelling Turing machine. This is not the most
elegant way to picture a self-embedded probabilistic model, nor the most elegant way to think about self-modelling
Turing machines, but it does make the connection from probabilistic models to quining and diagonalization explicit.
The Model
Let's write out a Turing machine as a probabilistic model.
Pieces:
Tape
Head
k-bit-per-timestep input channel
k-bit-per-timestep output channel
k-bit-per-timestep random bit channel
I'm including an input channel so we can have data come in at every timestep, rather than just putting all the input
on one tape initially (which would let the machine perform arbitrary computation between each data point arriving).
Similarly with the output channel: it will have a bit in it every timestep, so the machine can't perform arbitrary
computation between output bits. This is a signiﬁcant diﬀerence between this model and the usual model, and it
makes this model a lot more similar to real-world computational systems, like CPUs or brains or .... It is, eﬀectively, a
bounded computation model. This will play only a minor role for current purposes, but if you're thinking about how
decision-theoretic queries work, it becomes much more relevant: the machine will only have ﬁnite time to answer a
query before a decision must be made.
The relationships:
The tape-state at time t and position position, Tape(t)[position], is a function of Tape(t −1)[position] and 
Head(t) (if position = Head(t −1). position).
The head-state at time t, Head(t), is a function of Head(t −1), tape-state at the head's previous position 
Tape(t −1)[Head(t −1). position], and the input bits In(t) and Rand(t).
The output-state at time t, Out(t), is a function of Head(t).
Also, we'll assume that we know the initial state head0 of the head and tape0 of the tape, and that only a ﬁnite
section of the tape starts out nonzero. Put that all together into a probability distribution factorization, and we get
P [ T a p e , H e a d , O u t | I n , R a n d ] = I [ H e a d ( 0 ) = h e a d 0 ] I [ T a p e [ 0 ] = t a p e 0 ] ⋅
∏ t P [ H e a d ( t ) | H e a d ( t − 1 ) , T a p e ( t − 1 ) [ H e a d ( t − 1 ) . p o s i t i o n ] , I n ( t ) , R a n d ( t ) ] ⋅
∏ t P [ T a p e ( t ) | T a p e ( t − 1 ) , H e a d ( t − 1 ) . p o s i t i o n , H e a d ( t ) ] ⋅
∏ t P [ O u t ( t ) | H e a d ( t ) ]
Each line here handles the update for one component - the head, tape and output - with the initial conditions on the
ﬁrst line. We can also further break apart the tape-update term, since only the tape-position where the head is
located depends on the head-state:
P [ T a p e ( t ) | T a p e ( t − 1 ) , H e a d ( t − 1 ) . p o s i t i o n , H e a d ( t ) ] =

P [ T a p e ( t ) [ H e a d ( t − 1 ) . p o s i t i o n ] | T a p e ( t − 1 ) [ H e a d ( t − 1 ) . p o s i t i o n ] , H e a d ( t ) ] ⋅
∏ p o s i t i o n ≠ H e a d ( t − 1 ) . p o s i t i o n P [ T a p e ( t ) [ p o s i t i o n ] | T a p e ( t − 1 ) [ p o s i t i o n ] ]
Each of the "atomic" conditional probabilities in these formulas would be given by the machine's local update-rules -
e.g. head state as a function of previous head state, state of previous tape location, and inputs, or tape state as a
function of previous tape-state (usually the identity function). We can also incorporate a model of the rest of the
world, which would give P[In|Out] and P[Rand], with In at later times depending only on Out at earlier times so that
the whole thing works out to a well-behaved (i.e. acyclic) factorization.
The important thing to notice is that, while the distribution is over an inﬁnite space, we can express it in a ﬁnite set
of equations (i.e. the equations above). We can also perform ordinary probabilistic calculations using these
equations: it's just a plain old Bayes net, and there's a bounded number of nonzero variables at each timestep. More
to the point, we can hard-code a representation of these equations in the initial state of the Turing machine (quining
to ﬁt the representation of the initial state inside the representation of the model inside the initial state), and then
the machine itself can perform ordinary probabilistic calculations using these equations. We can treat the equations
deﬁning the model as a lazy data structure, and run queries on them.
Diagonalizing
So, what happens if we try to diagonalize our self-modelling Turing machine? What happens if we look at the ﬁrst
non-null output of the machine and then ﬂip it, and we program the machine to output the most likely value of this
ﬂipped output? Well, that scheme falls apart at "ﬁrst non-null output of the machine". There's no guarantee that the
machine ever outputs a non-null output. Let's write this out concretely, in terms of the probabilistic model and
queries involved.
We'll assume that Out consists of two bits. The machine always outputs "00" until its computation completes, at
which point it outputs either "10" if it wants to pass a logical-zero output or "11" for a logical-one output. The
program on the initial tape is some ordinary probabilistic reasoning program, and we hardcode our query into it.
What should our query be? Well, we want to look at the ﬁrst non-null output, so we'll have to write something like "
Out(mint t s. t. Out(t) ≠"00")" - the minimum time t such that the output is not "00". Then we want the machine to
output the least-likely value of that variable, so we'll ask for something like
a r g m i n v a l P [ O u t ( m i n t  t  s . t .  O u t ( t ) ≠ " 00 " ) = " 0 " + v a l ]
Then, when the machine goes to compute mint t s. t. Out(t) ≠"00", the computation may not complete at all, in
which case there will not be any time t which satisﬁes that condition.
The main thing to notice in this example is that the query itself was not well-deﬁned. It's not just that our machine
can't answer the query; even an outside observer with unlimited computational power would conclude that the
answer is not well-deﬁned, because the time t which it asks about does not exist. Our query is trying to address a
variable which doesn't exist. The "non-halting weirdness" comes from passing in weird queries to the model, not
from any weirdness in the model itself. If we stick to "normal" queries - i.e. ask about the value of a speciﬁc variable -
then there isn't any conceptual problem (though it may take a while for the query to run). So from this perspective,
the central problem of self-embedded world models is not representation or interpretation of the model, but rather
the algorithmic problem of expanding the set of queries we can answer "without any weirdness".
In this example, there is another possible behavior: the machine may output a logical zero with probability ½ and a
logical one with probability ½, using its random bit source. This would require a probabilistic reasoning algorithm
quite diﬀerent from what we normally use, but would be entirely self-consistent and non-weird. That's an example of
what it might mean to "expand the set of queries we can answer without any weirdness".
What Queries Do We Care About?
We do not care about all queries equally. Depending on the details of our underlying model/logic, there may be lots
of queries which are uncomputable/undeﬁned, but which we don't actually care about answering. We want a theory
of embedded agents, but that does not necessarily imply that we need to handle every possible query in some very
expressive logic.
So which queries do we need to handle, in order to support a minimum viable model of agency?

This is a hard question, because it depends on what decision theory we're using, and exactly what kind of
counterfactuals that decision theory contains (and of course some decision theories don't directly use a probabilistic
model at all). But there are at least some things we'll deﬁnitely want - in particular, if we're using a probabilistic
model at all, we'll want some way to do something like a Bayesian update. That doesn't necessarily mean updating
every probability of every state explicitly; we could update lazily, for instance, i.e. just store the input data directly
and then go look it up if and when it's actually relevant to a query. More generally, we want some data structure
which summarizes whatever info we have from the inputs in a form suitable to answering whatever queries we're
interested in.
(To me, this sounds like an obvious use-case for abstraction: throw out info which is unlikely to be relevant to future
queries of interest.)
Another interesting class of queries is optimization queries, of the sort needed for decision theories or game-
theoretic agents. One way to think about the decision theoretic problems of embedded agency is that we want to
ﬁgure out what the "right" class of queries is, in order to both (a) guarantee that the optimization query is actually
solved solved correctly and quickly, and (b) get good performance in a wide variety of environments. (Of course, this
isn't the only way to think about it.)

Sunzi's《Methods of War》- War
This is a translation of Chapter 2 of The Art of War by Sunzi. No English sources were
used.
孙⼦⽈：凡⽤兵之法，驰⻋千驷，⾰⻋千乘，带甲⼗万，千⾥馈粮，内外之费，宾客之⽤，㬵
漆之材，⻋甲之奉，⽇费千⾦，然后⼗万之师举矣。
The ordinary methods of war demand:
1,000 teams of 4 horses each,
1,000 wagons,
100,000 shields,
provisions to march 1,000 miles,
domestic and foreign expenses,
hospitality for guests,
construction materials for siege weapons,
armored vehicles,
salaries
...and an army of 100,000 soldiers.
其⽤战也胜，久则钝兵挫锐，攻城则⼒屈，久暴师则国⽤不⾜。
A long war is an expensive war.
夫钝兵挫锐，屈⼒殚货，则诸侯乘其弊⽽起，虽有智者，不能善其后矣。
An expensive war will cause your vassals to rebel against you.
故兵闻拙速，未睹巧之久也。夫兵久⽽国利者，未之有也。
There is no such thing as a beneﬁcial protracted war.
故不尽知⽤兵之害者，则不能尽知⽤兵之利也。
If you do not understand the costs of war then you do not know which wars are
worthwhile to ﬁght.
善⽤兵者，役不再籍，粮不三载，取⽤于国，因粮于敌，故军⾷可⾜也。
Do not conscript troops more than once. Do not resupply your army with grain more
than twice. Take what you need from the enemy. The enemy has ample grain and an
army of troops.
国之贫于师者远输，远输则百姓贫；
Resupplying an army over long distances impoverishes a country.
近师者贵卖，贵卖则百姓财竭，财竭则急于丘役。

Prices soar in wartime. Levying the peasantry under such circumstances will
impoverish them while extracting only forced labor.
⼒屈财殚，中原内虚于家，百姓之费，⼗去其七；
The central plains will go unfarmed. Seven tenths of the peasantry's labor will be
wasted.
公家之费，破军罢⻢，甲胄⽮弩，戟楯蔽橹，丘⽜⼤⻋，⼗去其六。
Supplying an army out of the public purse slows the army down. Horses sicken.
Shields split. Oxen tire. Six tenths is wasted.
故智将务⾷于敌，⾷敌⼀钟，当吾⼆⼗钟；萁秆⼀⽯，当吾⼆⼗⽯。
The wise general eats the enemy's food. A captured bowl of enemy food is worth
twenty bowls of your own. A captured ton of enemy grain is worth twenty tons of your
own.
故杀敌者，怒也；取敌之利者，货也。⻋战得⻋⼗乘以上，赏其先得者，⽽更其旌旗，⻋杂⽽
乘之，卒善⽽养之，是谓胜敌⽽益强。
Let your troops kill the enemy in anger, plunder the enemy in greed. A captured
enemy combat vehicle is worth no fewer than ten of your own. Reward your ﬁrst
soldier to capture one. Replace its ﬂag. Mix it in among your own.
A good soldier steals victory from the enemy.
故兵贵胜，不贵久。故知兵之将，⽣⺠之司命，国家安危之主也。
A valuable victory is a quick victory. A general who, understanding this, issues orders
to the people—thereupon is the fate of a state determined.

Spend twice as much eﬀort every time
you attempt to solve a problem
In brief:  in order to iteratively solve problems of unknown diﬃculty a good heuristic is to
double your eﬀorts every time you attempt it.
 
Imagine you want to solve a problem, such as getting an organization oﬀ the ground or
solving a research problem. The problem in question has a diﬃculty rating d, which you
cannot know in advance.
You can try to solve the problem as many times as you want, but you need to precommit in
advance how much eﬀort e you want to put into each attempt - this might be for example
because you need to plan in advance how you are going to spend your time.
We are going to assume that there is little transfer of knowledge between attempts, so that
each attempt succeeds iﬀ the amount of eﬀort you spend on the problem is greater than its
diﬃculty rating e > d.
The question is - how much eﬀort should you precommit to spend on each attempt so that
you solve the problem spending as little eﬀort as possible?
 
Let's consider one straightforward strategy. We ﬁrst spend 1 unit of eﬀort, then 2, then 3,
and so on.
We will have to spend in total 1 + 2 + ⋯+ d =
(d + 1)d ∈O(d2). The amount of eﬀort spent
scales quadratically with the diﬃculty of the problem.
Can we do better?
 
What if we double the eﬀort spent each time?
We will have to spend 1 + 2 + 4 + ⋯+ 2⌈log2 d⌉= 2 ⋅2⌈log2 d⌉−1 ≤4d −1 ∈O(d). Much better
than before! The amount of eﬀort spent now scales linearly with the diﬃculty of the problem.
In complexity terms, you cannot possibly do better - even if we knew in advance the
diﬃculty rating we would need to spend d ∈O(d) to solve it.
 
Is doubling the best scaling factor? Maybe if we increase the eﬀort by 1.5 each time we will
spend less total eﬀort.
12

In general, if we scaled our eﬀorts by a factor b then we will spend in total 
1 + b + b2 + ⋯+ b⌈logb d⌉=
≤
. This upper bound is tight at inﬁnitely many
points, and its minimum is found for b = 1 + √
 - which asymptotically approaches 2 as d
 grows. Hence 2 seems like a good heuristic scaling factor without additional information on
the expected diﬃculty. But this matters less than the core principle of increasing your eﬀort
exponentially.
 
Total eﬀort spent vs scaling factor, for diﬃculties 10, 1e4, 1e9. The eﬀort zigzags
up and down, hitting local minima for scaling factors that allow us to hit the
target diﬃculty exactly after a number of iterations. But if we look at the
silhouette delineated by the upper bounds of the function we can appreciate a
robust minimum slightly below 2. 
 
Real life is not as harsh in our assumptions - usually part of the eﬀort spent carries over
between attempts and we have information about the expected diﬃculty of a problem. But
in general I think this is a good heuristic to live by.
Here is an (admittedly shoehorned) example. Let us suppose you are unsure about what to
do with your career. You are considering research, but aren't sure yet. If you try out research,
you will learn more about that. But you are unsure of how much time you should spend
trying out research to gather enough information on whether this is for you.
In this situation, before committing to a three year PhD, you better make sure you spend
three months trying out research in an internship. And before that, it seems a wise use of
your time to allocate three days to try out research on your own. And you better spend three
minutes beforehand thinking about whether you like research.
 
Thank you to Pablo Villalobos for double-checking the math, creating the graphs and
discussing a draft of the post with me.
b⌈logb d⌉+1−1
b−1
b2d−1
b−1
d−1
d

Are the social sciences challenging
because of fundamental diﬃculties or
because of imposed ones?
Scholarship Status:
I'm not an expert in the social sciences. I've taken a few in-person and online courses
and read several books, but I'm very much speculating here. I also haven't done much
investigation for this speciﬁc post. Take this post to be something like, "my quick
thoughts of the week, based on existing world knowledge" rather than "the results of
an extensive research eﬀort." I would be very curious to get the take of people with
more experience in these areas. 
TLDR:
The social sciences, in particular psychology, sociology, and anthropology, are often
considered relatively ineﬀective compared to technical ﬁelds like math, engineering,
and computer science. Contemporary progress in the social sciences seems to be less
impactful, and college graduates in these ﬁelds face more challenging job prospects. 
One reason for this discrepancy could be that the social sciences are fundamentally
less tractable. An argument would go something like, "computers and machines can
be deeply understood, so we can make a lot of progress around them, but humans
and groups of humans are really messy and near impossible to make useful models
of."
I think one interesting take is that the social sciences aren't fundamentally more
diﬃcult than technical ﬁelds, but rather that they undergo substantial limitations.
Many of these limitations are intentional, and these might be the majority. By this I'm
mainly referring to certain expectations of privacy and other ethical expectations in
research, coupled with uncomfort around producing social science insights that are
"too powerful."
If this take is true, then it could change conversations around progress in the social
sciences to focus on possibly uncomfortable trade-oﬀs between research progress,
experimentation ethics, and long term risks. 
This is important to me because I could easily imagine advances in the social sciences
being much more net-positive than advances in technical ﬁelds, outside of AI. I'd like
ﬂying cars and better self driving vehicles a great deal, but I'd like to live in a kind,
coordinated, and intelligent world a whole lot more. 
Introduction
When I went to college (2008-2012), it was accepted as common wisdom that human
decision making was dramatically more complicated than machine behavior. The
math, science, and engineering classes used highly speciﬁed and deterministic
models. The psychology, anthropology, and marketing courses used what seemed like
highly sketchy heuristics, big conclusions drawn from narrow experiments, and

subjective ethnographic interpretations. Our reductionist and predictable models of
machines allowed for the engineering of technical systems, but our vague intuitions of
humans didn't allow us to do much to inﬂuence them. 
Perhaps it's telling that engineers keep on refusing to enter the domains of human
and social factors; we don't yet really have a ﬁeld of "cultural engineering" or
"personality engineering" for instance. Arguably cyberneticians and technocrats in the
1900s made attempts but fell out of favor.
Up until recently, I assumed that this discrepancy was due to fundamental diﬃculties
around humans. Even the most complex software setups were rather straightforward
compared to humans. After all, human brains are monumentally powerful compared to
software systems, so must be correspondingly challenging to deal with.
But recently I've been intrigued by a second hypothesis. That many aspects of the
social sciences aren't fundamentally more diﬃcult to understand than technical
systems, but rather that progress is deeply bottlenecked by ethical dilemmas and
potential dangerous truths. 
Intentional Limitations:
1. Political Agendas
Political agendas are probably the most obvious intentional challenge to the social
sciences. Generally, no one gets upset about which conclusions nuclear physicists
arrive at, but they complain on Twitter when new research is posted on sexual
orientation. There's already a fair bit of discussion of a left-bias in the social sciences
and it's something I hear many academics complain about. My impression is that this
is a limitation, but that the issue is a lot more complicated than a simple "we just need
more conservative scientists" answer. Conservatives on Twitter get very upset about
things too, and having two sides complain about opposite things doesn't cancel the
corresponding problems out.
So one challenge with agendas is that they preclude certain kinds of research. But I
think a deeper challenge is that they change incentives of researchers to actively
focus on providing evidence for existing assumptions, rather than actively search for
big truths. Think tanks are known for this; for a certain amount of money you could
generally get some macroeconomic work that supports seemingly any side of an
economic argument. My read is that many anthropologists and sociologists do their
work as part of a project to properly appreciate diversities of cultures and lifestyles.
There's a fair amount of work to understand issues on oppression; typically this seems
focused on defending the hypothesis that oppression has existed. 
There's a lot of value for agenda driven work, where agenda driven work is deﬁned as,
"there's a small group of people who know something, and they need a lot of
evidence to prove this to the many more people." I partake in work like this myself,
any writing to promote already discussed research around the validity of forecasting
ﬁts this description. However, this work seems very diﬀerent from work ﬁnding totally
new insights. Science done for the sake of convincing people of known things can be
looked at as essentially a highly respectable kind of marketing. Agenda driven science
uses the same tools as "innovation driven" science, but the change in goals seems
likely to produce correspondingly diﬀerent outcomes.  

2. Subject Privacy Concerns
I've worked a fair bit with web application architectures. They can be a total mess.
Client layers on top of client APIs, followed by the entire HTTP system, load balancers,
tangled spaghetti of microservices, several diﬀerent databases.  And compared to the
big players (Google, Facebook, Uber), this was all nice and simple.
One of the key things that makes it work is introspectability. If something is confusing,
you can typically either SSH into it and mess around, or try it out in a diﬀerent
environment. There are hundreds of organized sets of logs for all of the interactions in
and out. There's an entire industry of companies that do nothing but help other tech
companies set up sophisticated logging infrastructures. Splunk and Sumo Logic are
both public, with a combined market cap of around $35 Billion.
Managing all the required complexity would be basically impossible without all of this.
Now, in human land, we don't have any of this, mostly because it would invade
privacy. Psychological experiments typically consist of tiny snapshots of highly
homogenous clusters (college students, for example). Surveys can be given to more
people, but the main ones are highly limited in scope and are often quite narrow.
There's typically little room to "debug", which here would mean calling up particular
participants and getting a lot more information from them.
What Facebook can do now is far more expansive and sophisticated than anything I
know of in social science and survey design. They just have dramatically more and
better data. However, they don't seem to have that sophisticated of a team to
generate academic insights using this data, and their attempts to do actual
experimentation haven't gone very well. My guess is that the story "Facebook hires a
large team of psychologists to do investigation and testing on users" wouldn't be
received nicely, even if they took the absolute most prestigious and qualiﬁed
psychologists.
As artiﬁcial intelligence improves, it becomes possible to infer important information
from seemingly minor data. For example, it's possible to infer much of one's big 5
personality characteristics from their Facebook proﬁle, or the discussion about
inferring sexuality from proﬁle photos. Over time we could expect both that more data
be collected about people, and also that this data goes much further for analysis,
because we can make superior inferences from it. So the Facebook of tomorrow won't
just have more data, but might be able to infer a whole lot about each user.
I know if I was in social science, I would expect to be able to discover dramatically
more by working with the internal data of Facebook, NSA, or Chinese Government,
especially if I had a team of engineers help prune and run advanced inference on the
data.
This would be creepy on basically all current social standards. It could get very, very
creepy.
There could be ways of accepting reasonable trade-oﬀs somewhere. Perhaps we could
have better systems of diﬀerential privacy so scientists could get valuable insights
from large data sets without exposing any personal information. Maybe select groups
of people could purposely opt-in to extended study, ideally being paid substantially for
the privilege. Something like We Live in Public but more ordinary and on a larger
scale.  We may desire intensive monitoring and regulation of any groups handling this
kind of information. Those doing monitoring probably should be the most monitored.

On this note, I'd mention that I could imagine the Chinese Government being in a
position to spearhead social science research in a way not at all accepted in the
United States. Arguably their propaganda sophistication is already quite good. I'm not
sure if their improvement of propaganda has led to fundamental insights about human
behavior in general, but I would expect things to go in that direction, especially if they
made a signiﬁcant eﬀort. 
I imagine that this section on "privacy" could really be reframed as "ethical
procedures and limitations." Psychology has a long history of highly malicious
experiments and this has led to a heap of enforced polices and procedures. I imagine
that the main restrictions though are still the ones that were almost too obvious to be
listed as rules. Culture often produces more expansive and powerful rules than
bureaucracy does (thinking about ideas from Critical Theory and Slavoj Žižek).
3. Power
Let's step back a bit. It's diﬃcult to put a measure on progress of the social sciences,
but I think one decent aggregate metric would be the ability to make predictions
about individuals, organizations, and large societies. If we could predict them well, we
could do huge amounts of good. We could recommend very speciﬁc therapeutic
treatments to speciﬁc subpopulations and expect them to work. We could adjust the
education system to promote compassion, creativity, and ﬂourishing in ways that
would have tangible beneﬁts later on. We could quickly hone in on the most eﬀective
interventions to global paranoia, jingoism, and racism. 
But you can bet that if we got far better at "predicting human behavior", the
proverbial "bad guys" would be paying close attention.
In Spent, Geoﬀrey Miller wrote about attempts to investigate and promote ideas
around using evolutionary psychology to understand modern decision making.
Originally himself and a few others tried to discuss the ideas with academic
economists. They quickly realized that the people who were paying the closest
attention were really the people in marketing research.
Research into cognitive biases and the power of nudges was turned into a "nudge
unit" in the US, but I'm sure was more frequently used by growth hackers. I'm not
quite sure how advancements in the last few years of Neuroscience have so far helped
myself or my circle, but I do know they have been studied for Neuromarketing.
So I think that if we're nervous about physical technological advances being used for
destructive ends (environmental degradation and warfare come to mind), we should
be doubly so about social ones.
Militarily, it's interesting that there's public support for bomber planes and "advanced
interrogation", but information warfare and propaganda get a bad rap. There seems to
be a deeper stigma against propaganda than there is against nuclear weapons.
Related, the totalitarian environment of Nineteen Eighty-Four focused on social
technologies (an engineered language and cultural control) rather than the advanced
use of machines.
Sigmund Freud was the uncle of Edward Bernays, an Australian-American pioneer of
"public relations" who wrote the books Crystallizing Public Opinion and Propaganda,
both between 1925 and 1930. Edward extended ideas in psychology for his work. In
general the public seems to have had a positive association of propaganda at that
time as a tool for good. That association changed with the visible and evident use of

Propaganda during WWII very shortly afterwards. This seems to have been about as
big a reversal as with the public stance on eugenics. 
If the standard critiques of psychological study are that it's not eﬀective or useful, the
critique of propaganda would be anything but. It's too powerful. Perhaps it can be
used for massive amounts of good, but it can clearly be and historically was used for
catastrophic evil.
Unintentional Limitations:
There are, of course, many unintentional problems with the social sciences too.
There's the whole replication crisis for one. I imagine that better tooling could make a
huge diﬀerence. Positly comes to mind. The Social Sciences also could use more of the
basic stuﬀ; money, encouragement, and talent. I'm really not sure how these compare
to the above challenges.
What are the Social Sciences supposed to do
?
I think this all raises a question. What are the social sciences really for? More
speciﬁcally, what outputs are people in and around these ﬁelds hoping to accomplish
in the next 10 to 100 years? What would radical success look like? It's assumed that
fundamental breakthroughs in chemistry will lead to improvements in consumer
materials and that beneﬁts in engineering will lead to the once promised ﬂying cars.
With psychology, anthropology, and sociology, I'm really not sure what successes
would be both highly impactful and also socially tolerated.
If the path to impact is "improve the abilities of therapists and human resources
professionals", I imagine gains will be limited. I think that these professions are
important, but don't see changes in them improving the world by over 20% in the next
20 to 50 years. If the path is something like, "interesting knowledge that will be
transmitted by books and educational materials directly to its users", then the outputs
would need to be highly condensed. Most people don't have much extra time to study
material directly.
If the path is, "help prove out cases against racial and gender inequality" (and
similar), I could see this working to an extent, but this would seem like a fairly limited
agenda. Agenda driven scientiﬁc work is often too scientiﬁc to be convincing to most
people (who prefer short opinion pieces and ﬁction), and too narrow to be
groundbreaking. It serves a useful function, but generally not a function of radical
scientiﬁc progress.[1]
There are some research direction proposals that I could see being highly impactful,
but these correlate strongly with those venturing on being dangerous. This is
especially the case because many of them may require coordinated action, and it's
not clear which modern authorities are trusted enough to carry out large coordinated
action.
Possible research directions:

Systems to predict and signal which people and activities will lead to good or
bad things.
Support for aligning human intuitions with arbitrary traits. (Like, making children
particularly patriotic or empathetic)
Cultural engineering to optimize cultures for arbitrary characteristics.
Sophisticated chat bots that would outperform current humans at delivering
psychological help, friendship, and possibly romance.
Systems that would make near-optimal recommendations on what humans
should do on near all aspects of their lives.
Here's another way of looking at it. Instead of asking what people in the ﬁelds are
expecting, ask what regular people think will happen. What do most people expect of
the social sciences in 30, 60, 200 years? I'd guess that most people would assume
that psychology will make minor advances for the future of therapy, and that maybe
sociology and anthropology will provide more evidence in favor of contemporary
liberal values. Maybe they'll make a bunch of interesting National Geographic articles
too.
If you ask people what they expect from engineering I imagine they'd start chatting
about exciting science ﬁction scenarios. They might not know what a transistor is, but
they could understand that cars could drive themselves and maybe ﬂy too. This
seems like a symptom of the focus on technology in science ﬁction, though that could
be a symptom of more fundamental issues. Perhaps one should look to utopian
literature instead. I'm not well read on utopian ﬁction, but generally from what I know
there is a focus on "groups that seem well run" as opposed to "groups that are well
run due to sophisticated advances in the social sciences."
Predictions to anchor my views
Given the generality of the discussion above, it's hard to make incredibly precise
predictions that are meaningful. I'm mainly aiming for this piece to suggest a
perspective rather than convince people of it. 
Here are some slightly speciﬁc estimations I would make:
1. If all agenda driven social science incentives were removed, it would increase
"fundamental innovations" by 5% to 40%, 90% credence interval (but lose a lot
of other value).
2. If privacy concerns were totally removed, and social scientists could easily
partner with governments and companies (a big if!), it would increase
"fundamental innovations" by 20% to 1,000%.
3. If power concerns were totally removed, it would increase "fundamental
innovations" by 5% to 10,000%.
4. If $1 Billion were eﬀectively (think how it would be spent by an aligned tech
company, not that this will happen) spent on great software tooling for social
scientists, in the current climate, it would increase "fundamental innovations" by
2% to 80%. Note that I'd expect the government to be poor at spending, so if
they were to attempt this, I would expect it to cost them $100 Billion for the
equivalent impact.
5. If "fundamental innovations" in the social sciences were improved by 1000%
over the next 50 years, it would slightly increase the chances of global
totalitarianism, but it has a chance (>30%) of otherwise being dramatically
positive.

Takeaways
Apologies for the meandering path through this article. I'm attempting to make broad-
sweeping hypotheses of huge ﬁelds that I haven't ever worked in, this is exactly the
kind of analysis I typically am wary of.
At this point I'm curious to better understand if there are positive trajectories for the
social sciences that are both highly impactful and acceptable to both the key decision
makers and by society. I'm not sure if there really are.
Arguably the ﬁrst challenge is not to make the social sciences more eﬀective, but to
ﬁrst help clear up confusion over what they should be trying to accomplish in the ﬁrst
place. Perhaps the most exciting work is too hazardous or dangerous to attempt. Work
further outlining the costs and beneﬁts seems fairly tractable and important to me.
Many thanks to Elizabeth Van Nostrand, Soﬁa Davis-Fogel, Daniel Eth, and Nuño
Sempere for comments on this post. Some of them pointed out some severe
challenges that I haven't totally addressed, so any faults are really my own. 
 
[1] I realize that these ﬁelds are complicated collections of individuals and incentives
that are probably optimizing for a large set of goals, which likely include some of the
ones here mentioned. I'm not thinking there should be one universal goal, but I am
thinking that myself and many readers would be interested in the social sciences
through a consequentialist lens.  

Why those who care about
catastrophic and existential risk
should care about autonomous
weapons
(crossposted to EA forum here.)
Although I have not seen the argument made in any detail or in writing, I and the
Future of Life Institute (FLI) have gathered the strong impression that parts of the
eﬀective altruism ecosystem are skeptical of the importance of the issue of
autonomous weapons systems. This post explains why we think those interested in
avoiding catastrophic and existential risk, especially risk stemming from emerging
technologies, may want to have this issue higher on their list of concerns.
We will ﬁrst deﬁne some terminology and do some disambiguation, as there are many
classes of autonomous weapons that are often conﬂated; all classes have some issues
of concern, but some are much more problematic than others. We then detail three
basic motivations for research, advocacy, coordination, and policymaking around the
issue:
1. Governance of autonomous weapon systems is a dry-run, and precedent, for
governance of AGI. In the short term, AI-enabled weapons systems will share
many of the technical weaknesses and shortcomings of other AI systems, but
like general AI also raise safety concerns that are likely to increase rather than
decrease with capability advances. The stakes are intrinsically high (literally life-
or-death), and the context is an inevitably adversarial one involving states and
major corporations. The sort of global coordination amongst potentially
adversarial parties that will be required for governance of
transformative/general AI systems will not arise from nowhere, and autonomous
weapons oﬀer an invaluable precedent and arena in which to build experience,
capability, and best practices.
2. Some classes of lethal autonomous weapon systems constitute scalable
weapons of mass destruction (which may also have a much lower threshold for
ﬁrst use or accidental escalation), and hence a nascent catastrophic risk.
3. By increasing the probability of the initiation and/or escalation of armed conﬂict,
including catastrophic global armed conﬂict and/or nuclear war, autonomous
weapons represent a very high expected cost that overwhelmingly oﬀsets any
gain in life from substituting autonomous weapons for humans in armed conﬂict.
Classes of autonomous weapons
Because many things with very diﬀerent characteristics could fall under the rubric of
"autonomous weapon systems" (AWSs) it is worth distinguishing and classifying them.
First, let us split oﬀ cyberweapons - including AI-powered ones - as being an
important but distinct issue. Likewise, we'll set aside AI in other aspects of the military
not directly related to the use of force, from strategy to target identiﬁcation, where it
serves to augment human action and decision-making. Rather, we focus on systems
that have both (some form of) AI and physical armaments.

We now consider in turn these armaments' target types, which we will break into
categories of anti-personnel weapons, force-on-force (i.e. attacking manned enemy
vehicles or structures) weaponry, and those targeting other autonomous weapon
systems.
Anti-personnel AWSs can be further divided into lethal (or grossly injurious) ones
versus nonlethal ones. While an interesting topic,[1] we leave aside here non-lethal
anti-personnel autonomous weapon systems, which have a somewhat distinct set of
considerations.[2]
We regard force-on-force systems designed to attack manned military vehicles and
installations as relatively less intrinsically concerning. The targets of such weapons
will, with considerably higher probability, be valid military targets rather than civilian
ones, and insofar as they scale to mass damage, that damage will be to an
adversary's military. Of course if these weapons are highly eﬀective, the manned
targets they are designed to attack may quickly be replaced with unmanned ones.[3]
This brings us to force-on-force systems that attack other autonomous weapons (anti-
AWSs). These exist now, for example in the form of automated anti-missile systems,
and are likely to grow more prevalent and sophisticated. These raise a nuanced set of
considerations, as we'll see. Some types are quite uncontroversial: no one has to our
knowledge advocated for prohibiting, say, automated defenses on ships. On the other
hand, very eﬀective anti-ballistic missile systems could undermine the current nuclear
equilibrium based on mutual assured destruction. And while the prospect of robots
ﬁghting robots rather than humans ﬁghting humans is beguiling from the standpoint
of avoiding the horrors of war, we'll argue below that it is very unlikely for this to be a
net positive.
This leads to a fairly complex set of considerations. FLI and other organizations have
advocated for a prohibition against kinetic lethal anti-personnel weapons, with various
degrees of distinction between anti-personnel and force-on-force lethal autonomous
weapons, and various levels of concern and proposed regulation concerning some
classes of force-on-force autonomous weapons. Motivations for this advocacy vary,
but we start with one that is of particular important to FLI and to the EA/long-termist
community.
Lethal autonomous weapons systems are an early test for AGI safety, arms
race avoidance, value alignment, and governance
There are a surprising number of parallels between the issue of autonomous weapons
and some of the most challenging parts of the AGI safety issue. These parallels
include:
In both cases, a race condition is both natural and dangerous;
Military involvement is possible in AGI and inevitable for AWSs;
Involvement by national governments is likely in AGI and inevitable for AWSs;
Secrecy and information hazards are likely in both;
Major ethical/responsibility concerns exist for both, perhaps more explicitly in
AWSs;
In both cases, unpredictability and loss of control are key issues.
In both cases, early versions are potentially dangerous because of their
incompetence; later versions are dangerous because of their competence.

The danger of arms races has long been recognized as a potentially existential threat
in terms of AGI: if companies or countries worry that being second to realize a
technology could be catastrophic for their corporate or national interest, then safety
(and essentially all other) considerations will tend to fall to the wayside. When applied
to autonomous weapons, "arms race" is literal rather than metaphorical but similar
considerations apply. The general problem with arms races is that they are very easy
to lose, but very diﬃcult to win: you lose if you fail to compete, but you also lose if the
competition leads to a situation dramatically increasing the risk to both parties, or to
huge adverse side-eﬀects; and this appears likely to be the case in autonomous
weapons and AGI, just as it was in nuclear weapons.[4] Unfortunately, the current
international and national security context includes multiple parties fomenting a
"great powers" rivalry between the US and China that is feeding an arms race
narrative in AI in general, including in the military and potentially extending to AGI.
Managing to avoid an arms race in autonomous weapons - via multi-stakeholder
international agreement and other means - would set a very powerful precedent for
avoiding one more generally. Fortunately, there is reason to believe that this arms
race is avoidable.[5] The vast majority of AI researchers and developers are strongly
against an arms race in AWSs,[6] and AWSs enjoy very little popular support.[7] Thus
prohibition or strong governance of lethal autonomous weapons is a test instance in
which the overwhelming majority of AI researchers and developers agree. This
presents an opportunity to draw at least some line, in a globally coordinated way,
between what is and is not acceptable in delegating decisions, actions, and
responsibility to AI. And doing so would set a precedent for avoiding a race by
recognizing that even each participant's interests are better served by at least some
coordination and cooperation.
Good governance of AWSs will take exactly the sort of multilateral cooperation,
including getting militaries onboard, that is likely to be necessary with an overall
AI/AGI (ﬁgurative) arms race. The methods, institutions, and ideas necessary to
govern AGI in a beneﬁcial and stable multilateral system is very unlikely to arise
quickly or from nowhere. It might arise steadily from growth of current AI governance
institutions such as the OECD, international standards bodies, regulatory frameworks
such as that developing in the EU, etc. But these institutions tend to explicitly and
deliberately exclude discussion of military issues so as to make reaching agreements
easier. But this then avoids precisely the sorts of issues of national interest and
military and geopolitical power that would be at the forefront of the most disastrous
type of AGI race. Seeking to govern deeply unpopular AWSs (which also presently lack
strong interest groups pushing for them) provides the easiest possible opportunity for
a "win" in coordination amongst military powers.
Beyond race vs. cooperative dynamics, autonomous weapons and AGI present other
important parallels at the level of technical AI safety and alignment, and multi-agent
dynamics.
Lethal autonomous weapon systems are a special case of a more general problem in
AI safety and ethics that the technical capability of being eﬀective may be much
simpler than what is necessary to be moral or ethical or legal. Indeed the gap
between making an autonomous weapon that is eﬀective (successfully kills enemies)
and one that is moral (in the sense of, at minimum, being able to act in accord with
international law) may be larger than in any other AI application: the stakes are so
high, and the situations so complex, that the problem may well be AGI-complete.[8]

In the short-term, then, there are complex moral questions. In particular, who is
responsible for the decisions made by an AI system when the moral responsibility
cannot lie with the system? If an AI system is programmed to obey the "law of war,"
but then fails, who is at fault? On the ﬂip side, what happens if the AI system
"disagrees" with a human commander directing an illegal act? Even a weapon that is
very eﬀective at obeying such rules is unlikely to (be programmed to be able to)
disobey a direct "order" from its user: if such "insubordination" is possible it raises the
risk of incorrigible and intransigent intelligent weapons systems; but if not, it removes
an existing barrier to unconscionable military acts. While these concerns are not
foremost from the perspective of overall expected utility, for these and other reasons
we believe that delegating the decision to take a human life to machine systems is a
deep moral error, and doing so in the military sets a terrible precedent.
Things get even more complex when multiple cooperative and adversarial systems
are involved. As argued below, the unpredictability and adaptability of AWSs is an
issue that will increase with better AI rather than decrease. And when many such
agents interact, emergent eﬀects are likely that are even less predictable in advance.
This corresponds closely to the control problem in AI in general, and indicates a quite
pernicious problem of AI systems leaving humans unable to predict what they will do,
or eﬀectively intervene if what they do runs counter to the wishes of human
overseers.
In advanced AI in general, one of the most dangerous dynamics is the unwarranted
belief of AI developers, users, and funders that AI will - like most engineered
technologies - by default do what we want it to do. It is important that those who
would research, commission, or deploy autonomous weapons be fully cognizant of this
issue; and we might hope that the cautious mindset this could engender could bleed
into or be transplanted into safety considerations for powerful AI systems in and out of
the military.
Lethal autonomous weapons as WMDs
There is a very strong case to classify some anti-personnel AWSs as weapons of mass
destruction. We regard as the key deﬁning characteristic of WMDs[9] that a single
person's agency directed through the weapon can directly cause many fatalities with
very little additional support structure (like an army to command.) This is not possible
with "conventional" weapons systems like guns, aircraft, and tanks, where the deaths
caused scale roughly linearly with the number of people involved in causing those
deaths.
With this deﬁnition, some anti-personnel lethal AWSs (such as microdrone munition-
carrying "slaughterbots") would easily qualify. These weapons are essentially
(microdrone)+(bullet)+(smartphone components), and with near-future technology
and eﬃciency of scale, slaughterbots could plausibly be as inexpensive as $100 each
to manufacture en masse. Even with a 50% success rate and doubling of the cost to
account for delivery, this is $400/fatality. Nuclear weapons cost billions to develop,
then tens to hundreds of millions per warhead. A nuclear strike against a major city is
likely to have hundreds of thousands of fatalities (for example a 100 kiloton strike
against downtown San Francisco would cause an estimated 200K fatalities and 400K
injuries.) 100,000 kills worth of slaughterbots, at a cost of $40M, would be just as cost-
eﬀective to manufacture and deploy, and dramatically cheaper to develop. They are
more bulky than a nuclear warhead but could plausibly still ﬁt in a 40' shipping

container (and unlike nuclear, chemical and biological weapons are safe to transport,
hard to detect, and can easily be deployed remotely.)
This is possible with near-future technology.[10] It is not hard to imagine even more
miniaturized weaponry, in a continuum that could reach all the way to
nanotechnology. And unlike (to ﬁrst approximation) for nuclear weapons, eﬀectiveness
and cost-eﬃciency are likely to signiﬁcantly increase with technological improvement.
[11] Thus if even a fraction of the resources that have been put into nuclear weapons
were put into anti-personnel lethal AWSs, they could potentially become as large of a
threat. Consider that it took less than 20 years from the 1945 Trinity test until the
Cuban Missile Crisis that almost led to a global catastrophe, and that a determined but
relatively minor program by a major military could likely develop a slaughterbot-type
WMD within a handful of years.
One crucial diﬀerence between AWs and other WMDs is that the former's ability to
discriminate among potential targets is much better, and this capability should
increase with time. A second is that Autonomous WMD would, unlike other WMDs,
leave the targeted territory relatively undamaged and quickly inhabitable.
In certain ways these are major advantages: a (somewhat more) responsible actor
could use this capability to target only military personnel insofar as they are
distinguishable, or target only the leadership structure of some rogue organization,
without harming civilians or other bystanders. Even if such distinctions are diﬃcult,
such weapons could relatively easily never target children, the wounded, etc. And a
military victory would not necessarily be accompanied by the physical destruction of
an adversary's infrastructure and economy.
The unfortunate ﬂip-side of these diﬀerences, however, is that anti-personnel lethal
AWSs are much more likely to be used. In terms of "bad actors," along with the
advantages of being safe to transport and hard to detect, the ability to selectively
attack particular types of people who have been identiﬁed as worthy of killing will help
assuage the moral qualms that might otherwise discourage mass killing. Particular
ethnic groups, languages, uniforms, clothing, or individual identities (culled from the
internet and matched using facial recognition) could all provide a basis for targeting
and rationalization. And scalable destruction of physical assets would make
autonomous WMDs far more strategically eﬀective for seizing territory.
Autonomous WMDs would pose all of the same sorts of threats that other ones do,[12]
from acts of terror to geopolitical destabilization to catastrophic conﬂict between
major powers. Tens of billions of USD are spent by the US and other states to prevent
terrorist actions using WMDs and to prevent the "wrong" states from acquiring them.
And recall that a primary (claimed) reason for the Iraq war (at trillions of USD in total
cost) was its (claimed) possession of WMDs. It thus seems foolish in the extreme to
allow - let alone implicitly encourage - the development of a new class of WMDs that
could proliferate much more easily than nuclear weapons.
Lethal autonomous weapons as destabilizing elements in and out of war
On the list of most important things in the world, retaining global international peace
and stability rates very highly; instability is a critical risk factor for global catastrophic
or X-risk. Even nuclear weapons, probably the greatest current catastrophic risk, are
arguably stabilizing against large-scale war. In contrast, there are many compelling

reasons to see autonomous weapons as a destabilizing eﬀect, perhaps profoundly so.
[13]
For a start, AWSs like slaughterbots are ideal tools of assassination and terror, hence
deeply politically destabilizing. The usual obstacles to one individual killing another -
technical diﬃculty, fear of being caught, physical risk during execution, and innate
moral aversion - are all lowered or eliminated using a programmable autonomous
weapon. All else being equal, if lethal AWSs proliferate, this will make both political
assassinations and acts of terror inevitably more possible, and dramatically so if the
current rate is limited by any of the above obstacles. Our sociopolitical systems react
very strongly to both types of violence, and the consequences are unpredictable but
could be very large-scale. Tallying up the economic cost of the largest terror attacks to
date - those on 9/11 - surely reaches into trillions of $USD, with an accompanying
social cost of surveillance, global conﬂict, and so on.
Second, like drone warfare, lethals AWSs are likely to further (and more widely) lower
the threshold of state violence toward other states. The US, for one, has shown little
reluctance to strike targets of interest in certain other countries, and lethal AWSs
could diminish the reluctance even more by lowering the level of collateral damage.
[14] This type of action might spread to other countries that currently lack the US's
technical ability to accomplish such strikes. Lethal (or nonlethal) AWSs could also
increase states' ability to perpetrate violence against its own citizens; whether this
increases or decreases stability of those states, seems, however, unclear.
Third, AWSs of all types threaten to upset the status quo of military power. The
advantage of major military powers rests on decades of technological advantage
coupled with vast levels of spending on training and equipment. A signiﬁcant part of
this investment and advantage would be nulliﬁed by a new class of weapon that
evolves on software rather than hardware timescale. Moreover, even if the current
capability "ranking" of military powers were preserved, for a weapon that strongly
favors oﬀense (as some have argued for antipersonnel AWSs) there may be no
plausible technical advantage that suﬃces[15] - indeed this is a key reason that major
military powers are so concerned about nuclear proliferation.
Finally, and probably most worrisome, if there is an open arms race in AWSs of all
types, we see a dramatically increased risk of accidental triggering or escalation of
armed conﬂict.[16] A crucial desirable feature of AWSs from the military point of view is
to be able to understand and predict[17] how they will operate in a given situation:
under what conditions will they take action on what sorts of targets, and how. This is a
very diﬃcult technical problem because, given the variety of situations in which an
AWS might be placed, it could easily fall outside the context of its training data. But it
is a very crucial one: without such an understanding, ﬁelding an AWS would raise a
spectrum of potential unintended consequences.
But now consider a situation in which AWSs are designed to attack and defend against
other AWSs. In this case, predictability of how a given AWS will function turns from a
desirable feature (for military decision makers to understand how their weapon will
function) into an exploitable liability.[18] There will then be a very strong conﬂict
between the desire to make an AWS predictable to its user, and the necessity of
making it unpredictable and unexploitable to its adversary. This is likely to manifest as
a parallel conﬂict between a simple set of clear and followable rules (making the AWS
more predictable) versus a high degree of ﬂexibility and "improvisation" (making the
AWS more eﬀective but less predictable.) This competition would happen alongside a
competition in the speed of the OODA (Observe, Orient, Decide, Act) loop. The net

eﬀect seems to inevitably point to a situation in which AWSs react to each other in a
way that is both unpredictable in advance, and too fast for humans to intervene.
There seems little opportunity for such conﬂict between such weapons to de-escalate.
Inadvertent military conﬂict is already a major problem when humans are involved
who fully understand the stakes. It seems very dangerous to have a situation in which
the ability to resist or forestall such escalation would be seen as a major and
exploitable military disadvantage.
Keeping the threshold for war high is very obviously very important but it is worth
looking at the numbers. A large-scale nuclear war is unbelievably costly: it would most
likely kill 1-7Bn in the ﬁrst year and wipe out a large fraction of Earth's economic
activity (i.e. of order one quadrillion USD or more, a decade worth of world GDP.)Some
current estimates of the likelihood of global-power nuclear war over the next few
decades range from ~0.5-20%. So just a 10% increase in this probability, due to an
increase in the probability of conﬂict that leads to nuclear war, costs in expectation
~500K - 150m lives and ~$0.1-10Tn (not counting huge downstream life-loss and
economic losses). Insofar as saving the lives of soldiers is an argument in favor of
deploying AWSs, it seems exceedingly unlikely that substituting lethal AWSs for
soldiers will ever save this many lives or value: AWSs are unlikely to save any lives in
a global thermonuclear war, and it is hard to imagine a conventional war of large
enough scale that AWSs could substitute for this many humans, without the war
escalating into a nuclear one. In other words, imagine a war with human combatants
in which N are expected to die, with probability Pn of that or another related war
escalating into a nuclear exchange costing M lives. We suppose that we might replace
these N human combatants with autonomous ones but at the cost of increasing the
probability to Py. The expected deaths are N + pnM in the human-combatant case and
PyM in the autonomous combatant case, with a diﬀerence in fatalities of (
Py −Pn)(M −N). Given how much larger M (~1-7 Bn) is than N (tens of thousands at
most) it only takes a small diﬀerence (Py −Pn) for this to be a very poor exchange.
What should be done?
We've argued above that the issue of autonomous weapons is not simply concerns
about soulless robots killing people or discomfort with the inevitable applications of AI
to military purposes. Rather, particular properties of autonomous weapons seem likely
to lead, in expectation, to a substantially more dangerous world. Moreover, actions to
mitigate this danger may even help - via precedent and capability-building - in
mitigating others. The issue is also relatively tractable - at least for now, and in
comparison to more intractable-but-important issues like nuclear accident risk or the
problematic business model of certain big tech companies. Although involvement of
militaries makes it diﬃcult, there is as yet relatively little strong corporate interest in
the issue.[19] International negotiations exist and are underway (though struggling to
make signiﬁcant headway.) It is also relatively neglected, with a small number of
NGOs working at high activity, and relatively little public awareness of the issue. It is
thus a good target for action by usual criteria.

Arguments against being concerned with autonomous weapons appear to fall into
three general classes:[20]The ﬁrst is that autonomous weapons are a net good. The
second is that autonomous weapons are an inevitability, and there's little or nothing
to be done about it. The third is simply that this is "somebody else's problem," and
low-impact relative to other issues to which eﬀort and resources could be devoted.[21]
We've argued above against all three positions: the expected utility of widespread
autonomous weapons is likely to be highly negative (due to increase probability of
large-scale war, if nothing else), the issue is addressable (with multiple examples of
past successful arms-control agreements), currently tractable if diﬃcult, and success
would also improve the probability of positive results in even more high-stakes arenas
including global AGI governance.
If the issue of autonomous weapons is important, tractable and neglected, it is worth
asking what success would look like. Many of the above concerns could be
substantially mitigated via an international agreement governing autonomous
weapons; unfortunately they are unlikely to be signiﬁcantly impacted by lesser
measures. Arguments against such an agreement tend to focus on how hard or
eﬀective it would be, or conﬂate very distinct considerations or weapons classes. But
there are many possible provisions such an agreement could include that would be
net-good and that we believe many countries (including major military powers) might
agree on. For example:
Some particular, well-deﬁned, classes of weapons could be prohibited (as
biological weapons, laser blinding weapons, space-based nuclear weapons, etc.,
are currently). Weapons with high potential for abuse and relatively little real
military advantage to major powers (like slaughterbots) should be ﬁrst in line.
Automated primarily defensive weaponry targeting missiles or other unmanned
objects, or non-injurious AWSs, very probably should not be prohibited in
general. The grey area in the middle should be worked out in multilateral
negotiation.
For whatever is not prohibited, there could be agreements (supplemented by
internal regulations) regarding proliferation, tracking, attribution, human control,
etc., to AWSs; for some examples see this "Roadmapping exercise," which
emerged as a sketch of consensus recommendations from a meeting between
technical experts with a very wide range of views on autonomous weapons.
Highlighting the risks of autonomous weapons may also encourage militaries to invest
substantially in eﬀective defensive technologies (especially those that are non-AI
and/or that are purely defensive rather than force-on-force) against lethal autonomous
weapons, including the prohibited varieties. This could lead to (an imperfect but far
less problematic than our current trajectory) scenario in which anti-personnel AWSs
are generally prohibited, yet defended against, and other AWSs are either prohibited
or governed by a strong set of agreements aimed at maintaining a stable detente in
terms of AI weapons.
FLI has advanced the view - widely shared in the AI research community - that the
world will be very ill-served by an arms race and unfettered buildup in autonomous
weaponry. Our conﬁdence in this is quite high. We have further argued here that the
stakes are signiﬁcantly greater than many have appreciated, which has motivated
both FLI's advocacy in this area as well as this posting. Less clear is how much and
what can be done about the dynamics driving us in that direction. We welcome
feedback both regarding the arguments put forth in this piece, and more generally
about what actions can be taken to best mitigate the long-term risks that autonomous
weapons may pose.

I thank FLI staﬀ and especially Jared Brown and Emilia Javorsky for helpful feedback
and notes on this piece.
Notes
1. As a major advantage, nonlethal autonomous weapons need not defend
themselves and so can take on signiﬁcant harm in order to prevent harm while
subduing a human. On the other hand if such weapons become _too _eﬀective
they may make it too easy and "low-cost" for authoritarian governments to
subdue their populace. ↩ 
2. Though we would note that converting a nonlethal autonomous weapon into a
lethal one could require relatively small modiﬁcation, as it would really amount
to using the same software on diﬀerent hardware (weapons). ↩ 
3. Autonomy also created new capabilities - like swarms - that are wholly new and
will subvert existing weapons categories. The versatility of small, scalable, lethal
AWS is of note here, as they might be quickly repurposed for a variety of target
types, with many combining to attack a larger target. ↩ 
4. The claim here is not that nuclear weapons are without beneﬁt (as they arguably
have been a stabilizing inﬂuence so far), but the arms race to weapons numbers
far beyond deterrence probably is. Understanding of nuclear winter laid bare the
lose-lose nature of the nuclear arms race: even if one power were able to
perform a magically eﬀective ﬁrst strike to eliminate all of the enemy's
weapons, that power would still ﬁnd itself with a starving population. ↩ 
5. AI will be unavoidably tied to military capability, as it has appropriate roles in the
military that would be unpreventable even if this were desirable. However this is
very diﬀerent from an unchecked arms race, and de-linking AI and weaponry as
much as possible seems a net win. ↩ 
6. For example in polling for the Asilomar Principles among many of the world's
foremost AI researchers, Principle 18, "An arms race in lethal autonomous
weapons should be avoided," polled the very highest. ↩ 
7. This survey shows about 61% pro and 22% con for their use. This article points
to a more recent EU poll with high (73%) support for an international treaty
prohibiting them. It should be noted that both surveys were commissioned by
the Campaign to Stop Killer Robot. This study argues that opinions can easily
change due to additional factors, and in general we should assume that public
understanding of autonomous weapons and their implications is fairly low. ↩ 
8. There is signiﬁcant literature and debate on the diﬃculty of satisfying
requirements of international law in distinction, proportionality; see e.g. this
general analysis, and this discussion of general issues of human vs. machine
control. Beyond the question of legality are moral questions, as explored in
detail here for example. ↩ 
9. The term "WMD" is somewhat poorly deﬁned, sometimes conﬂated with the trio
of chemical, biological and nuclear weapons. But if we deﬁne WMDs in terms of
characteristics such that the term could at least in principle apply both to and

beyond nuclear, chemical and biological weapons, then it's hard to avoid
including anti-personnel AWSs. One might include additional or alternative
characteristics that (a) WMDs must be very destructive, and/or (b) that they are
highly indiscriminate, and/or (c) that they somehow oﬀend human sensibilities
through their mode of killing. However, (a) chemical and biological weapons are
not necessarily destructive (other than to life/humans); (b) if biological weapons
are made more discriminate, e.g. to attack only people with some given set of
genetic markers, they would almost certainly still be classed as WMDs and
arguably be of even more concern; (c) "oﬀending sensibilities" is rather murkily
deﬁned. ↩ 
10. It has been argued that increasing levels of autonomy in loitering munition
systems represent a slippery slope, behaving functionally as lethal autonomous
weapons on the battleﬁeld. Some of the systems identiﬁed as of highest concern
and also of lower cost relative to large drones have been deployed in recent
drone conﬂicts in Libya and Nagorno-Karabakh. ↩ 
11. While speculating on particular technologies is probably not worthwhile, note
that the physical limits are quite lax. For example, ten million gnat-sized drones
carrying a poison (or noncontagious bioweapon) payload could ﬁt into a suitcase
and ﬂy at 1 km/hr (as gnats do). ↩ 
12. Note that autonomous WMDs could also be combined with or enable other ones:
miniature autonomous weapons could eﬃciently deliver a tiny chemical,
biological or radiological payload, combining the high lethality of existing WMDs
with the precision of autonomous ones. ↩ 
13. For some analyses of this issue see this UNIDIR report and this piece. Even the
dissertation by Paul Scharre, concludes that "The widespread deployment of
fully autonomous weapons is therefore likely to undermine stability because of
the risk of unintended lethal engagements." and recommends regulatory
approaches to mitigate the issue. ↩ 
14. In the case of the US, this eﬀect is likely to be present even if lethal AWSs were
prohibited - human-piloted microdrones or swarms should be able to provide
most of the advantages as lethal AWSs, except in rare circumstances when the
signal can be blocked. ↩ 
15. Israel presents a particularly important case. While its small population
motivates replacing or augmenting human soldiers with machines, to us it
seems unwise to seek unchecked global development of lethal AWSs, when it is
surrounded by adversaries perfectly capable of developing and ﬁelding them. ↩ 
16. This RAND publication lays out the argument in some detail. ↩ 
17. For detailed discussion of these terms, see e.g. this UNIDIR report. ↩ 
18. Autonomous weapons developers are already thinking along these lines of
course; see for example this article about planning to undermine drone swarms
by predicting and intervening in their dynamics. ↩ 
19. While arms manufacturers will tend to disfavor limitations on arms, few if any
are currently proﬁting from the sorts of weapons that might be prohibited by
international agreement, and there is plenty of scope for proﬁt-making in
designing defenses against lethal autonomous weapons, etc. ↩ 

20. We leave out disingenuous arguments against straw men such as "But if we give
up lethal autonomous weapons and allow others to develop them, we lose the
war." No one serious, to our knowledge, is advocating this - the whole point of
multilateral arms control agreements is that all parties are subject to them.
Ironically, though, this self-defeating position is the one taken at least formally
by the US (among others), for which current policy largely disallows (though see
this re-interpretation) fully autonomous lethal weapons, even while the US
argues against a treaty creating such a prohibition for other countries. ↩ 
21. A more pernicious argument that we have heard is that advocacy regarding
autonomous weapons is antagonistic to the US military and government, which
could lead to lack of inﬂuence in other matters. This seems terribly misguided to
us. We strongly believe US national security is served, rather than hindered, by
agreements and limitations on autonomous weapons and their proliferation.
There is a real danger that US policymakers and military planners are failing to
realize this precisely due to lack of input from experts who understand the
issues surrounding AI systems best. Moreover, neither the US government nor
the US military establishment are monolithic institutions, but huge complexes
with many distinct agents and interests. ↩ 

Squiggle: An Overview
Overview
I've spent a fair bit of time over the last several years iterating on a text-based probability
distribution editor (the 5 to 10 input editor in Guesstimate and Foretold). Recently I've added
some programming language functionality to it, and have decided to refocus it as a domain-
speciﬁc language.
The language is currently called Squiggle. Squiggle is made for expressing distributions and
functions that return distributions. I hope that it can be used one day for submitting complex
predictions on Foretold and other platforms. 
Right now Squiggle is very much a research endeavor. I'm making signiﬁcant sacriﬁces for
stability and deployment in order to test out exciting possible features. If it were being
developed in a tech company, it would be in the "research" or "labs" division.
You can mess with the current version of Squiggle here. Consider it in pre-alpha stage. If you
do try it out, please do contact me with questions and concerns. It is still fairly buggy and
undocumented.
I expect to spend a lot of time on Squiggle in the next several months or years. I'm curious to
get feedback from the community. In the short term I'd like to get high-level feedback, in the
longer term I'd appreciate user testing. If you have thoughts or would care to just have a call
and chat, please reach out! We (The Quantiﬁed Uncertainty Research Institute) have some
funding now, so I'm also interested in contractors or hires if someone is a really great ﬁt.
Squiggle was previously introduced in a short talk that was transcribed here, and Nuño
Sempere wrote a post about using it here. 
 
Note: the code for this has developed since my time on Guesstimate. With Guesstimate, I
had one cofounder, Matthew McDermott. During the last two years, I've had a lot of help
from a handful of programmers and enthusiasts. Many thanks to Sebastian Kosch and Nuño
Sempere, who both contributed.  I'll refer to this vague collective as "we" throughout this
post.
Video Demo
A Quick Tour

The syntax is forked from Guesstimate and Foretold.
A simple normal distribution
normal(5,2)
You may notice that unlike Guesstimate, the distribution is nearly perfectly smooth. It's this
way because it doesn't use sampling for (many) functions where it doesn't need to.
Lognormal shorthand
5 to 10
This results in a lognormal distribution with 5 to 10 being the 5th and 95th conﬁdence
intervals respectively.
You can also write lognormal distributions as: lognormal(1,2) or lognormal({mean: 3, stdev:
8}).
Mix distributions with the multimodal function
multimodal(normal(5,2), uniform(14,19), [.2, .8])
You can also use the shorthand mm(), and add an array at the end to represent the weights
of each combined distribution. 
Note: Right now, in the demo, I believe "multimodal" is broken, but you can use "mm".
Mix distributions with discrete data
Note: This is particularly buggy.
multimodal(0, 10, normal(4,5), [.4,.1, .5])

Variables
            expected_case = normal(5,2) 
long_tail = 3 to 1000 
multimodal(expected_case, long_tail, [.2,.8]) 
          
Simple calculations
When calculations are done on two distributions, and there is no trivial symbolic solution the
system will use Monte Carlo sampling for these select combinations. This assumes they are
perfectly independent.
multimodal(normal(5,2) + uniform(10,3), (5 to 10) + 10) * 100
Pointwise calculations
We have an inﬁx for what can be described as pointwise distribution calculations.
Calculations are done along the y-axis instead of the x-axis, so to speak. "Pointwise"
multiplication is equivalent to an independent Bayesian update. After each calculation, the
distributions are renormalized.
normal(10,4) .* normal(14,3)
First-Class Functions
When a function is written, we can display a plot of that function for many values of a single
variable. The below plots treat the single variable input on the x-axis, and show various
percentiles going from the median outwards.
            myFunction(t) = normal(t,10) 
myFunction 
          

            myFunction(t) = normal(t^3,t^3.1) 
myFunction 
          

Reasons to Focus on Functions
Up until recently, Squiggle didn't have function support. Going forward this will be the
primary feature. 
Functions are useful for two distinct purposes. First, they allow composition of models.
Second, they can be used directly to be submitted as predictions. For instance, in theory you
could predict, "For any point in time T, and company N, from now until 2050, this function
will predict the market cap of the company." 
At this point I'm convinced of a few things:

It's possible to intuitively write distributions and functions that return distributions, with
the right tooling.
Functions that return distributions are highly preferable to speciﬁc distributions, if
possible.
It would also be great if existing forecasting models could be distilled into common
formats.
There's very little activity in this space now.
There's a high amount of value of information to further exploring the space.
Writing a small DSL like this will be a fair bit of work, but can be feasible if the
functionality is kept limited.
Also, there are several other useful aspects about having a simple language equivalent
for Guesstimate style models.
I think that this is a highly neglected area and I'm surprised it hasn't been explored more. It's
possible that doing a good job is too challenging for a small team, but I think it's worth
investigating further.
What Squiggle is Meant For
The ﬁrst main purpose of Squiggle is to help facilitate the creation of judgementally
estimated distributions and functions.
Existing solutions assume the use of either data analysis and models, or judgemental
estimation for points, but not judgemental estimation to intuit models. Squiggle is meant to
allow people to estimate functions in situations where there is very little data available, and
it's assumed all or most variables will be intuitively estimated.
A second possible use case is to embed the results of computational models. Functions in
Squiggle are rather portable and composable. Squiggle (or better future tools) could help
make the results of these models interoperable.
One thing that Squiggle is not meant for is heavy calculation. It's not a probabilistic
programming language, because it doesn't specialize in inference. Squiggle is a high-level
language and is not great for performance optimization. The idea is that if you need to do
heavy computational modeling, you'd do so using separate tools, then convert the results to
lookup tables or other simple functions that you could express in Squiggle.
One analogy is to think about the online estimation "calculators" and "model explorers". See
the microCOVID Project calculator and the COVID-19 Predictions. In both of these, I assume
there was some data analysis and processing stage done on the local machines of the

analysts. The results were translated into some processed format (like a set of CSV ﬁles), and
then custom code was written for a front end to analyze and display that data. 
If they were to use a hypothetical front end uniﬁed format, this would mean converting their
results into a Javascript function that could be called using a standardized interface. This
standardization would make it easier for these calculators to be called by third party wigets
and UIs, or for them to be downloaded and called from other workﬂows. The priority here is
that the calculators could be run quickly and that the necessary code and data is minimized
in size. Heavy calculation and analysis would still happen separately.
Future "Comprehensive" Uses
On the more comprehensive end, it would be interesting to ﬁgure out how individuals or
collectives could make large clusters of these functions, where many functions call other
functions, and continuous data is pulled in. The latter would probably require some
server/database setup that ingests Squiggle ﬁles.
I think the comprehensive end is signiﬁcantly more exciting than simpler use cases but also
signiﬁcantly more challenging. It's equivalent to going from Docker the core technology, to
Docker hub, then making an attempt at Kubernetes. Here we barely have a prototype of the
proverbial Docker, so there's a lot of work to do.
Why doesn't this exist already?
I will brieﬂy pause here to ﬂag that I believe the comprehensive end seems fairly obvious as
a goal and I'm quite surprised it hasn't really been attempted yet, from what I can tell. I
imagine such work could be useful to many important actors, conditional on them
understanding how to use it.
My best guess is this is due to some mix between:
It's too technical for many people to be comfortable with.
There's a fair amount of work to be done, and it's diﬃcult to monetize quickly.
There's been an odd, long-standing cultural bias against clearly intuitive estimates.
The work is substantially harder than I realize.
Related Tools
Guesstimate
I previously made Guesstimate and take a lot of inspiration from it. Squiggle will be a
language that uses pure text, not a spreadsheet. Perhaps Squiggle could one day be made
available within Guesstimate cells. 
Ergo
Ought has a Python library called Ergo with a lot of tooling for judgemental forecasting. It's
written in Python so works well with the Python ecosystem. My impression is that it's made
much more to do calculations of speciﬁc distributions than to represent functions. Maybe
Ergo results could eventually be embedded into Squiggle functions.
Elicit
Elicit is also made by Ought. It does a few things, I recommend just checking it out. Perhaps
Squiggle could one day be an option in Elicit as a forecasting format.
Causal

Causal is a startup that makes it simple to represent distributions over time. It seems fairly
optimized for clever businesses. I imagine it probably is going to be the most polished and
easy to use tool in its targeted use cases for quite a while. Causal has an innovative UI with
HTML blocks for the diﬀerent distributions; it's not either a spreadsheet-like Guesstimate or a
programming language, but something in between. 
Spreadsheets
Spreadsheets are really good at organizing large tables of parameters for complex
estimations. Regular text ﬁles aren't. I could imagine ways Squiggle could have native
support for something like Markdown Tables that get converted into small editable
spreadsheets when being edited. Another solution would be to allow the use of JSON or TOML
in the language, and auto-translate that into easier tools like tables in editors that allow for
them.[2]
Probabilistic Programming Languages
There are a bunch of powerful Probabilistic Programming Languages out there. These
typically specialize in doing inference on speciﬁc data sets. Hopefully, they could be
complementary to Squiggle in the long term. As said earlier, Probabilistic Programming
Languages are great for computationally intense operations, and Squiggle is not.
Prediction Markets and Prediction Tournaments
Most of these tools have fairly simple inputs or forecasting types. If Squiggle becomes
polished, I plan to encourage its use for these platforms. I would like to see Squiggle as an
open-source, standardized language, but it will be a while (if ever) for it to be stable enough.
Declarative Programming Languages
Many declarative programming languages seem relevant. There are several logical or
ontological languages, but my impression is that most assume certainty, which seems vastly
suboptimal. I think that there's a lot of exploration for languages that allow users to basically
state all of their beliefs probabilistically, including statements about the relationships
between these beliefs. The purpose wouldn't be to ﬁnd one speciﬁc variable (as often true
with probabilistic programming languages), but to more to express one's beliefs to those
interested, or do various kinds of resulting analyses.
Knowledge Graphs
Knowledge graphs seem like the best tool for describing semantic relationships in ways that
anyone outside a small group could understand. I tried making my own small knowledge
graph library called Ken, which we've been using a little in Foretold. If Squiggle winds up
achieving the comprehensive vision mentioned, I imagine there will be a knowledge graph
somewhere.
For example, someone could write a function that takes in a "standard location schema" and
returns a calculation of the number of piano tuners at that location. Later when someone
queries Wikipedia for a town, it will recognize that that town has data on Wikidata, which can
be easily converted into the necessary schema.
Next Steps
Right now I'm the only active developer of Squiggle. My work is split between Squiggle,
writing blog posts and content, and other administrative and organizational duties for QURI.  
My ﬁrst plan is to add some documentation, clean up the internals, and begin writing short
programs for personal and group use. If things go well and we could ﬁnd a good developer to

hire, I would be excited to see what we could do after a year or two.
Ambitious versions of Squiggle would be a lot of work (as in, 50 to 5000+ engineer years
work), so I want to take things one step at a time.  I would hope that if progress is suﬃciently
exciting, it would be possible to either raise suﬃcient funding or encourage other startups
and companies to attempt their own similar solutions. 
Footnotes
[1] The main challenge comes from having a language that represents symbolic
mathematics and programming statements. Both of these independently seem challenging,
and I have yet to ﬁnd a great way to combine them. If you read this and have suggestions for
learning about making mathematical languages (like Wolfram), please do let me know.
[2] I have a distaste for JSON in cases that are primarily written and read by users. JSON was
really optimized for simplicity for programming, not people. My guess is that it was a mistake
to have so many modern conﬁguration systems be in JSON instead of TOML or similar.

Confucianism in AI Alignment
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
I hear there's a thing where people write a lot in November, so I'm going to try writing
a blog post every day. Disclaimer: this post is less polished than my median. And my
median post isn't very polished to begin with.
Imagine a large corporation - we'll call it BigCo. BigCo knows that quality management
is high-value, so they have a special program to choose new managers. They run the
candidates through a program involving lots of management exercises, simulations,
and tests, and select those who perform best.
Of course, the exercises and simulations and tests are not a perfect proxy for the
would-be managers' real skills and habits. The rules can be gamed. Within a few years
of starting the program, BigCo notices a drastic disconnect between performance in
the program and performance in practice. The candidates who perform best in the
program are those who game the rules, not those who manage well, so of course
many candidates devote all their eﬀort to gaming the rules.
How should this problem be solved?
Ancient Chinese scholars had a few competing schools of thought on this question,
most notably the Confucianists and the Legalists. The (stylized) Confucianists' answer
was: the candidates should be virtuous and not abuse the rules. BigCo should
demonstrate virtue and benevolence in general, and in return their workers should
show loyalty and obedience. I'm not an expert, but as far as I can tell this is not a
straw man - though stylized and adapted to a modern context, it accurately captures
the spirit of Confucian thought.
The (stylized) Legalists instead took the position obvious to any student of modern
economics: this is an incentive design problem, and BigCo leadership should design
less abusable incentives.
If you have decent intuition for economics, it probably seems like the Legalist position
is basically right and the Confucian position is Just Wrong. I don't want to discourage
this intuition, but I expect that many people who have this intuition cannot fully spell
out why the Confucian answer is Just Wrong, other than "it has no hope of working in
practice". After all, the whole thing is worded as a moral assertion - what people
should do, how the problem should be solved. Surely the Confucian ideal of everyone
working together in harmony is not wrong as an ideal? It may not be possible in
practice, but that doesn't mean we shouldn't try to bring the world closer to the
Confucian vision.
Now, there is room to argue with Confucianism on a purely moral front - everyone
working together in harmony is not synonymous with everyone receiving what they
deserve. Harmony does not imply justice. Also, there's the issue of the system being
vulnerable to small numbers of bad agents. These are fun arguments to have if you're
the sort of person who enjoys endless political/philosophical debates, but I bring it up
to emphasize that they are NOT the arguments I'm going to talk about here.

The relevant argument here is not a moral claim, but a purely factual claim: the
Confucian ideal would not actually solve the problem, even if it were fully
implemented (i.e. zero bad actors). Even if BigCo senior management were virtuous
and benevolent, and their workers were loyal and did not game the rules, the poor
rules would still cause problems.
The key here is that the rules play more than one role. They act as:
Conscious incentives
Unconscious incentives
Selection rules
In the Confucian ideal, the workers all ignore the bad incentives provided by the rules,
so conscious incentives are no longer an issue (as long as we're pretending that the
Confucian ideal is plausible in the ﬁrst place). Unconscious incentives are harder to
ﬁght - when people are rewarded for X, they tend to do more X, regardless of whether
they consciously intended to do so. But let's assume a particularly strong form of
Confucianism, where everyone ﬁghts hard against their unconscious biases.
That still leaves selection eﬀects.
Even if everyone is ignoring the bad incentives, people are still diﬀerent. Some people
will naturally act in ways which play more to the loopholes and weaknesses in the
rules, even if they don't intend to do so. (And of course, if there's even just a few bad
actors, then they'll deﬁnitely still abuse the rules.) And BigCo will disproportionately
select those people as their new managers. It's not necessarily maliciousness, it's just
Goodhart's Law: make decisions based on a less-than-perfect proxy, and it will cease
to be a good proxy.
Takeaway: even a particularly strong version of the Confucian ideal would not be
suﬃcient to solve BigCo's problem. Conversely, the Legalist answer - i.e. ﬁxing the
incentive structure - would be suﬃcient. Indeed, ﬁxing the incentive structure seems
not only suﬃcient but necessary; selection eﬀects will perpetuate problems even if
everyone is harmoniously working for the good of the collective.
Analogy to AI Alignment
The modern Ml paradigm: we have a system that we train oﬄine. During that training,
we select parameters which perform well in simulations/tests/etc. Alas, some parts of
the parameter space may abuse loopholes in the parameter-selection rules. In
extreme cases, we might even see malicious inner optimizers: subagents smart
enough to intentionally abuse loopholes in the parameter-selection rules.
How should we solve this problem?
One intuitive approach: ﬁnd some way to either remove or align the inner optimizers.
I'll call this the "generalized Confucianist" approach. It's essentially the Confucianist
answer from earlier, with most of the moralizing stripped out. Most importantly, it
makes the same mistake: it ignores selection eﬀects.
Even if we set up a training process so that it does not create any inner optimizers,
we'll still be selecting for the same bad behaviors which a malicious inner optimizer
would utilize.

The basic problem is that "optimization" is an internal property, not a behavioral
property. A malicious optimizer might do some learning and reasoning to ﬁgure out
that behavior X exploits a weakness in the parameter selection goal/algorithm. But
some other parameters could just happen to perform behavior X "by accident",
without any malicious intent at all. The parameter selection goal/algorithm will be just
as weak to this "accidental" abuse as to the "intentional" abuse of an inner optimizer.
The equivalent of the Legalists' solution to the problem would be to ﬁx the parameter-
selection rule: design a training goal and process which aren't abusable, or at least
aren't abusable by anything in the parameter space. In alignment jargon: solve the
outer alignment problem, and build a secure outer optimizer.
As with the Confucian solution to the BigCo problem, the Confucian solution is not
suﬃcient for AI alignment. Even if we avoid creating misaligned inner optimizers, bad
parameter-selection rules would still select for the same behavior that the inner
optimizers would display. The only diﬀerence is that we'd select for rules which
behave badly "by accident".
Conversely, the Legalist solution would be suﬃcient to solve the problem, and seems
necessary if we want to keep the general framework of optimization.
The main takeaway I want to emphasize here is that making our outer objective
"secure" against abuse is part of the outer alignment problem. This means outer
alignment is a lot harder than I think a lot of people imagine. If our proxy for human
values has loopholes which a hypothetical inner optimizer could exploit, then it's a
bad proxy. If an inner optimizer could exploit some distribution shift between the
training and deployment environments, then performance-in-training is a bad proxy
for performance-in-deployment. In general, outer alignment contains an implicit "for
all" quantiﬁer: for all possible parameter values, our training objective should give a
high value only if those parameters would actually perform well in practice.
The ﬂip side is that, since we probably need to build the Legalist solution anyway, the
Confucian solution isn't really necessary. We don't necessarily need to make any
special eﬀort to avoid inner optimizers, because our selection criteria need to be
secure against whatever shenanigans the inner optimizers could attempt anyway.
That said, I do think there are some good reasons to work on inner optimizers. The
biggest is imperfect optimization. In this context: our outer optimizer is not going to
check every single point in the parameter space, so the basin of attraction of any
misaligned behavior matters. If we expect that malicious inner optimizers will take up
a larger chunk of the parameter space than "accidental" bad behavior, then it makes
sense to worry more about "intentional" than "accidental" maligness. At this point, we
don't really know how to tell how much of a parameter space is taken up by malicious
agents, or any sort of inner agents; one example of this kind of problem is Paul's
question about whether minimal circuits are daemon-free.
Taking the analogy back to the BigCo problem: if it's very rare for workers to
accidentally game the rules, and most rule-gaming is intentional, then the Confucian
solution makes a lot more sense.
I also expect some people will argue that malicious inner optimizers would be more
dangerous than accidental bad behavior. I don't think this argument quite works - in
suﬃciently-rish parameter spaces, I'd expect that there are non-agenty parameter
combinations which exhibit the same behavior as any agency combinations.
Optimization is an internal property, not a behavioral property. But a slight

modiﬁcation of this argument seems plausible: more dangerous behaviors take up a
larger fraction of the agenty chunks of parameter space than the non-agenty chunks.
It's not that misaligned inner optimizers are each individually more dangerous than
their behaviorally-identical counterparts, it's that misaligned optimizers are more
dangerous on average. This would be a natural consequence to expect from
instrumental convergence, for instance: a large chunk of agenty parameter space all
converges to the same misbehavior. Again, this threat depends on imperfect
optimization - if the optimizer is perfect, then "basin of attraction" doesn't matter.
Again taking the analogy back to the BigCo problem: if most accidental-rule-abusers
only abuse the rules a little, but intentional-rule-abusers usually do it a lot, then the
Confucian solution can help a lot.
Of course, even in the cases where the Confucian solution makes relatively more
sense, it's still just an imperfect patch; it still won't ﬁx "accidental" abuse of the rules.
The Legalist approach is the full solution. The selection rules are the real problem
here, and ﬁxing the selection rules is the best possible solution.

A guide to Iterated Ampliﬁcation &
Debate
Crossposted from the AI Alignment Forum. May contain more technical jargon than
usual.
This post is about two proposals for aligning AI systems in a scalable way:
Iterated Distillation and Ampliﬁcation (often just called 'Iterated Ampliﬁcation'), or
IDA for short,[1] is a proposal by Paul Christiano.
Debate is an IDA-inspired proposal by Geoﬀrey Irving.
This post is written to be as easy to understand as possible, so if you found existing
explanations of IDA confusing, or if you just never bothered because it seemed
intimidating, this post is for you. The only prerequisite is knowing about the concept of
outer alignment (and knowing about inner alignment is helpful as well). Roughly,
Outer alignment is aligning the training signal or training data we give to our
model with what we want.
If the model we ﬁnd implements its own optimization process, then inner
alignment is aligning [the thing the model is optimizing for] with the training
signal.
See also this post for an overview and this paper or my ELI12 edition for more details
on inner alignment.
1. Motivation / Reframing AI Risk
Why do we need a fancy alignment scheme?
There has been some debate a few months back about whether the classical
arguments of the kind made in Superintelligence for why AI is dangerous hold up to
scrutiny. I think a charitable reading of the book can interpret it as primarily defending
one claim, which is also an answer to the leading question. Namely,
It is hard to deﬁne a scalable training procedure that is not outer-
misaligned.
For example, a language model (GPT-3 style) is outer-misaligned because the objective
we train for is to predict the most likely next word, which says nothing about being
'useful' or 'friendly'. Similarly, a question-answering system trained with Reinforcement
Learning is outer-misaligned because the objective we train for is 'optimize how much
the human likes the answer', not 'optimize for a true and useful answer'.
I'll refer to this claim as (∗). If (∗) true, it is a problem even under the most optimistic
assumptions. For example, we can suppose that
1. progress is gradual all the way, and we can test everything before we deploy it;

2. we are likely to maintain control of AI systems (and can turn them oﬀ whenever
we want to) for a while after they exceed our capabilities;
3. it takes at least another 50 years for AI to exceed human capabilities across a
broad set of tasks.
Even then, (∗) remains a problem. The only way to build an outer-aligned AI system is
to build an outer-aligned AI system, and we can't do it if we don't know how to do it.
In the past, people have given many examples of how outer alignment could fail (there
are a lot of those in Superintelligence, and I've given two more above). But the primary
reason to believe (∗) is that it has taken people a long time to come up with a
formalized training scheme that is not clearly outer-misaligned. IDA and Debate are two
such schemes.
If outer alignment works out, that alone is not suﬃcient. To solve the entire alignment
problem (or even just Intent Alignment[2]), we would like to have conﬁdence that an AI
system is
1. outer-aligned; and
2. inner-aligned (or not using an inner optimizer); and
3. training competitive; and
4. performance-competitive.
Thus, IDA and Debate are a long way from having solved the entire problem, but the
fact that they may be outer-aligned is reason to get excited, especially if you think the
alignment problem is hard.
2. The Key Idea
Training AI systems requires a training signal. In some cases, this signal is easy to
provide regardless of how capable the system is - for example, it is always easy to see
whether a system has won a game of Go, even if the system plays at superhuman
level. But most cases we care about are not of this form. For example, if an AI system
makes long-term economic decisions, we only know how good the decisions are after
they've been in place for years, and this is insuﬃcient for a training signal.
In such cases, since we cannot wait to observe the full eﬀects of a decision, any
mechanism for a more rapid training signal has to involve exercising judgment to
estimate how good the decisions are ahead of time. This is a problem once we assume
that the system is more capable than we are.
To the rescue comes the following idea:
The AI system we train has to help us during training.
IDA and Debate provide two approaches to do this.

3. Iterated Distillation and
Ampliﬁcation
Before we begin, here are other possible resources to understand IDA:
The LessWrong/Alignment Forum sequence (written by Paul Christiano)
The very long 80k hours podcast with Paul Christiano
The attempted complete explanation of the scheme by Chi Nguyen
The FAQ written by Alex Zhu
A video by Robert Miles (who makes lots of AI-Alignment relevant youtube
content)
This is Hannah.
Hannah, or H for short, is a pretty smart human. In particular, she can answer
questions up to some level of competence.
As a ﬁrst step to realize IDA, we wish to distill Hannah's competence at this question-
answering task into an AI system (or 'model') A1. We assume A1 will be slightly less
competent than Hannah, therefore Hannah can provide a safe training signal.

A1 may be trained by reinforcement learning or by supervised learning of any form.[3]
The basic approach of IDA leaves the distillation step as a black box, so any
implementation is ﬁne, as long as the following is true:
Given an agent as input, we obtain a model that imitates the agent's behavior at
some task but runs much faster.
The output model is only slightly less competent than the input agent at this task.
This process is alignment-preserving. In other words, if H is honest, then A0
should be honest as well.
If we applied A1 to the same question-answering task, it would perform worse:
However, A1 has vastly improved speed: it may answer questions in a few milliseconds
that would have taken H several hours. This fact lets us boost performance through a
step we call ampliﬁcation:
In the general formulation of the IDA scheme, ampliﬁcation is also a black box, but in
this post, we consider the basic variant, which we call stock IDA. In stock IDA,

ampliﬁcation is realized by giving H access to the model A1. The idea is that this new
'agent' (consisting of H with access to A1) is more competent than Hannah is by
herself.
If it is not obvious why, imagine you had access to a slightly dumber version of yourself
that ran at 10000 times your speed. Anytime you have a (sub)-question that does not
require your full intellect, you can relegate it to this slightly dumber version and obtain
an answer at once. This allows you to eﬀectively think for longer than you otherwise
could.
Thus, we conjecture that this combined 'agent' has improved performance (compared
to H) at the same question-answering task.
Here is a diﬀerent way of describing what happened. Our combined 'agent' looks like
this:
Since A1 tries to imitate H, we could think of Hannah as having access to an
(imperfect) copy of herself. But since A1 thinks much faster than H, it is more accurate
to view her as having access to many copies of herself, like so:

Where the gray circle means 'this is a model that tries to behave like the thing in the
circle.'
At this point, we've covered one distillation and one ampliﬁcation step. You might
guess what happens next:
We train a new model A2 to imitate the agent [H
access
⟶
A1] on the question-answering
task. Since [H
access
⟶
A1] is more competent than H, this means that A2 will be more
competent than A1 (which was trained to imitate just H).
In this example, A2 is almost exactly as competent as H. This is a good time to mention
of my performance numbers are made-up - the three properties they're meant to
convey are that
performance goes up in each ampliﬁcation step; and

performance goes down in each distillation step; but
performance goes up in each (ampliﬁcation step, distillation step) pair.
After each distillation step, we end up with some model Ak. While Ak was trained in a
very particular way, it is nonetheless just a model, which can answer questions very
quickly. Each Ak performs better than its predecessor Ak−1 without a loss of speed.
The next ampliﬁcation step looks like this:
Note that, in each ampliﬁcation step, we always give Hannah access to our newest
model. The Ak's get better and better, but Hannah remains the same human.
This new 'agent' is again more competent at the question-answering task:
Now we could train a model A3 to imitate the behavior of [H
access
⟶
A2] on the question-
answering task, which would then be less competent than the system above, but more
competent than A2 (and in our case, more competent than H). It would still be a model
and thus be extremely fast. Then, we could give Hannah access to A3, and so on.
One way to summarize this process is that we're trying to create a model that imitates
the behavior of a human with access to itself. In particular, each model Ak imitates the
behavior of [H
access
⟶
Ak−1]. Does this process top out at some point? It's conceivable
(though by no means obvious) that it does not top out until Ak is superintelligent. If so,

and if distillation and ampliﬁcation are both alignment-preserving, our scheme would
be both aligned and performance-competitive.
Recall that our 'agent' [H
access
⟶
A2] now looks like this:
Since A2 tries to imitate [H
access
⟶
A1], we can alternatively depict this as

Once again, we draw more than one of these since A2 is much faster than [H
access
⟶
A1],
so it is as if H had access to a lot of these, not just one. (Also not just three, but I only
have that much space.)
Since each A1 tries to imitate H, we can depict this further like so:

Thus, insofar as the imitation step 'works' (i.e., insofar as we can ignore the circles),
the resulting system will behave as if it were composed of Hannah consulting many
copies of herself, each of which consulting many copies of herself. This is after
precisely four steps, i.e., distillation → ampliﬁcation → distillation → ampliﬁcation. You
can guess how it would look if we did more steps.
The name 'Hannah' is a bit on-the-nose as her name starts with 'H', which also stands
for 'human'. Thus, the tree above consists of a human consulting humans consulting
humans consulting humans consulting humans consulting humans consulting
humans...
We call the entire tree HCH,[4] which is a recursive acronym for Humans consulting
HCH. Generally, HCH is considered to have inﬁnite depth.
Note that there is an important caveat hidden in the clause 'insofar as the imitation
step works'. In each distillation step, we are training a model to predict the answers of

a system that thinks for much longer than itself. Thus, each Ak is only more competent
than Ak−1 insofar as it is possible to solve problems in less time through better
algorithms. There are strong reasons to believe that this is the case for a large class of
tasks, but we know that it isn't possible for every task. For example, an HCH tree can
play perfect chess (literally perfect, not just superhuman) by searching the entire chess
tree.[5] A model trained by IDA cannot do the same.
In the aforementioned LessWrong sequence, the illustration for the Distillation →
Ampliﬁcation process looks like this:
Alternatively, if we consider all of the Ak's to be the same AI system that gets upgraded
over time, we have the following (where r denotes a reward signal).
4. Factored Cognition
Informally, the Factored Cognition Hypothesis says that each question can be
decomposed into easier subquestions such that the answer to the original question
follows from the answer to the subquestions. Factored Cognition plays a crucial role for
the applicability of both Debate and many instances of IDA.[6]
Here is an illustration, where the top block is a question, each layer below a block is a
set of subquestions whose answers determine the top-level question, and
darkness/size of the blocks corresponds to diﬃculty:

We might now hope that the absolute diﬃculties look something like this:
Where the lower part is meant to indicate that we can decompose all of the above
questions such that they eventually bottom out in the lowest stripe of questions smart
humans can answer in 15 minutes.
I see two ways to illustrate why Factored Cognition is important for stock IDA. One is
the HCH picture - insofar as the imitations 'work', a model trained via stock IDA
behaves just like a tree of humans consulting each other. Thus, if the model is
supposed to be superintelligent, then we better hope that any question a
superintelligent AI could answer can be recursively decomposed into subquestions,
until we end up with something Hannah can answer by herself. (Otherwise, stock IDA
may not be performance-competitive.) In other words, we better hope that the
Factored Cognition Hypothesis holds.
Another way is to look at just one ampliﬁcation step in the procedure. Suppose that we
have successfully trained model A8, which is already smarter than H, and now want to

use this to create the smarter agent [H
access
⟶
A8]. Suppose that A8 is already smart
enough to answer super hard questions. We want the new agent to be smarter than A8,
so we want it to be able to answer super-duper hard questions. In other words, we're in
this position:
This means that, to answer this question, Hannah has to do the following:
She has to take the question Q and decompose it into subquestions q1, q2, q3, q4, such
that the subquestions imply the answer to Q, and each qi is at most super hard. Then,
she can use A8 to answer the qi, receive answers ai, and, on their basis, output an
answer a for Q.

This means that she requires the Factored Cognition Hypothesis to hold for this
particular step (the one from super-duper hard to super hard). If the Factored Cognition
Hypothesis fails for any one jump of diﬃculty, performance might grind to a halt at that
level.
Both views point to the same phenomenon because they describe the same idea: HCH
is idealized stock IDA, i.e., it is what stock IDA hopes to approximate in the limit. Both
the concrete training procedure and the ideal utilize Factored Cognition.
It is also conceivable that a decomposition of the kind that Hannah needs to solve this
problem does exist, but she is not smart enough to ﬁnd it. This problem can be
considered a motivation for Debate.[7]
5. Debate
Before we begin, here are other possible resources to understand Debate:
The original paper
The AI Alignment podcast episode with Geoﬀrey Irving
Suppose a smart agent X makes the following argument:
She wants to argue that D holds. Thus, she claims that A is true, that A implies B
because {argument symbolized by leftmost arrow}, that B implies C because
{conjunction of the arguments made by middle arrows} and that C implies D because
{argument made by rightmost arrow}.
Then comes forth an equally smart agent Y  to claim that

X cannot give up on the step since the entire argument depends on it, so she has to
defend it. Unlike in normal debates, both X and Y  now forget about the surrounding
context: the steps from A to B and from C to D no longer matter (and neither does the
ﬁrst conjunct of the step from B to C). The remaining debate is entirely about the
second conjunct of the step from B to C.
Thus, we zoom into this step. It turns out there is more going on; the step does itself
has structure to it.
Then sayeth X:
Then sayeth Y :
Now, X has to defend this step, and so on. Eventually, the steps become so simple that
Hannah can recognize the ﬂaw for herself. The step from B23112 to B23113 was false;
therefore the step from B2311 to B2312 was false; therefore the step from B231 to B232
was false; therefore the step from B23 to B24 = C was false; therefore the step from B2
to C was false; therefore the argument that A implies D was false. X was wrong; Y  was
right.
In the Debate game, an AI system X debates Y , an identical copy of itself. The input
to the game is a question; the output is a transcript of the arguments made by X and Y
. The game proceeds in rounds: ﬁrst, X says something, then Y , then X, then Y , and so

on. Ideally, the ﬁrst statement should always be the answer to the input question. In
the end, a human H decides who won the game. In doing so, H should adhere to the
following principles:
Enforce recursive zooming into statements. If Y  points to a particular part of X's
argument that she claims is wrong, and X responds by changing topics, then the
game should end immediately, and Y  should win.
If a subquestion is resolved in favor of either party, always judge the entire
debate based on who was right on that subquestion.
To illustrate why the second point makes sense, suppose that the debate above ends
diﬀerently. X and Y  still zoom into the problem until they hit step B23112 →B23113, but
this time, Hannah thinks the step is valid. In this case, you might suspect that this is
not conclusive since the argument could fail at another step. However, since Y  was
free to critique any part of the entire argument, chose this particular step, and failed to
establish a ﬂaw, we conclude that she could not have done better anywhere else.[8]
Thus, based solely on verifying the validity of step B23112 →B23113, Hannah concludes
that every part of the argument was valid and that X won the debate.
This also illustrates why Debate can be powerful. If complexity is reduced at every step
(i.e., B23112 →B23113 is easier to verify than B2311 →B2312 and so on), then a hard
problem can be reduced to a much simpler problem, and the judge only has to verify
the simple one.[9] On this note, recall the question-diﬃculty hierarchy from the
previous chapter:
The recursive zooming into statements from Debate is closely analogous to the
decomposition into subquestions from HCH. The important diﬀerence is how the tree
gets sliced up during the training process. If [H
access
⟶
A8] is asked a super-duper hard

question Q, but A8 can only answer super hard questions, then it's her job to
decompose Q into q1,...,qk such that the qi's determine Q and all of them are at most
super hard. In other words, we're taking a local slice of the tree.
Of course, each distillation step will require many such slices, and every subsequent
distillation step will require many such slices at a higher level of the tree.
Conversely, each Debate game slices an entire vertical path out of the tree.
If Debate is ever implemented, we won't start oﬀ with superintelligent agents, and so
we won't start oﬀ with super-duper6-hard questions either. Consequently, the paths
may be very short in the beginning and grow longer over time. Either way, each path
should always go all the way to the bottom.[10]
The other diﬀerence is what I've mentioned at the end of the previous chapter: in
Debate, the human never has to deal with ﬁguring out how to decompose a question.
At most, she has to ﬁgure out whether a decomposition is valid.[11] The way Geoﬀrey

Irving put it is that a debate transcript (as depicted above) corresponds to a single path
through the tree of ampliﬁcation.[12] Crucially, it is a path chosen by the two Debate
agents.
6. Comparison
Both IDA and Debate...
may or may not be outer-aligned
try to utilize the AI systems they're trying to train during the training process
are designed to scale up to superintelligence
rely on some version of the Factored Cognition Hypothesis to be applicable[13]
since they traverse the tree of diﬃcult problems/questions
However, IDA...
carves a local slice out of the tree at each training step
has no built-in solution for decomposing questions into subquestions
A separate model may be trained for this purpose, or the questions may go
meta, i.e., "what is a good way to decompose this question?"
Insofar as this makes the decompositions worse, it implies that a shallow
HCH tree is less powerful than a shallow Debate tree.
can look very diﬀerent depending on how the ampliﬁcation and distillation
black boxes are implemented
only approximates HCH insofar as all distillation steps 'work'
Whereas Debate...
carves a vertical slice/path out of the tree at each training step
Therefore, it relies on the claim that such a path reliably provides
meaningful information about the entire tree.
probably won't be training-competitive in the above form since each round
requires human input
This means one has to train a second model to imitate the behavior of a
human judge, which introduces further diﬃculties.
requires that humans can accurately determine the winner of a debate with
debaters on every level of competence between zero and superintelligence
could maybe tackle Inner Alignment concerns by allowing debaters to win the
debate by demonstrating Inner Alignment failure in the other debater via the use
of transparency tools
7. Outlook
Although this post is written to work as a standalone, it also functions as a prequel to a
sequence on Factored Cognition. Unlike this post, which is summarizing existing work,
the sequence will be mostly original content.
If you've read everything up to this point, you already have most of the required
background knowledge. Beyond that, familiarity with basic mathematical notation will
be required for posts one and two. The sequence will probably start dropping within a
week.

1. As far as I know, the proposal is most commonly referred to as just 'Iterated
Ampliﬁcation', yet is most commonly abbreviated as 'IDA' (though I've seen 'IA'
as well). Either way, all four names refer to the same scheme. ↩ 
2. Intent Alignment is aligning [what the AI system is trying to do] with [what we
want]. This makes it the union of outer and inner alignment. Some people
consider this the entire alignment problem. It does not include 'capability
robustness'. ↩ 
3. I think the details of the distillation step strongly depend on whether IDA is used
to train an autonomous agent (one which takes agents by itself), or a non-
autonomous agent, one which only takes actions if queried by the user.
For the autonomous case, you can think of the model as an 'AI assistant', a
system that autonomously takes actions to assist you in various activities. In this
case, the most likely implementation involves reinforcement learning.
For the non-autonomous case, you can think of the model as an oracle: it only
uses its output channels as a response to explicit queries from the user. In this
case, the distillation step may be implemented either via reinforcement learning
or via supervised learning on a set of (question, answer) pairs.
From a safety perspective, I strongly prefer the non-autonomous version, which is
why the post is written with that in mind. However, this may not be
representative of the original agenda. The sequence on IDA does not address this
distinction explicitly. ↩ 
4. Note that, in the theoretical HCH tree, time freezes for a node whenever she asks
something to a subtree and resumes once the subtree has delivered the answer,
so that every node has the experience of receiving answers instantaneously. ↩ 
5. It's a bit too complicated to explain in detail how this works, but the gist is that
the tree can play through all possible combinations of moves and counter-moves
by asking each subtree to explore the game given a particular next move. ↩ 
6. In particular, it is relevant for stock IDA where the ampliﬁcation step is
implemented by giving a human access to the current model. In principle, one
could also implement ampliﬁcation diﬀerently, in which case it may not rely on
Factored Cognition. However, such an implementation would also no longer
imitate HCH in the limit, and thus, one would need an entirely diﬀerent argument
for why IDA might be outer-aligned. ↩ 
7. Geoﬀrey Irving has described Debate as a 'variant of IDA'. ↩ 
8. This is the step where we rely on debaters being very powerful. If Y  is too weak
to ﬁnd the problematic part of the argument, Debate may fail. ↩ 
9. Formally, there is a result that, if the judge can solve problems in the complexity
class P, then optimal play in the debate game can solve problems in the
complexity class PSPACE. ↩ 

10. Given such a path p, the value |p| (the total number of nodes in such a path) is
bounded by the depth of the tree, which means that it grows logarithmically with
the total size of the tree. This is the formal reason why we can expect the size of
Debate transcripts to remain reasonably small even if Debate is applied to
extremely hard problems. ↩ 
11. Note that even that can be settled via debate: if Y  claims that the decomposition
of X is ﬂawed, then X has to defend the decomposition, and both agents zoom
into that as the subproblem that will decide the debate. Similarly, the question of
how to decompose a question in IDA can, in principle, itself be solved by
decomposing the question 'how do I decompose this question' and solving that
with help from the model. ↩ 
12. This is from the podcast episode I've linked to at the start of the chapter. Here is
the relevant part of the conversation:
Geoﬀrey: [...] Now, here is the correspondence. In ampliﬁcation, the human does
the decomposition, but I could instead have another agent do the decomposition.
I could say I have a question, and instead of a human saying, "Well, this question
breaks down into subquestions X, Y, and Z," I could have a debater saying, "The
subquestion that is most likely to falsify this answer is Y." It could've picked at any
other question, but it picked Y. You could imagine that if you replace a human
doing the decomposition with another agent in debate pointing at the ﬂaws in the
arguments, debate would kind of pick out a path through this tree. A single
debate transcript, in some sense, corresponds to a single path through the tree of
ampliﬁcation.
Lucas: Does the single path through the tree of ampliﬁcation elucidate the truth?
Geoﬀrey: Yes. The reason it does is it's not an arbitrarily chosen path. We're sort
of choosing the path that is the most problematic for the arguments. ↩ 
13. To be precise, this is true for stock IDA, where ampliﬁcation is realized by giving
the human access to the model. Factored Cognition may not play a role in
versions of IDA that implement ampliﬁcation diﬀerently. ↩ 

