
Medical Paradigms
1. Prediction-based medicine (PBM)
2. Taking vitamin D3 with K2 in the morning
3. The Dogma of Evidence-based Medicine
4. Phage therapy in a post-antibiotics world
5. Using the Quantiﬁed Self paradigma for COVID-19
6. Anatomy, diseases and aging damage
7. Hypothesis: lab mice have more active transposons then wild mice
8. War on Cancer II
9. Orexin and the quest for more waking hours

Prediction-based medicine (PBM)
We need a new paradigm for doing medicine. I make the case by ﬁrst speaking about
the problems of our current paradigm of evidence-based medicine.
The status quo of evidence-based medicine
While biology moves forward and the cost of genetic-sequencing dropped a lot faster
than Moore's law the opposite is true for the development of new drugs. In the current
status quo the development of new drugs rises exponentially with Eroom's law. While
average lifespan increased greatly about the last century in Canada the average life
span at age 90 increased only 1.9 years over the last century. In 2008 the Centers for
Disease Control and Prevention reported that life expectancy in the US declined from
77.9 to 77.8 years. After Worldbank data Germany increased average lifespan by two
years over the last decade which is not enough for the dream of radical lifespan
increases in our lifetime.
When it costs 80 million to test whether an intervention works and most attempts
show that the intervention doesn't work we have a problem. We end up paying billions
for every new intervention.
Eric Ries wrote "The Lean Startup". In it he argues that it's the job of a startup to
produce validated learning. He proposes that companies that work with small batch
sizes can produce more innovation because they can learn faster how to build good
products. The existing process in medicine doesn't allow for small batch innovation
because the measuring stick for whether an intervention works is too expensive.
In addition the evidence-based approach rests on the assumption that we don't build
bespoke interventions for every client. As long as a treatment doesn't generalize
about multiple diﬀerent patients, it's not possible to test it with a trial. In principle a
double-blind trial can't give you evidence that a bespoke intervention that targets the
speciﬁc DNA proﬁle of a patient and his co-morbidity works.
The ideal of prediction-based medicine
The evidence-based approach also assumes that practitioners are exchangeable. It
doesn't model the fact that diﬀerent physical therapist or psychologists have diﬀerent
skill levels. It doesn't provide a mechanism to reward highly skilled practitioners but it
treats every practitioner that uses the same treatment intervention the same way.
Its strong focus on asking whether a treatment beats a placebo in double-blind studies
makes it hard to compare diﬀerent treatments against each other. In the absence of
an ability to predict the eﬀect sizes of diﬀerent drugs with the literature the treatment
that wins on the market is often the treatment that's best promoted by a
pharmaceutical company.
How could a diﬀerent system work? What's the alternative to making treatment
decisions based on big and expensive studies that provide evidence?

I propose that a treatment provider should provide a patient with the credence that
the treatment provider estimates for treatment outcomes that are of interest to the
client.
If Bob wants to stop smoking and asks doctor Alice whether the treatment Alice
provides will result in Bob not smoking in a year, Alice should provide him with her
credence estimation. In addition Alice's credence estimations can be entered in a
central database. This allows Bob to see Alice's Brier score that reﬂects the ability of
Alice to predict the eﬀects of her treatment recommendations.
In this framework Alice's expertise isn't backed up by having gotten an academic
degree and recommending interventions that are studied with expensive gold-
standard studies. Her expertise is backed by her track record.
This means that Alice can charge money based on the quality of her skills. If Alice is
extremely good she can make a lot of money with her intervention without having to
pay billions for running trials.
Why don't we pay doctors in the present system based on their skills? We can't
measure their skills in the present paradigm, because we can't easily compare the
outcomes of diﬀerent doctors. Hard patients get send to doctors with good reputations
and as a result every doctor has an excuse for getting bad outcomes. In the status
quo he can just assert that his patients were hard.
In prediction-based medicine a doctor can write down a higher credence for a positive
treatment outcome for an easy patient than a hard patient. Patients can ask multiple
doctors and are given good data to choose the treatment that provides the best
outcome for which they are willing to pay.
In addition to giving the patient a more informed choice about the advantages of
diﬀerent treatment options this process helps the treatment provider to increase his
skills. They learn about where they make errors in the estimation of treatment
outcomes.
The provider can also innovate new treatments in small batches. Whenever he
understands a treatment well enough to make predictions about its outcomes he's in
business. He can easily iterate on his treatment and improve it.
The way to bring prediction-based medicine
into reality
I don't propose to get rid of evidence-based medicine. It has its place and I don't have
any problem with it for the cases where it works well.
It works quite poorly for body work interventions and psychological interventions that
are highly skill based. I have seen hypnosis achieve great eﬀects but at the same time
there are also many hypnotists who don't achieve great eﬀects. In the status quo a
patient who seeks hypnosis treatment has no eﬀective way to judge the quality of the
treatment before he's buying.
A minimal viable product might be a website that's Uber for body workers and
hypnotists. The website lists the treatment providers. The patient can enter his issue

and every treatment provider can oﬀer his credence of solving the issue of the patient
and the price of his treatment.
Before getting shown the treatment providers, a prospective patient would take a
standardized test to diagnose the illness. The information from the standardized test
will allow the treatment providers make better predictions about the likelihood that
they can cure the patient. Other standardized tests that aren't disease speciﬁc like the
OCEAN personality index can also be provided to the patient.
Following the ideas of David Burn's TEAM framework, the treatment provider can also
tell the patient to take tests between treatments sessions to keep better track of the
progression of the patient.
When making the purchasing decision the patient agrees to a contract that includes
him paying a ﬁne, if he doesn't report the treatment outcome after 3 months, 6
months and 1 year. This produces a comprehensive database of claims that allows us
to measure how well the treatment providers are calibrated. 
Various Quantiﬁed Self gadgets can be used to gather data. Many countries have
centralized electronic health records that could be linked to a user account.
The startup has a clear business model. It can take a cut of every transaction. It has
strong network eﬀects and it's harder for a treatment provider to switch because all
his prediction track record is hosted on the website.
Thanks to various people from the Berlin Lesswrong crowd who gave valuable
feedback for the draft of this article.

Taking vitamin D3 with K2 in the
morning
Epistemological status: I studied bioinformatics but I'm no domain expert and layout
my current view on the issue based on the facts I found in the literature. This is not
health advice.
tl,dr:
There's a strong evolutionary selection for producing enough Vitamin D. Vitamin D
works together with Vitamin K2, and as a result, high Vitamin D supplementation
should come with Vitamin K2 supplementation.
Personally, I decided to consume every morning 10 000 IU of Vitamin D3 and 200 µg
of K2 (all-trans MK7) and believe that it's likely beneﬁcial for many fellow rationalists
who don't spend a lot of time exposing a lot of skin to the sun to take the same
supplements.
The importance of Vitamin D
Gwern's meta-analysis on Vitamin D suggests that the existing academic publications
suggest that this costs 0.33 years of life. Later in this article, I will make the case why
I believe that the literature is likely biased to underrate the eﬀect for systematic
reasons.
Which Vitamin D should be supplemented?
Humans produce Vitamin D3. While Vitamin D2 that gets produced by mushrooms can
also be used by humans, it makes sense to supplement Vitamin D3 as it's the form of
Vitamin D around which evolution optimized us given that it's what's available in the
natural environment. Besides the evolutionary argument, The case against
ergocalciferol (vitamin D2) as a vitamin supplement lists a few other reasons as well.
How much Vitamin D3 should be taken?
According to the Evaluation, Treatment, and Prevention of Vitamin D Deﬁciency: an
Endocrine Society Clinical Practice Guideline:
For clinical care, it appears that all current methodologies are adequate if one targets
a 25(OH)D value higher than current cut points; for example, a value of 40 ng/ml (100
nmol/L) is without toxicity and virtually ensures that the individual's "true" value is
greater than 30 ng/ml (75 nmol/L). A clinical approach of targeting a higher 25(OH)D
value seems prudent in that improving vitamin D status should reduce multiple
adverse consequences of vitamin D deﬁciency at an extremely low cost with minimal
toxicity risk.
In a German study they found median 25(OH)D levels of 19.8 ng/ml. While this isn't a
meta-review it illustrates that plenty of people is strongly under the recommendation
of the Endocrine Society. I'm German and I used to take 5000 IU vitamin D3 per day
and got tested by my doctor and got a value of 33ng/ml. That's a bit over the
recommended minimum of 30 ng/ml but not enough to get over the inherent error of

the essay. As a result, I upped my daily D3 intake to 10000 IU which is maintenance
tolerable upper limits recommended by the Endocrine Society Guideline.
How high is 10000 IU? According to the Endocrine Society Guideline, when an adult
wearing a bathing suit is exposed to one minimal erythemal dose of UV radiation (a
slight pinkness to the skin 24 h after exposure), the amount of vitamin D produced is
equivalent to ingesting between 10,000 and 25,000 IU. At the same time, 10000 IU is
much higher than the recommended daily intake of 600 IU by IOM (US), 600 IU by
EFSA (EU), and 800 IU (20 µg Vitamin D) by the DGE in Germany.
Dimitrios T. Papadimitriou argues in The Big Vitamin D Mistake that 10 000 IU should
be consumed to reach the 40 ng/ml 25(OH)D blood level and that due to statistical
errors it's assumed in the oﬃcial guidelines that less Vitamin D3 supplementation is
required to reach that level.
Above I spoke about my belief, that the existing studies underrate the beneﬁts of
Vitamin D3 supplementation. I believe that for two reasons. The ﬁrst is about the
timing of Vitamin D3 supplementation and the second is about K2.
The eﬀect of timing of Vitamin D supplementation
The studies we have generally make the assumption that timing doesn't matter and
blood 25(OH)D levels are the only interesting variable. Given that we don't have a
routine clinical essay to measure vitamin D3 or vitamin D2 serum concentration we
can't focus on their serum levels.
Another name for 25(OH)D level is calcifediol (or 25-hydroxyvitamin D / calcidiol) while
the substance we supplement or that our skin produces in response to UVB light
exposure is cholecalciferol (or Vitamin D3). Calcifediol gets produced in our liver from
cholecalciferol. Additionally, our kidney turns calcifediol into calcitriol (1,25-
dihydroxyvitamin D). Both calcifediol and calcitriol are used in many diﬀerent
pathways. Calcifediol has a biological half-life of 2-3 weeks while calcitriol has a half-
life of 4-15 h.
The mainstream belief that timing is irrelevant is supported by the fact that calcitriol
levels don't get directly aﬀected by calcifediol levels. At the same time, Seth Roberts
gathered examples of multiple people whose sleep improved when they took Vitamin
D3 in the morning and whose sleep got worse when they took it in the evening.
Multiple folks in Quantiﬁed Self replicated that eﬀect for themselves but unfortunately,
there aren't studies that investigated it deeper.
Given that there's no harm in taking it in the morning I personally take my Vitamin D3
in the morning even when there's no high-quality evidence for it.
The role of K2
The second important variable is K2. Atli Arnarson makes the case in Is Vitamin D
Harmful Without Vitamin K? that Vitamin D toxicity at high doses is usually about K2
deﬁciency because both are needed in the same pathways and more K2 is needed
when there's more Vitamin D. Vitamin D toxicity leads to hypercalcemia where there's
too much Calcium in the blood. Calcifediol moves some calcium from the bone to the
blood and K2 is needed to put the calcium in the bones. Hypercalcemia is bad
because it lead to blood vessel calciﬁcation. Observational studies link low Vitamin K2
levels to blood vessel calciﬁcation with K2 being more important than K1.

Why might we have a K2 deﬁciency compared to the ancestral environment? K2 is
found in animal liver and fermented foods which means food in which bacteria grew.
In the ancestral environment, it was hard to prevent bacteria from growing in the food
that was consumed and thus the related consumption was higher. Seth Roberts makes
here the point that we value the taste of umami that primarily comes from eating
microbe-rich food in the ancestral environment. Today, most westerners also don't eat
animal liver while ancestral humans consumed it.
Which K2?
There are multiple forms of K2 (menaquinone) that can theoretically be used to
supplement. The most commonly discussed are MK-4 and MK-7. According to Sato et al
that MK-4 has from supplements has no direct bioavailability coming to the conclusion
"MK-7 is a better supplier for MK-4 in vivo than MK-4 itself." Schurgers et all write
however that he has unpublished data that suggest MK-4 bioavailability. It would be
good to have more research to get more clear about MK-4 bioavailability. There's also
MK-5, MK-8, and MK-9 however it's not widely used as a supplement and there's more
research needed.
Given the current research, it seems sensible to me to go with pure MK-7
supplements.
MK-7 exists in a trans- and a cis-form where only the trans-form is used by humans.
Given that some supplements contain a mix of both forms it's desirable to buy a MK-7
supplement that speciﬁes that it's all-trans (or 99% all-trans).
Conclusion
On that basis, I have decided for myself to consume for now 10000 IU of Vitamin D3
per day and 200 µg Vitamin K2 (all-trans MK-7)*. I take it every morning. I don't have
strong reasons for 200 µg but it's the default size of supplements and there's no
known toxicity of it. While going out into the sun would also be a way to acquire
Vitamin D3, it causes wrinkles in the face that while I don't try to minimize sun
exposure I won't maximize it when I can get my Vitamin D3 through a cheap
supplement.
* I brought my current K2 supplement before doing this research and it doesn't specify
trans vs. cis but I will buy all-trans MK7 the next time.

The Dogma of Evidence-based
Medicine
In polite society it's currently fashionable to be in favor of Evidence-based Medicine
and proclaim that we don't have enough of it. In this article I want to argue that this
preference isn't backed up by good reasons. The paradigm of Evidence-based
Medicine isn't backed up by evidence that proves the virtues of the paradigm but by
faith.
What's Evidence-based Medicine in the ﬁrst place? The term was deﬁned in a scientiﬁc
paper by Guyatt et al in their paper "Evidence-Based Medicine - A New Approach to
Teaching the Practice of Medicine" in 1992. According to the paper Evidence-based
medicine requires new skills of the physician, including eﬃcient literature searching
and the application of formal rules of evidence evaluating the clinical literature.
Evidence-based Medicine was supposed to be about replace theory-based medicine
with empirically-backed medicine.
They make the assumption that physicians who learn the skill of literature searching
and applications of formal rules of evidence will produce better clinical results for their
patients. Theoretically there are valid reasons why someone might believe in this
assumption. For a community who sincerely believes in evidence-based thinking
instead of practicing belief-in-belief I would however expect that they test their
assumptions.
It would be possible to run a controlled study whereby some doctors get more classes
on learning those eﬃcient literature searching skills and the skills of application of
formal rules of evidence. If the cost of that experiment would be too big, it would even
be possible to seek correlation evidence. To my knowledge, nobody tried to run either
study.
I opened a question on Skeptics.StackExchange to ﬁnd out whether anybody could
ﬁnd studies who prove core assumptions of Evidence-Based Medicine and nobody
replied with studies that validated the idea that teaching doctors more of those
evidence based skills improves patient outcomes.
Brienne Yudkowsky wrote on Facebook that she thinks that the Hamming question for
epistemic rationality might be "To which topics, or under what circumstances, do you
apply diﬀerent epistemic laws?". For many people medicine is such a ﬁeld. The
majority of supposed defenders of Evidence-based Medicine accept without evidence
from controlled studies that those Evidence-based methods of practicing medicine are
better. At the same time, they ﬁght alternative medicine paradigms for not providing
enough studies that back up their claims.
According to the core assumption in Evidence-based Medicine results that are found in
one patient population generally generalize to other patients populations. If that
would be true it should be easy to replicate studies. In reality replication often fails
even when there's a lot of attention invested to get comparable patient populations.
In real world clinical settings the patient population is more diverse than the carefully
chosen patient population of a trial. In the clinical trial patients often only take one
drug and while in normal clinical practice patients often take multiple drugs to ﬁght
multiple diseases.

Another part of the core Evidence-based Medicine dogma is the dualistic notion that
doctors should focus on creating clinical eﬀects for their patients through proper
intervention and not through placebo eﬀects. This means that while patients don't
care if they get better because of mind or matter, doctors are primarily focused on the
matter. An alternative therapist who might get clinical eﬀects for their patients by
spending an hour talking to them get rejected in favor of a doctor who interviews a
patient for 5 minutes and then gives them a pill. These dogmatic beliefs about how to
think about the placebo eﬀect are also largely formed without scientiﬁc investigation
of the placebo eﬀect. There's a strong double standard about what kinds of beliefs
need studies to back them up and what can be accepted without empirical evidence
because they make theoretic sense.
There's a belief that placebo blinding procedures generally result in patients not
knowing whether or not the the patient got the placebo or verum. Rabkin et al
investigated in their paper "How blind is blind?" how well patients can tell what they
got. 78% of the patients and 87% of the doctors could correctly distinguish between
placebo and verum when they were asked. A research community that would sincerely
belief in the tenets of evidence-based medicine would start asking patients in every
trial for their subjective belief of whether they got placebo. They behavior of the
community we do have that keeps following their established rituals without
questioning those rituals looks more like belief-in-belief.
One hypothesis is that patients know whether they take a placebo or verum because
verum has side eﬀects. In an environment where the placebo controlled eﬀects of
antidepressant as Kirsch et al described in their paper only makes on average 1.8 out
of a 50 point scale, there's the question whether antidepressants with high side-eﬀect
unblind themselves and are thus better in direct comparison to antidepressants with
less side eﬀects. Unfortunately, the ethical review boards don't care about those
issues and rather focus on preventing consent forms getting signed with pencils.
There's one Evidence-based Medicine belief that will look very strange to future
students who want to make sense of our beliefs. It's the belief that the blind man sees
better. The belief that it's bad to clearly see the object under investigation in all it's
details. It's true that the practice of blinding can helps us from falling victim to various
biases but having access to less data also prevents us from seeing real patterns.
Ironically, this blindly leads to researchers not being interested in the subjective
experience of their patients to the point that they don't gather data about whether the
patients think that they got verum.
Why do we think we need Evidence-based Medicine in the ﬁrst place? We don't want
to trust in human authorities. We want science to free us of the need to trust
authorities. Instead of asking us how we can develop justiﬁed trust in human
authorities, we dream for objective knowledge that transcends human authorities.
I proposed in my post about Prediction-based Medicine a system in which we let
doctors make predictions about the outcomes of their treatment and use the quality of
those predictions to establish authority. Once we solve the problem of trust the
knowledge production itself can get more diverse. One scientists might understand a
disease better by doing phenomenological investigation of the subjective experience
of patients. Another scientists might use a lot of sensors and run machining learning
algorithms to better understand disease. Both proﬁt if they don't have to ﬁt inside the
bureaucracy of Evidence-based Medicine and can focus on producing knowledge that
helps doctors make better predictions about how to treat their patients.

It won't be as Hahnemann said "Wer heilt, hat Recht" ("He who cures is right") but "He
who can predict in advance that he will cure the patient and then actually cures the
patient is right".

Phage therapy in a post-antibiotics
world
When Fleming discovered the ﬁrst natural product antibiotic Penicillin in 1928, the
discovery was groundbreaking for medicine. Antibiotics were a tool with brute force.
Without knowing details about the illness from which a patient was suﬀering
antibiotics allowed a doctor to ﬁght illnesses due to bacteria.
Penicillin proved to be very useful for preventing wounds in the second World War
from getting infected and research went into scaling up the production of it. After the
war it came to be called a wonder drug. Economically, the fact that one antibiotica
can be used for many diﬀerent illnesses made it in the middle of the 20st century very
proﬁtable to patent new antibiotics and bring them to market.
Besides antibiotics phage therapy was another approach that was used a bit within
the 1920s and 1930s. Phages cause a trillion trillion successful infections of bacteria
per second. They destroy up to 40 percent of all bacterial cells in the ocean every day.
Phage therapy is using the power of phages to kill viruses to ﬁght bacteria in patients.
Phage therapy had the problem of being a solution that only targeted very speciﬁc
bacterial species and sometimes only speciﬁc strains of bacteria. Frequently, phage
therapy failed because it was not targeted towards the bacteria strain with which a
patient was infected. It stopped being used in the West after antibiotics became a
popular way to ﬁght bacteria.
While Western health authorities managed to get a framework that allows new ﬂu
vaccines to be approved in a short time frame to react to a changing virus, we lack a
regulatory system that allows new phage cocktails that are needed to deal with
evolving bacteria to be approved without going through multiple years of clinical
trials.
The property of being a very speciﬁc treatment has the drawback that it's necessary
to test the patient, to know which bacteria infects the patient, to be able to choose
the right treatment. In the past it was both expensive and time consuming to test for
the bacteria that causes an infection.
In Poland there's the Phage Therapy Unit which provides Phage therapy for chronic
drug-resistant bacterial infections but they operate under an exception for
experimental procedures. They published a review titled Facing Antibiotic Resistance:
Staphylococcus aureus Phages as a Medical Tool about using phage therapy for
treating staphylococcus and argue in another paper that their way of treating patients
might be more cost-eﬀective than conventional treatment with antibiotics.
The cost of providing medical treatment matters a lot and causes our health care
systems to spend more and more money. DNA sequencing is the one central
technology that fell a lot in price in the last decades and while it doesn't fall faster
than Moore's law anymore there is still hope that continued progress will allow it to be
cheaper in the future. Whole-Genome Sequencing (WGS) can not only be used to
sequence human DNA but can also be used to sequence bacteria DNA of infections. In
several countries WGS-based pathogen typing is already in the trial phase for
implementation as a routine tool for the monitoring and detection of multidrug-
resistant bacteria pathogens.

As this sequencing becomes common place, doctors will have the relevant data to
target speciﬁc strains in their patients with phage therapy. I predict that there will be a
multi-billion dollar company that uses machine learning to pick the right phage
cocktail to treat an infection based on the results from WGS-based pathogen typing.
Phage therapy will get around antibiotic resistance and it will only kill harmful
bacteria, while not killing friendly bacteria the way antibiotics do. A company that
uses machine learning to iterate on their phage cocktails will give us a more eﬀective
alternative to antibiotics.

Using the Quantiﬁed Self paradigma
for COVID-19
Petri Hollmén traveled to Tyrol on the 5th of March. He had a bottle of hand sanitizer
with him, used it a lot and washed his hands like never before.
Sunday, the 8th he returned home to hear a day afterwards that Tyrol was declared a
COVID-19 epidemic area. He decided to work from home given the higher risk of
having been in an epidemic area. On Thursday the 12th he woke up feeling normal
but his Oura ring measured that his readiness was down to 54 from being normally at
80-90 which was mostly due to having a 1°C elevated temperature at his ﬁnger at
night.
Even though he felt normal, he went to the doctor and given that he was from an
epidemic area, they decided to test him. He tested positive and went to self-
quarantine for 14 days. He measured his temperature several times during the
following day and it always came back with 36.5°C. The Oura ring provided evidence
that led to his diagnosis that wouldn't have been available otherwise.
While he didn't have true fever as deﬁned by the oﬃcial gold standard he did have a
kind of clinical relevant fever. It's my impression that our medical community is too
focused on their gold standards that are based on old and outdated technology like
mercurial thermometers.
Even when new measurements like nightly ﬁnger temperature don't match with the
gold standard there are still cases where the information allows for better clinical
decision making.
Today, we have cheap sensors and machine learning that provide us with a diﬀerent
context of making medical decisions then going to the doctors oﬃce.
Testing by doctors is very important in the ﬁght against COVID-19 but people need to
know when it's time to go to the doctor. Hollmén needed his Oura to know that it was
time to get tested professionally.
We need to get good at catching cases of COVID-19 as fast as possible when they
happen in the wild if we want to avoid that millions die without us choking our
economy by long-term quarantines.
Analysis of Fitbit users found that their resting heart rate and total amount of sleep
can be used to predict the oﬃcial state numbers for inﬂuenza-like illness.
It's very likely that lower heart rate variance and a higher minimum of the nightly
heartrate happens in at least some of the COVID-19 cases. Unfortunately, the WHO is
stuck in the last century and the oﬃcial symptoms charts tell us nothing about how
common either of those metrics are in COVID-19 patients. Lack of access to those
metrics in the oﬃcial statistics means it's harder for people who have an Oura Ring,
an Apple watch or another device that can measure nightly heartrate to make good
decisions about when to go to the doctor or self-quarantine.
Given that Apple sold around 50 million Apple watches between 2018 and 2019, a
sizable portion of people could make better decisions if we would have more

information about how COVID-19 aﬀects heart rate.
Even more people have access to a smart phone with a decent camera. Having a sore
throat is a typical symptom for many virus infections like COVID-19 and a good
machine learning algorithm could produce valuable data from those images.
A priori it's unclear about how much we can learn from such pictures. If a throat of a
patient is red due to inﬂammation a doctor who looks at it, can't distinguish whether
it's due to snoring or a virus infection.
If a machine learning algorithm could have access to a steady stream of daily imagine
of a person's throat the algorithm could understand a person's baseline and use that
insight to factor out the eﬀects of snoring.
When the gold standard of diagnosing the throat is to look at one image at a particular
point in time at the doctor's oﬃce there's potentially a big improvement to be gained
by looking at a series over multiple days. We don't know how useful such a diagnostic
tool is before building it.
Ideally, users of a new app would take an image of their throat every morning after
getting up and every evening before going to sleep. They would also measure their
temperature with a normal thermometer at both points and enter information about
subjective symptoms. If a person gets a proper COVID-19 test, they should also be
able to enter the data.
At ﬁrst we would train the machine learning algorithm to use the images to predict
temperature. With enough users our algorithm can learn how the throat of a person
having ﬂu diﬀers from their baseline whether or not they are snoring.
As we have more users and some of our users get COVID-19 lab tests our machine
learning algorithm can learn to predict the test results directly. It's the nature of
advanced technology that we don't know how powerful a tool is before it's developed.
Most clinical trials for new drugs ﬁnd that they don't live up to their promise.
We need more dakka for COVID-19. Creating an app that does the above function
doesn't cost much and the cost of the project should be worth the potential beneﬁts of
catching COVID-19 cases faster and thus preventing people from unknowingly
infecting their friends.

Anatomy, diseases and aging damage
In this post I will talk about how I diagnosed a medical issue of a displaced muscle
tendon in myself and solved it. I analyse how our medical ontology is currently ill-
equipped to categorize the problem. I discuss the implications for thinking about
human aging and why we need to think broader than the seven hallmarks.
Personal experience with a displaced muscle tendon
While I'm normally using a trackball as a mouse, two years ago I went to go co-
working and used a normal mouse. I made a bad movement while using the mouse
and afterwards my right hand hurt a bit. A few days later my hand was relatively okay,
but my hand and arm were still more tense than before.
I asked multiple bodywork people to ﬁx it, but while the arm got more relaxed the
issue didn't fully resolve. This week I decided to investigate how my right hand and
left hand diﬀer to ﬁnd out what's going on. I noticed that if I extend my right arm my
right hand goes in the direction of the ulna side unless I add tension to keep it in
place.
When palpating the ulna head from the dorsal side of my left hand I was touching the
ulna head directly. When doing the same thing on the right side, there was something
above the ulna head. I formed the hypothesis: "Maybe, the thing I'm palpating is out
of place. How about I move it laterally?" I used my ﬁngers to slowly push it laterally.
Afterwards, my right arm started relaxing. I ﬁxed the problem that I produced two
years ago in 10-15 seconds of action. I looked up the anatomy and deduced that I
moved the tendon of the muscle extensor carpi ulnaris. The tendon is supposed to be
lateral of the ulna head and not dorsal. This explains why my  hand moved before
when extending my arm. Part of extending the arm involves turning the ulna and as
the ulna turns, the ulna head presses a bit in the dorsal direction and pushed on the
tendon. As a result of pushing on the tendon the extensor carpi ulnaris contract
resulting in the movement I observed.
Untreated, this issue might have resulted down the line in carpal tunnel syndrome or
back pain down the line. Plausibly, it would have even produced those eﬀects in the
two years if I wouldn't regularly do eﬀective interventions to remove tension.
Conceptualizing the displaced muscle tendon as a ICD 11 illness
Did I have an illness that I cured and if yes, what illness? The current oﬃcial ontology
for illnesses is written down in the International Statistical Classiﬁcation of Diseases
and Related Health Problems (ICD), currently at version 11.
Given that the issue was about the extensor carpi ulnaris I would expect to ﬁnd a way
to specify it in NC36.5 Injury of other extensor muscle, fascia or tendon at forearm
level.
Other extensor muscle means that we are not talking about muscles of thumb or other
ﬁngers that have their own codes.  This code does allow me to specify that the issue is
about XA9304 Extensor carpi ulnaris muscle and on the right side with XK9K Right.
NC36.5 gives me four sub-choices:

NC36.50 Strain or sprain of other extensor muscle, fascia or tendon at forearm level
NC36.41 Laceration of extensor muscle, fascia or tendon of other ﬁnger at forearm
level
NC36.4Y Other speciﬁed injury of extensor muscle, fascia or tendon of other ﬁnger at
forearm level
NC36.4Z Injury of extensor muscle, fascia or tendon of other ﬁnger at forearm level,
unspeciﬁed
While ICD 11 doesn't give me a deﬁnition of what they mean with strain, Medical-
Dictionary gives me for strain "3. an  overstretching  or  overexertion  of  some  part 
of  the  musculature".
The nearest I found for sprain on Medical-Dictionary is:1. An injury to a ligament as a
result of abnormal or excessive forces applied to a joint, but without dislocation or
fracture.
This is diﬀerent from the dislocation I had, my problem was not that the muscle was
permanently stretched but that it got put under tension if I used my arm.
Giving that the muscle was dislocated in a way that stayed dislocated for two years,
this seems to be inapplicable. This means that the only way to express it in ICD-11
terms would have been NC36.4Y and use free-text. If my issue would have been with a
joint or ligament I could have used NC33 Dislocation or strain or sprain of joints or
ligaments of elbow.
To me the inability of ICD-11 to express my issue directly is interesting because it
points to a lack of medical interest in the issue. It's illustrative of how anatomy is
currently a neglected research topic. If you are doubtful about how anatomy is
neglected, the fact that the lymphatic system extends into our brains was only
discovered in 2015.
Conceptualizing the displaced muscle tendon as aging damage
As people age they usually become more tense and stiﬀ. Many people develop back
pain as they age and it becomes more common with advancing age. I consider it
plausible that a lot of diﬀerent untreated damage that's in nature similar to my
dislocated muscle tendon contributes to this problem.
While only a minority of people will develop a dislocated extensor carpi ulnaris tendon,
if we solve all aging damage that develops in all humans, a myriad of diﬀerent classes
of unrepaired damage are likely to still kill everybody as more and more of it
accumulates in individual.
Given that damage like this can accumulate even if the speciﬁc type of damage
doesn't exist in every aging individual, Aubrey's idea that it's enough to cure the
seven types of damage he identiﬁed or the nine hallmarks is ﬂawed.
What do we need to go forward from here?
A lot of progress in our biomedical knowledge of the last two decades is driven by
open-source bioinformatics. Databases like UniProt provide every researcher the
ability to freely access data about genes and proteins and do science with them.

We created those databases by funding molecular biology centric approaches. Given
that we have access to fMRI technology we can use it for more than pretty pictures of
brains. The data about where muscles happen to be is accessible via fMRI and we can
use computer analysis to ﬁnd a lot more on fMRI's that doctors currently see with the
limited focus of their ﬁeld of expertise. While proprietary fMRI software might
successfully diagnose some medical issue that doctors don't see, we need open
scientiﬁc exchange to conceptualize medical issues.
We need an open system that takes in data like fMRI data and that translates them
into 3D anatomical models like the anatomical model of BioDigital, ZygoteBody or
Anatomy3dAtlas. We need those models to study how the anatomy of individual
humans diﬀers, diagnose anatomical problems like dislocated muscles in a systematic
way and study the eﬀects of our interventions. Once we conceptualize the problems
we need ICD codes to get the problems into our medical system.
Besides improving our general medical knowledge, 3D anatomical models of
individual patients that can be explored in VR would help physiotherapists and other
bodyworkers work more eﬀectively.
If you care about illnesses such as cancer, better understanding of anatomy might
help us detect abnormal anatomy due to cancer better.
If the goal you care about is ending aging, developing technology like this is important
to ﬁnd more of the accumulating damage that goes beyond the nine hallmarks. 

Hypothesis: lab mice have more
active transposons then wild mice
As johnswentworth recounts in Core Pathways of Aging, as an organism ages active
transposons within it's stem cells duplicate and that mechanism might lead to
increased average transposons count in stem cells. Those transposons then produce
DNA damage which in turn leads to cell senescence.
If that hypothesis is true, there's evolutionary pressure to keep the count of active
transposons low. That evolutionary pressure is greater in organism that reproduce at a
later age then for organisms that reproduce at an earlier age.
As Bret Weinstein describes, breeding protocols for lab mice have lab mice
reproducing at an earlier age then mice that live in the wild because it's economical to
make the mice reproduce at a young age. Weinstein made the hypothesis that this
leads to laboratory mice having elongated telomeres.
I hereby make the hypothesis that if we investigate the average amount of active
transposons in laboratory mice and lab mice, we will ﬁnd that the wild mice have less
active transposons then the wild mice, because there's less evolutionary pressure in
the laboratory mice to remove mutations that lead to increased active transposon
count.
If investigation ﬁnds this hypothesis to be true, approaches to reduce transposon
count should get more attention by antiaging researchers.

War on Cancer II
Epistemic status: This isn't medical advice. To the extent that it's advice it's health
policy advice. I'm no domain expert. If you are faced with the prospect of cancer,
consult with multiple experts. 
Introduction
Richard Nixon declared the war against cancer in 1971. Beau Biden, the son of Joe
Biden, died in 2015 due to brain cancer. Having their child die before them is one of
the worst experiences a parent can have. Joe Biden, then the vice-president, decided
to start the Vice President's Cancer Moonshot. On the campaign trail on his run for
presidency he declared in 2019: "I promise you if I'm elected president, you're going
to see the single most important thing that changes America: We're gonna cure
cancer." Joe Biden essentially declared the second war against cancer.
With both Nixon and Biden wanting to ﬁght wars on cancer the issue is essentially
bipartisan. Biden has personal reasons for ﬁghting the war that distinguish him from
other recent presidents but they are not party-political. I applaud the ideal of ﬁghting
wars against cancer instead of burning resources to ﬁght expensive wars against
human people.
While I applaud the principle, the ﬁrst war against cancer failed. In this article I want
to lay out the problems with the policies that came with the last war on cancer and
make a case of how we can approach health policy in a better way. 
The diﬀerence between the cancer survival
rate and the cancer death rate
One mainstream view of the war on cancer is that it was partly a success. Vincent
DeVita writes in The 'War on Cancer' and its impact:
Relative survival rates for all cancers have increased 70%, since the passage of
the Act [National Cancer Act of 1971]
To a layperson that claim might seem impressive but as Sarah Constantin writes in Is
cancer progress stagnating:
But the War on Cancer seems to have disappointing results. Cancer deaths have
only fallen by 5% since 1950, at a rate of 200 deaths a year per 100,000
individuals. (By contrast, heart disease deaths are a third of what they were in
1950,  thanks to innovations like statins, stents, and bypass surgery.)
There are three possible explanations of why those two numbers diverge.
The longer lifespans thesis
While it's true that we increased lifespan and a higher lifespan increases the likelihood
of getting cancer you would expect the same with heart disease. Given that we see a

lot of success at cutting heart disease deaths but not cancer deaths, this thesis
doesn't explain what's going on and why we aren't getting more progress in cancer
which is the area at which we throw the most research dollars.
The toxic environment thesis
The ﬁrst explanation is the toxic environment thesis. According to it we have a lot
more cancer causing substances in our environment and as a result even though we
are better at curing cancer, we have a similar amount of cancer deaths. In the last
decades we drastically reduced air pollution, removed cancer causing substances
such as asbestos and reduced smoking rates. We have regulations that try to remove
cancer causing substances from the market. Given those eﬀorts, we should expect a
less toxic cancer causing environment.
If we have a more toxic environment, there's a signiﬁcant unresolved policy failure.
Scenarios such as microplastics causing as much cancer as the substances we
removed, are scary and underexplored. There are strong lobbying interests against
seriously studying products for safety issues that are currently not under our radar
and the narrow focus of US cancer policy doesn't ﬁght against them to see whether
we have a lot of unknown toxic substances in our current environment.
The goodharting thesis
There's an easy and reliable way to signiﬁcantly increase the cancer survival rate. If
you double the amount of people that are diagnosed with cancer and the healthy
people you diagnosed with cancer don't die due to cancer you massively increase
your relative cancer survival rate.
One of the insights at the time the war against cancer started is that it's much easier
to treat a small cancer in its early stages than it is to treat a larger cancer after time
passed. Out of this insight, the idea that a good way to ﬁght cancer is to increase the
amount of early cancer diagnoses arose. Public health campaigns that taught the
populations about early signs of cancer got started and especially in the US expensive
medical imaging was promoted to catch cancer in its early stages.
This resulted in the US having the ﬁfth place at highest cancer survival rates while at
the same time having the ﬁfth highest cancer death rate and only the 40th place at
life expectancy.  Cancer treatment that happens when it wasn't needed is very costly.
Some women lost their breasts without gaining anything in return.
After passing the aﬀordable care act, a taskforce of the Obama administration decided
that they believe in the goodharting thesis enough to cut back on testing in 2009. In
addition to reducing the amount of women who needlessly lose their breasts this
policy was also a way to save healthcare costs and critics argued that it was a health
care policy that reduced treatment quality to save costs.
Initially, the American Cancer Society spoke against it and it took them till 2015 to
come around to recommend the same policies. The new language was still very broad
and they never really owned up to the fact that the recommended policies that made
their members a lot of money that needlessly took women's breasts for a long time.
Without the American Cancer Society owning up to their problematic past, it's not an
organization to listen to when we want better cancer policy.

Trump administrations reduction of regulations
Given the high cost of drug approval the Trump administration decided to create a
right-to-try law for patients with life-threatening diseases to bypass the FDA's
application process for "compassionate use" of experimental drugs. The American
Cancer Society was again at the forefront of ﬁghting the new policies.
What's cancer?
Given that we don't want to fall into the goodharting trap again, it's worth exploring
what cancer happens to be.
Cells gather mutations as they divide and are exposed to external stressors. They
have non-perfect repair mechanisms. When the genes for the repair mechanisms
mutate there are a lot more mutations. Some mutations lead to cells constantly
dividing even when they are in a situation where a normal cell wouldn't divide. Some
mutations make the cell ignore signals to self-destruct. Some mutations lead to the
cell producing telomerase to escape the hay ﬂick limit that limits how often a cell can
divide.
Mutations happen all the time and in the normal case the immune system catches the
mutated cell and eliminates it. The immune system can eliminate cancer cells
because they have a diﬀerent cell surface then regular cells. When proteins get
broken down inside a cell the cell presents substrings of the proteins on its cell wall
via a process called antigen presentation. When genes mutate there are some
substrings in the mutated proteins that don't appear in the other cells in the
organism.  Genes that are normally only expressed in fetal development can mutate
to be expressed in adult humans and then the existence of the corresponding proteins
is a signal for the cell being cancerous. Cancer cells can mutate to shut down antigen
presentation and therefore don't show their mutated proteins. In that process they
however also stop presenting the antigens that a normal cell presents which provides
a diﬀerent avenue for their recognition.
The immune system can fail to do its job either broadly in the body or in a speciﬁc
location and problematic cells replicate in a way that results in cancer. Sometimes the
immune system starts eﬀectively ﬁghting the cancer after it's already visible on
imaging methods. Sometimes the genes for telomerase don't get expressed and while
a cell cluster mutates in a visible way it stops growing when it goes against the hay
ﬂick limit.
Currently, we don't have a good idea to what extent the overall mutation rate due to
environmental stressors, global immune system failure or local immune system failure
in a speciﬁc part of the body is the driving force for cancer.
Our goal should either be to ﬁnd out which people actually need treatment to survive
their cancer or ﬁnd treatments that have no harmful side-eﬀects for early stage
cancer so that it doesn't matter if we treat it even when it would go away on it's own.
For those that would die without treatment it's okay to have treatments with serious
side-eﬀects, and we need better treatments for late stage cancers as well.
How much does cancer matter?

Suzanne Wu argued that given that curing all cancer would only extend lifespan by
three years money would be better spent ﬁghting aging then ﬁghting cancer. While we
want to extend lifespan a lot longer than three years, curing cancer would allow us to
use other therapies more aggressively that we currently don't use because they have
the risk of causing cancer. Anti-aging therapies to regrow parts of the body come
inherently with cancer risks and we would get further with them if we wouldn't need to
worry about cancer.
Growth hormone increases the speed in which cancer grows and for anti-aging
therapies there's a good chance that we will want to inject growth hormone.
Is Cancer a Disease?
The Atlantic wrote an article titled Cancer Isn't a Disease. That headline comes out of
asking a person working in biotech "What is a common and/or annoying
misconception about your vocation?" The slogan is that cancer isn't one disease but a
cancer is a collection of diseases.
The background of this statement is that diﬀerent cancers indeed react diﬀerently to
many treatments. Contrary to the reality of pharma companies doing many clinical
trials of cancer drugs the person they ask asserts: "That fact alone—that cancer is a
collection of diseases—dissuades Pharma from attacking it, with the absence of
blockbuster potential. It's becoming reminiscent of antibiotics, albeit for somewhat
diﬀerent reasons."
If you ask yourself why Pharma develops drugs that are targeted at individual cancers
a large part of the answer is the Orphan Drug Act of 1983. Under the Orphan Drug Act
drugs, vaccines, and diagnostic agents would qualify for orphan status if they were
intended to treat a disease aﬀecting less than 200,000 American citizens. Orphan
status inturn reduced the regulatory barriers for bringing drugs to market. Given that
regulatory barriers constitute a major part of the cost of developing drugs, this
encourages Pharma to develop drugs for orphan diseases instead of more general
solutions. 
Orphan drugs also make it easier to charge higher prices EvaluatePharma® estimates
based on an analysis based on the top 100 drugs in the US in 2018an  that the mean
cost per patient per year of an orphan drug was $150,854 versus $33,654 for a non-
orphan drug. The report predicts:
Pipeline orphan drugs account for over a third of total R&D pipeline sales through to
2024, with the annual growth rate from sales forecast to be 163% compared to 146%
for non-orphan R&D products.
As a society we prefer if drug companies develop cheaper drugs that help more
patients but set up our regulatory environment to encourage expensive drugs that
help fewer people. 
Orphan drug status also provides a few other advantages to drug companies that I
won't list here, see the EvaluatePharma® report for more information.
Eﬀective use of research money

Within the NIH the National Cancer Institute had a budget of $6.9 billion in 2020. In
contrast, the human genome project spent $5,1 billion in 2020 dollars between 1990
and 2003 to accelerate DNA sequencing technology and uncover the human genome. 
While the knowledge about the human genome that they published wasn't very
useful, the technology that came out of the human genome project that allowed for
cheap DNA sequencing to be developed turned out to be very useful.
Even if we only look at cancer the ability to sequence the genome of a cancer and
thus get data about the mutations of the cancer of a particular cancer is plausible
worth more then all the cancer research that the NCI funded in that timeframe.
Sequencing is a basic building block for eﬀective immunotherapy which is one of the
most promising technologies to tackle cancer in the coming years as I will discuss
later.
New technology often allows a research task to be done for a tenth of the price in a
decade. A high percentage of public research dollars should go into technology
development. 
Cloud labs
When researchers use their equipment in their own lab, their priority isn't to improve
their research technology but to make scientiﬁc ﬁndings in the domain of their grant
to publish papers. If the scientists would instead concentrate on doing their science
and outsource the execution of their experiments to cloud lab companies, the cloud
lab companies could focus on bringing the cost of the experiments.
Besides allowing the cloud lab itself to optimize their technology, cloud labs also help
with researcher productivity. EmeraldCloudLab for example claims that scientists who
use their platform increase the amount of samples per year from 2,220 to 7,064, take
1 year to publish a paper instead of 1.96 years, reduce cost per paper from $146.3 K
to $107.6 K and reduce time to ﬁrst publication quality data from 3 months to 24
hours.
Cloud labs seem to be currently held back by requiring researchers to think diﬀerently
about the way their lab works and having Phd students do less cheap manual work.
Grant giving should earmark for a large portion of grants that aren't about building
new technological capability part of the grant to pay for cloud lab costs.
Theoretic research
Researchers seek large research budgets, universities seek professors that are likely
to bring in large research budgets. Running expensive experiments comes with more
research costs than theoretical research. As a result of this dynamic we don't have
professors who research cancer completely on the theoretical level and integrate the
large amount of information we have into coherent theories on a basic level. We
should give out a type of grant that's focused on theoretical research without
experiments. 
If we would have more theoretical researchers instead of just researchers who focus
on experiments we would have understood the goodharting problem of cancer testing
earlier. We don't need to spend as much on theoretical research as we spend on
experimental research but giving 1% of total cancer research money to theoretical

research where the involved researchers aren't engaging in experiments would be
great.
Treatment perspectives for cancer
While we should be open to a lot of diﬀerent treatment modalities I will discuss
approaches here where I'm conﬁdent that executing them well will improve cancer
care.
Cancer Immunotherapy
In the last decade cancer immunotherapy appeared on the scene. The idea of cancer
immunotherapy is to help the body ﬁght the cancer more eﬀectively. 
As we describe above, many cancer cells present antigens about their mutations on
their cell wands. If we get the body to build antibodies against those antigens, the
immune system uses those antigens to detect and ﬁght cancer cells. In the beginning
there was the hope that targeting proteins that normally don't get expressed in adults
is enough to attack the cancer. Clinical trials suggest that it isn't. Fortunately, we can
use gene sequencing to learn about all the mutations in a cancer. With the help of
computer models we can determine which of those mutations will be displayed as
antigen on the cell wall and vaccinate patients against those mutations that get
displayed. 
The beneﬁt of this method is that it puts little stress on the patient, so it matters less
when we use the method with a patient that doesn't need treatment. For patients with
more advanced cancers we can combine this method with other methods.
Multiple technology platforms might be usable for cancer vaccines. We could use
traditional adjuvants, we could also use mRNA vaccines. Given that the technology is
advanced enough that multiple companies are doing clinical trials, the ﬁeld does not
need non-commercial research money.
When it comes to cancer cells that remove antigen presentation mechanisms, natural
killer cell based therapies already lead to approved cancer treatments. Like cancer
vaccines the treatments have little toxicity. At this stage there are still many
challenges that need research to optimize treatment. Just like cancer vaccines that are
individualized to individual patients are better, natural kill cell based therapies that
are individualized to target the cancer of a speciﬁc person are likely more eﬀective
and there are many open research questions that need to be solved, so the ﬁeld
needs funding. 
We need technology to determine which antigens on the surface of cells of a particular
cancer are missing. 
We need technology to eﬀectively grow natural killer cells in the lab that are
specialized to be sensitive to particular missing antigens and not attack when
antigens that are generally missing in a particular patient. While we are at it, we have
to study whether we can increase in-vivo persistence.
Nutrient optimization

It's likely that the blood nutrient content of cancer patients frequently deviates from
optimal levels, given that cancer is taxing the organism a lot. While in most cases
nutrient optimization won't be enough to cure cancer alone it can easily be used in
combination with other therapies to improve treatment success. 
Needed technology
Blood testing
After Theranos failing the appetite to invest into a new generation of general blood
testing technology is currently low. Having better and cheaper blood testing
technology would allow us to take less blood from patients to get information about
what goes on in the organism of cancer beyond cancer markers. 
Understanding better what goes on in the whole body when it suﬀers from cancer
helps us to progress science.
Therapeutically, understanding more variables of a patient gives us more points to
intervene.
3D open source anatomical model creation
Human anatomy is a neglected research ﬁeld. Important aspects of human anatomy
such as the lymphatic system existing in the brain have only been found in 2015.
Operating cancer with our current understanding of anatomy produces needless
damage that we could avoid if we would understand human anatomy better.
Understanding anatomy better and what diﬀerences among healthy humans are
normal will allow us to understand abnormal anatomy to detect cancer when it
happens. Cancers produce stress on the organism by pressuring other parts of the
body. A better understanding of anatomy is needed to understand the eﬀects better.
Sometimes anatomy will create a microenvironment that has eﬀects on the cancer.
Surgery comes with side eﬀects and those can be reduced with better understanding
of the underlying anatomy.
Instead of just gathering a 3D atlas of cancers, 3D models should include larger parts
of the body. Instead of just having the 3D models as raw data we need open-source
tools that turn the raw data into models with which both researchers and clinicians
can better interact. 
While commercial providers might produce 3D models out of raw data, having the
models based on open-source software is essential for researchers who study aspects
of the models and need to adapt them for their research questions.
We already scan the whole body of cancer patients to discover possible metastases.
Better software would allow us to get more out of the data that we already gather.
Open Research questions:
What roles do transposons play in cancer?

With next-generation sequencing that sequences the DNA 100 base pairs at a time,
it's not possible to see when a transposon that's 6000 bases in length gets duplicated.
Third-generation sequencing brings us the ability to sequence 10000 base pairs at a
time so that we can see how often a transposon is replicated. 
Biologists often don't care for what they can't see and transposons just move into our
view.The fact that while transposons regularly replicate within DNA the transposon
count of our species stays constant. There needs to be a mechanism of how increased
transposons reduce the ﬁtness of individuals. The most plausible mechanism is that
they regularly cut the DNA and induce mutations. Given that cancer happens
downstream from DNA mutations, cancer might be one way that individuals with a
transposon count that's too high get wiped out. 
PGBD5 that codes for a transposase is expressed in a majority of pediatric solid
tumors while it's possible that PGBD5 only gets expressed after the cancer grows a
bit, it's also possible that PGBD5 produces mutations that create the majority of
pediatric solid tumors. 
If that's the case, we have to check whether PGBD5 is needed or whether we can
vaccinate against PGBD5 to let the immune system kill cells that express it long
before cancer gets developed. We have to rethink which substances are
cancerogenous based on how they interact with transposons.
The main approach shouldn't be to think about how we can shut down DNA repair in
cells aﬀected by PGBD5 but how we can generally prevent PGBD5 from bringing cells
into a cancerous state. Shutting down repair is not a strategy that's likely to be a
permanent solution as it just means that another cell that has problems with PGBD5 is
going to mutate into a cancerous state. We should focus applied research in a way
that actually has a chance of creating major progress.
How does the microenvironment around cancer aﬀect
cancer formation?
We don't understand well how the immune system sometimes fails at detecting
cancer and ﬁghting it. It's plausible that there are conditions under which the immune
system works less well in certain parts of the body.
Drug approval
Prediction-based medicine (PBM) for compassionate use
The current state of aﬀairs where doctors can give parents unwarranted hope when
they sell treatments to the patients under the compassionate use clause sets bad
incentives. To the extent that drugs get used without a company producing studies for
them, that also removes the incentives to fund studies to investigate the merits of
treatments.
We need a new mechanism to deal with patients whom we give treatments under
compassionate use. I propose to use the mechanism of Prediction-based medicine
(PBM). In PBM a doctor has to tell the patients about the likely outcomes of a
treatment and submit his prediction to a central authority. That central authority then

publishes aggregated data about the quality of the predictions of individual doctors
and hospitals. 
In the case of cancer, I propose telling the patient about the 1-year, 3-year and 5-year
survival rate when he uses the treatment.
Under Prediction-based medicine doctors are incentivised to give patients drugs when
they have justiﬁed belief that the drug will help the patients without an expensive
approval process. Pharma companies in turn are incentivised to run studies that allow
doctors that care about being a doctor with good prediction accuracy to make good
predictions. 
A side-eﬀect of this system is that we can identify the best doctors at knowing
whether a drug will help a patient in the absence of a formal drug approval, how we
should evolve our standards for formal drug approval so that we approve drugs that
work with a minimum of bureaucracy.
Pharma companies can also hire the best doctors at estimating the usefulness of
drugs to guide them at making decisions about which clinical trials to run.
Drug approval denationalization
Currently, the incentives of the FDA are more about not approving drugs that pose risk
of criticism. Drug approval denationalization is about creating competition between
drug approval agencies of developed countries. In it we take a list of countries in
whose systems we trust and declare that approval by any of those countries is enough
to bring the drug to market. 
End the Orphan drug act
The idea that a drug that's taken by 300,000 patients needs diﬀerent evidence then a
drug that's taken by 100,000 is ﬂawed. In both cases patients deserve drugs that work
and that don't put them at risk. To the extent we need to allow usage of drugs where
approval is too expensive, Prediction-based medicine is a better system.
Conclusion
The US discourse goodharted on the cancer survival rate which is a bad metric,
instead the success of cancer policy should be measured by reducing death due to
cancer. Spending money on tool building is often higher return then bringing another
substance that might produce a small eﬀect that changes little in the big picture to
market. 
While reducing the barriers to bringing new drugs to market, we still need to know
which drugs work and experimenting with Prediction-based Medicine for drug use for
compassionate use is a good way to get started. The Orphan drug act sets bad
incentives as we want cheap drugs that help a lot of people instead of expensive
drugs that help few people. 
General platforms like cancer immunotherapy that can be adapted to diﬀerent types
of cancers are more promising than narrow drugs that only work in very speciﬁc kinds
of cancer.

While I consider the areas towards which I point to be important, research should
always be open for new approaches and not be too much committed to old strategy
given that the nature of science means, that the domain under investigation is
uncertain.

Orexin and the quest for more waking
hours
A week ago I was skeptical about the prospect of radically reducing human sleep
needs. After reading John Boyle's Cause area: Short-sleeper genes, I decided to
research the area more deeply and updated to believe that it's more likely that we can
reduce human sleep needs without signiﬁcant negative side eﬀects. It might increase
risk-taking which has both positive and negative eﬀects. The one friend I have that
has short-sleeper genes is a startup founder. 
Boyle suggested that one of the best actions to attempt would be using orexin or an
orexin agonist as a drug, but that there's currently a lack of funding for doing so. 
Given the way the FDA and EMA work, drugs only get approved when they are able to
cure illnesses, with an illness being anything that has an ICD code. According to that
notion, people who suﬀer from having to sleep more than four hours don't have an
illness and thus drugs can't be approved for that purpose. In practice, this results in
the NIH not being interested to fund the research of Ying-Hui, about people who need
a lot less sleep and are still well rested, that Boyle discussed. 
DEC2-P384R and orexin biology
The DEC2 gene produces prepro-orexin, which is 131 amino acids long. People with
the DEC2-P384R mutation produce more prepro-orexin and have a reduced need for
sleep. From prepro-orexin our body generates orexin A, which is 33 amino acids long,
and orexin B, which is 28 amino acids long. Orexin A is highly conserved and has
the same molecular structure in humans, mice, rats, and cows, while human orexin B
diﬀers from rodent orexin B. While orexin B doesn't cross the blood-brain barrier,
orexin A does. I didn't ﬁnd information on whether or not prepro-orexin passes the
barrier, but it likely doesn't given its size.
According to Uniprot:
Orexin-A binds to both OX1R and OX2R with a high aﬃnity, whereas orexin-B binds
only to OX2R with a similar high aﬃnity.
The literature is sometimes unclear when they use the term orexin about whether the
author means prepro-orexin, orexin A, and orexin B or a mix of them. Hypocretin is an
alternative name in the literature for orexin, hypocretin-1 for orexin A, and hypocretin-
1 for orexin B. 
Do we get a free lunch?
From an evolutionary perspective, it seems beneﬁcial to have a lower sleep
requirement, so we have to ask ourselves why DEC2-P384R didn't provide a signiﬁcant
enough advantage to spread the mutation to the whole human population.
Energy expenditure hypothesis

In the search for evolutionary disadvantages, I found an article by Dyan Sellayah et al
where they say:
Here, we review a fat-burning mechanism that is turned on by the brain hormone
orexin during high-caloric food consumption. Remarkably, the same hormone also
induces feeding, and its levels correlate with lean body mass in both rodents and
humans. Intriguingly, loss of orexin prevents thermogenic energy expenditure
while inducing obesity in the face of hypophagia. Thus, orexin is a unique
neuropeptide that promotes both feeding and energy expenditure, conferring
resistance to weight gain.
Evolutionarily, for most of human history, a mutation that caused someone to eat
more while burning their fat reserves for thermogenic energy, instead of using the
energy for necessary metabolic processes, was a clear disadvantage. 
This makes me hopeful that in our current world, where we have access to as much
food as we want, DEC2-P384R comes without clear negative side eﬀects. 
Stress hypothesis
The caveﬁsh Astyanax mexicanus has evolved to need only 80% hours of sleep
compared to related surface ﬁsh, while having a similar lifespan. Astyanax mexicanus
have OX2R receptors that are more sensitive, and have an increased blood level of
orexin.
A key environmental diﬀerence for Astyanax mexicanus is that they live in an
environment without predators. This makes them less anxious, and it's plausible that
increasing orexin will make people less anxious and more willing to take risks.
If that's what comes with reducing human sleep needs, we might be okay with it.
Sleeping less, having a stronger drive for action, and willingness to take more risks
sounds like a good package in today's environment for most people. It might be
negative for individuals with high aggression or low IQ who are more likely to commit
crimes if they feel less inhibition. 
If we need sleep to deal with the eﬀects of stress, it makes sense for genes that
reduce stress to lead to less sleep. This hypothesis would also be supported by some
people who need less sleep after meditating a lot, given that meditation is another
way to reduce stress. 
Orexin and narcolepsy
Lucie Barateau et al write in Treatment Options for Narcolepsy:
Narcolepsy type 1 is characterized by excessive daytime sleepiness and cataplexy
and is associated with hypocretin-1 deﬁciency. On the other hand, in narcolepsy
type 2, cerebrospinal ﬂuid hypocretin-1 levels are normal and cataplexy absent. 
Given that orexin A (hypocretin-1) passes the blood-brain barrier while orexin B
doesn't, it's possible to measure orexin A deﬁciency in the blood but not measure
whether or not someone is orexin B deﬁcient. Narcolepsy type 1 patients are likely
both orexin A and orexin B deﬁcient. Narcolepsy type 1 is estimated to have a
prevalence of 25 to 50 per 100,000 people according to UpToDate. In a double-blind

experiment intranasal orexin A supplementation of patients with Narcolepsy type 1
helped them with having faster reaction times and making fewer errors.
If you are a naive reader, you might expect that we give people with narcolepsy type
1 orexin-A as a supplement because that would be obvious. We don't. You might
expect that someone tried to bring it to market as a drug and ran a clinical trial. They
didn't.
The problem seems to be that the solution is too obvious. The patent oﬃce likely
decided that the solution would be too obvious to give out a patent for it, and thus the
narcoleptic patients are without orexin-A supplementation unless they go
through eﬀorts to procure it themselves.
Clinical trials
Instead of giving patients orexin-A, multiple companies recently invested in clinical
trials for orexin agonists. An orexin agonist is a substance that binds to the orexin
receptors just like orexin does. Unfortunately, when you select a molecule for binding
to a certain receptor you are generally choosing molecules that easily bind in general,
which often leads to oﬀ-target eﬀects where other receptors are also aﬀected.
Scott Alexander's post on how his hospital pharmacy didn't have any melatonin but
only what's eﬀectively an expensive melatonin agonist is worth reading to understand
the problem of how hard it is for unpatented natural substances to exist in our medical
system. 
Researchers at Takeda got breakthrough therapy status for their oral orexin agonist to
treat narcolepsy type 1, but their trial ended prematurely because a safety signal
emerged in the trial. The likely hypothesis for the safety signal is that their drug not
only binds to the orexin receptors but also has other interactions, which is a common
problem when developing artiﬁcial agonists instead of the natural substance to which
the body is already adapted. 
Fortunately, there are more clinical trials underway for orexin agonists for narcolepsy
type 1.
Possible actions
We can hope that the clinical trials for orexin agonists ﬁnd a drug that gets approved
for Narcolepsy type 1 patients, and then non-narcoleptics can use that drug oﬀ-label. 
We could fund studies for orexin-A supplementation with philanthropic money with the
hope of both helping Narcolepsy type 1 patients, and using the drug after it got FDA
approval oﬀ-label to reduce sleep needs in the general population. Given that there's
a market failure because of the inability to patent orexin-A as a treatment, using
philanthropic money has justiﬁcation. This approach has the beneﬁt that orexin-A
would be available without patent protection, and thus a lot cheaper to procure. 
Daring individuals might buy orexin-A from a nootropics store and experiment
themselves. It helped rhesus monkeys to deal better with sleeping less than their
normal amount of hours. If you think about it, then I would recommend that you do
additional research in addition to what you read from me. This post is very much not
medical advice. 

The caveﬁsh seem to eat more in an environment with plenty of food than the surface
ﬁsh and have less stress. We want our farm animals to eat a lot and have less stress.
From an animal welfare standpoint, replacing the orexin system of chicken, pigs, or
cows with the orexin system of caveﬁsh might help them be happier and be
economically beneﬁcial. This might make sense as an EA startup. You get more happy
animals and potentially show that reducing sleep needs in a nonhuman species works
well enough to motivate us to invest research dollars into reducing human sleep
needs.
Invest more into researching the other short sleeper genes that interact with diﬀerent
systems than the orexin system.

