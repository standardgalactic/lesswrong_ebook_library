
Keith Stanovich: What Intelligence Tests Miss
1. What Cost for Irrationality?
2. A Taxonomy of Bias: The Cognitive Miser
3. A Taxonomy of Bias: Mindware Problems
4. What Intelligence Tests Miss: The psychology of rational thought

What Cost for Irrationality?
This is the ﬁrst part in a mini-sequence presenting content from Keith E. Stanovich's
excellent book What Intelligence Tests Miss: The psychology of rational thought. It will
culminate in a review of the book itself.
People who care a lot about rationality may frequently be asked why they do so. There
are various answers, but I think that many of ones discussed here won't be very
persuasive to people who don't already have an interest in the issue. But in real life,
most people don't try to stay healthy because of various far-mode arguments for the
virtue of health: instead, they try to stay healthy in order to avoid various forms of
illness. In the same spirit, I present you with a list of real-world events that have been
caused by failures of rationality.
What happens if you, or the people around you, are not rational? Well, in order from
least serious to worst, you may...
Have a worse quality of living. Status Quo bias is a general human tendency to
prefer the default state, regardless of whether the default is actually good or not. In
the 1980's, Paciﬁc Gas and Electric conducted a survey of their customers. Because
the company was serving a lot of people in a variety of regions, some of their
customers suﬀered from more outages than others. Paciﬁc Gas asked customers with
unreliable service whether they'd be willing to pay extra for more reliable service, and
customers with reliable service whether they'd be willing to accept a less reliable
service in exchange for a discount. The customers were presented with increases and
decreases of various percentages, and asked which ones they'd be willing to accept.
The percentages were same for both groups, only with the other having increases
instead of decreases. Even though both groups had the same income, customers of
both groups overwhelmingly wanted to stay with their status quo. Yet the service
diﬀerence between the groups was large: the unreliable service group suﬀered 15
outages per year of 4 hours' average duration and the reliable service group suﬀered
3 outages per year of 2 hours' average duration! (Though note caveats.)
A study by Philips Electronics found that one half of their products had nothing wrong
in them, but the consumers couldn't ﬁgure out how to use the devices. This can be
partially explained by egocentric bias on behalf of the engineers. Cognitive scientist
Chip Heath notes that he has "a DVD remote control with 52 buttons on it, and every
one of them is there because some engineer along the line knew how to use that
button and believed I would want to use it, too. People who design products are
experts... and they can't imagine what it's like to be as ignorant as the rest of us."
Suﬀer ﬁnancial harm. John Allen Paulos is a professor of mathematics at Temple
University. Yet he fell prey to serious irrationality which began when he purchased
WorldCom stock at $47 per share in early 2000. As bad news about the industry
began mounting, WorldCom's stock price started falling - and as it did so, Paulos kept
buying, regardless of accumulating evidence that he should be selling. Later on, he
admitted that his "purchases were not completely rational" and that "I bought shares
even though I knew better". He was still buying - partially on borrowed money - when
the stock price was $5. When it momentarily rose to $7, he ﬁnally decided to sell.
Unfortunately, he didn't get oﬀ from work until the market closed, and on the next

market day the stock had lost a third of its value. Paulos ﬁnally sold everything, at a
huge loss.
Stock market losses due to irrationality are not atypical. From the beginning of 1998 to
the end of 2001, the Firsthand Technology Value mutual fund had an average gain of
16 percent per year. Yet the average investor who invested in the fund lost 31.6
percent of her money over the same period. Investors actually lost a total of $1.9
billion by investing in a fund which was producing 16 percent of a proﬁt per year. That
happened because the fund was very volatile, causing people to invest and cash out
at exactly the wrong times. When it gained, it gained a lot, and when it lost, it lost a
lot. When people saw that it had been making losses, they sold, and when they saw it
had been making gains, they bought. In other words, they bought when high and sold
when low - exactly the opposite of what you're supposed to do if you want to make a
proﬁt. Reporting on a study of 700 mutual funds during 1998-2001, ﬁnanical reporter
Jason Zweig noted that "to a remarkable degree, investors underperformed their
funds' reported returns - sometimes by as much as 75 percentage points per year."
Be manipulated and robbed of personal autonomy. Subjects were asked to
divide 100 usable livers to 200 children awaiting a transplant. With two groups of
children, group A with 100 children and group B with 100 children, the overwhelming
response was to allocate 50 livers to each, which seems reasonable. But when group A
had 100 children, each with an 80 percent chance of surviving when transplanted, and
group B had 100 children, each with a 20 percent chance of surviving when
transplanted, people still chose the equal allocation method even if this caused the
unnecessary deaths of 30 children. Well, that's just a question of values and not
rationality, right? Turns out that if the patients were ranked from 1 to 200 in terms of
prognosis, people were relatively comfortable with distributing organs to the top 100
patients. It was only when the question was framed as "group A versus group B" that
people suddenly felt they didn't want to abandon group B entirely. Of course, these
are exactly the same dilemma. One could almost say that the person who got to
choose which framing to use was getting to decide on behalf of the people being
asked the question.
Two groups of subjects were given information about eliminating aﬃrmative action
and adopting a race-neutral policy at several universities. One group was told that
under race-neutral conditions, the probability of a black student being admitted would
decline from 42 percent to 13 percent and the probability of a white student being
admitted would rise from 25 percent to 27 percent. The other group was told that
under race-neutral admissions, the number of black students being admitted would
decrease by 725 and the number of white students would increase by 725. These two
framings were both saying the same thing, but you can probably guess the outcome:
support for aﬃrmative action was much higher in the percentage group.
In a hypothetical country, a family with no children and an income of $35,000 pays
$4,000 in tax, while a family with no children and an income of $100,000 pays
$26,000 in tax. Now suppose that there's a $500 tax reduction for having a child for a
family with an income of $35,000. Should the family with an income of $100,000 be
given a larger reduction because of their higher income? Here, most people would say
no. But suppose that instead, the baseline is that a family of two children with an
income of $35,000 pays $3,000 in tax and that a family of two children with an
income of $100,000 pays $25,000 in tax. We propose to make the families with no
children pay more tax - that is, have a "childless penalty". Say that the family with the

income of $100,000 and one child has their taxes set at $26,000 and the same family
with no children has their taxes set at $27,000 - there's a childless penalty of $1,000
per child. Should the poorer family which makes $35,000 and has no children also pay
the same $2,000 childless penalty as the richer family? Here, most people would also
say no - they'd want the "bonus" for children to be equal for low- and high-income
families, but they do not want the "penalty" for lacking children to be the high for
same and low income.
End up falsely accused or imprisoned. In 2003, an attorney was released from
prison in England when her conviction of murdering her two infants was overturned.
Five months later, another person was released from prison when her charge of
having murdered her children was also overturned. In both cases, the evidence
presented against them had been ambiguous. What had convinced the jury was that
in both cases, a pediatrician had testiﬁed that the odds of two children in the same
family dying of infant death syndrome was 73 million to 1. Unfortunately, he had
arrived to this ﬁgure by squaring the odds of a single death. Squaring the odds of a
single event to arrive at the odds for it happening twice only works if the two events
are independent. But that assumption is likely to be false in the case of multiple
deaths in the same family, where numerous environmental and genetic factors may
have aﬀected both deaths.
In the late 1980s and early 1990s, many parents were excited and overjoyed to hear
of a technique coming out of Australia that enabled previously totally non-verbal
autistic children to communicate. It was uncritically promoted in highly visible media
such as 60 Minutes, Parade magazine and the Washington Post. The claim was that
autistic individuals and other children with developmental disabilities who'd previously
been nonverbal had typed highly literate messages on a keyboard when their hands
and arms had been supported over by the typewriter by a sympathetic "facilitator". As
Stanovich describes: "Throughout the early 1990s, behavioral science researchers the
world over watched in horriﬁed anticipation, almost as if observing cars crash in slow
motion, while a predictable tragedy unfolded before their eyes." The hopes of
countless parents were dashed when it was shown that the "facilitators" had been -
consciously or unconsciously - directing the children's hands on the right keys. It
should have been obvious that spreading such news before the technique had been
properly scientiﬁcally examined was dangerously irresponsible - and it gets worse.
During some "faciliation" sessions, children "reported" having been sexually abused
by their parents, and were removed from their homes as a result. (Though they were
eventually returned.)
End up dead. After 9/11, people became afraid of ﬂying and started doing so less.
Instead, they began driving more. Unfortunately, car travel has a much higher chance
of death than air travel. Researchers have estimated that over 300 more people died
in the last months of 2001 because they drove instead of ﬂying. Another group
calculated that for ﬂying to be as dangerous as driving, there would have to be an
incident on the scale of 9/11 once a month!
Have your society collapse. Possibly even more horrifying is the tale of Albania,
which had previously been a communist dictatorship but had made considerable
ﬁnancial progress from 1992 to 1997. In 1997, however, one half of the adult
population had fallen victim to Ponzi schemes. In a Ponzi scheme, the investment
itself isn't actually making any money, but rather early investors are paid oﬀ with the
money from late investors, and eventually the system has to collapse when no new
investors can be recruited. But when schemes oﬀering a 30 percent monthly return
began to become popular in Albania, competitors oﬀering a 50-60 or even a 100

percent monthly return soon showed up, and people couldn't resist the temptation.
Eventually both the government and economy of Albania collapsed. Stanovich
describes:
People took out mortgages on their homes in order to participate. Others sold
their homes. Many put their entire life savings into the schemes. At their height,
an amount equal to 50 percent of the country's GDP was invested in Ponzi
schemes. Before the schemes collapsed, they actually began to compete with
wage income and distort the economy. For example, one business owner saw his
workforce quickly slip from 130 employees to 70 because people began to think
they could invest in the Ponzi schemes instead of actually working for their
income.
The estimated death toll was between 1,700 and 2,000.

A Taxonomy of Bias: The Cognitive
Miser
This is the second part in a mini-sequence presenting content from Keith E.
Stanovich's excellent book What Intelligence Tests Miss: The psychology of rational
thought. It will culminate in a review of the book itself.
Noting that there are many diﬀerent kinds of bias, Keith Stanovich proposes a
classiﬁcation scheme for bias that has two primary categories: the Cognitive Miser,
and Mindware Problems. Today, I will discuss the Cognitive Miser category, which has
the subcategories of Default to the Autonomous Mind, Serial Associative Cognition
with a Focal Bias, and Override Failure.
The Cognitive Miser
Cognitive science suggests that our brains use two diﬀerent kinds of systems for
reasoning: Type 1 and Type 2. Type 1 is quick, dirty and parallel, and requires little
energy. Type 2 is energy-consuming, slow and serial. Because Type 2 processing is
expensive and can only work on one or at most a couple of things at a time, humans
have evolved to default to Type 1 processing whenever possible. We are "cognitive
misers" - we avoid unnecessarily spending Type 2 cognitive resources and prefer to
use Type 1 heuristics, even though this might be harmful in a modern-day
environment.
Stanovich further subdivides Type 2 processing into what he calls the algorithmic mind
and the reﬂective mind. He argues that the reason why high-IQ people can fall prey to
bias almost as easily as low-IQ people is that intelligence tests measure the
eﬀectiveness of the algorithmic mind, whereas many reasons for bias can be found in
the reﬂective mind. An important function of the algorithmic mind is to carry out
cognitive decoupling - to create copies of our mental representations about things, so
that the copies can be used in simulations without aﬀecting the original
representations. For instance, a person wondering how to get a fruit down from a high
tree will imagine various ways of getting to the fruit, and by doing so he operates on a
mental concept that has been copied and decoupled from the concept of the actual
fruit. Even when he imagines the things he might do to the fruit, he never confuses
the fruit he has imagined in his mind with the fruit that's still hanging in the tree (the
two concepts are decoupled). If he did, he might end up believing that he could get
the fruit down by simply imagining himself taking it down. High performance on IQ
tests indicates an advanced ability for cognitive decoupling.
In contrast, the reﬂective mind embodies various higher-level goals as well as thinking
dispositions. Various psychological tests of thinking dispositions measure things such
as the tendency to collect information before making up one's mind, the tendency to
seek various points of view before coming to a conclusion, the disposition to think
extensively about a problem before responding, the tendency to calibrate the degree
of strength of one's opinion to the degree of evidence available, the tendency to think
about future consequences before taking action, the tendency to explicitly weigh
pluses and minuses of situations before making a decision, and the tendency to seek
nuance and avoid absolutism. All things being equal, a high-IQ person would have a
better chance of avoiding bias if they stopped to think things through, but a higher
algorithmic eﬃciency doesn't help them if it's not in their nature to ever bother doing

so. In tests of rational thinking where the subjects are explicitly instructed to consider
the issue in a detached and objective manner, there's a correlation of .3 - .4 between
IQ and test performance. But if such instructions are not given, and people are free to
reason in a biased or unbiased way as they wish (like in real life), the correlation
between IQ and rationality falls to nearly zero!
Modeling the mind purely in terms of Type 1 and Type 2 systems would do a poor job
of explaining the question of why intelligent people only do better at good thinking if
you tell them in advance what "good thinking" is. It is much better explained by a
three-level model where the reﬂective mind may choose to "make a function call" to
the algorithmic mind, which in turn will attempt to override the autonomous Type 1
processes. A failure of rationality may happen either if the reﬂective mind fails to
activate the algorithmic mind, or if the algorithmic mind fails to override the
autonomous mind. This gives us a three-level classiﬁcation of this kind of bias.
Default to the Autonomous Mind
Defaulting to the autonomous mind is the most shallow kind of thought, where no
Type 2 processing is done at all. The reﬂective mind fails to react and activate the
algorithmic mind. Stanovich considers biases such as impulsively associative thinking
and aﬀect substitution (evaluating something primarily based on its aﬀect) to be
caused by this one.
Serial Associative Cognition with a Focal Bias
In this mode of thinking, Type 2 processes are engaged, but they are too conservative
in their use of resources. For instance, consider the following problem (answer in rot13
below):
Jack is looking at Anne but Anne is looking at George. Jack is married but George is
not. Is a married person looking at an unmarried person? A) Yes B) No C) Cannot be
determined.
Gur pbeerpg nafjre, juvpu yrﬀ guna 20 creprag bs crbcyr trg, vf N. Vs Naar vf zneevrq,
gura gur nafjre vf "Lrf", orpnhfr fur'f ybbxvat ng Trbetr jub'f hazneevrq. Vs Naar vf
hazneevrq, gura gur nafjre vf "Lrf" orpnhfr fur'f orvat ybbxrq ng ol Wnpx, jub'f
zneevrq.
In this example, people frequently concentrate too much on a single detail and get the
answer wrong. There are numerous biases of similar kind. For instance, when asked to
guess the amount of murders in Detroit (which is located in Michigan) they give a
higher number than when asked to guess the number of murders in Michigan. This is
because people are using crude aﬀect-laden images of the locations in question to
generate their guess. Vividness, salience and accessibility of various pieces of
information have an overly strong eﬀect to our thinking, becoming the main focal
point of our evaluation. Focal bias is also involved biases such as framing eﬀects (the
presented frame is taken as focal), the Wason selection task, motivated cognition, and
conﬁrmation bias.
Override Failure
In an override failure, Type 2 processes notice that Type 1 systems are attempting to
apply rules or heuristics that are not applicable to the situation at hand. As a result,
the Type 2 processes attempt to initiate an override and take the Type 1 systems

oﬄine, but for whatever reason they fail to do so. Override failures can be divided into
two categories: "cold" and "hot" ones.
Premise: All living things need water
Premise: Roses need water
Conclusion: Roses are living things
The above reasoning is invalid ("Living things" implies "need water", but "need water"
does not imply "living thing"), but many people will instinctively accept it, because the
conclusion is a true one. It's an example of a cold override, where you need to
override a natural response with a rule-based one. In another example, test subjects
were presented with two cans of jelly beans. One of the cans had nine white jelly
beans and one red jelly bean. The other had eight red jelly beans and ninety-two white
jelly beans. The subjects were told to pick one of the cans and then draw a jelly bean
at random from their chosen can: if they got a red one, they'd win a dollar. Most
picked the can with one red jelly bean (a 10% chance) but 30 to 40 percent of the
subjects picked the one with the worse (8%) odds. Many of them knew that they were
doing a mistake, but having a higher absolute amount of beans was too enticing to
them. One commented afterwards: "I picked the one with more red jelly beans
because it looked like there were more ways to get a winner, even though I knew
there were also more whites, and that the percents were against me."
A "hot" override, on the other hand, is one where strong emotions are involved. In
what's likely to be a somewhat controversial example around here, Stanovich
discusses the trolley problem. He notes that most people would choose to ﬂip the
switch sending the trolley to the track where it kills one person instead of ﬁve, but
that most people would also say "no" to pushing the fat man on the tracks. He notes
that this kind of a scenario feels more "yucky". Brain scans of people being presented
various variations of this dilemma show more emotional activity in the more personal
variations. The people answering "yes" to the "fat man"-type dilemmas took a longer
time to answer, and scans of their brain indicated activity in the regions associated
with overriding the emotional brain. They were using Type 2 processing to override the
eﬀects of Type 1 emotions.
Stanovich identiﬁes denominator neglect (the jelly bean problem), belief bias eﬀects
("roses are living things"), self-control problems such as the inability to delay
gratiﬁcation, as well as moral judgement failures as being caused by an override
failure.

A Taxonomy of Bias: Mindware
Problems
This is the third part in a mini-sequence presenting content from Keith E. Stanovich's
excellent book What Intelligence Tests Miss: The psychology of rational thought. It will
culminate in a review of the book itself.
Noting that there are many diﬀerent kinds of bias, Keith Stanovich proposes a
classiﬁcation scheme for bias that has two primary categories: the Cognitive Miser,
and Mindware Gaps. Last time, I discussed the Cognitive Miser category. Today, I will
discuss Mindware Problems, which has the subcategories of Mindware Gaps and
Corrupted Mindware.
Mindware Problems
Stanovich deﬁnes "mindware" as "a generic label for the rules, knowledge,
procedures, and strategies that a person can retrieve from memory in order to aid
decision making and problem solving".
Mindware Gaps
Previously, I mentioned two tragic cases. In one, a pediatrician incorrectly testiﬁed the
odds of a two children in the same family suﬀering infant death syndrome to be 73
million to 1. In the other, people bought into a story of "facilitated communication"
helping previously non-verbal children to communicate, without looking at it in a
critical manner. Stanovich uses these two as examples of a mindware gap. The people
involved were lacking critical mindware: in one case, that of probabilistic reasoning, in
the other, that of scientiﬁc thinking. One of the reasons why so many intelligent
people can act in an irrational manner is that they're simply missing the mindware
necessary for rational decision-making.
Much of the useful mindware is a matter of knowledge: knowledge of Bayes' theorem,
taking into account alternative hypotheses and falsiﬁability, awareness of the
conjunction fallacy, and so on. Stanovich also mentions something he calls strategic
mindware, which refers to the disposition towards engaging the reﬂective mind in
problem solving. These were previously mentioned as thinking dispositions, and some
of them can be measured by performance-based tasks. For instance, in the Matching
Familiar Figures Test (MFFT), participants are presented with a picture of an object,
and told to ﬁnd the correct match from an array of six other similar pictures. Reﬂective
people have long response times and few errors, while impulsive people have short
response times and numerous errors. These types of mindware are closer to
strategies, tendencies, procedures, and dispositions than to knowledge structures.
Stanovich identiﬁes mindware gaps to be involved in at least conjunction errors and
ignoring base rates (missing probability knowledge), as well as the Wason selection
task and conﬁrmation bias (not considering alternate hypotheses). Incorrect lay
psychological theories are identiﬁed as a combination of a mindware gap and
contaminated mindware (see below). For instance, people are often blind to their own
biases, because they incorrectly think that biased thinking on their part would be
detectable by conscious introspection. In addition to bias blind spot, lay psychological
theory is likely to be involved in errors of aﬀective forecasting (the forecasting of one's
future emotional state).

Contaminated Mindware
I previously also mentioned the case of Albania, where a full one half of the adult
population fell victim to Ponzi schemes. As another example, in the 1980s
psychotherapists thought they had found a way to uncover repressed memories of
childhood sexual abuse. The ideas spread via professional association but without any
evidence of them actually being correct. Even when the patients had no memories of
abuse prior to entering therapy, and during therapy began to come up with elaborate
memories of being abused in rituals with satanic overtones, nobody questioned their
theories or sought any kind of independent evidence. The belief system of the
therapists was basically "if the patient thinks she was abused then she was", and the
mindware represented by this belief system required only the patient and the
therapist to believe in the story. Several people were convicted of abuse charges
because of this mindware.
Even though mindware gaps were deﬁnitely involved in both of these cases, they are
also examples of contaminated mindware sweeping through a population. Stanovich
deﬁnes contaminated mindware as mindware that leads to maladaptive actions and
resists critical evaluation. Contaminated mindware is often somewhat complicated
and may be just as enticing to high-IQ people than low-IQ people, if not more so. In a
survey of paranormal beliefs conducted on members of a Mensa club in Canada, 44%
believed in astrology, 51% believed in biorhytms, and 56% believed in extraterrestrial
visitors.
To explain why we acquire mindware that is harmful to us, Stanovich draws upon
memetic theory. In the same way that organisms are built to advance the interests of
the genes rather than for the interest of the gene hosts themselves, beliefs may
spread without being true or helping the human who holds the belief in any way. Chain
letters such as "send me to ﬁve other people or experience misfortune" are passed on
despite being both untrue and useless to the people passing them on, surviving
because of their own "self-replicative" properties. Memetic theory shifts our
perspective from "how do people acquire beliefs" to "how do beliefs acquire people".
(Here, we're treating "memes" and "mindware" as rough synonyms, with the main
diﬀerence being one of emphasis.)
Stanovich lists four reasons for why mindware might spread:
1. Mindware survives and spreads because it is helpful to the people that store it.
2. Certain mindware proliferates because it is a good ﬁt to pre-existing genetic
dispositions or domain-speciﬁc evolutionary modules.
3. Certain mindware spreads because it facilitates the replication of genes that
make vehicles that are good hosts for that particular mindware (e.g. religious
beliefs that urge people to have more children).
4. Mindware survives and spreads because of the self-perpetuating properties of
the mindware itself.
Stanovich notes that reasons 1-3 are relatively uncontroversial, with 1 being a
standard assumption in cultural anthropology, 2 being emphasized by evolutionary
psychology, and 3 being meant to capture the types of eﬀects emphasized by
theorists stressing gene/culture co-evolution.
There are several subversive ways by which contaminated mindware might spread. It
might mimick the properties of beneﬁcial mindware and disguise itself as one, it might
cause its bearers to want to kill anyone who speaks up against it, it might be

unfalsiﬁable and prevent us from replacing it, it might contain beliefs that actively
prohibit evaluation ("blind faith is a virtue") or it might impose a prohibitively high
cost on testing it (some groups practicing female genital mutilation believe that if a
baby's head touches the clitoris during delivery, the baby will die).
Stanovich identiﬁes contaminated mindware to be involved in at least self-centered
biases (self and egocentric processing) and conﬁrmation bias (evaluation-disabling
strategies), as well combining with mindware gaps to cause problems in the form of
lay psychological theory.

What Intelligence Tests Miss: The
psychology of rational thought
This is the fourth and ﬁnal part in a mini-sequence presenting Keith E. Stanovich's
excellent book What Intelligence Tests Miss: The psychology of rational thought.
If you want to give people a single book to introduce people to the themes and ideas
discussed on Less Wrong, What Intelligence Tests Miss is probably the best currenty
existing book for doing so. It does have a somewhat diﬀerent view on the study of bias
than we on LW: while Eliezer concentrated on the idea of the map and the territory
and aspiring to the ideal of a perfect decision-maker, Stanovich's perspective is more
akin to bias as a thing that prevents people from taking full advantage of their
intelligence. Regardless, for someone less easily persuaded by LW's somewhat
abstract ideals, reading Stanovich's concrete examples ﬁrst and then proceeding to
the Sequences is likely to make the content presented in the sequences much more
interesting. Even some of our terminology such as "carving reality at the joints" and
the instrumental/epistemic rationality distinction will be more familiar to somebody
who was ﬁrst read What Intelligence Tests Miss.
Below is a chapter-by-chapter summary of the book.
Inside George W. Bush's Mind: Hints at What IQ Tests Miss is a brief
introductory chapter. It starts with the example of president George W. Bush,
mentioning that the president's opponents frequently argued against his intelligence,
and even his supporters implicitly conceded the point by arguing that even though he
didn't have "school smarts" he did have "street smarts". Both groups were purportedly
surprised when it was revealed that the president's IQ was around 120, roughly the
same as his 2004 presidential candidate opponent John Kerry. Stanovich then goes on
to say that this should not be surprising, for IQ tests do not tap into the tendency to
actually think in an analytical manner, and that IQ had been overvalued as a concept.
For instance, university admissions frequently depend on tests such as the SAT, which
are pretty much pure IQ tests. The chapter ends by a disclaimer that the book is not
an attempt to say that IQ tests measure nothing important, or that there would be
many kinds of intelligence. IQ does measure something real and important, but that
doesn't change the fact that people overvalue it and are generally confused about
what it actually does measure.
Dysrationalia: Separating Rationality and Intelligence talks about the
phenomenon informally described as "smart but acting stupid". Stanovich notes that if
we used a broad deﬁnition of intelligence, where intelligence only meant acting in an
optimal manner, then this expression wouldn't make any sense. Rather, it's a sign that
people are intuitively aware of IQ and rationality as measuring two separate qualities.
Stanovich then brings up the concept of dyslexia, which the DSM IV deﬁnes as
"reading achievement that falls substantially below that expected given the
individual's chronological age, measured intelligence, and age-appropriate education".
Similarly, the diagnostic criterion for mathematics disorder (dyscalculia) is
"mathematical ability that falls substantially below that expected for the individual's
chronological age, measured intelligence, and age-appropriate education". He argues
that since we have a precedent for creating new disability categories when someone's
ability in an important skill domain is below what would be expected for their
intelligence, it would make sense to also have a category for "dysrationalia":

Dysrationalia is the inability to think and behave rationally despite adequate
intelligence. It is a general term that refers to a heterogenous group of disorders
manifested by signiﬁcant diﬃculties in belief formation, in the assessment of
belief consistency, and/or in the determination of action to achieve one's goals.
Although dysrationalia may occur concomitantly with other handicapping
conditions (e.g. sensory impairment), dysrationalia is not the result of those
conditions. The key diagnostic criterion for dysrationalia is a level of rationality, as
demonstrated in thinking and behavior, that is signiﬁcantly below the level of the
individual's intellectual capacity (as determined by an individually administered IQ
test).
The Reﬂective Mind, the Algorithmic Mind, and the Autonomous Mind
presents a three-level model of the mind, which I mostly covered in A Taxonomy of
Bias: The Cognitive Miser. At the end, we return to the example of George W. Bush,
and are shown a bunch of quotes from the president's supporters describing him. His
speechwriter called him "sometimes glib, even dogmatic; often uncurious and as a
result ill-informed"; John McCain said Bush never asks for his opinion and that the
president "wasn't intellectually curious". The same sentiment was echoed by a senior
oﬃcial in Iraq who had observed Bush in various videoconferences and said that the
president's "obvious lack of interest in long, detailed discussions, had a chilling
eﬀect". On the other hand, other people were quoted as saying that Bush was
"extraordinarily intelligent, but was not interested in learning unless it had practical
value". Tony Blair repeatedly told his associates that Bush was "very bright". This is
taken as evidence that while Bush is indeed intelligent, he does not have thinking
dispositions that would have make him make use of his intelligence: he has
dysrationalia.
Cutting Intelligence Down to Size further criticizes the trend of treating the word
"intelligence" in a manner that is too broad. Stanovich points out that even critics of
the IQ concept who introduce terms such as "social intelligence" and "bodily-
kinesthetic intelligence" are probably shooting themselves in the foot. By giving
everything valuable the label of intelligence, these critics are actually increasing the
esteem of IQ tests, and therefore making people think that IQ measures more than it
does.
Consider a thought experiment. Imagine that someone objected to the emphasis
given to horsepower (engine power) when evaluating automobiles. They feel that
horsepower looms too large in people's thinking. In an attempt to deemphasize
horsepower, they then being to term the other features of the car things like
"braking horsepower" and "cornering horsepower" and "comfort horsepower".
Would such a strategy make people less likely to look to engine power as an
indicator of the "goodness" of a car? I think not. [...] Just as calling "all good car
things" horsepower would emphasize horsepower, I would argue that calling "all
good cognitive things" intelligence will contribute to the deiﬁcation of MAMBIT
[Mental Abilities Measured By Intelligence Tests].
Stanovich then continues to argue in favor of separating rationality and intelligence,
citing surveys that suggest that folk psychology does already distinguish between the
two. He also brings up the chilling eﬀect that deifying intelligence seems to be having
on society. Reviews about a book discussing the maltreatment of boys labeled
feebleminded seemed to concentrate on the stories of the boys who were later found
to have normal IQs, implying that abusive treatment of boys who actually did have a
low IQ was okay. Various parents seem to take a diagnosis of low mental ability as
much more shocking than a diagnosis such as ADHD or learning disability that

stresses the presence of normal IQ, even though the life problems associated with
some emotional and behavior disorders are much more severe than those associated
with many forms of moderate or mild intellectual disability.
Why Intelligent People Doing Foolish Things Is No Surprise brieﬂy introduces
the concept of the cognitive miser, explaining that conserving energy and not thinking
about things too much is a perfectly understandable tendency given our evolutionary
past.
The Cognitive Miser: Ways to Avoid Thinking discusses the cognitive miser
further, starting with the "Jack is looking at Anne but Anne is looking at George"
problem, noting that one could arrive at the correct answer via disjunctive reasoning
("either Anne is married, in which case the answer is yes, or Anne is unmarried, in
which case the answer is also yes") but most people won't bother. It then discusses
attribute substitution (instead of directly evaluating X, consider the correlated and
easier to evaluate quality Y), vividness/salience/accessibility eﬀects, anchoring eﬀects
and the recognition heuristic. Stanovich emphasizes that he does not say that
heuristics are always bad, but rather that one shouldn't always rely on them.
Framing and the Cognitive Miser extensively discusses various framing eﬀects,
and at the end notes that high-IQ people are not usually any more likely to avoid
producing inconsistent answers to various framings unless they are speciﬁcally
instructed to try to be consistent. This is mentioned to be a general phenomenon: if
intelligent people have to notice themselves that an issue of rationality is involved,
they do little better than their counterparts of lower intelligence.
Myside Processing: Heads I Win - Tails I Win Too! discusses "myside bias",
people evaluating situations in terms of their own perspective. Americans will provide
much stronger support for the USA banning an unsafe German car than for Germany
banning an unsafe American car. People will much more easily pick up on
inconsistencies in the actions of their political opponents than the politicians they
support. They will also be generally overconﬁdent, be appalled at others exhibiting the
same unsafe behaviors they themselves exhibit, underestimate the degree to which
biases inﬂuence our own thinking, and assume people understand their messages
better than they actually do. The end of the chapter surveys research on the linkage
between intelligence and the tendency to fall prey to these biases. It notes that
intelligent people again do moderately better, but only when speciﬁcally instructed to
avoid bias.
A Diﬀerent Pitfall of the Cognitive Miser: Thinking a Lot, but Losing takes up
the problem of failing to override your autonomous processing even when it would be
necessary. Most of this chapter is covered by my previous discussion of override
failures in the Cognitive Miser post.
Mindware Gaps introduces in more detail a diﬀerent failure mode: that of mindware
gaps. It also introduces and explains the concepts of Bayes' theorem, falsiﬁability,
base rates and the conjunction error as crucial mindware for avoiding many failures of
rationality. It notes that thinking dispositions for actually actively analyzing things
could be called "strategic mindware". The chapter concludes by noting that the useful
mindware discussed in the chapter is not widely and systematically taught, leaving
even intelligent people gaps in their mindware that makes them subject to failures of
rationality.

I mostly covered the contents of Contaminated Mindware in my post about
mindware problems.
How Many Ways Can Thinking Go Wrong? A Taxonomy of Irrational Thinking
Tendencies and Their Relation to Intelligence summarizes the content of the
previous chapters and organizes the various biases into a taxonomy of biases that has
the main categories of the Cognitive Miser, Mindware Problems, and Mr. Spock
Syndrome. I did not previously cover Mr. Spock Syndrome because as Stanovich says,
it is not a fully cognitive category. People with the syndrome have a reduced ability to
feel emotions, which messes up their ability to behave appropriately in various
situations even though their intelligence remains intact. Stanovich notes that the
syndrome is most obvious with people who have suﬀered severe brain damage, but
diﬃculties of emotional regulation and awareness do seem to also correlate negatively
with some tests of rationality, as well as positive life outcomes, even when
intelligence is controlled for.
The Social Beneﬁts of Increasing Human Rationality - and Meliorating
Irrationality concludes the book by arguing that while increasing the average
intelligence of people would have only small if any eﬀects on general well-being, we
could reap vast social beneﬁts if we actually tried to make people more rational.
There's evidence that rationality would be much more malleable than intelligence.
Disjunctive reasoning, the tendency to consider all possible states of the world when
deciding among options, is noted to be a rational thinking skill of high generality that
can be taught. There also don't seem to be strong intelligence-related limitations on
the ability to think disjunctively. Much other useful mindware, like that of scientiﬁc and
probabilistic reasoning. While these might be challenging to people with a lower IQ,
techniques such as implementation intention may be easier to learn.
An implementation intention is formed when the individual marks the cue-action
sequence with the conscious, verbal declaration of "when X occurs, I will do Y."
Often with the aid of the context-ﬁxing properties of language, the triggering of
this cue-action sequence on just a few occasions is enough to establish it in the
autonomous mind. Finally, research has shown that an even more minimalist
cognitive strategy of forming mental goals (whether or not they have
implementation intentions) can be eﬃcacious. For example, people perform better
at a task when they are told to form a mental goal ("set a speciﬁc, challenging
goal for yourself") for their performance as opposed to being given the generic
motivational instructions ("do your best").
Stanovich also argues in favor of libertarian paternalism: shaping the environment so
that people are still free to choose what they want, but so that the default choice is
generally the best one. For instance, countries with an opt-out policy for organ
donation have far more donors than the countries with an opt-in policy. This is not
because the people in one country would be any more or less selﬁsh than those in
other countries, but because people in general tend to go with the default option. He
also argues that it would be perfectly possible though expensive to develop general
rationality tests that would be akin to intelligence tests, and that also using RQ
proxies for things such as college admission would have great social beneﬁts.
In studies cited in this book, it has been shown that:
Psychologists have found ways of presenting statistical information so that we
can make more rational decisions related to medical matters and in any
situation where statistics are involved.

Cognitive psychologists have shown that a few simple changes in presenting
information in accord with default biases could vastly increase the frequency of
organ donations, thus saving thousands of lives.
Americans annually pay millions of dollars for advice on how to invest their
money in the stock market, when following a few simple principles from decision
theory would lead to returns on their investments superior to any of this advice.
These principles would help people avoid the cognitive biases that lead them to
reduce their returns - overreacting to chance events, overconﬁdence, wishful
thinking, hindsight bias, misunderstanding of probability.
Decision scientists have found that people are extremely poor at assessing
environmental risks. This is mainly because vividness biases dominate people's
judgment to an inordinate extent. People could improve, and this would make a
huge diﬀerence because these poor assessments come to aﬀect public policy
(causing policy makers to implement policy A, which saves one life for each $3.2
million spent, instead of policy B, which would have saved one life for every
$220,000 spent, for example).
Psychologists from various specialty areas are beginning to pinpoint the
cognitive illusions that sustain pathological gambling behavior -
pseudodiagnosticity, belief perseverance, over-reacting to chance events,
cognitive impulsivity, misunderstanding probability - behavior that destroys
thousands of lives each year.
Cognitive psychologists have studied the overconﬁdence eﬀect in human
judgment - that people miscalibrate their future performance, usually by making
overoptimistic predictions. Psychologists have studied ways to help people avoid
these problems in self-monitoring, making it easier for people to plan for the
future (overconﬁdent people get more unpleasant surprises).
Social psychological research has found that controlling the explosion of choices
in our lives is one of the keys to happiness - that constraining choice often
makes people happier.
Simple changes in the way that pension plans are organized and administered
could make retirement more comfortable for millions of people.
Probabilistic reasoning is perhaps the most studied topic in the decision-making
ﬁeld, and many of the cognitive reforms that have been examined - for example,
eliminating base-rate neglect - could improve practices in courtrooms, where
poor thinking about probabilities have been shown to impede justice.
These are just a small sampling of the teachable reasoning strategies and
environmental ﬁxes that could make a diﬀerence in people's lives, and they are more
related to rationality than intelligence. They are examples of the types of outcomes
that would result if we all became more rational thinkers and decision makers. They
are the types of outcomes that would be multiplied if schools, businesses, and
government focused on the parts of cognition that intelligence tests miss. Instead, we
continue to pay far more attention to intelligence than to rational thinking. It is as if
intelligence has become totemic in our culture, and we choose to pursue it rather than
the reasoning strategies that could transform our world.

